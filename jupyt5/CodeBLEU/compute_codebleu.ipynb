{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2efb83-e0b5-4f80-9493-4e491391503f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:15:19.364732Z",
     "iopub.status.busy": "2021-08-19T14:15:19.364415Z",
     "iopub.status.idle": "2021-08-19T14:15:19.380153Z",
     "shell.execute_reply": "2021-08-19T14:15:19.379612Z",
     "shell.execute_reply.started": "2021-08-19T14:15:19.364658Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DFG_python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab2c4cc2c5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweighted_ngram_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msyntax_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataflow_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/junjie/DataScience/CodeBLEU/syntax_match.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Licensed under the MIT license.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFG_python\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_java\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_ruby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_go\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_php\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_javascript\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDFG_csharp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m from parser import (remove_comments_and_docstrings,\n\u001b[1;32m      6\u001b[0m                    \u001b[0mtree_to_token_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DFG_python'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import json\n",
    "import re\n",
    "from rouge import Rouge\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "def read_json(name):\n",
    "    with open(name,'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    return json_file\n",
    "def write_json(file,path):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "def read_txt_last(fname):\n",
    "    with open(fname, 'r', encoding='utf-8') as f: \n",
    "        lines = f.readlines()  \n",
    "        return lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cba2d7e-c8a3-4654-8808-4fa434b87c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:15:20.104258Z",
     "iopub.status.busy": "2021-08-19T14:15:20.103996Z",
     "iopub.status.idle": "2021-08-19T14:15:20.108034Z",
     "shell.execute_reply": "2021-08-19T14:15:20.107411Z",
     "shell.execute_reply.started": "2021-08-19T14:15:20.104230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang = 'python'\n",
    "params = '0.1,0.4,0.1,0.4'\n",
    "alpha,beta,gamma,theta = [float(x) for x in params.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d79fc71-a429-41fc-aaeb-9f2c856427da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:15:20.771803Z",
     "iopub.status.busy": "2021-08-19T14:15:20.771562Z",
     "iopub.status.idle": "2021-08-19T14:15:20.777013Z",
     "shell.execute_reply": "2021-08-19T14:15:20.776292Z",
     "shell.execute_reply.started": "2021-08-19T14:15:20.771778Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.columns = ['A' ]\n"
     ]
    }
   ],
   "source": [
    "code = 'df.columns = [\\'A\\' madeupword0087]'\n",
    "def remove_madeupword(code):\n",
    "    item_list = re.split(r'(madeupword\\d{4})',code)\n",
    "    item_effective = []\n",
    "    for s in item_list:\n",
    "        if 'madeupword' in s:\n",
    "            continue\n",
    "        else:\n",
    "            item_effective.append(s)\n",
    "    return ''.join(item_effective)\n",
    "#print(item_effective)\n",
    "print(remove_madeupword(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fa7ee1-b311-4bde-8b40-81c842625567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:15:21.219137Z",
     "iopub.status.busy": "2021-08-19T14:15:21.218897Z",
     "iopub.status.idle": "2021-08-19T14:15:21.224868Z",
     "shell.execute_reply": "2021-08-19T14:15:21.224214Z",
     "shell.execute_reply.started": "2021-08-19T14:15:21.219111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_replace = {\n",
    "    \"Ġ\": \" \",\n",
    "    \"Ċ\": \"\\n\",\n",
    "    \"ĉ\": \"\\t\",\n",
    "    'madeupword0001': '\\'jupyter_string\\''\n",
    "}\n",
    "def remove_madeupword(code):\n",
    "    item_list = re.split(r'(madeupword\\d{4})',code)\n",
    "    item_effective = []\n",
    "    for s in item_list:\n",
    "        if 'madeupword' in s:\n",
    "            continue\n",
    "        else:\n",
    "            item_effective.append(s)\n",
    "    return ''.join(item_effective)\n",
    "\n",
    "def decode(hyp):\n",
    "    hyp = ''.join(hyp)\n",
    "    for s, t in post_replace.items():\n",
    "        hyp = hyp.replace(s, t)\n",
    "    hyp = remove_madeupword(hyp)\n",
    "    return hyp\n",
    "def decode_string(hyp):\n",
    "    hyp = ''.join(hyp.split())\n",
    "    for s, t in post_replace.items():\n",
    "        hyp = hyp.replace(s, t)\n",
    "    hyp = remove_madeupword(hyp)\n",
    "    return hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ea3eb-d050-435d-9999-efc2e0d63066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162d6c30-2f9a-4e60-a3c0-ac5b960badaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:21:02.374078Z",
     "iopub.status.busy": "2021-08-19T14:21:02.373823Z",
     "iopub.status.idle": "2021-08-19T14:21:02.386041Z",
     "shell.execute_reply": "2021-08-19T14:21:02.385351Z",
     "shell.execute_reply.started": "2021-08-19T14:21:02.374050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_devdiv_codebleu(prefix):\n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    reference = []\n",
    "\n",
    "    result = read_json(prefix+'/result/split_generation_results.json')\n",
    "    for item in result:\n",
    "        hypothesis.append(decode_string(item['generation']))\n",
    "        reference.append(decode_string(item['target']))\n",
    "        references.append([decode_string(item['target'])])\n",
    "    \n",
    "    tokenized_hyps = [x.split() for x in hypothesis]\n",
    "    tokenized_refs = [[x.split()] for x in reference]\n",
    "    ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "    # calculate weighted ngram match\n",
    "    keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "    def make_weights(reference_tokens, key_word_list):\n",
    "        return {token:1 if token in key_word_list else 0.2 \\\n",
    "                for token in reference_tokens}\n",
    "    tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "    weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "    # calculate syntax match\n",
    "    syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "    # calculate dataflow match\n",
    "    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "                        format(ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score))\n",
    "\n",
    "    code_bleu_score = alpha*ngram_match_score\\\n",
    "                    + beta*weighted_ngram_match_score\\\n",
    "                    + gamma*syntax_match_score\\\n",
    "                    + theta*dataflow_match_score\n",
    "\n",
    "    print('CodeBLEU score: ', code_bleu_score)\n",
    "\n",
    "    \n",
    "def get_codegpt_codebleu(file_prefix):\n",
    "    reference = [x for x in open(file_prefix+'test.gold', 'r', encoding='utf-8').readlines()]\n",
    "    references = [[x] for x in open(file_prefix+'test.gold', 'r', encoding='utf-8').readlines()]\n",
    "    hypothesis = [x for x in open(file_prefix+'test.output', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    tokenized_hyps = [x.split() for x in hypothesis]\n",
    "    tokenized_refs = [[x.split()] for x in reference]\n",
    "    ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "    # calculate weighted ngram match\n",
    "    keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "    def make_weights(reference_tokens, key_word_list):\n",
    "        return {token:1 if token in key_word_list else 0.2 \\\n",
    "                for token in reference_tokens}\n",
    "    tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "    weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "    # calculate syntax match\n",
    "    syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "    # calculate dataflow match\n",
    "    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "                        format(ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score))\n",
    "\n",
    "    code_bleu_score = alpha*ngram_match_score\\\n",
    "                    + beta*weighted_ngram_match_score\\\n",
    "                    + gamma*syntax_match_score\\\n",
    "                    + theta*dataflow_match_score\n",
    "\n",
    "    print('CodeBLEU score: ', code_bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3083b40b-b401-4f81-8cfd-43c8c42136ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:21:02.796535Z",
     "iopub.status.busy": "2021-08-19T14:21:02.796283Z",
     "iopub.status.idle": "2021-08-19T14:21:07.882986Z",
     "shell.execute_reply": "2021-08-19T14:21:07.882349Z",
     "shell.execute_reply.started": "2021-08-19T14:21:02.796509Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rounge-L] Score\n",
      "{'r': 0.20279555654402748, 'p': 0.26721891184061547, 'f': 0.21414256356827652}\n",
      "[CodeBLEU] Score\n",
      "ngram match: 0.006786179774655879, weighted ngram match: 0.018638786927896875, syntax_match: 0.18370909570000493, dataflow_match: 0.10819802221950922\n",
      "CodeBLEU score:  0.06978425120642852\n"
     ]
    }
   ],
   "source": [
    "def get_metrics_codegpt(prefix):\n",
    "    \n",
    "    print('[Rounge-L] Score')\n",
    "    references = [x for x in open(file_prefix+'test.gold', 'r', encoding='utf-8').readlines()]\n",
    "    hypothesis = [x for x in open(file_prefix+'test.output', 'r', encoding='utf-8').readlines()]\n",
    "    result = []\n",
    "    for ref,hyp in zip(references,hypothesis):\n",
    "        result.append({'generation':hyp,'target':ref})\n",
    "    \n",
    "    \n",
    "    hyps, refs = map(list, zip(*[[d['generation'], d['target']] for d in result]))\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hyps, refs,avg=True)\n",
    "    print(scores['rouge-l'])\n",
    "    \n",
    "    print('[CodeBLEU] Score')\n",
    "    \n",
    "    get_codegpt_codebleu(prefix)\n",
    "\n",
    "file_prefix = '/mnt/jpz_azure_home/save/codegpt_Ori_fix_whitespace_nocolmcc_3_mcct_200_ck_clean_ori_code_mcl_1000//codegpt_adapted_juice/'\n",
    "get_metrics_codegpt(file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6530bd7e-eed6-4e74-8bd8-54e8e57bfab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:48:00.077350Z",
     "iopub.status.busy": "2021-08-23T19:48:00.077099Z",
     "iopub.status.idle": "2021-08-23T19:48:00.083435Z",
     "shell.execute_reply": "2021-08-23T19:48:00.082827Z",
     "shell.execute_reply.started": "2021-08-23T19:48:00.077324Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics_devdiv(prefix):\n",
    "    translations_ori = []\n",
    "    reference_text_ori = []\n",
    "\n",
    "    print('[BLEU] Score')\n",
    "    generate_text_path = prefix+'/result/generate-test.txt'\n",
    "    print(read_txt_last(generate_text_path))\n",
    "    \n",
    "    result = read_json(prefix+'/result/split_generation_results.json')\n",
    "    for item in result:\n",
    "        translations_ori.append(decode_string(item['generation']))\n",
    "        reference_text_ori.append(decode_string(item['target']))\n",
    "    count = 0\n",
    "    for i,j in zip(translations_ori,reference_text_ori):\n",
    "        if i==j:\n",
    "            count += 1\n",
    "    print('[EM] Score')\n",
    "    print(len(translations_ori))\n",
    "    print(count)\n",
    "    print(count/2000*100)\n",
    "    \n",
    "    print('[Rounge-L] Score')\n",
    "    hyps, refs = map(list, zip(*[[d['generation'], d['target']] for d in result]))\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hyps, refs,avg=True)\n",
    "    print(scores['rouge-l'])\n",
    "    \n",
    "    print('[CodeBLEU] Score')\n",
    "    get_devdiv_codebleu(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c30d7d4d-dd57-41d0-a29d-34673ab34590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-30T18:32:49.938748Z",
     "iopub.status.busy": "2021-08-30T18:32:49.938514Z",
     "iopub.status.idle": "2021-08-30T18:32:57.302072Z",
     "shell.execute_reply": "2021-08-30T18:32:57.301398Z",
     "shell.execute_reply.started": "2021-08-30T18:32:49.938722Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEU] Score\n",
      "P-3513\t-2.5666 -2.0677 -0.0000 -0.0212 -0.0023 -0.0004 -0.2398 -0.7950 -0.2773 -0.0011 -0.0031 -0.0001 -0.1817 -1.2833 -0.8154 -0.0000 -0.0000 -0.1386 -0.3908 0.0000 -0.0035 -0.0003 -0.0011 -0.1119 -1.4566 -0.0000 -0.0002 -0.0243 -0.0003 -0.4965 -0.8648 -0.1124 -0.0003 -0.0009 -0.0000 -0.0000 -0.0001 -0.0020 -0.0024 -0.0000 -0.0047 -0.0000 -0.0002 -0.0000 -0.0000 0.0000 -0.0002 0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0002 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 -0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 -0.0000 -0.0001 0.0000 0.0000 0.0000 -0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0001 -0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 -0.0000 0.0000 0.0000 -0.0000 -0.0000 0.0000 0.0000 0.0000 0.0000 -0.0001 -0.0000 -0.0000\n",
      "\n",
      "[EM] Score\n",
      "608\n",
      "0\n",
      "0.0\n",
      "[Rounge-L] Score\n",
      "{'r': 0.3615756830365, 'p': 0.10888634630053404, 'f': 0.14846370152717422}\n",
      "[CodeBLEU] Score\n",
      "ngram match: 0.01571599724841923, weighted ngram match: 0.055120986001724125, syntax_match: 0.3595594539339181, dataflow_match: 0.4288864388092613\n",
      "CodeBLEU score:  0.23113051504262794\n"
     ]
    }
   ],
   "source": [
    "prefix = '/mnt/jpz_azure_home/dataset/juice_filter/Exe_colist_noimcom_nodef_4kdev_resample_mcc_3_mcct_200_ck_clean_ori_code_mcl_1000////'\n",
    "get_metrics_devdiv(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9047f61-6288-4c04-bfb4-10a04128455b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:45:33.997353Z",
     "iopub.status.busy": "2021-08-23T19:45:33.997120Z",
     "iopub.status.idle": "2021-08-23T19:45:34.086102Z",
     "shell.execute_reply": "2021-08-23T19:45:34.085431Z",
     "shell.execute_reply.started": "2021-08-23T19:45:33.997327Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plt.plot(range(training_epochs),cost_history)\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(range(training_epochs),test_cost)\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.show()\n",
      "\n",
      "----------------------------------------\n",
      "plt.plot(range(len(cost_history[1:])),cost_history[1:])\n",
      "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "prefix = '/mnt/jpz_azure_home/dataset/juice_filter/Ori_fix_whitespace_colist_noimcom_mcc_3_mcct_100_ck_clean_ori_code_mcl_1000//'\n",
    "result = read_json(prefix+'/result/split_generation_results.json')\n",
    "translations_ori = []\n",
    "reference_text_ori = []\n",
    "for item in result:\n",
    "    translations_ori.append(decode_string(item['generation']))\n",
    "    reference_text_ori.append(decode_string(item['target']))\n",
    "count = 0\n",
    "for i,j in zip(translations_ori,reference_text_ori):\n",
    "    print(i)\n",
    "    print('-'*40)\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34d72b10-e8c7-4cf4-98eb-1c200d2ef5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:42:55.110732Z",
     "iopub.status.busy": "2021-08-23T19:42:55.110490Z",
     "iopub.status.idle": "2021-08-23T19:42:55.114157Z",
     "shell.execute_reply": "2021-08-23T19:42:55.113508Z",
     "shell.execute_reply.started": "2021-08-23T19:42:55.110705Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl t . plot ( range ( training _ ep och s ), cost _ history ) Ċ pl t . yl abel ( madeupword0001 ) Ċ pl t . x label ( madeupword0001 ) Ċ pl t . show () Ċ Ċ pl t . plot ( range ( training _ ep och s ), test _ cost ) Ċ pl t . yl abel ( madeupword0001 ) Ċ pl t . x label ( madeupword0001 ) Ċ pl t . show () Ċ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38416445-2355-43c7-b57f-eac3a8f2c4a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:43:10.197918Z",
     "iopub.status.busy": "2021-08-23T19:43:10.197683Z",
     "iopub.status.idle": "2021-08-23T19:43:10.201422Z",
     "shell.execute_reply": "2021-08-23T19:43:10.200754Z",
     "shell.execute_reply.started": "2021-08-23T19:43:10.197892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl t . plot ( range ( len ( cost _ history [ 1 : ]) ), cost _ history [ 1 : ]) Ċ pl t . axis ([ 0 , training _ ep och s , 0 , np . max ( cost _ history ) ]) Ċ pl t . show ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5893e441-7ea3-44a5-8a82-410aacafbd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:43:33.086349Z",
     "iopub.status.busy": "2021-08-23T19:43:33.086117Z",
     "iopub.status.idle": "2021-08-23T19:43:33.090250Z",
     "shell.execute_reply": "2021-08-23T19:43:33.089569Z",
     "shell.execute_reply.started": "2021-08-23T19:43:33.086323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plt.plot(range(training_epochs),cost_history)\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(range(training_epochs),test_cost)\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode_string(result[0]['generation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7363ea2-8390-411e-9eae-afb1980b2968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:43:49.102482Z",
     "iopub.status.busy": "2021-08-23T19:43:49.102249Z",
     "iopub.status.idle": "2021-08-23T19:43:49.106206Z",
     "shell.execute_reply": "2021-08-23T19:43:49.105554Z",
     "shell.execute_reply.started": "2021-08-23T19:43:49.102456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plt.plot(range(len(cost_history[1:])),cost_history[1:])\n",
      "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(decode_string(result[0]['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29342623-8d8b-4aa4-839f-91921b478357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:46:42.969435Z",
     "iopub.status.busy": "2021-08-23T19:46:42.969160Z",
     "iopub.status.idle": "2021-08-23T19:46:42.973106Z",
     "shell.execute_reply": "2021-08-23T19:46:42.972476Z",
     "shell.execute_reply.started": "2021-08-23T19:46:42.969409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.plot(cost_history)\n"
     ]
    }
   ],
   "source": [
    "print(decode_string(result[1]['generation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef809744-d7ad-4b78-bcd8-f644273c6f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:46:49.556213Z",
     "iopub.status.busy": "2021-08-23T19:46:49.555977Z",
     "iopub.status.idle": "2021-08-23T19:46:49.559973Z",
     "shell.execute_reply": "2021-08-23T19:46:49.559336Z",
     "shell.execute_reply.started": "2021-08-23T19:46:49.556187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_y = sess.run(y_, feed_dict={X: valid_dataset})\n",
      "mse = tf.reduce_mean(tf.square(pred_y - valid_labels))\n",
      "print('jupyter_string' % sess.run(mse)) \n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(valid_labels, pred_y)\n",
      "ax.plot([valid_labels.min(), valid_labels.max()], [valid_labels.min(), valid_labels.max()], 'jupyter_string', lw=3)\n",
      "ax.set_xlabel('jupyter_string')\n",
      "ax.set_ylabel('jupyter_string')\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(decode_string(result[1]['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60ebeec-49c1-4633-b824-5380a6dd5ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:52:42.733799Z",
     "iopub.status.busy": "2021-08-19T14:52:42.733555Z",
     "iopub.status.idle": "2021-08-19T14:52:42.915332Z",
     "shell.execute_reply": "2021-08-19T14:52:42.914523Z",
     "shell.execute_reply.started": "2021-08-19T14:52:42.733772Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1882\n",
      "157\n",
      "7.85\n"
     ]
    }
   ],
   "source": [
    "translations_ori = []\n",
    "reference_text_ori = []\n",
    "\n",
    "generate_text_path = prefix+'/result/generate-test.txt'\n",
    "\n",
    "\n",
    "result = read_json(prefix+'/result/split_generation_results.json')\n",
    "for item in result:\n",
    "    translations_ori.append(decode(item['generation']))\n",
    "    reference_text_ori.append(decode(item['target']))\n",
    "count = 0\n",
    "for i,j in zip(translations_ori,reference_text_ori):\n",
    "    if i==j:\n",
    "        count += 1\n",
    "print(len(translations_ori))\n",
    "print(count)\n",
    "print(count/2000*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddcae23d-8781-4030-bc60-53dce8f53ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:53:19.789001Z",
     "iopub.status.busy": "2021-08-19T14:53:19.788761Z",
     "iopub.status.idle": "2021-08-19T14:53:19.892463Z",
     "shell.execute_reply": "2021-08-19T14:53:19.891794Z",
     "shell.execute_reply.started": "2021-08-19T14:53:19.788974Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypothesis = []\n",
    "references = []\n",
    "reference = []\n",
    "\n",
    "result = read_json(prefix+'/result/split_generation_results.json')\n",
    "for item in result:\n",
    "    hypothesis.append(decode_string(item['generation']))\n",
    "    reference.append(decode_string(item['target']))\n",
    "    references.append([decode_string(item['target'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a2f9f66-fc2b-4cf8-bfda-edf355bbaa55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-12T15:06:15.524068Z",
     "iopub.status.busy": "2021-08-12T15:06:15.523822Z",
     "iopub.status.idle": "2021-08-12T15:06:16.139277Z",
     "shell.execute_reply": "2021-08-12T15:06:16.138579Z",
     "shell.execute_reply.started": "2021-08-12T15:06:15.524042Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset = train_dataset.reshape(train_dataset.shape[0], train_dataset.shape[1], 1)\n",
      "valid_dataset = valid_dataset.reshape(valid_dataset.shape[0], train_dataset.shape[1], 1)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45524cc0-2e20-4250-9c67-67b3ee33b467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-12T15:06:20.997741Z",
     "iopub.status.busy": "2021-08-12T15:06:20.997494Z",
     "iopub.status.idle": "2021-08-12T15:06:21.001370Z",
     "shell.execute_reply": "2021-08-12T15:06:21.000693Z",
     "shell.execute_reply.started": "2021-08-12T15:06:20.997716Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def feature_normalize(dataset):\n",
      "    mu = np.mean(dataset,axis=0)\n",
      "    sigma = np.std(dataset,axis=0)\n",
      "    return (dataset - mu)/sigma\n",
      "def append_bias_reshape(features,labels = None):\n",
      "    n_training_samples = features.shape[0]\n",
      "    n_dim = features.shape[1]\n",
      "    f = np.reshape(np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim+1])\n",
      "    if labels is None:\n",
      "        return f\n",
      "    labels = (labels-min(labels))/(max(labels)-min(labels))\n",
      "    l = np.reshape(labels,[n_training_samples,1])\n",
      "    return f, l\n",
      "\n",
      "normalized_train_features = feature_normalize(train_dataset)\n",
      "normalized_valid_features = feature_normalize(valid_dataset)\n",
      "\n",
      "\n",
      "\n",
      "train_dataset, train_labels = append_bias_reshape(normalized_train_features,train_labels)\n",
      "valid_dataset, valid_labels = append_bias_reshape(normalized_valid_features,valid_labels)\n",
      "\n",
      "\n",
      "train_dataset = np.nan_to_num(train_dataset)\n",
      "valid_dataset = np.nan_to_num(valid_dataset)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reference[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32055b97-b844-4e2f-9e6b-b809470e00ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-10T08:04:15.965025Z",
     "iopub.status.busy": "2021-08-10T08:04:15.964791Z",
     "iopub.status.idle": "2021-08-10T08:04:15.969407Z",
     "shell.execute_reply": "2021-08-10T08:04:15.968355Z",
     "shell.execute_reply.started": "2021-08-10T08:04:15.965000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data3=pd.read_csv(jupyter_string) \\ny=np.asarray(data3.iloc[:,-1]) \\nX=np.asarray(data3.iloc[:,0:-1]) ']\n"
     ]
    }
   ],
   "source": [
    "print(references[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d109d3e2-fd18-42ed-909a-ce148936323f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-16T16:52:29.598968Z",
     "iopub.status.busy": "2021-08-16T16:52:29.598741Z",
     "iopub.status.idle": "2021-08-16T16:52:29.816160Z",
     "shell.execute_reply": "2021-08-16T16:52:29.815395Z",
     "shell.execute_reply.started": "2021-08-16T16:52:29.598944Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_prefix = '/mnt/jpz_azure_home/save/codegpt_Ori_fix_whitespace_nocolmcc_3_mcct_200_ck_clean_ori_code_mcl_1000//codegpt_adapted_juice/'\n",
    "reference = [x for x in open(file_prefix+'test.gold', 'r', encoding='utf-8').readlines()]\n",
    "references = [[x] for x in open(file_prefix+'test.gold', 'r', encoding='utf-8').readlines()]\n",
    "hypothesis = [x for x in open(file_prefix+'test.output', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1f1e8-1914-43b8-bba1-530a83cde197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce5a186-f3f7-4e99-9d47-e381ff8a9e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T00:58:49.115315Z",
     "iopub.status.busy": "2021-08-19T00:58:49.115077Z",
     "iopub.status.idle": "2021-08-19T00:58:49.124577Z",
     "shell.execute_reply": "2021-08-19T00:58:49.124012Z",
     "shell.execute_reply.started": "2021-08-19T00:58:49.115290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_hyps = [x.split() for x in hypothesis]\n",
    "tokenized_refs = [[x.split()] for x in reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b47108f4-ff16-48b7-aeb5-61dde0daee0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T00:58:49.842968Z",
     "iopub.status.busy": "2021-08-19T00:58:49.842739Z",
     "iopub.status.idle": "2021-08-19T00:58:54.061567Z",
     "shell.execute_reply": "2021-08-19T00:58:54.060926Z",
     "shell.execute_reply.started": "2021-08-19T00:58:49.842944Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.10568825685129822, weighted ngram match: 0.10639678937554656, syntax_match: 0.39840607495958796, dataflow_match: 0.2660349127182045\n",
      "CodeBLEU score:  0.19938211401858907\n"
     ]
    }
   ],
   "source": [
    "ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "# calculate weighted ngram match\n",
    "keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "def make_weights(reference_tokens, key_word_list):\n",
    "    return {token:1 if token in key_word_list else 0.2 \\\n",
    "            for token in reference_tokens}\n",
    "tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "            for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "# calculate syntax match\n",
    "syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "# calculate dataflow match\n",
    "dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "                    format(ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score))\n",
    "\n",
    "code_bleu_score = alpha*ngram_match_score\\\n",
    "                + beta*weighted_ngram_match_score\\\n",
    "                + gamma*syntax_match_score\\\n",
    "                + theta*dataflow_match_score\n",
    "\n",
    "print('CodeBLEU score: ', code_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d685ab9-17f2-409e-8a47-a7a360b5dccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa1f4da8-775b-44c8-9fac-eb22fe5ea4b7",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-19T15:45:54.588839Z",
     "iopub.status.busy": "2021-08-19T15:45:54.588606Z",
     "iopub.status.idle": "2021-08-19T15:45:55.807077Z",
     "shell.execute_reply": "2021-08-19T15:45:55.806443Z",
     "shell.execute_reply.started": "2021-08-19T15:45:54.588813Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca.explained_variance_ratio_\n",
      "=====\n",
      "pcaDF = pd.DataFrame(data=pca.components_, index=X.columns)\n",
      "\n",
      "\n",
      "pcaDF.head()\n",
      "----------------------------------------\n",
      "pca1 = PCA(n_components=1)\n",
      "pca1.fit(X)\n",
      "X_pca1 = pca1.transform(X)\n",
      "\n",
      "plt.scatter(X_pca1[:, 0], X_pca1[:, 1])\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "pcaDF[0].plot(kind='jupyter_string', figsize=(15,6))\n",
      "----------------------------------------\n",
      "submission_df = pd.read_csv(submission_filename)\n",
      "submission_df\n",
      "=====\n",
      "df1 = pd.read_csv('jupyter_string', \n",
      "                  usecols=[0, 1, 2], header=0, names=['jupyter_string', 'jupyter_string', 'jupyter_string'])\n",
      "\n",
      "df2 = pd.read_csv(submission_filename, \n",
      "                  usecols=[0, 1, 2], header=0, names=['jupyter_string', 'jupyter_string', 'jupyter_string'])\n",
      "\n",
      "df = pd.merge(df1, df2, how='jupyter_string', \n",
      "              left_on=['jupyter_string', 'jupyter_string', 'jupyter_string'], \n",
      "              right_on=['jupyter_string', 'jupyter_string', 'jupyter_string'])\n",
      "\n",
      "df[(df['jupyter_string'] != df['jupyter_string']) | \n",
      "   (df['jupyter_string'] != df['jupyter_string']) | \n",
      "   (df['jupyter_string'] != df['jupyter_string'])]\n",
      "----------------------------------------\n",
      "Xy_train_sj = Xy_train[Xy_train['city'] == 'jupyter_string']\n",
      "Xy_train_iq = Xy_train[Xy_train['city'] == 'jupyter_string']\n",
      "=====\n",
      "Xy_sj = Xy_train.loc[Xy_train.city == 'jupyter_string', :]\n",
      "Xy_iq = Xy_train.loc[Xy_train.city == 'jupyter_string', :]\n",
      "print('jupyter_string')\n",
      "print('jupyter_string')\n",
      "----------------------------------------\n",
      "X_train = X_train.reset_index(drop=True)\n",
      "=====\n",
      "X_train = X_train.reset_index()\n",
      "X_test = X_test.reset_index()\n",
      "----------------------------------------\n",
      "X_train_sj['jupyter_string'] = normalize(X_train_sj['jupyter_string'])\n",
      "X_train_sj['jupyter_string'] = normalize(X_train_sj['jupyter_string'])\n",
      "X_train_sj['jupyter_string'] = normalize(X_train_sj['jupyter_string'])\n",
      "\n",
      "X_train_iq['jupyter_string'] = normalize(X_train_iq['jupyter_string'])\n",
      "X_train_iq['jupyter_string'] = normalize(X_train_iq['jupyter_string'])\n",
      "X_train_iq['jupyter_string'] = normalize(X_train_iq['jupyter_string'])\n",
      "=====\n",
      "features_to_normalize = features + new_features\n",
      "\n",
      "X_train_sj[features_to_normalize] = X_train_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_train_iq[features_to_normalize] = X_train_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_sj[features_to_normalize] = X_test_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_iq[features_to_normalize] = X_test_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "----------------------------------------\n",
      "X_train_sj.reset_index(drop=True, inplace=True)\n",
      "X_train_iq.reset_index(drop=True, inplace=True)\n",
      "X_test_sj.reset_index(drop=True, inplace=True)\n",
      "X_test_iq.reset_index(drop=True, inplace=True)\n",
      "=====\n",
      "X_train = pd.concat([X_train_sj, X_train_iq], axis=0)\n",
      "X_train.set_index('jupyter_string', inplace=True)\n",
      "X_train.head()\n",
      "----------------------------------------\n",
      "reg = LinearRegression()\n",
      "print('jupyter_string', round(train_predict_score(reg, X_train_sj, y_train_sj), 4))\n",
      "print('jupyter_string', round(train_predict_score(reg, X_train_iq, y_train_iq), 4))\n",
      "print('jupyter_string', round(train_cross_val_score(reg, X_train_sj, y_train_sj), 4))\n",
      "print('jupyter_string', round(train_cross_val_score(reg, X_train_iq, y_train_iq), 4))\n",
      "=====\n",
      "X_train_iq.head()\n",
      "----------------------------------------\n",
      "y_pred_iq = reg_iq_1.predict(X_test_iq)\n",
      "y_pred_iq2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "predict_sj = X_test_sj[keys].copy()\n",
      "predict_iq = X_test_iq[keys].copy()\n",
      "----------------------------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "y_sj_pred_final = np.array([sum(x)/2.0 for x in zip(y_sj_pred, y_sj_pred_2) ])\n",
      "----------------------------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "y_iq_pred_final = np.array([sum(x)/2.0 for x in zip(y_iq_pred, y_iq_pred_2) ])\n",
      "=====\n",
      "predict_sj['total_cases'] = y_sj_pred.round().astype(int)\n",
      "predict_sj.head()\n",
      "----------------------------------------\n",
      "predict_sj.to_csv('jupyter_string', index=False)\n",
      "predict_iq.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "predict_df = pd.concat([predict_sj, predict_iq], axis=0)\n",
      "----------------------------------------\n",
      "loans['grade'].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(10,6))\n",
      "loans['grade'].value_counts().plot(kind='jupyter_string')\n",
      "plt.tick_params(axis='jupyter_string', labelsize=18)\n",
      "plt.xticks(rotation='jupyter_string')\n",
      "plt.tick_params(axis='jupyter_string', labelsize=18)\n",
      "plt.title('jupyter_string', fontsize=18)\n",
      "plt.xlabel('jupyter_string', fontsize=18)\n",
      "plt.ylabel('jupyter_string', fontsize=18)\n",
      "----------------------------------------\n",
      "predictions = small_model.predict(validation_data.ix[:, validation_data.columns != 'jupyter_string'])\n",
      "predictions\n",
      "=====\n",
      "validation_safe_loans = validation_data[validation_data[target] == 1]\n",
      "validation_risky_loans = validation_data[validation_data[target] == -1]\n",
      "\n",
      "sample_validation_data_risky = validation_risky_loans[0:2]\n",
      "sample_validation_data_safe = validation_safe_loans[0:2]\n",
      "\n",
      "sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)\n",
      "sample_validation_data\n",
      "----------------------------------------\n",
      "loans['safe_loans'].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(10,6))\n",
      "loans['jupyter_string'].value_counts().plot(kind='jupyter_string')\n",
      "plt.tick_params(axis='jupyter_string', labelsize=18)\n",
      "plt.xticks(rotation='jupyter_string')\n",
      "plt.tick_params(axis='jupyter_string', labelsize=18)\n",
      "plt.title('jupyter_string', fontsize=18)\n",
      "plt.xlabel('jupyter_string', fontsize=18)\n",
      "plt.ylabel('jupyter_string', fontsize=18)\n",
      "----------------------------------------\n",
      "sample_validation_data['jupyter_string'] = decision_tree_model.predict(sample_validation_data)\n",
      "=====\n",
      "samp_vald_data_prob = decision_tree_model.predict_proba(sample_validation_data.ix[:, sample_validation_data.columns != 'jupyter_string'])[:,1]\n",
      "----------------------------------------\n",
      "b_train.head()\n",
      "=====\n",
      "b_test = pd.read_csv('jupyter_string')\n",
      "print('jupyter_string'.format(b_test.shape))\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "test_data = xgb.DMatrix(X_test)\n",
      "y_score = bst.predict(test_data)\n",
      "y_score = np.hstack((1 - y_score.reshape(y_score.size,1),\n",
      "                     y_score.reshape(y_score.size,1)))\n",
      "=====\n",
      "b_test['jupyter_string'] = submission_prediction \n",
      "b_test.head()\n",
      "----------------------------------------\n",
      "data = data[['release_date', 'budget', 'tagline', 'revenue_adj', 'homepage', 'vote_average', 'overview', 'imdb_id', 'runtime', 'budget_adj', 'popularity', 'vote_count', 'cast', 'genres', 'keywords', 'production_companies', 'original_title', 'director', 'release_year', 'id']]\n",
      "=====\n",
      "data = data[data[\"cast\"].isnull() == False]\n",
      "data = data[data[\"genres\"].isnull() == False]\n",
      "\n",
      "data = data[data.budget_adj != 0]\n",
      "data = data[data.revenue_adj != 0]\n",
      "----------------------------------------\n",
      "cast = data['cast'].str.split('jupyter_string', expand=True)\n",
      "cast.columns = ['cast_1', 'cast_2', 'cast_3', 'cast_4', 'cast_5']\n",
      "cast.head()\n",
      "=====\n",
      "actor_dict = {}\n",
      "\n",
      "actors = data[\"cast\"]\n",
      "actors = actors.str.split('jupyter_string')\n",
      "actors = np.array(actors)\n",
      "for actorList in actors:\n",
      "    \n",
      "    for actor in actorList:\n",
      "        actor = actor.lstrip() \n",
      "        if actor not in actor_dict:\n",
      "            actor_dict[actor] = 1\n",
      "        else:\n",
      "            actor_dict[actor] += 1\n",
      "                \n",
      "\n",
      "\n",
      "sorted_actor_dict = sorted(actor_dict.items(), key = operator.itemgetter(1), reverse = True)\n",
      "\n",
      "\n",
      "\n",
      "x_axis = list()\n",
      "y_axis = list()\n",
      "\n",
      "for item in sorted_actor_dict[0:20]:\n",
      "    x_axis.append(item[0])\n",
      "    y_axis.append(item[1])\n",
      "\n",
      "\n",
      "sns.set(rc={'jupyter_string':(12,10)}, font_scale=1.4)\n",
      "ax = sns.barplot(x_axis, y_axis, palette='jupyter_string')\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel='jupyter_string', ylabel='jupyter_string', title = 'jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "genres = pd.read_csv('jupyter_string')\n",
      "genres['jupyter_string'] = genres['genres'].str.extract('jupyter_string', expand=True)\n",
      "genres['jupyter_string'] = genres['genres'].str.extract('jupyter_string', expand=True)\n",
      "=====\n",
      "year_set = set()\n",
      "genre_set = set()\n",
      "genres_and_year = data[[\"genres\", \"release_year\"]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "production_year = genres_and_year[\"release_year\"]\n",
      "production_year = production_year.drop_duplicates()\n",
      "for year in production_year:\n",
      "    if year not in year_set:\n",
      "        year_set.add(year)\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split('jupyter_string')\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_set:\n",
      "                genre_set.add(genre)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gerne_count_per_year_df = pd.DataFrame(index = year_set, columns=genre_set)\n",
      "gerne_count_per_year_df[:] = 0  \n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split('jupyter_string')\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_dict:\n",
      "                genre_dict[genre] = 1\n",
      "            else:\n",
      "                genre_dict[genre] = genre_dict[genre] + 1\n",
      "                    \n",
      "    aux_df = pd.DataFrame(genre_dict, index = [year])\n",
      "    gerne_count_per_year_df.loc[year, aux_df.columns] = gerne_count_per_year_df.loc[year, aux_df.columns] + aux_df.loc[year]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "most_popular_genre_by_year = pd.DataFrame([gerne_count_per_year_df.idxmax(axis = 1).values,\n",
      "                                          gerne_count_per_year_df.apply( max, axis=1 ).values],\n",
      "                                          columns = gerne_count_per_year_df.index,\n",
      "                                         index = ['jupyter_string', 'jupyter_string'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "gms_14.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "nbinom_model = smf.glm(formula='jupyter_string', data=gls_10, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14.loc[:,'jupyter_string'] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team1'],r['team2'], gls_10), axis=1)\n",
      "gms_14.loc[:,'jupyter_string'] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team2'],r['team1'], gls_10), axis=1)\n",
      "\n",
      "evaluate_predictions(gms_14)\n",
      "----------------------------------------\n",
      "surveys.isnull().head()\n",
      "=====\n",
      "pd.isnull(surveys_df)\n",
      "----------------------------------------\n",
      "surveys_df['weight'].isnull().sum()\n",
      "=====\n",
      "empty_weights = surveys_df[pd.isnull(surveys_df['weight'])]['weight']\n",
      "print(empty_weights)\n",
      "----------------------------------------\n",
      "nosex_df['weight'].isnull().sum()\n",
      "=====\n",
      "grouped = sex_df.groupby(['plot_id', 'sex'])['weight']\n",
      "grouped.mean().unstack().plot(kind='jupyter_string', stacked='jupyter_string')\n",
      "----------------------------------------\n",
      "ref_surveys_df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "surveys_df.head()\n",
      "----------------------------------------\n",
      "lens.sort_values(['jupyter_string'], ascending=False).head(25)\n",
      "=====\n",
      "most_rated=lens.groupby('jupyter_string').size().sort_values(ascending=False)[:25]\n",
      "most_rated\n",
      "----------------------------------------\n",
      "gms_14.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "regr = smf.glm(formula='jupyter_string', data=gls_exc_14, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14['jupyter_string'] = gms_14.apply(lambda r: pred_team_gls(regr, r['team1'],r['team2'], gls_exc_14), axis=1)\n",
      "gms_14['jupyter_string'] = gms_14.apply(lambda r: pred_team_gls(regr, r['team2'],r['team1'], gls_exc_14), axis=1)\n",
      "evaluate_predictions(gms_14)\n",
      "----------------------------------------\n",
      "lens.groupby('jupyter_string').agg({'jupyter_string': [np.size, np.mean]})\n",
      "=====\n",
      "lens.set_index('jupyter_string', inplace=True)\n",
      "\n",
      "----------------------------------------\n",
      "print('jupyter_string')\n",
      "print(data.M_Firstname.value_counts())\n",
      "print('jupyter_string')\n",
      "print(data.F_Firstname.value_counts())\n",
      "=====\n",
      "import numpy as np \n",
      "\n",
      "print(data['AccusedRef'].shape[0], 'jupyter_string')\n",
      "\n",
      "datamale=data.loc[(data['Sex'] =='jupyter_string')] \n",
      "datafemale=data.loc[(data['Sex'] =='jupyter_string')] \n",
      "datasexunknow=data.shape[0]-datamale.shape[0]-datafemale.shape[0]\n",
      "\n",
      "print(datamale.shape[0], 'jupyter_string')\n",
      "print(datafemale.shape[0], 'jupyter_string')\n",
      "print(datasexunknow, 'jupyter_string')\n",
      "----------------------------------------\n",
      "datachild=data.loc[(data['Age']<16)] \n",
      "print(datachild.shape[0], 'jupyter_string')\n",
      "\n",
      "print('jupyter_string',np.min(datachild['Age'])) \n",
      "print('jupyter_string',np.max(datachild['Age'])) \n",
      "print('jupyter_string', int(np.mean(datachild['Age'])) )\n",
      "=====\n",
      "def male_famle_child(witches):\n",
      "    age,sex = witches\n",
      "     \n",
      "    if age < 16:\n",
      "        return 'jupyter_string'\n",
      "    else:\n",
      "        return sex\n",
      "\n",
      "data['jupyter_string'] = data[[\"Age\",\"Sex\"]].apply(male_famle_child,axis=1)\n",
      "count=data['jupyter_string'].value_counts()\n",
      "print(count)\n",
      "\n",
      "datachild=data.loc[(data['jupyter_string'] =='jupyter_string')]\n",
      "print(datachild.loc[:'FirstName' ,'Notes'])\n",
      "----------------------------------------\n",
      "data['jupyter_string'] = data[[\"Age\",\"Sex\"]].apply(male_famle_child,axis=1)\n",
      "data['jupyter_string'].value_counts()\n",
      "=====\n",
      "data1=dataage.loc[~dataage.Age_estcareer]\n",
      "print(data1.shape[0], 'jupyter_string')\n",
      "data2=dataage.loc[~dataage.Age_estchild]\n",
      "print(data2.shape[0], 'jupyter_string')\n",
      "----------------------------------------\n",
      "data1['Res_county'].value_counts()\n",
      "=====\n",
      "dcounty=pd.crosstab(data.Res_county,data.Sex, margins=True)\n",
      "dcounty.sort_values(by='jupyter_string', inplace=True)\n",
      "print(dcounty)\n",
      "----------------------------------------\n",
      "data.FirstName.value_counts()\n",
      "=====\n",
      "count=data[\"FirstName\"].value_counts()\n",
      "print(count)\n",
      "----------------------------------------\n",
      "week_timeAnalysisKS_trend_success.countProj.diff().plot(title='jupyter_string', figsize=(30,20),label='jupyter_string')\n",
      "week_timeAnalysisKS_trend_fail.countProj.diff().plot(title='jupyter_string', figsize=(30,20),label='jupyter_string')\n",
      "plt.legend(loc='jupyter_string')\n",
      "=====\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS.index,\n",
      "          y=week_timeAnalysisKS.countProj)]\n",
      "\n",
      "\n",
      "week_timeAnalysisKS_seasonality = week_timeAnalysisKS[['jupyter_string']].diff()\n",
      "seasonality = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_seasonality.index,\n",
      "          y=week_timeAnalysisKS_seasonality.countProj)]\n",
      "\n",
      "fig = dict(data=seasonality, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = 'jupyter_string')\n",
      "----------------------------------------\n",
      "ks.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "ks.describe()\n",
      "----------------------------------------\n",
      "ks.corr()\n",
      "=====\n",
      "ks = ks[(ks[\"launched\"].dt.year > 1970)]\n",
      "----------------------------------------\n",
      "country_mapping.head()\n",
      "=====\n",
      "ks= pd.merge(ks,country_mapping, how='jupyter_string', left_on=\"country\", right_on=\"Code\" )\n",
      "\n",
      "\n",
      "\n",
      "ks= ks.drop([\"country\", \"Code\"], axis=1)\n",
      "ks=ks.rename(columns={'jupyter_string': 'jupyter_string'})\n",
      "ks.tail()\n",
      "----------------------------------------\n",
      "ks = ks[ks.country != 'jupyter_string']\n",
      "ks = ks[ks.country != 'jupyter_string']\n",
      "=====\n",
      "ks = ks[(ks['jupyter_string'].notna())]\n",
      "----------------------------------------\n",
      "ks['jupyter_string']=ks[\"usd_pledged_real\"]/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks[\"usd_pledged_real\"]/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks[\"usd_pledged_real\"]/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks[\"usd_pledged_real\"]/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks[\"usd_pledged_real\"]/ks['jupyter_string']\n",
      "=====\n",
      "ks.describe()\n",
      "----------------------------------------\n",
      "ks['state'].value_counts().plot(kind='jupyter_string')\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "sns.countplot(x=\"state\",data=ks, ax=ax,palette='jupyter_string')\n",
      "\n",
      "----------------------------------------\n",
      "ks.state.value_counts()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "ax1=sns.countplot(x=\"state\",data=ks, ax=ax,palette='jupyter_string')\n",
      "\n",
      "for p in ax1.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+3000 ,'jupyter_string'.format(height), ha='jupyter_string') \n",
      "----------------------------------------\n",
      "ks['main_category'].value_counts()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax2=sns.countplot(x=\"main_category\",data=ks, ax=ax, order = ks['main_category'].value_counts().index)\n",
      "\n",
      "for p in ax2.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+1000 ,'jupyter_string'.format(height), ha='jupyter_string') \n",
      "----------------------------------------\n",
      "ks.country.value_counts()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax4=sns.countplot(y='jupyter_string',data=ks,order = ks['jupyter_string'].value_counts().index,ax=ax)\n",
      "\n",
      "\n",
      "for p in ax4.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,'jupyter_string'.format(width))\n",
      "----------------------------------------\n",
      "ks[ks['jupyter_string']<70000].groupby('main_category').state.value_counts().unstack()\n",
      "=====\n",
      "plt.figure(figsize=(20,20)) \n",
      "\n",
      "sns.boxplot(x=\"main_category\", y='jupyter_string',data=ks[(ks['jupyter_string']<70000)], palette='jupyter_string',  hue=\"state\")\n",
      "----------------------------------------\n",
      "ks[(ks['jupyter_string']<70000)][['usd_goal_real','project_length']].describe()\n",
      "=====\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "ks1 = ks[['jupyter_string','usd_goal_real', 'jupyter_string']]\n",
      "\n",
      "plt.yscale('jupyter_string')\n",
      "\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "plt.yscale('jupyter_string')\n",
      "sns.boxplot(data=ks1[['jupyter_string']])\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "plt.yscale('jupyter_string')\n",
      "sns.boxplot(data=ks1[[\"usd_goal_real\"]])\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "sns.boxplot(data=ks1[['jupyter_string']])\n",
      "\n",
      "----------------------------------------\n",
      "ks1 = ks1[ks1.project_length < 10]\n",
      "ks1 = ks1[ks1.project_length > 18]\n",
      "=====\n",
      "mainKS=ks[((ks['jupyter_string']<=1200) & (ks['jupyter_string']<=90))&(ks[\"usd_goal_real\"]<=38000) &(ks['jupyter_string']>0.0002)& (ks['jupyter_string']>=5) & (ks[\"usd_goal_real\"]>0.01)]\n",
      "ksUpperOutliers=ks[((ks['jupyter_string']>1200)| (ks['jupyter_string']>90) | (ks[\"usd_goal_real\"]>38000))]\n",
      "ksLowerOutliers=ks[(ks['jupyter_string']<=0.0002)| (ks['jupyter_string']<5)| (ks[\"usd_goal_real\"]<=0.01)]\n",
      "\n",
      "print('jupyter_string', len(ks.index))\n",
      "print('jupyter_string', len(mainKS.index))\n",
      "print('jupyter_string', len(ksUpperOutliers.index))\n",
      "print('jupyter_string', len(ksLowerOutliers.index))\n",
      "\n",
      "----------------------------------------\n",
      "sns.distplot(mainKS['goal_per_day'])\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.rcParams['jupyter_string'] = True\n",
      "sns.distplot(mainKS.goal_per_day, bins=80, kde = False, hist_kws=dict(edgecolor='jupyter_string'))\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(y='jupyter_string',data=mainKS)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "ks['jupyter_string']=ks['backers']/ks['pledged_per_backer']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "ks['jupyter_string']=ks['jupyter_string']/ks['jupyter_string']\n",
      "=====\n",
      "Top5=ks.sort_values(by='jupyter_string',ascending=False)\n",
      "Top5.head()\n",
      "----------------------------------------\n",
      "df = pd.DataFrame({'jupyter_string': [75, 80, 79, 60], 'jupyter_string': ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']})\n",
      "=====\n",
      "df = pd.read_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "df['Pclass'].value_counts()\n",
      "=====\n",
      "df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "sns.barplot(x='Name', y='Survived', data=df)\n",
      "=====\n",
      "df.Name.head(10)\n",
      "----------------------------------------\n",
      "df.Sex.fillna('jupyter_string', inplace=True)\n",
      "=====\n",
      "df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "df[['Age', 'Survived']].groupby(['Age'], as_index=False).mean()\n",
      "=====\n",
      "df.Age.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "sns.barplot(x='SibSp', y='Survived', data=df)\n",
      "=====\n",
      "df.SibSp.isnull().sum(axis=0), df.Parch.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "df.SibSp.fillna(0, inplace=True)\n",
      "df.Parch.fillna(0, inplace=True)\n",
      "=====\n",
      "df['jupyter_string'] = df['SibSp'] + df['Parch'] + 1\n",
      "\n",
      "\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "sns.barplot(x='jupyter_string', y='Survived', data=df)\n",
      "=====\n",
      "df['jupyter_string'].value_counts()\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = np.where(df['jupyter_string'] > 4, 0, 1)\n",
      "df['jupyter_string'].value_counts()\n",
      "=====\n",
      "df.Family = df.Family.map(lambda x: 0 if x > 4 else x)\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'], as_index=False).mean()\n",
      "=====\n",
      "df['jupyter_string'].value_counts()\n",
      "----------------------------------------\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'], as_index=False).mean()\n",
      "=====\n",
      "df.Ticket.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "df.Ticket.fillna('jupyter_string', inplace=True)\n",
      "=====\n",
      "df.Ticket = df.Ticket.map(lambda x: x[0])\n",
      "\n",
      "\n",
      "df[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "sns.factorplot(x='Ticket', y='Survived', data=df, kind='jupyter_string')\n",
      "=====\n",
      "df['Ticket'].value_counts()\n",
      "----------------------------------------\n",
      "sns.barplot(x='Ticket', y='Survived', data=df)\n",
      "plt.show()\n",
      "=====\n",
      "df[['Ticket', 'Fare']].groupby(['Ticket'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "df[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean()\n",
      "=====\n",
      "df.Fare.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "sns.boxplot('Sex','Fare',data=df)\n",
      "plt.show()\n",
      "=====\n",
      "df[['Pclass', 'Fare']].groupby(['Pclass']).mean()\n",
      "----------------------------------------\n",
      "df[['Embarked', 'Fare']].groupby(['Embarked']).median()\n",
      "=====\n",
      "guess_Fare = df.Fare.loc[ (df.Ticket == 'jupyter_string') & (df.Pclass == 3) & (df.Embarked == 'jupyter_string')].median()\n",
      "df.Fare.fillna(guess_Fare , inplace=True)\n",
      "\n",
      "\n",
      "df[['Fare', 'Survived']].groupby(['Survived'],as_index=False).mean()\n",
      "----------------------------------------\n",
      "sns.barplot(x='Embarked', y='Survived', data=df)\n",
      "=====\n",
      "grid = sns.FacetGrid(df, hue='Survived', size=4, aspect=1.5)\n",
      "grid.map(plt.hist, 'Fare', alpha=.5, bins=range(0,210,10))\n",
      "grid.add_legend()\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "sns.factorplot(x='jupyter_string', y='Survived', data=df, kind='jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "df.Cabin.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "df['Embarked'].fillna('jupyter_string', inplace=True)\n",
      "=====\n",
      "df.Embarked.isnull().sum(axis=0)\n",
      "----------------------------------------\n",
      "df.Embarked.fillna('jupyter_string', inplace=True)\n",
      "=====\n",
      "df.describe(include=['jupyter_string']) \n",
      "----------------------------------------\n",
      "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
      "=====\n",
      "grid = sns.FacetGrid(df, col='jupyter_string', size=3, aspect=0.8, sharey=False)\n",
      "grid.map(plt.hist, 'Age', alpha=.5, bins=range(0,105,5))\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
      "=====\n",
      "df[['jupyter_string', 'Age']].groupby(['jupyter_string']).mean()\n",
      "----------------------------------------\n",
      "grid = sns.FacetGrid(df, col='jupyter_string', size=3, aspect=0.8, sharey=False)\n",
      "grid.map(plt.hist, 'Age', alpha=.5, bins=range(0,105,5))\n",
      "plt.show()\n",
      "=====\n",
      "df[['jupyter_string', 'Age']].groupby(['jupyter_string']).std()\n",
      "----------------------------------------\n",
      "sns.distplot(df.temperature)\n",
      "=====\n",
      "df.loc[df.gender == 'jupyter_string', 'gender'] = 0\n",
      "df.loc[df.gender == 'jupyter_string', 'gender'] = 1\n",
      "\n",
      "chi2, p, dof, ex = scipy.stats.chi2_contingency(df.head())\n",
      "print('jupyter_string', chi2)\n",
      "print('jupyter_string', p)\n",
      "print('jupyter_string',dof)\n",
      "\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = pd.cut(df['Age'], 5)\n",
      "df['jupyter_string'] = pd.cut(df['Age'], 5)\n",
      "df['jupyter_string'] = pd.cut(df['Age'], 5)\n",
      "df['jupyter_string'] = pd.cut(df['Age'], 5)\n",
      "=====\n",
      "bins = [ 0, 4, 12, 18, 30, 50, 65, 100] \n",
      "age_index = (1,2,3,4,5,6,7) \n",
      "df['jupyter_string'] = pd.cut(df.Age, bins, labels=age_index).astype(int)\n",
      "\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'],as_index=False).mean()\n",
      "----------------------------------------\n",
      "df['jupyter_string'].value_counts()\n",
      "=====\n",
      "df[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean()\n",
      "----------------------------------------\n",
      "df[['jupyter_string', 'Survived']].groupby(['jupyter_string'],as_index=False).mean()\n",
      "=====\n",
      "df['Ticket'].value_counts()\n",
      "----------------------------------------\n",
      "df.groupby('gender')['heart_rate'].describe()\n",
      "=====\n",
      "scipy.stats.ttest_1samp(df.temperature, 98.6)\n",
      "----------------------------------------\n",
      "train_y.head()\n",
      "=====\n",
      "test_all = pd.read_csv('jupyter_string')\n",
      "test_x = test_all.loc[:, \"Class\" : \"Fare\"]\n",
      "test_y = test_all.loc[:, \"Survived\"]\n",
      "----------------------------------------\n",
      "train_x = train_x.fillna(train_x.mean())\n",
      "test_x = test_x.fillna(test_x.mean())\n",
      "validate_x = validate_x.fillna(validate_x.mean())\n",
      "=====\n",
      "df = df.drop(['Id', 'Name'], axis=1)\n",
      "\n",
      "\n",
      "df = df.dropna(how='jupyter_string', axis=0)\n",
      "\n",
      "df = df.replace(['jupyter_string','jupyter_string'], [0,1])\n",
      "\n",
      "print(df.shape)\n",
      "df.head()\n",
      "----------------------------------------\n",
      "mean_confidence_interval(df.heart_rate)\n",
      "=====\n",
      "m = df.loc[df.gender == 1, 'temperature']\n",
      "f = df.loc[df.gender == 0, 'temperature']\n",
      "----------------------------------------\n",
      "m_diff = m.mean() - f.mean()\n",
      "m_diff\n",
      "=====\n",
      "diff_mean = m.mean() - f.mean()\n",
      "m_var = np.var(m)\n",
      "f_var = np.var(f)\n",
      "var = math.sqrt((m_var/len(m)) + (f_var/len(f)))\n",
      "\n",
      "lim = 1.96 * var\n",
      "up = diff_mean + lim\n",
      "low = diff_mean - lim\n",
      "\n",
      "test = 1.65 * var\n",
      "print('jupyter_string',abs(diff_mean))\n",
      "print('jupyter_string',test)\n",
      "print('jupyter_string', up,'jupyter_string',low,'jupyter_string')\n",
      "----------------------------------------\n",
      "df.describe()\n",
      "=====\n",
      "df = df.fillna(0)\n",
      "----------------------------------------\n",
      "X_train.head()\n",
      "=====\n",
      "Y_oh_train=to_categorical(Y_train,num_classes=5).astype('jupyter_string')\n",
      "Y_oh_test=to_categorical(Y_test,num_classes=5).astype('jupyter_string')\n",
      "----------------------------------------\n",
      "data = pd.read_csv('jupyter_string')\n",
      "data.head()\n",
      "=====\n",
      "X = pd.read_csv('jupyter_string', sep='jupyter_string', header=0, usecols=range(13), thousands='jupyter_string')\n",
      "Y = pd.read_csv('jupyter_string', sep='jupyter_string', header=0, usecols=[13], encoding='jupyter_string')\n",
      "Y = Y.replace({'jupyter_string': 0}, regex=True)\n",
      "Y = Y.replace({'jupyter_string': 1}, regex=True)\n",
      "Y = Y.replace({'jupyter_string': 2}, regex=True)\n",
      "\n",
      "X = X.values.astype(np.float64)\n",
      "Y = Y.values[:,0].astype(np.float64)\n",
      "\n",
      "\n",
      "from sklearn import preprocessing\n",
      "\n",
      "X_NORMAL = preprocessing.normalize(X)\n",
      "X_STD = preprocessing.scale(X)\n",
      "X_NORMAL_STD = preprocessing.scale(X_NORMAL)\n",
      "\n",
      "----------------------------------------\n",
      "U, s, V = np.linalg.svd(X, full_matrices=False)\n",
      "\n",
      "\n",
      "print('jupyter_string')\n",
      "print(s)\n",
      "\n",
      "\n",
      "print('jupyter_string')\n",
      "print(U)\n",
      "print('jupyter_string')\n",
      "print(V)\n",
      "=====\n",
      "s_rank = 12\n",
      "S = np.zeros(X.shape, dtype=complex)\n",
      "S[:s_rank, :s_rank] = np.diag(s)[:s_rank, :s_rank]\n",
      "\n",
      "X_SVD = np.dot(U, np.dot(S, V));\n",
      "X_SVD = np.real(X_SVD);\n",
      "----------------------------------------\n",
      "X_EDITED = np.zeros(X_EDITED.shape, dtype=complex)\n",
      "X_EDITED[:s_rank, :s_rank] = np.diag(s)[:s_rank, :s_rank]\n",
      "\n",
      "X_EDITED = np.real(X_EDITED);\n",
      "=====\n",
      "used_columns = [1,6,10,11,12]\n",
      "X_EDITED = X[:, used_columns]\n",
      "----------------------------------------\n",
      "clf = svm.SVC(kernel='jupyter_string')\n",
      "clf.fit(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print('jupyter_string', accuracy_score(y_test, y_pred))\n",
      "=====\n",
      "svc = svm.SVC(kernel='jupyter_string', C=Penalty_C)\n",
      "accuracy_svc = cross_val_score(svc, X, Y, cv=cross_val_k, scoring='jupyter_string').mean()\n",
      "print('jupyter_string', accuracy_svc)\n",
      "----------------------------------------\n",
      "billboard = pd.read_csv('jupyter_string')\n",
      "billboard.head()\n",
      "=====\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "import pprint as pprint\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "plt.style.use('jupyter_string')\n",
      "----------------------------------------\n",
      "sns.pairplot(df)\n",
      "=====\n",
      "df.corr()\n",
      "----------------------------------------\n",
      "bb_df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "bb_df['artist.inverted'].value_counts()\n",
      "----------------------------------------\n",
      "bb_df[['x64th.week', 'x34th.week', 'song_len', 'x49th.week', 'x55th.week', 'x50th.week', 'x38th.week', 'time', 'genre', 'x18th.week', 'x29th.week', 'x44th.week', 'x47th.week', 'x59th.week', 'x22nd.week', 'x46th.week', 'x76th.week', 'year_entered', 'x28th.week', 'x4th.week', 'x17th.week', 'x37th.week', 'x63rd.week', 'month_entered', 'x51st.week', 'x48th.week', 'x62nd.week', 'x10th.week', 'x30th.week', 'Time_to_peak_prop', 'Avg_position', 'x54th.week', 'x75th.week', 'x74th.week', 'x68th.week', 'x19th.week', 'x65th.week', 'x20th.week', 'x14th.week', 'x9th.week', 'x3rd.week', 'x56th.week', 'x16th.week', 'x67th.week', 'x33rd.week', 'x12th.week', 'x43rd.week', 'x24th.week', 'year', 'Season_entered', 'x61st.week', 'x7th.week', 'date.entered', 'x53rd.week', 'x1st.week', 'x66th.week', 'x27th.week', 'x69th.week', 'x40th.week', 'x32nd.week', 'artist.inverted', 'x23rd.week', 'x36th.week', 'x70th.week', 'x2nd.week', 'x72nd.week', 'x6th.week', 'x25th.week', 'x39th.week', 'x52nd.week', 'x57th.week', 'x26th.week', 'Time_to_peak_bin', 'x15th.week', 'x45th.week', 'x31st.week', 'x8th.week', 'time_on_bb_weeks', 'date.peaked', 'x11th.week', 'x58th.week', 'x21st.week', 'x42nd.week', 'x35th.week', 'x13th.week', 'x41st.week', 'track', 'x60th.week', 'x73rd.week', 'x5th.week', 'x71st.week']].describe()\n",
      "=====\n",
      "bb_df['jupyter_string'] = bb_df['time'].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "----------------------------------------\n",
      "sns.distplot(bb_small_df['jupyter_string'])\n",
      "plt.show()\n",
      "=====\n",
      "bb_small_df.hist(figsize=(15,15));\n",
      "----------------------------------------\n",
      "bb_small_df.hist(figsize=(15,15));\n",
      "=====\n",
      "bb_small_df['jupyter_string'].plot(kind = 'jupyter_string', figsize = (15,7),\\\n",
      "                                     bins = 50,\\\n",
      "                                    fontsize = 15\\\n",
      "                                    ).set_title('jupyter_string', \\\n",
      "                                                fontsize = 20, y = 1.01)\n",
      "\n",
      "plt.savefig('jupyter_string');\n",
      "----------------------------------------\n",
      "peaked_early = df[df.peaked == 1]['Avg_position']\n",
      "peaked_later = df[df.peaked == 0]['Avg_position']\n",
      "=====\n",
      "shuf_peak_df = bb_small_df[['jupyter_string', 'jupyter_string']]\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "rs = pd.read_csv('jupyter_string', index_col=0)\n",
      "----------------------------------------\n",
      "rs.head()\n",
      "=====\n",
      "pd.set_option('jupyter_string', 500)\n",
      "----------------------------------------\n",
      "model = smf.ols(formula=formula, data=df)\n",
      "res=model.fit()\n",
      "res.summary()\n",
      "=====\n",
      "plt.scatter(df['medv'], res.fittedvalues)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "from sklearn import linear_model, preprocessing\n",
      "\n",
      "sstat = pd.read_csv('jupyter_string',index_col=1)\n",
      "ss = sstat.fillna(0)\n",
      "print(ss)\n",
      "----------------------------------------\n",
      "ss = pd.read_csv('jupyter_string')\n",
      "ss.columns = ['FG','FGA','FG%','3P','3PA','3P%','2P','2PA','2P%','eFG%','FT','FTA','FT%','ORB','DRB','TRB','AST','STL','BLK','TOV','PF','jupyter_string']\n",
      "=====\n",
      "sstat8 = pd.read_csv('jupyter_string',index_col=1)\n",
      "ss8 = sstat8.fillna(0)\n",
      "----------------------------------------\n",
      "yhat = logreg.predict(xts)\n",
      "acc = np.mean(yhat == yts)\n",
      "print('jupyter_string'.format(acc))\n",
      "=====\n",
      "nprt = 10\n",
      "Ierr = np.where(ytr != yhat)[0]\n",
      "print('jupyter_string',Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = 'jupyter_string'.format(ytr[ind], yhat[ind])\n",
      "    print(title)\n",
      "----------------------------------------\n",
      "plt.scatter(ytr, yhat)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "from sklearn.metrics import confusion_matrix\n",
      "C = confusion_matrix(ytr,yhat)\n",
      "a=np.where((ytr=='jupyter_string')&(yhat=='jupyter_string'))[0]\n",
      "b=np.where((ytr=='jupyter_string'))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='jupyter_string')&(yhat=='jupyter_string'))[0]\n",
      "b=np.where((ytr=='jupyter_string'))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='jupyter_string')&(yhat=='jupyter_string'))[0]\n",
      "b=np.where((ytr=='jupyter_string'))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='PF')&(yhat=='PF'))[0]\n",
      "b=np.where((ytr=='PF'))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='jupyter_string')&(yhat=='jupyter_string'))[0]\n",
      "b=np.where((ytr=='jupyter_string'))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "\n",
      "Csum = np.sum(C,1)\n",
      "C = C / Csum[None,:]\n",
      "\n",
      "\n",
      "print(np.array_str(C, precision=3, suppress_small=True))\n",
      "plt.imshow(C, interpolation='jupyter_string')\n",
      "x=[0,1,2,3,4,5]\n",
      "y=[0,1,2,3,4,5]\n",
      "ygroup_labels = ['jupyter_string', 'PF','jupyter_string','jupyter_string','jupyter_string','jupyter_string']  \n",
      "xgroup_labels = ['jupyter_string', 'PF','jupyter_string','jupyter_string','jupyter_string','jupyter_string'] \n",
      "plt.xticks(x, xgroup_labels, rotation=90)  \n",
      "plt.yticks(y, ygroup_labels, rotation=0)\n",
      "plt.colorbar()\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "----------------------------------------\n",
      "svc.fit(xts,yts)\n",
      "yhat_ts = svc.predict(xts)\n",
      "acc = np.mean(yhat_ts == yts)\n",
      "print('jupyter_string'.format(acc))\n",
      "=====\n",
      "Ierr = np.where(yhat_tr != ytr)[0]\n",
      "nprt = 2\n",
      "print('jupyter_string',Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = 'jupyter_string'.format(ytr[ind], yhat_tr[ind])\n",
      "    print(title)\n",
      "----------------------------------------\n",
      "plt.scatter(yhat_tr, ytr)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "yhat_ts = svc.predict(xts)\n",
      "acc = np.mean(yhat_ts == yts)\n",
      "print('jupyter_string'.format(acc))\n",
      "----------------------------------------\n",
      "corrmat = data_df.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat)\n",
      "=====\n",
      "feature_df = data_df.drop(['Y1', 'Y2'], axis=1)\n",
      "----------------------------------------\n",
      "corr = feature_df.corr()\n",
      "sns.heatmap(corr)\n",
      "=====\n",
      "y1_corrs = feature_df.corrwith(data_df.Y1)\n",
      "y1_corrs.plot(kind='jupyter_string')\n",
      "----------------------------------------\n",
      "sns.heatmap(y1_corrs)\n",
      "=====\n",
      "f_corrs = feature_df.corr()\n",
      "sns.heatmap(f_corrs, annot=True)\n",
      "----------------------------------------\n",
      "lambdas = np.logspace(-3,-2,100)\n",
      "R_2_OS=[]\n",
      "X_train0, X_valid, y_train0, y_valid = train_test_split(X_train,\n",
      "                                    y_train, test_size = 0.3, random_state = 10)\n",
      "for a in lambdas:\n",
      "    R_2_OS.append(Regularization_fit_lambda(1,X_train0,y_train0,a,p=0.4,Graph=True,logl=False))\n",
      "plt.plot(lambdas,R_2_OS)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "lambdas = np.exp(np.linspace(0,13,200))\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.3,Graph=True)\n",
      "print('jupyter_string'.format(lambda_r_optimal))\n",
      "----------------------------------------\n",
      "petal.head()\n",
      "=====\n",
      "train_p,test_p = train_test_split(petal, test_size=0.3, random_state=0) \n",
      "train_x_p = train_p[['PetalWidthCm','PetalLengthCm']]\n",
      "train_y_p = train_p.Species\n",
      "\n",
      "test_x_p = test_p[['PetalWidthCm','PetalLengthCm']]\n",
      "test_y_p = test_p.Species\n",
      "----------------------------------------\n",
      "X = iris.drop(['Id', 'Species'], axis=1)\n",
      "y = iris['Species']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "=====\n",
      "plt.figure(figsize=(8,4))\n",
      "sns.heatmap(iris.corr(), annot=True, cmap='jupyter_string') \n",
      "plt.show()\n",
      "----------------------------------------\n",
      "stats.ttest_ind(data['Congruent'], data['Incongruent'], equal_var=False)\n",
      "=====\n",
      "alpha = 0.05\n",
      "\n",
      "dof = len(data['Congruent']) - 1\n",
      "\n",
      "data['jupyter_string'] = data['Congruent']-data['Incongruent']\n",
      "\n",
      "\n",
      "diff_mean = data['jupyter_string'].mean()\n",
      "diff_std = data['jupyter_string'].std()\n",
      "diff_stderr = diff_std/len(data['Congruent'])**0.5\n",
      "t_crit = stats.t.ppf(1-alpha/2, dof)\n",
      "lower = diff_mean - t_crit*diff_stderr\n",
      "upper = diff_mean + t_crit*diff_stderr\n",
      "t_stat = (diff_mean-0)/(diff_std/len(data['Congruent'])**0.5)\n",
      "pvalue = stats.t.sf(np.abs(t_stat), dof)*2\n",
      "print('jupyter_string' + 'jupyter_string' % t_crit + 'jupyter_string' + 'jupyter_string' % t_crit)\n",
      "print('jupyter_string' + 'jupyter_string' % lower + 'jupyter_string' + 'jupyter_string' % upper + 'jupyter_string')\n",
      "print('jupyter_string' + 'jupyter_string' % t_stat)\n",
      "print('jupyter_string' + 'jupyter_string' % pvalue)\n",
      "----------------------------------------\n",
      "data.describe()\n",
      "=====\n",
      "con_mean = data['Congruent'].mean()\n",
      "con_median = data['Congruent'].median()\n",
      "con_std = data['Congruent'].std()\n",
      "incon_mean = data['Incongruent'].mean()\n",
      "incon_median = data['Incongruent'].median()\n",
      "incon_std = data['Incongruent'].std()\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = df['jupyter_string'].fillna('jupyter_string')\n",
      "df['jupyter_string'] = df['jupyter_string'].fillna('jupyter_string')\n",
      "df['jupyter_string'] = df['jupyter_string'].fillna('jupyter_string')\n",
      "df['jupyter_string'] = df['jupyter_string'].fillna('jupyter_string')\n",
      "df['jupyter_string'] = df['jupyter_string'].fillna('jupyter_string')\n",
      "=====\n",
      "df_pop1 = df.copy(deep=True)\n",
      "df_pop1 = df_pop1[df.A_Se != 'jupyter_string']\n",
      "df_pop1 = df_pop1[df.A_As != 'jupyter_string']\n",
      "\n",
      "\n",
      "df_pop2 = df.copy(deep=True)\n",
      "df_pop2 = df_pop2[df.A_As != 'jupyter_string']\n",
      "df_pop2 = df_pop2[df.A_Pb != 'jupyter_string']\n",
      "----------------------------------------\n",
      "df_pop1.A_Se = df_pop1.A_Se.fillna(0)\n",
      "df_pop1.A_As = df_pop1.A_As.fillna(0)\n",
      "df_pop1.A_Pb = df_pop1.A_Pb.fillna(0)\n",
      "\n",
      "\n",
      "df_pop2.A_Se = df_pop2.A_Se.fillna(0)\n",
      "df_pop2.A_As = df_pop2.A_As.fillna(0)\n",
      "df_pop2.A_Pb = df_pop2.A_Pb.fillna(0)\n",
      "=====\n",
      "df2 = df.replace('jupyter_string', 0)\n",
      "----------------------------------------\n",
      "df2 = df2.replace('jupyter_string', 0)\n",
      "=====\n",
      "df2.head()\n",
      "----------------------------------------\n",
      "df_pop1.columns = ['Longitude', 'A_As', 'A_Pb', 'Ã¯Â»Â¿SiteID', 'A_Se', 'Latitude', 'SiteID']\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop2[[\"Latitude\", \"Longitude\"]], weights=df_pop2[\"A_As\"],\n",
      "    max_intensity=80, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "----------------------------------------\n",
      "X_train = data_train[['18', '22', '8', '12', '26', '3', '27', '4', '21', '5', '33', '36', '20', '28', '32', '17', '15', '34', '24', '14', '29', '6', '35', '31', '10', 'Unnamed: 0', '19', '38', '2', '7', '1', '37', '39', '9', '11', '23', '40', '30', '13', '16', '25', '0']]\n",
      "y_train = data_train['18']\n",
      "\n",
      "X_test = data_test[['18', '22', '8', '12', '26', '3', '27', '4', '21', '5', '33', '36', '20', '28', '32', '17', '15', '34', '24', '14', '29', '6', '35', '31', '10', 'Unnamed: 0', '19', '38', '2', '7', '1', '37', '39', '9', '11', '23', '40', '30', '13', '16', '25', '0']]\n",
      "y_test = data_test['18']\n",
      "=====\n",
      "data_test.head()\n",
      "----------------------------------------\n",
      "plt.scatter(cars.horsepower,cars.mpg)\n",
      "plt.show()\n",
      "=====\n",
      "import  seaborn as sns\n",
      "\n",
      "ax = sns.regplot(x = \"weight\", y = \"mpg\", data = cars)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "y_pred_lasso = lasso.predict(polyX_test)\n",
      "y_pred_ridge = ridge.predict(polyX_test)\n",
      "y_pred_eNet = eNet.predict(polyX_test)\n",
      "=====\n",
      "plot_learning_curve(lasso, 'jupyter_string', polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "AAnn = pd.read_csv('jupyter_string')\n",
      "AUF2015= pd.read_csv('jupyter_string')\n",
      "\n",
      "\n",
      "AUF2015.rename(columns={' Primary Key': 'jupyter_string', ' Effect on Officer': 'jupyter_string', 'Officer Yrs of Service': 'jupyter_string'}, inplace=True)\n",
      "AAnn.rename(columns={'jupyter_string':'jupyter_string', 'Council District': 'jupyter_string'}, inplace=True)\n",
      "\n",
      "\n",
      "AUF2015.columns = AUF2015.columns.str.replace('jupyter_string','jupyter_string')\n",
      "AAnn.columns = AAnn.columns.str.replace('jupyter_string','jupyter_string')\n",
      "\n",
      "\n",
      "AUF2015 = AUF2015.drop_duplicates(subset='jupyter_string', keep='jupyter_string', inplace = False)\n",
      "\n",
      "\n",
      "stack = pd.merge(AAnn, AUF2015, left_on='jupyter_string', right_on='jupyter_string', how='jupyter_string', indicator = True)\n",
      "stack.loc[ stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[ stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[ stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack.loc[stack.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "\n",
      "=====\n",
      "stack['jupyter_string'].value_counts()\n",
      "----------------------------------------\n",
      "crime = pd.read_csv('jupyter_string')\n",
      "crime.columns = ['coords', 'GO X Coordinate', 'GO Location Zip', 'GO Census Tract', 'Composite Socioeconomic Data for City Council Districts', 'GO District', 'GOPrimaryKey', 'Unnamed: 7', 'count', 'Clearance Date', 'Clearance Status', 'Officer Yrs of Service', 'Nature of Contact', 'GOYCoordinate', 'Unnamed: 14', 'Weapon Used 3', 'Subject Conduct Desc', ' Reason Desc', 'Unnamed: 10', 'GO Location', 'Subject Effects', 'crime', 'Number Shots', 'Time Occurred', 'Weapon Used 4', 'GOXCoordinate', 'Subject Resistance', 'geometry', 'Ã¯Â»Â¿Composite Socioeconomic Data for City Council Districts', 'Unnamed: 11', 'YCoord', 'table_ind', 'Unnamed: 3', 'Unnamed: 5', 'Officer Commission Date', 'Subject Sex', 'Council District', 'RIN', 'Y-Coordinate', 'X-Coordinate', 'XCoord', 'Unnamed: 1', 'GO Primary Key', 'Subject Race', 'force', 'RIN', 'Y-Coordinate', 'X-Coordinate', 'XCoord', 'Unnamed: 13', 'Unnamed: 4', 'UF', 'Unnamed: 6', 'Weapon Used 1', 'council_di', 'Unnamed: 12', 'GO Highest Offense Desc', 'GO Y Coordinate', 'Date Occurred', 'Key', 'Master Subject ID', 'Subject Ethnicity', 'Area Command', 'Effect on Officer', 'Unnamed: 2', 'R2R Level', 'Area Command', 'Weapon Used 5', 'Unnamed: 15', 'Unnamed: 16']\n",
      "=====\n",
      "crime_types = pd.merge(pd.get_dummies(crime_table.loc[:,  'Highest NIBRS/UCR Offense Description']), crime_table, left_index = True, right_index = True, how = 'jupyter_string')\n",
      "----------------------------------------\n",
      "crime_table_types.head()\n",
      "=====\n",
      "stack_types = pd.merge(crime_table_types, AUF2015, left_on='GO Primary Key', right_on=' Primary Key', how='jupyter_string', indicator = True)\n",
      "stack_types.loc[ stack_types.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack_types.loc[ stack_types.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "stack_types.loc[ stack_types.loc[:, 'jupyter_string'] == 'jupyter_string'  , 'jupyter_string'] = 'jupyter_string'\n",
      "\n",
      "stack_types.loc[:, 'jupyter_string'] = 0\n",
      "stack_types.loc[ stack_types.loc[:, 'jupyter_string'] != 'jupyter_string' , 'jupyter_string'] = 1\n",
      "\n",
      "stack_types.loc[:, 'jupyter_string'] = 0\n",
      "stack_types.loc[ stack_types.loc[:, 'jupyter_string'] != 'jupyter_string' , 'jupyter_string'] = 1\n",
      "----------------------------------------\n",
      "model_types['jupyter_string'] = smf.ols('jupyter_string' + 'jupyter_string', data = stack_types).fit()\n",
      "=====\n",
      "ENTER_CRIME_STRING_HERE = 'jupyter_string'\n",
      "model_types[ENTER_CRIME_STRING_HERE].summary()\n",
      "----------------------------------------\n",
      "districts.head()\n",
      "=====\n",
      "AUF2015.rename(columns={' Primary Key': 'jupyter_string', ' Effect on Officer': 'jupyter_string', 'jupyter_string':'jupyter_string', 'Officer Yrs of Service': 'jupyter_string'}, inplace=True)\n",
      "AAnn.rename(columns={'jupyter_string':'jupyter_string', 'Council District': 'jupyter_string'}, inplace=True)\n",
      "\n",
      "\n",
      "AUF2015.columns = AUF2015.columns.str.replace('jupyter_string','jupyter_string')\n",
      "AAnn.columns = AAnn.columns.str.replace('jupyter_string','jupyter_string')\n",
      "\n",
      "\n",
      "AUF2015 = AUF2015.drop_duplicates(subset='jupyter_string', keep='jupyter_string', inplace = False)\n",
      "----------------------------------------\n",
      "_ = pm.plot_posterior(trace[burnin:], varnames=['jupyter_string', 'jupyter_string'])\n",
      "=====\n",
      "x_lim = 60\n",
      "y_pred = trace[burnin:].get_values('jupyter_string')\n",
      "\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype='jupyter_string', color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel('jupyter_string')\n",
      "_ = plt.title('jupyter_string')\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='jupyter_string')\n",
      "_ = plt.xlabel('jupyter_string')\n",
      "_ = plt.ylabel('jupyter_string')\n",
      "_ = plt.title('jupyter_string')\n",
      "\n",
      "plt.tight_layout()\n",
      "----------------------------------------\n",
      "pm.model_to_graphviz(model)\n",
      "=====\n",
      "prob_pois = trace[burnin:]['jupyter_string'].mean()\n",
      "prob_nb = 1 - prob_pois\n",
      "BF = (prob_nb/prob_pois)*(prior_model_prob/(1-prior_model_prob))\n",
      "print('jupyter_string' % BF)\n",
      "----------------------------------------\n",
      "movies = pd.read_csv('jupyter_string')\n",
      "lines = pd.read_csv('jupyter_string')\n",
      "characters = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "conversation_df = pd.read_csv('jupyter_string',encoding='jupyter_string', sep='jupyter_string',warn_bad_lines =False,header=None)\n",
      "lines_df = pd.read_csv('jupyter_string',sep='jupyter_string',error_bad_lines=False,warn_bad_lines =False,header=None)\n",
      "characters_df = pd.read_csv('jupyter_string',sep='jupyter_string',warn_bad_lines =False,error_bad_lines=False,header=None)\n",
      "\n",
      "----------------------------------------\n",
      "dialogues.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "cul_sentence = 'jupyter_string'.join(dialogues)\n",
      "nltk.download('jupyter_string')\n",
      "----------------------------------------\n",
      "characters_df.head()\n",
      "=====\n",
      "print('jupyter_string')\n",
      "characters_df.columns=['jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string']\n",
      "characters_df.head(5)\n",
      "----------------------------------------\n",
      "bigrams = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
      "bigrams.score_ngrams(nltk.collocations.BigramAssocMeasures())\n",
      "=====\n",
      "biCharWords = nltk.bigrams(char_final_tokens)\n",
      "biFdist = nltk.FreqDist(biCharWords)\n",
      "print(len(biFdist))\n",
      "biFdist.plot(20, cumulative=False)\n",
      "----------------------------------------\n",
      "merged_df.to_csv('jupyter_string')\n",
      "=====\n",
      "merged_df.dropna(how='jupyter_string', inplace=True)\n",
      "----------------------------------------\n",
      "char_dialogues.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "char_cul_sentence = 'jupyter_string'.join(char_dialogues)\n",
      "----------------------------------------\n",
      "char_cul_sentence[:10]\n",
      "=====\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "tknzr = TweetTokenizer()\n",
      "char_final_tokens = tknzr.tokenize(char_cul_sentence)\n",
      "----------------------------------------\n",
      "pd.pivot_table(stacked, index='jupyter_string', columns='jupyter_string', values='jupyter_string', aggfunc='jupyter_string')\n",
      "=====\n",
      "mean_power = df.mean()\n",
      "mean_power.head()\n",
      "----------------------------------------\n",
      "rules = convert_apriori_results_to_pandas_df(results)\n",
      "\n",
      "\n",
      "rules.head()\n",
      "=====\n",
      "result_df = result_df.sort_values(by='jupyter_string', ascending=False)\n",
      "print(result_df.head(10))\n",
      "----------------------------------------\n",
      "result_df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "result_df = result_df.sort_values(by='jupyter_string', ascending=False)\n",
      "print(result_df.head(10))\n",
      "----------------------------------------\n",
      "my_dataframe.head()\n",
      "=====\n",
      "bitcoin_price_analysis = pd.read_csv('jupyter_string')\n",
      "bcktstdf=pd.read_csv('jupyter_string')\n",
      "tmp=bitcoin_price_analysis.head()\n",
      "----------------------------------------\n",
      "ms_columns\n",
      "=====\n",
      "df1=bitcoin_price_analysis\n",
      "missing_columns=df1.columns[df1.isnull().any()].tolist()\n",
      "----------------------------------------\n",
      "df1[['Date','btc_total_bitcoins']].set_index('Date').plot()\n",
      "=====\n",
      "my_dataframe['btc_total_bitcoins'].interpolate(method='jupyter_string',axis=0,inplace=True)\n",
      "----------------------------------------\n",
      "plt.plot(my_dataframe['btc_total_bitcoins'])\n",
      "plt.show()\n",
      "=====\n",
      "df1['btc_total_bitcoins'].interpolate(method='jupyter_string',axis=0,inplace=True)\n",
      "----------------------------------------\n",
      "my_dataframe[['btc_total_bitcoins']] = my_dataframe[['btc_total_bitcoins']].astype('jupyter_string')\n",
      "=====\n",
      "import datetime as dt\n",
      "\n",
      "test_dataframe['Date']=test_dataframe['Date'].astype('jupyter_string')\n",
      "test_dataframe['Date'] = pd.to_datetime(test_dataframe['Date']).dt.strftime('jupyter_string')\n",
      "\n",
      "test_dataframe['Date'] = pd.to_datetime(test_dataframe['Date'])\n",
      "test_dataframe['Date']=test_dataframe['Date'].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "my_dataframe['Date']=my_dataframe['Date'].astype('jupyter_string')\n",
      "my_dataframe['Date'] = pd.to_datetime(my_dataframe['Date']).dt.strftime('jupyter_string')\n",
      "\n",
      "my_dataframe['Date'] = pd.to_datetime(my_dataframe['Date'])\n",
      "my_dataframe['Date']=my_dataframe['Date'].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "my_dataframe['Date']\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "y_hat = lm.predict(my_dataframe[['btc_total_bitcoins']])\n",
      "=====\n",
      "import datetime as dt\n",
      "\n",
      "bcktstdf['Date']=bcktstdf['Date'].astype('jupyter_string')\n",
      "bcktstdf['Date'] = pd.to_datetime(bcktstdf['Date']).dt.strftime('jupyter_string')\n",
      "\n",
      "bcktstdf['Date'] = pd.to_datetime(bcktstdf['Date'])\n",
      "bcktstdf['Date']=bcktstdf['Date'].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "df1['Date']=df1['Date'].astype('jupyter_string')\n",
      "df1['Date'] = pd.to_datetime(df1['Date']).dt.strftime('jupyter_string')\n",
      "\n",
      "df1['Date'] = pd.to_datetime(df1['Date'])\n",
      "df1['Date']=df1['Date'].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "df = pd.merge(df, housedf, on='house', how='jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "pd.merge(mean_power, housedf).head()\n",
      "----------------------------------------\n",
      "male_survived_group = titanic_data[titanic_data['Sex'] == 'jupyter_string']\n",
      "female_survived_group = titanic_data[titanic_data['Sex'] == 'jupyter_string']\n",
      "display_hist(male_survived_group['Age'],'jupyter_string')\n",
      "display_hist(female_survived_group['Age'],'jupyter_string')\n",
      "=====\n",
      "groupedby_Sex = titanic_df.groupby('Sex',as_index = False)\n",
      "----------------------------------------\n",
      "titanic_df.dropna(inplace = True)\n",
      "=====\n",
      "titanic_df.head()\n",
      "----------------------------------------\n",
      "survived = titanic_df[titanic_df['Survived'] == 1]\n",
      "not_survived = titanic_df[titanic_df['Survived'] == 0]\n",
      "=====\n",
      "groupedby_Survived = titanic_df.groupby('Survived',as_index = False)\n",
      "survival_count = groupedby_Survived.count()\n",
      "print('jupyter_string'+str(titanic_df.shape[0]))\n",
      "print('jupyter_string'+str(survival_count.loc[1,'PassengerId']))\n",
      "print('jupyter_string'+str(survival_count.loc[0,'PassengerId']))\n",
      "----------------------------------------\n",
      "groupedby_Survived = titanic_df.groupby('Survived',as_index = False)\n",
      "groupedby_Survived.count()\n",
      "=====\n",
      "percentage_survived = (survival_count.loc[1,'PassengerId']/titanic_df.shape[0])*100\n",
      "percentage_not_survived = (survival_count.loc[0,'PassengerId']/titanic_df.shape[0])*100\n",
      "sizes = [percentage_survived,percentage_not_survived]\n",
      "labels = ['Survived','jupyter_string']\n",
      "plt.pie(sizes, labels=labels, autopct='jupyter_string', startangle=90)\n",
      "x = plt.title('jupyter_string')\n",
      "----------------------------------------\n",
      "survival_pclass = titanic_df.groupby('Pclass')['Survived'].mean()\n",
      "survival_pclass\n",
      "=====\n",
      "groupedby_Pclass = titanic_df.groupby('Pclass',as_index = False)\n",
      "----------------------------------------\n",
      "titanic_df['Fare'].hist()\n",
      "plt.xlabel(\"Fare\")\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string')\n",
      "=====\n",
      "survived_group = groupedby_Survived.get_group(1)\n",
      "not_survived_group = groupedby_Survived.get_group(0)\n",
      "----------------------------------------\n",
      "customer_data = customer_data.set_index('CustomerID')\n",
      "=====\n",
      "customer_df = invoice_data.join([product_data, sales_data, agg_cart_data])\n",
      "\n",
      "\n",
      "customer_df.head()\n",
      "----------------------------------------\n",
      "df.shape\n",
      "=====\n",
      "df.head(10)\n",
      "----------------------------------------\n",
      "df['Country'].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(6,7))\n",
      "\n",
      "\n",
      "sns.countplot(y='Country',data=df)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df.dropna(subset=['CustomerID'], inplace=True)\n",
      "=====\n",
      "df = df[df.CustomerID.notnull()]\n",
      "----------------------------------------\n",
      "SPX_data_dump.head()\n",
      "=====\n",
      "SPX_data_dump['Date'] = pd.to_datetime(SPX_data_dump.Date, format='jupyter_string')\n",
      "SPX_data_dump = SPX_data_dump.sort_values(by='Date', ascending=1)\n",
      "SPX_data_dump.index = np.arange(rows)[::-1]\n",
      "display(SPX_data_dump.head(n=5))\n",
      "----------------------------------------\n",
      "sns.factorplot(x='jupyter_string', y='jupyter_string', hue='jupyter_string', data=merged, kind='jupyter_string')\n",
      "=====\n",
      "grp = merged.groupby('type')\n",
      "type(grp)\n",
      "----------------------------------------\n",
      "SPX_data_dump['jupyter_string'] = SPX_data_dump['Close_SPX'] - SPX_data_dump['Open_SPX']\n",
      "SPX_data_dump['jupyter_string'] = SPX_data_dump['High_SPX'] - SPX_data_dump['Low_SPX']\n",
      "SPX_data_dump['jupyter_string'] = SPX_data_dump['Close_VIX'] - SPX_data_dump['Open_VIX']\n",
      "SPX_data_dump['jupyter_string'] = SPX_data_dump['High_VIX'] - SPX_data_dump['Low_VIX']\n",
      "=====\n",
      "SPX_data_dump = SPX_data_dump.sort_index()\n",
      "display(SPX_data_dump.head(n=5))\n",
      "----------------------------------------\n",
      "SPX_data_modified['jupyter_string'] = SPX_data_modified['jupyter_string'] - SPX_data_modified['jupyter_string']\n",
      "SPX_data_modified['jupyter_string'] = SPX_data_modified['jupyter_string'] / SPX_data_modified['jupyter_string']\n",
      "SPX_data_modified['jupyter_string'] = SPX_data_modified['jupyter_string'] / SPX_data_modified['jupyter_string']\n",
      "=====\n",
      "SPX_data_modified.to_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "SPX_data_modified = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "X = SPX_data_modified.iloc[:,0:(cols_new-1)]\n",
      "Y = SPX_data_modified.iloc[:,(cols_new-1)]\n",
      "print('jupyter_string')\n",
      "display(X.head(n=5))\n",
      "print('jupyter_string')\n",
      "display(Y.head(n=5))\n",
      "----------------------------------------\n",
      "X_train.to_csv('jupyter_string')\n",
      "Y_train.to_csv('jupyter_string')\n",
      "X_cv.to_csv('jupyter_string')\n",
      "Y_cv.to_csv('jupyter_string')\n",
      "X_test.to_csv('jupyter_string')\n",
      "Y_test.to_csv('jupyter_string')\n",
      "=====\n",
      "a = pd.concat([Y_test,Y_cv,Y_train])\n",
      "if pd.DataFrame.equals(a,Y):\n",
      "    print('jupyter_string')\n",
      "else:\n",
      "    print('jupyter_string')\n",
      "\n",
      "b = pd.concat([X_test,X_cv,X_train])\n",
      "if pd.DataFrame.equals(b,X):\n",
      "    print('jupyter_string')\n",
      "else:\n",
      "    print('jupyter_string')\n",
      "----------------------------------------\n",
      "start = datetime.strptime('jupyter_string', 'jupyter_string')\n",
      "end = datetime.strptime('jupyter_string', 'jupyter_string')\n",
      "\n",
      "start_date = start.strftime('jupyter_string')\n",
      "end_date = end.strftime('jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "\n",
      "start_date = datetime.strptime(start_date, 'jupyter_string')\n",
      "end_date = datetime.strptime(end_date, 'jupyter_string')\n",
      "=====\n",
      "from sklearn.metrics import f1_score, accuracy_score\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
      "\n",
      "benchmark_Y_pred_train = np.empty(Y_train.shape[0])\n",
      "benchmark_Y_pred_train.fill(1)\n",
      "cashflows_train = (X_train.iloc[0,0],X_train.iloc[0,int(close_index[0])]),\\\n",
      "                  (X_train.iloc[X_train.shape[0]-1,0],-X_train.iloc[X_train.shape[0]-1,int(open_index[0])])\n",
      "print('jupyter_string'.\\\n",
      "      format(accuracy_score(Y_train, benchmark_Y_pred_train)))\n",
      "print('jupyter_string'.format(f1_score(Y_train, benchmark_Y_pred_train)))\n",
      "print('jupyter_string'.format(100*xirr(cashflows_train)))\n",
      "\n",
      "benchmark_Y_pred_cv = np.empty(Y_cv.shape[0])\n",
      "benchmark_Y_pred_cv.fill(1)\n",
      "cashflows_cv = (X_cv.iloc[0,0],X_cv.iloc[0,int(close_index[0])]),\\\n",
      "               (X_cv.iloc[X_cv.shape[0]-1,0],-X_cv.iloc[X_cv.shape[0]-1,int(open_index[0])])\n",
      "print('jupyter_string'.\\\n",
      "      format(accuracy_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print('jupyter_string'.format(f1_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print('jupyter_string'.format(100*xirr(cashflows_cv)))\n",
      "\n",
      "benchmark_Y_pred_test = np.empty(Y_test.shape[0])\n",
      "benchmark_Y_pred_test.fill(1)\n",
      "cashflows_test = (X_test.iloc[0,0],X_test.iloc[0,int(close_index[0])]),\\\n",
      "                 (X_test.iloc[X_test.shape[0]-1,0],-X_test.iloc[X_test.shape[0]-1,int(open_index[0])])\n",
      "print('jupyter_string'.format(accuracy_score(Y_test, benchmark_Y_pred_test)))\n",
      "print('jupyter_string'.format(f1_score(Y_test, benchmark_Y_pred_test)))\n",
      "print('jupyter_string'.format(100*xirr(cashflows_test)))\n",
      "----------------------------------------\n",
      "g = grp.mean()\n",
      "g.head()\n",
      "=====\n",
      "grp.agg(['jupyter_string', 'jupyter_string'])\n",
      "----------------------------------------\n",
      "best_rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
      "print('jupyter_string', best_rf_F1)\n",
      "=====\n",
      "best_rf_feature_importance = pd.Series(best_rf.feature_importances_, index=\\\n",
      "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
      "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
      "print(best_rf_feature_importance.sort_values(ascending=False))\n",
      "----------------------------------------\n",
      "best_xgb_Y_test_pred = best_xgb.predict(X_test.iloc[:,engineered_features_start_index:X_test.shape[1]])\n",
      "best_xgb_F1 = f1_score(Y_test, best_xgb_Y_test_pred)\n",
      "=====\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX')\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX')\n",
      "date_index = np.where(SPX_data_modified.columns.values == 'Date')\n",
      "\n",
      "def convert_label_to_cash_sign(label):\n",
      "    if label == 1:\n",
      "        cash_sign = -1\n",
      "    else:\n",
      "        cash_sign = 1\n",
      "    \n",
      "    return(cash_sign)\n",
      "\n",
      "def get_cashflows(best_clf, X):\n",
      "    Y_pred = best_clf.predict(X.iloc[:,engineered_features_start_index:X.shape[1]])\n",
      "    \n",
      "    \n",
      "    \n",
      "    length = len(Y_pred)\n",
      "    prev_cash_sign = convert_label_to_cash_sign(Y_pred[length-1])\n",
      "    net_position = -prev_cash_sign\n",
      "    cashflows = ((X.iloc[length-1, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                  X.iloc[length-1, int(open_index[0])]*prev_cash_sign),)\n",
      "\n",
      "    for i in np.arange(length-2, 1, -1):\n",
      "        cur_cash_sign = convert_label_to_cash_sign(Y_pred[i])\n",
      "        if cur_cash_sign != prev_cash_sign or net_position == 0:\n",
      "            net_position = net_position - cur_cash_sign\n",
      "            cashflows = (cashflows) + ((X.iloc[i, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                                         X.iloc[i, int(open_index[0])]*cur_cash_sign),)\n",
      "        prev_cash_sign = cur_cash_sign\n",
      "\n",
      "    cur_cash_sign = convert_label_to_cash_sign(Y_pred[0])\n",
      "    \n",
      "    \n",
      "    \n",
      "    if net_position == 0:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),) + \\\n",
      "                                ((X.iloc[0, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*cur_cash_sign),)\n",
      "    elif cur_cash_sign != prev_cash_sign:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),)\n",
      "        net_position = net_position - cur_cash_sign\n",
      "    else:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime('jupyter_string'),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*prev_cash_sign),)\n",
      "        net_position = net_position + prev_cash_sign\n",
      "    \n",
      "    return(cashflows)\n",
      "\n",
      "best_dt_cv_cashflows = get_cashflows(best_dt, X_cv)\n",
      "best_svm_cv_cashflows = get_cashflows(best_svm, X_cv)\n",
      "best_rf_cv_cashflows = get_cashflows(best_rf, X_cv)\n",
      "best_xgb_cv_cashflows = get_cashflows(best_xgb, X_cv)\n",
      "----------------------------------------\n",
      "sns.distplot(F1_list)\n",
      "sns.distplot(XIRR_list)\n",
      "plt.legend(['jupyter_string','jupyter_string'])\n",
      "plt.show()\n",
      "=====\n",
      "import plotly\n",
      "plotly.tools.set_credentials_file(username='jupyter_string', api_key='jupyter_string')\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.tools import FigureFactory as FF\n",
      "import scipy\n",
      "\n",
      "x = F1_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color='jupyter_string'))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title='jupyter_string'\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename='jupyter_string')\n",
      "----------------------------------------\n",
      "grp.agg(['jupyter_string', 'jupyter_string'])\n",
      "=====\n",
      "merged.pivot(index='jupyter_string', columns='type', values='jupyter_string').head()\n",
      "----------------------------------------\n",
      "df = pandas.read_csv('jupyter_string')\n",
      "=====\n",
      "df = pd.DataFrame()\n",
      "----------------------------------------\n",
      "times[['tripminutes', 'tripduration']].mean()\n",
      "=====\n",
      "import numpy as np\n",
      "np.exp(data['jupyter_string'])\n",
      "----------------------------------------\n",
      "pd.value_counts(data['birthyear'])\n",
      "=====\n",
      "pd.value_counts(2015 - data['birthyear']).sort_index()\n",
      "----------------------------------------\n",
      "pd.value_counts(times.dayofweek).sort_index()\n",
      "=====\n",
      "pd.value_counts(times.dayofweek, sort=False)\n",
      "----------------------------------------\n",
      "data.tripduration.plot.hist()\n",
      "=====\n",
      "data['jupyter_string'].plot.hist(bins=100)\n",
      "----------------------------------------\n",
      "neigh = NearestNeighbors(n_neighbors=3)\n",
      "neigh.fit(X)\n",
      "distances, indices = neigh.kneighbors(X)\n",
      "=====\n",
      "num_neighbors= 3\n",
      "\n",
      "\n",
      "input=[2.6,1.7]\n",
      "\n",
      "\n",
      "sns.lmplot(x='jupyter_string',y='jupyter_string',data=X,fit_reg=False)\n",
      "plt.plot(input[0],input[1],'jupyter_string')\n",
      "----------------------------------------\n",
      "user1='jupyter_string'\n",
      "user2='jupyter_string'\n",
      "print('jupyter_string', pearson_score(ratingsdf,user1,user2))\n",
      "=====\n",
      "ratingsdf = pd.read_json('jupyter_string')\n",
      "----------------------------------------\n",
      "similar_users = find_similar_users(ratings,'jupyter_string',3)\n",
      "similar_users\n",
      "=====\n",
      "user='jupyter_string'\n",
      "scores = find_similar_users(ratingsdf,user,3)\n",
      "print('jupyter_string', user)\n",
      "scores.head()\n",
      "----------------------------------------\n",
      "titanic.groupby('Sex')['Survived'].mean()\n",
      "=====\n",
      "st.norm.sf(abs(z))\n",
      "----------------------------------------\n",
      "newDF = pd_data[['Age','SibSp','Parch','Fare']].dropna().copy()\n",
      "\n",
      "sns.pairplot(newDF, vars=['Age','SibSp','Parch','Fare'], hue='Survived', palette='jupyter_string')\n",
      "=====\n",
      "pd_data.sample(n=5)\n",
      "----------------------------------------\n",
      "weather.sort_values(by='wind_gust', ascending=False).head()\n",
      "=====\n",
      "weather[(weather['month']==2) & (weather['day']==12) & \n",
      "(weather['year']==2013)].sort_values(by=['wind_speed'], ascending=False).head(5)\n",
      "----------------------------------------\n",
      "df.temperature.describe()\n",
      "=====\n",
      "def ecdf(data):\n",
      "    x = np.sort(data)\n",
      "    y = 1.* np.arange(1,len(data)+1) / (len(data))\n",
      "    return x, y\n",
      "\n",
      "temp = np.array(df['temperature'])\n",
      "mu = np.mean(temp)\n",
      "sigma = np.std(temp)\n",
      "bs_samples = np.random.normal(mu,sigma,10000)\n",
      "\n",
      "x_theo, y_theo = ecdf(bs_samples)\n",
      "x ,y = ecdf(temp)\n",
      "\n",
      "\n",
      "_ = plt.plot(x_theo, y_theo)\n",
      "_ = plt.plot(x, y, marker='jupyter_string', linestyle='jupyter_string')\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel('jupyter_string')\n",
      "_ = plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "dc_airbnb.info()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "np.random.seed(1)\n",
      "\n",
      "dc_listings = pd.read_csv('jupyter_string')\n",
      "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
      "stripped_commas = dc_listings['price'].str.replace('jupyter_string', 'jupyter_string')\n",
      "stripped_dollars = stripped_commas.str.replace('jupyter_string', 'jupyter_string')\n",
      "dc_listings['price'] = stripped_dollars.astype('jupyter_string')\n",
      "----------------------------------------\n",
      "dc_listings.info()\n",
      "=====\n",
      "dc_listings.head(1)\n",
      "----------------------------------------\n",
      "vote['jupyter_string'] = vote['jupyter_string'] - vote['jupyter_string']\n",
      "vote['jupyter_string'] = vote['jupyter_string'] - vote['jupyter_string']\n",
      "=====\n",
      "vote['jupyter_string'] = vote.Gen1 / (vote.Gen1 + vote.Gen2)\n",
      "vote['jupyter_string'] = vote.Pri1 / (vote.Pri1 + vote.Pri2)\n",
      "vote['jupyter_string'] = vote.GenShare - vote.PriShare\n",
      "vote['jupyter_string'] = vote.DIME1 - vote.DIME2\n",
      "vote['jupyter_string'] = vote.PriOpp / vote.PriTotal * 100\n",
      "vote['jupyter_string'] = vote.OppShare * vote.DIMEChange\n",
      "vote.head(5)\n",
      "----------------------------------------\n",
      "titanic_data = titanic_data.dropna(subset=['Embarked'])\n",
      "=====\n",
      "titanic_data = titanic_data.dropna()\n",
      "----------------------------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data = titanic_data.drop('PassengerId', axis=1)\n",
      "----------------------------------------\n",
      "titanic_data['Survived'].value_counts()\n",
      "=====\n",
      "bins = np.linspace(0, 80, 9)\n",
      "plt.hist(titanic_data['Age'],bins, alpha=0.5)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "titanic_data['Embarked'].value_counts()\n",
      "=====\n",
      "data_by_class = titanic_data.groupby('Pclass')\n",
      "----------------------------------------\n",
      "data_by_survived = titanic_data.groupby('Survived')\n",
      "=====\n",
      "import seaborn as sns\n",
      "data_male = titanic_data.loc[lambda df: df.Sex == 'jupyter_string', :]\n",
      "data_female = titanic_data.loc[lambda df: df.Sex == 'jupyter_string', :]\n",
      "\n",
      "\n",
      "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
      "data_male['Survived'].value_counts().plot(kind= 'jupyter_string', \\\n",
      "                                          title='jupyter_string', ax=axes[0], \\\n",
      "                                          figsize=(8, 4),autopct='jupyter_string', \\\n",
      "                                          fontsize=10, \\\n",
      "                                          colors = ['jupyter_string','jupyter_string'],\\\n",
      "                                          legend=True)\n",
      "\n",
      "data_female['Survived'].value_counts().plot(kind= 'jupyter_string', \\\n",
      "                                          title='jupyter_string', ax=axes[1], \\\n",
      "                                          figsize=(8, 4),autopct='jupyter_string', \\\n",
      "                                          fontsize=10, \\\n",
      "                                          colors = ['jupyter_string','jupyter_string'],\\\n",
      "                                          legend='jupyter_string')\n",
      "plt.show()\n",
      "\n",
      "'''jupyter_string'''\n",
      "----------------------------------------\n",
      "sns.factorplot(x=\"Age_group\", y=\"Survived\", hue=\"Sex\", data=data_by_class, kind='jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "def group_by_age(i):\n",
      "    return str(int(i/10)*10)+'jupyter_string'+str(int(i/10)*10+10)\n",
      "titanic_data['jupyter_string'] = titanic_data['Age'].apply(group_by_age)\n",
      "----------------------------------------\n",
      "sns.factorplot(x='jupyter_string', y=\"Survived\", hue=\"Sex\", data=titanic_data, kind='jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "data_by_age = titanic_data.groupby(['Pclass','jupyter_string'],as_index=False)[['Survived']].mean()\n",
      "data_by_age\n",
      "----------------------------------------\n",
      "sns.factorplot(x=\"Sex\", y=\"Survived\", hue='jupyter_string', data=data_by_age, kind='jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "sib_count_survived_female = survived_female['SibSp'].value_counts()\n",
      "sib_count_no_survived_female = non_survived_female['SibSp'].value_counts()\n",
      "----------------------------------------\n",
      "sns.distplot(leaf_data.average-contrast)\n",
      "=====\n",
      "leaf_data.average_contrast.plot(kind = 'jupyter_string')\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "leaf_data[['eccentricity', 'smoothness']].corr()\n",
      "=====\n",
      "leaf_data.smoothness.corr(leaf_data.eccentricity)\n",
      "----------------------------------------\n",
      "df_mur.head()\n",
      "=====\n",
      "df_mur=df_mur[[\"Country/Territory\",\"Number of homicides by firearm\",\"Average total all civilian firearms\"]]\n",
      "df_ms\n",
      "df_pop=df_pop[['jupyter_string','jupyter_string']]\n",
      "df_gdp=df_gdp[['jupyter_string','jupyter_string']]\n",
      "\n",
      "----------------------------------------\n",
      "model2.summary()\n",
      "=====\n",
      "plt.scatter('jupyter_string', 'jupyter_string', data = df)\n",
      "\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string', weight='jupyter_string', fontsize=14);\n",
      "----------------------------------------\n",
      "vote.loc[vote.Party == 'jupyter_string', 'Party'] = 'jupyter_string'\n",
      "vote.loc[vote.Party == 'jupyter_string', 'Party'] = 'jupyter_string'\n",
      "vote.loc[vote.Party == 'jupyter_string', 'Party'] = 'jupyter_string'\n",
      "=====\n",
      "wooutlie = vote.loc[(vote.VoteChange < .15) | (vote.DIMEChange > -.5)]\n",
      "model_nooutlie = sm.OLS(wooutlie.VoteChange, sm.add_constant(wooutlie.DIMEChange)).fit()\n",
      "model_nooutlie.summary()\n",
      "----------------------------------------\n",
      "train_set = train_set[train_set['capital_gain'] != 'jupyter_string']\n",
      "train_set = train_set[train_set['capital_loss'] != 'jupyter_string']\n",
      "=====\n",
      "train_set=train_set.replace(regex='jupyter_string',value=pd.np.nan).dropna(how='jupyter_string')\n",
      "train_set.head()\n",
      "----------------------------------------\n",
      "train_set['hours_per_week'] = train_set['hours_per_week'].astype('jupyter_string')\n",
      "=====\n",
      "sns.kdeplot(train_set.hours_per_week,shade=True)\n",
      "train_set.hours_per_week.describe()\n",
      "----------------------------------------\n",
      "train_set.hours_per_week_group.replace([1,2,3,4,5],[1,2,3,4,5],inplace=True)\n",
      "train_set.hours_per_week_group.replace([6,7],[1,2],inplace=True)\n",
      "train_set.hours_per_week_group.replace([8,9],[1,2],inplace=True)\n",
      "train_set.hours_per_week_group.replace([10,11],[1,2],inplace=True)\n",
      "train_set.hours_per_week_group.replace([12,13],[1,2],inplace=True)\n",
      "train_set.hours_per_week_group.replace([14,15],[1,2],inplace=True)\n",
      "train_set.hours_per_week_group.replace([16,17],[1,2],inplace=True)\n",
      "=====\n",
      "train_set.hours_per_week=train_set.hours_per_week.astype(int)\n",
      "train_set.loc[train_set.hours_per_week < 40,'jupyter_string'] = 'jupyter_string'\n",
      "train_set.loc[(train_set.hours_per_week >= 40) & (train_set.hours_per_week <= 45),'jupyter_string'] = 'jupyter_string'\n",
      "train_set.loc[(train_set.hours_per_week > 45) & (train_set.hours_per_week <= 60),'jupyter_string'] = 'jupyter_string'\n",
      "train_set.loc[(train_set.hours_per_week > 60) & (train_set.hours_per_week <= 80),'jupyter_string'] = 'jupyter_string'\n",
      "train_set.loc[train_set.hours_per_week > 80,'jupyter_string'] = 'jupyter_string'\n",
      "----------------------------------------\n",
      "train_set.hours_per_week.describe()\n",
      "=====\n",
      "train_set['jupyter_string'].head()\n",
      "----------------------------------------\n",
      "test_set.columns = ['native_region', 'capital_loss', 'wage_class', 'hours_per_week_group', 'capital_gain']\n",
      "=====\n",
      "train_set.native_region.unique()\n",
      "\n",
      "----------------------------------------\n",
      "train_set[['capital_gain', 'capital_loss']].describe()\n",
      "=====\n",
      "sns.kdeplot(train_set.capital_gain,shade=True)\n",
      "train_set.capital_gain.describe()\n",
      "----------------------------------------\n",
      "train_set = pd.concat([train_set, workclass, education, hours_per_week_group, marital_status, occupation, relationship, race, sex, native_region, wage_class], axis=1)\n",
      "=====\n",
      "X_train = pd.DataFrame({'jupyter_string':train_set.age,'jupyter_string':workclass[0],'jupyter_string':train_set.fnlwgt,'jupyter_string':education[0],'jupyter_string':hours_per_week_group[0],'jupyter_string':marital_status[0],'jupyter_string':occupation[0],'jupyter_string':relationship[0],'jupyter_string':race[0],'jupyter_string':sex[0],'jupyter_string':train_set.capital_gain,'jupyter_string':train_set.capital_loss,'jupyter_string':hours_per_week_group[0],'jupyter_string':native_region[0]})\n",
      "Y_train = wage_class[0]\n",
      "X_train.head()\n",
      "----------------------------------------\n",
      "test_set.columns = ['native_region', 'capital_loss', 'wage_class', 'hours_per_week_group', 'capital_gain']\n",
      "workclass = pd.factorize(test_set.workclass)\n",
      "education = pd.factorize(test_set.education)\n",
      "hours_per_week_group = pd.factorize(test_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(test_set.marital_status)\n",
      "occupation = pd.factorize(test_set.occupation)\n",
      "relationship = pd.factorize(test_set.relationship)\n",
      "race = pd.factorize(test_set.race)\n",
      "sex = pd.factorize(test_set.sex)\n",
      "native_region = pd.factorize(test_set.native_region) \n",
      "wage_class = pd.factorize(test_set.wage_class)\n",
      "=====\n",
      "test_set=test_set.replace(regex='jupyter_string',value=pd.np.nan).dropna(how='jupyter_string') \n",
      "test_set.head()\n",
      "----------------------------------------\n",
      "test_set['hours_per_week'].describe()\n",
      "=====\n",
      "test_set.hours_per_week=test_set.hours_per_week.astype(int)\n",
      "test_set.loc[test_set.hours_per_week < 40,'jupyter_string'] = 'jupyter_string'\n",
      "test_set.loc[(test_set.hours_per_week >= 40) & (test_set.hours_per_week <= 45),'jupyter_string'] = 'jupyter_string'\n",
      "test_set.loc[(test_set.hours_per_week > 45) & (test_set.hours_per_week <= 60),'jupyter_string'] = 'jupyter_string'\n",
      "test_set.loc[(test_set.hours_per_week > 60) & (test_set.hours_per_week <= 80),'jupyter_string'] = 'jupyter_string'\n",
      "test_set.loc[test_set.hours_per_week > 80,'jupyter_string'] = 'jupyter_string'\n",
      "----------------------------------------\n",
      "test_set['jupyter_string'] = test_set['capital_gain'] / test_set['capital_loss']\n",
      "=====\n",
      "capital_gain_mean = test_set.capital_gain.mean()\n",
      "capital_loss_mean = test_set.capital_loss.mean()\n",
      "----------------------------------------\n",
      "train_set['jupyter_string'] = train_set['jupyter_string'].astype('jupyter_string')\n",
      "train_set['jupyter_string'] = train_set['jupyter_string'].astype('jupyter_string')\n",
      "=====\n",
      "workclass = pd.factorize(test_set.workclass)\n",
      "education = pd.factorize(test_set.education)\n",
      "hours_per_week_group = pd.factorize(test_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(test_set.marital_status)\n",
      "occupation = pd.factorize(test_set.occupation)\n",
      "relationship = pd.factorize(test_set.relationship)\n",
      "race = pd.factorize(test_set.race)\n",
      "sex = pd.factorize(test_set.sex)\n",
      "native_region = pd.factorize(test_set.native_region) \n",
      "wage_class = pd.factorize(test_set.wage_class)\n",
      "----------------------------------------\n",
      "model = xgb.XGBClassifier(max_depth=5, min_child_weight=2)\n",
      "model.fit(X_train, Y_train)\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.8,\n",
      " colsample_bytree=0.8,\n",
      " objective= 'jupyter_string',\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()['jupyter_string'], nfold=5,\n",
      "            metrics='jupyter_string', early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print ('jupyter_string' % accuracy_score(Y_test, Y_pred))\n",
      "print ('jupyter_string' % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind='jupyter_string', title='jupyter_string')\n",
      "pt.ylabel('jupyter_string')\n",
      "pt.show()\n",
      "----------------------------------------\n",
      "age_median = df['age'].median()\n",
      "age_information_gain = 0\n",
      "\n",
      "for i in range(len(df['age'])):\n",
      "    if df['age'][i] < age_median:\n",
      "        age_left = df['age'][i]\n",
      "    else:\n",
      "        age_left = df['age'][i]\n",
      "        \n",
      "    if df['age'][i] > age_median:\n",
      "        age_right = df['age'][i]\n",
      "    else:\n",
      "        age_right = df['age'][i]\n",
      "        \n",
      "    if df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "    elif df['income'][i] == 'jupyter_string':\n",
      "        age_information_gain += 1\n",
      "=====\n",
      "import numpy\n",
      "\n",
      "def calc_entropy(column):\n",
      "    \"\"'jupyter_string'\"\"\n",
      "    \n",
      "    counts = numpy.bincount(column)\n",
      "    \n",
      "    \n",
      "    probabilities = counts / float(len(column))\n",
      "    \n",
      "    \n",
      "    entropy = 0\n",
      "    \n",
      "    \n",
      "    for prob in probabilities:\n",
      "        if prob > 0:\n",
      "            entropy += prob * math.log(prob, 2)\n",
      "    \n",
      "    return -entropy\n",
      "\n",
      "\n",
      "entropy = calc_entropy([1,1,0,0,1])\n",
      "print('jupyter_string', entropy)\n",
      "\n",
      "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
      "print('jupyter_string', information_gain)\n",
      "\n",
      "income_entropy = calc_entropy(income[\"high_income\"])\n",
      "\n",
      "median_age = income[\"age\"].median()\n",
      "\n",
      "left_split = income[income[\"age\"] <= median_age]\n",
      "right_split = income[income[\"age\"] > median_age]\n",
      "\n",
      "age_information_gain = income_entropy - ((float(left_split.shape[0]) / income.shape[0]) * calc_entropy(left_split[\"high_income\"]) + ((float(right_split.shape[0]) / income.shape[0]) * calc_entropy(right_split[\"high_income\"])))\n",
      "print('jupyter_string', age_information_gain)\n",
      "----------------------------------------\n",
      "private_incomes = data[data['workclass'] == 'jupyter_string']\n",
      "public_incomes = data[data['workclass'] != 'jupyter_string']\n",
      "=====\n",
      "private_incomes = income[income[\"workclass\"] == 4]\n",
      "public_incomes = income[income[\"workclass\"] != 4]\n",
      "\n",
      "print('jupyter_string', private_incomes.shape)\n",
      "print('jupyter_string', public_incomes.shape)\n",
      "----------------------------------------\n",
      "income = pd.read_csv('jupyter_string')\n",
      "income.head()\n",
      "=====\n",
      "import pandas\n",
      "\n",
      "\n",
      "income = pandas.read_csv('jupyter_string', index_col=False)\n",
      "income.head(5)\n",
      "----------------------------------------\n",
      "sample_mean = df.temperature.mean()\n",
      "sample_mean\n",
      "=====\n",
      "df['temperature'].mean()\n",
      "----------------------------------------\n",
      "studentInfo.to_csv('jupyter_string')\n",
      "otherInfo.to_csv('jupyter_string')\n",
      "=====\n",
      "studentInfo.info()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import pprint\n",
      "\n",
      "filepath = 'jupyter_string'\n",
      "df = pd.read_csv(filepath)\n",
      "pp = pprint.PrettyPrinter()\n",
      "\n",
      "print('jupyter_string')\n",
      "pp.pprint(df.head(1).T)\n",
      "\n",
      "----------------------------------------\n",
      "sns.distplot(sub_df['Count_3-6_months_late'])\n",
      "plt.show()\n",
      "sns.distplot(sub_df['Count_6-12_months_late'])\n",
      "plt.show()\n",
      "=====\n",
      "num_cols = 2 \n",
      "num_rows = len(numerical_vars) // 2 + 1\n",
      "\n",
      "fig = sub_df.hist(layout = (num_rows,num_cols), \n",
      "                    bins=30,\n",
      "                    figsize=(50,50),\n",
      "                   ylabelsize=50,\n",
      "                   xlabelsize=50)\n",
      "titles = [x.title.set_size(50) for x in fig.ravel()] \n",
      "\n",
      "----------------------------------------\n",
      "corr = df.corr()\n",
      "sns.heatmap(corr)\n",
      "=====\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def corr_matrix_generator(corr,drop_duplicates = True):\n",
      "    if drop_duplicates:    \n",
      "        mask = np.zeros_like(corr, dtype=np.bool)\n",
      "        mask[np.triu_indices_from(mask)] = True\n",
      "        \n",
      "    sns.set_style(style = 'jupyter_string')\n",
      "    f, ax = plt.subplots(figsize=(11, 9))\n",
      "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
      "    if drop_duplicates:\n",
      "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={'jupyter_string': .5})\n",
      "    else:\n",
      "        sns.heatmap(corr, cmap=cmap, \n",
      "                square=True,\n",
      "                linewidth=.5, cbar_kws={'jupyter_string': .5}, ax=ax)\n",
      "        \n",
      "        \n",
      "\n",
      "    \n",
      "corr = sub_df.corr()\n",
      "\n",
      "print('jupyter_string')\n",
      "print(sub_df['renewal'].value_counts())\n",
      "print('jupyter_string')\n",
      "corr_matrix_generator(corr)\n",
      "\n",
      "----------------------------------------\n",
      "sns.set_style(style = 'jupyter_string')\n",
      "f, ax = plt.subplots(figsize=(11, 9))\n",
      "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
      "sns.heatmap(train.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={'jupyter_string': .5})\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import xgboost as xgb\n",
      "from sklearn import metrics,cross_validation\n",
      "from matplotlib import pyplot as plt\n",
      "import itertools\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "test_df = pd.read_csv('jupyter_string')\n",
      "train_df = pd.read_csv('jupyter_string')\n",
      "train_df.drop('renewal',inplace=True,axis=1)\n",
      "assert(list(test_df.columns) ==  list(train_df.columns))\n",
      "\n",
      "train_df['jupyter_string'] = 0\n",
      "test_df['jupyter_string'] = 1\n",
      "\n",
      "combined_df = train_df.append(test_df)\n",
      "combined_df = combined_df.sample(frac=1).reset_index(drop=True) \n",
      "print(len(combined_df))\n",
      "print(combined_df.head(1).T)\n",
      "\n",
      "\n",
      "combined_df = dummy_encode(combined_df)\n",
      "combined_df = combined_df.drop('id',axis=1)\n",
      "----------------------------------------\n",
      "new_df_data = dta.iloc[:,:-1]\n",
      "new_df_labels = dta.iloc[:,-1]\n",
      "=====\n",
      "new_df_data = dta.iloc[:, 0:10]\n",
      "new_df_labels = dta.loc[:, 'fifty_k']\n",
      "new_df_data.head()\n",
      "----------------------------------------\n",
      "star_wars[star_wars.columns[9:15]] = star_wars[star_wars.columns[9:15]].astype(float)\n",
      "=====\n",
      "seen = 'Have you seen any of the 6 films in the Star Wars franchise?'\n",
      "fan = 'Do you consider yourself to be a fan of the Star Wars film franchise?'\n",
      "fan_st = 'Do you consider yourself to be a fan of the Star Trek franchise?'\n",
      "\n",
      "\n",
      "yes_no_map = {\n",
      "    'jupyter_string': True,\n",
      "    'jupyter_string': False\n",
      "}\n",
      "\n",
      "star_wars[seen] = star_wars[seen].map(yes_no_map)\n",
      "star_wars[fan] = star_wars[fan].map(yes_no_map)\n",
      "star_wars[fan_st] = star_wars[fan_st].map(yes_no_map)\n",
      "\n",
      "----------------------------------------\n",
      "males = star_wars[star_wars[\"Gender\"] == 'jupyter_string']\n",
      "females = star_wars[star_wars[\"Gender\"] == 'jupyter_string']\n",
      "=====\n",
      "from textwrap import wrap\n",
      "movies =  ['jupyter_string'.join(wrap(l, 20)) for l in movie_names]\n",
      "males = star_wars[star_wars['Gender'] == 'jupyter_string']\n",
      "females = star_wars[star_wars['Gender'] == 'jupyter_string']\n",
      "\n",
      "ranking_males = males[males.columns[9:15]].mean()\n",
      "ranking_females = females[females.columns[9:15]].mean()\n",
      "seen_males = males[males.columns[3:9]].sum()\n",
      "seen_females = females[females.columns[3:9]].sum()\n",
      "\n",
      "plot_mf = [ranking_males, ranking_females, seen_males, seen_females]\n",
      "titles_mf = ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']\n",
      "\n",
      "def plot_bar(var_ls, tit_ls):\n",
      "    \n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "    for i, each in enumerate(var_ls):\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        ax.bar(movies, each)\n",
      "        ax.tick_params(rotation = 90)\n",
      "        \n",
      "        ax.set_title(tit_ls[i])\n",
      "        for key,spine in ax.spines.items():\n",
      "                spine.set_visible(False)\n",
      "        if i == 0 or i == 1:\n",
      "            ax.get_xaxis().set_visible(False)\n",
      "        if i == 1 or i == 3:\n",
      "            ax.get_yaxis().set_visible(False)\n",
      "\n",
      "plot_bar(plot_mf,titles_mf)\n",
      "----------------------------------------\n",
      "plt.boxplot((outfield,'jupyter_string',des_hit,'jupyter_string'), labels=('jupyter_string','jupyter_string','jupyter_string','jupyter_string'))\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.ylim(0, 0.5)\n",
      "=====\n",
      "x = np.linspace(0.01, 6, num=250)\n",
      "groups = 4\n",
      "samples = outfield.count() + infield.count() + des_hit.count() + catcher.count()\n",
      "df1 = groups - 1\n",
      "df2 = samples - groups\n",
      "y = f.pdf(x, df1, df2)\n",
      "x_right = np.linspace(F_statistic, 6)\n",
      "y_right = f.pdf(x_right, df1, df2)\n",
      "plt.fill_between(x_right, 0, y_right, facecolor='jupyter_string')\n",
      "plt.plot(x, y, 'jupyter_string', lw=1)\n",
      "----------------------------------------\n",
      "df.Brewery.value_counts().head(10)\n",
      "=====\n",
      "df['Location'].describe()\n",
      "----------------------------------------\n",
      "df.ABV_clean.value_counts()\n",
      "=====\n",
      "df['ABV'].str.replace('jupyter_string', 'jupyter_string').head()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "fluTrain=pd.read_csv('jupyter_string',parse_dates=True)\n",
      "----------------------------------------\n",
      "brooklyn = df[df['Location'] == 'jupyter_string']\n",
      "brooklyn\n",
      "=====\n",
      "Brook_beer = df[df.Location == 'jupyter_string']\n",
      "Brook_beer\n",
      "----------------------------------------\n",
      "brooklyn = df[df.Location == 'jupyter_string']\n",
      "brooklyn.Brewery.value_counts()\n",
      "=====\n",
      "Brook_beer['Brewery'].value_counts().head(1)\n",
      "----------------------------------------\n",
      "brooklyn_beer[brooklyn_beer['Brewery'] == 'jupyter_string'].Style.value_counts().head(5)\n",
      "=====\n",
      "Sixpoint = Brook_beer[Brook_beer.Brewery == 'jupyter_string']\n",
      "----------------------------------------\n",
      "Sixpoint.Style.value_counts().head(5)\n",
      "=====\n",
      "Sixpoint['Style'].value_counts().head(5)\n",
      "----------------------------------------\n",
      "Brooklyn_beer[Brooklyn_beer.Brewery == 'jupyter_string'].Brewery.value_counts()\n",
      "=====\n",
      "NY_beer = df[df['Location'].str.contains('jupyter_string', na=False)]\n",
      "NY_beer\n",
      "----------------------------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(20).plot(kind='jupyter_string')\n",
      "=====\n",
      "choice_beer = df[(df['Style'] == 'jupyter_string') | (df['Style'] == 'jupyter_string') | (df['Style'] == 'jupyter_string')]\n",
      "----------------------------------------\n",
      "choice_beer.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(5)\n",
      "=====\n",
      "choice_beer[\"IBUs\"].mean().astype(int)\n",
      "----------------------------------------\n",
      "choice_beer[choice_beer[\"Style\"] == 'jupyter_string'][\"IBUs\"].mean()\n",
      "=====\n",
      "IPA_beer = df[df['Style'].str.contains('jupyter_string', na=False)]\n",
      "----------------------------------------\n",
      "wheat_beer = pd.read_csv('jupyter_string')\n",
      "IPA_beer = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "IPA_beer['jupyter_string'].mean()\n",
      "----------------------------------------\n",
      "_ = plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog.Weight, dflog.Height, c=dflog.Gender)\n",
      "_ = plt.title('jupyter_string')\n",
      "_ = plt.xlabel('Weight')\n",
      "_ = plt.ylabel('Height')\n",
      "=====\n",
      "dflog_dummies = pd.get_dummies(dflog, drop_first = True)\n",
      "plt_gen_color = {0:('jupyter_string','jupyter_string'), 1:('jupyter_string','jupyter_string')}\n",
      "patches = [mpl.patches.Patch(color=color, label=label) for color, label in plt_gen_color.values()]\n",
      "----------------------------------------\n",
      "D.to_csv('jupyter_string')\n",
      "=====\n",
      "D.detected()\n",
      "----------------------------------------\n",
      "plt.plot(p_MgO_PMN, label = 'jupyter_string')\n",
      "plt.plot(p_MgO_AMN, label = 'jupyter_string')\n",
      "plt.plot(p_Pt_PMN, label = 'jupyter_string')\n",
      "plt.plot(p_Au_AMN, label = 'jupyter_string')\n",
      "plt.legend()\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "f, axarr = plt.subplots(2, 2, \\\n",
      "                    figsize=(8,6))\n",
      "ax = [axarr[0,0], axarr[0,1], axarr[1,0], axarr[1,1]]\n",
      "ms = 8; mew = 0.7\n",
      "label = ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']\n",
      "\n",
      "for i in range(4):\n",
      "    ax[i].axhline(y=0, c='jupyter_string', ls='jupyter_string')\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            unp.nominal_values(p_Au_AMN[i]) - unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_AMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Au_AMN[i]),\\\n",
      "            fmt='jupyter_string', mec='jupyter_string', mew=mew, label = 'jupyter_string', \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            unp.nominal_values(p_Pt_PMN[i]) - unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_PMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Pt_PMN[i]), \\\n",
      "            fmt='jupyter_string', mec='jupyter_string', mew=mew, label='jupyter_string', \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].set_xlabel('jupyter_string'); ax[i].set_ylabel('jupyter_string')\n",
      "    l = ax[i].legend(loc=3, numpoints = 1, fontsize = 10)\n",
      "    l.get_frame().set_linewidth(0.5)\n",
      "    plt.tight_layout(pad=0.4)\n",
      "    ax[i].set_ylim(-5.,3.); ax[i].set_xlim(0.,140.)\n",
      "    ax[i].set_xticks(ax[i].get_xticks()[::2]); ax[i].set_yticks(ax[i].get_yticks()[::2])\n",
      "    ax[i].text(0.08, 0.83,label[i], horizontalalignment='jupyter_string',\\\n",
      "            verticalalignment='jupyter_string', transform = ax[i].transAxes,\\\n",
      "              fontsize = 24)\n",
      "\n",
      "plt.savefig('jupyter_string', bbox_inches='jupyter_string', \\\n",
      "                        pad_inches=0.1)\n",
      "----------------------------------------\n",
      "plt.scatter(fitted.fittedvalues, fitted.resid)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "=====\n",
      "fluTrain.corr()\n",
      "----------------------------------------\n",
      "mod1Result.summary()\n",
      "=====\n",
      "from statsmodels.stats.outliers_influence import OLSInfluence\n",
      "influenceResults=OLSInfluence(mod1Result)\n",
      "influenceResults.summary_frame().head()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "df = pd.read_csv('jupyter_string', index_col='jupyter_string', parse_dates = True)\n",
      "df\n",
      "----------------------------------------\n",
      "df.plot()\n",
      "plt.show()\n",
      "=====\n",
      "names = df['jupyter_string']\n",
      "names\n",
      "----------------------------------------\n",
      "scaler = preprocessing.StandardScaler().fit(data)\n",
      "data = scaler.transform(data)\n",
      "=====\n",
      "from sklearn import preprocessing\n",
      "preprocessing.scale(digits.data)\n",
      "----------------------------------------\n",
      "rmse_test = np.sqrt(mean_squared_error(test.ILI, test.predict_test))\n",
      "rmse_test\n",
      "=====\n",
      "SSE=((test.predict_test - test.ILI)**2).sum()\n",
      "nrow,ncol=test.shape\n",
      "RMSE=math.sqrt(SSE/nrow)\n",
      "RMSE\n",
      "----------------------------------------\n",
      "plt.scatter(iris_X_train[:, 0], iris_X_train[:, 1], c=iris_y_train)\n",
      "plt.scatter(iris_X_test[:, 0], iris_X_test[:, 1], c=iris_y_test)\n",
      "plt.show()\n",
      "=====\n",
      "X = np.array([\n",
      "    [ .5],\n",
      "    [1]\n",
      "])\n",
      "\n",
      "y = [0.5, 1]\n",
      "\n",
      "test = np.c_[ 0, 2, 2].T\n",
      "\n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(X, y)\n",
      "\n",
      "plt.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = 0.1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    plt.plot(test, regr.predict(test))\n",
      "    plt.scatter(this_X, y, s=3)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "train = pd.read_csv('jupyter_string')\n",
      "test = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "fluTrain['jupyter_string']=fluTrain.ILI.shift(2)\n",
      "fluTrain.ILI_lag2[0:10]\n",
      "----------------------------------------\n",
      "por = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "por.info()\n",
      "por.isnull().sum()\n",
      "----------------------------------------\n",
      "math.head()\n",
      "=====\n",
      "sns.pairplot(math_gp)\n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "----------------------------------------\n",
      "df_test.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "df_test.head(10)\n",
      "----------------------------------------\n",
      "sns.distplot(df['LoanAmount'])\n",
      "plt.show()\n",
      "=====\n",
      "df['ApplicantIncome'].hist(bins=50)\n",
      "----------------------------------------\n",
      "df['LoanAmount'].hist(bins=50)\n",
      "=====\n",
      "df.boxplot(column='ApplicantIncome')\n",
      "----------------------------------------\n",
      "df.select_dtypes(include=['jupyter_string']).head()\n",
      "=====\n",
      "df['Gender'].value_counts()\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = np.log(df['CoapplicantIncome'])\n",
      "=====\n",
      "df['jupyter_string'] = df['CoapplicantIncome'].apply(lambda x: math.log(x) if x != 0 else x)\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = np.sqrt(df['jupyter_string'])\n",
      "df['jupyter_string'] = np.sqrt(df['jupyter_string'])\n",
      "=====\n",
      "df['jupyter_string'].hist(bins=20)\n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "df['jupyter_string'].hist(bins=20)\n",
      "----------------------------------------\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "=====\n",
      "df['Loan_Amount_Term'].fillna(360.0, inplace=True)\n",
      "----------------------------------------\n",
      "df.apply(lambda x: sum(x.isnull()), axis=0)\n",
      "=====\n",
      "df['Self_Employed'].fillna('jupyter_string', inplace=True)\n",
      "----------------------------------------\n",
      "test['ILI_lag2'] = test['ILI'].shift(1)\n",
      "test['ILI_lag2'].fillna(test['ILI'], inplace=True)\n",
      "test['ILI_lag2'].head()\n",
      "=====\n",
      "test['jupyter_string']=test.ILI.shift(2)\n",
      "test.ILI_lag2.head()\n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = df['Gender'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "df['jupyter_string'] = df['Married'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "df['jupyter_string'] = df['Dependents'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "df['jupyter_string'] = df['Education'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "df['jupyter_string'] = df['Self_Employed'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "df['jupyter_string'] = df['Property_Area'].map({'jupyter_string':0, 'jupyter_string':1}).astype(int)\n",
      "=====\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "var_mod = ['Married', 'Dependents', 'Education', 'Self_Employed','Property_Area']\n",
      "le = LabelEncoder()\n",
      "for i in var_mod:\n",
      "    df[i] = le.fit_transform(df[i].astype(str))\n",
      "    \n",
      "df['Credit_History'].fillna('jupyter_string', inplace=True)\n",
      "----------------------------------------\n",
      "gender_marriege.div(gender_marriege.sum(1).astype(float), axis=0)\n",
      "=====\n",
      "gender_marriege.plot(kind='jupyter_string', stacked=True, color=['jupyter_string', 'jupyter_string'], grid=False)\n",
      "----------------------------------------\n",
      "FluTest['ILI_lag2'].fillna(method='jupyter_string', inplace=True)\n",
      "FluTest['ILI'].fillna(method='jupyter_string', inplace=True)\n",
      "FluTest['Queries'].fillna(method='jupyter_string', inplace=True)\n",
      "=====\n",
      "test.head()\n",
      "----------------------------------------\n",
      "gender_credit = pd.crosstab(df['Gender'], df['Credit_History'])\n",
      "gender_credit.plot(kind='jupyter_string', stacked=True, color=['jupyter_string', 'jupyter_string'], grid=False)\n",
      "=====\n",
      "df_test = pd.read_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "test['Queries'].fillna(method='jupyter_string', inplace=True)\n",
      "test['Queries'].fillna(method='jupyter_string', inplace=True)\n",
      "=====\n",
      "fluTrain.tail()\n",
      "----------------------------------------\n",
      "dfcars.to_csv('jupyter_string')\n",
      "=====\n",
      "dfcars.to_csv('jupyter_string', index=False)\n",
      "----------------------------------------\n",
      "np.mean(dfcars.mpg < 20)\n",
      "=====\n",
      "(dfcars.mpg < 20).mean()\n",
      "----------------------------------------\n",
      "ppd.describe()\n",
      "=====\n",
      "ppd[\"investigative_findings\"].value_counts()\n",
      "----------------------------------------\n",
      "test['jupyter_string']=np.exp(A0+Q0*test.Queries)+Q1*(np.log(test.ILI))\n",
      "=====\n",
      "test.head()\n",
      "RMSE_test=math.sqrt(((test.predict_test-test.ILI)**2).mean())\n",
      "----------------------------------------\n",
      "tcfindings[tcfindings[\"dist_occurrence\"] == 'jupyter_string'][\"po_race\"].value_counts()\n",
      "=====\n",
      "ppd['jupyter_string'] = ppd[[\"po_initials\",\"po_race\",\"po_sex\",\"dist_occurrence\"]].apply(lambda x: 'jupyter_string'.join(x.astype(str)), axis=1)\n",
      "----------------------------------------\n",
      "sns.distplot(df['temperature'])\n",
      "=====\n",
      "sns.distplot(df.temperature, bins=25)\n",
      "----------------------------------------\n",
      "sns.distplot(df.temperature)\n",
      "plt.show()\n",
      "=====\n",
      "print ('jupyter_string').format(df.temperature.size)\n",
      "----------------------------------------\n",
      "sns.distplot(data['steering'])\n",
      "plt.show()\n",
      "=====\n",
      "binwidth = 0.025\n",
      "\n",
      "print (data.steering_angle[0:100])\n",
      "\n",
      "mysteer=data.steering_angle\n",
      "mysteer1 =mysteer[1:]\n",
      "\n",
      "\n",
      "steer_floats = [float(x) for x in mysteer1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "\n",
      "\n",
      "plt.title('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "df2 = pd.DataFrame({'jupyter_string':range(10), 'jupyter_string': np.random.randint(10, 100, size=10)})\n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "df['Temp'].min(), df['jupyter_string'].max()\n",
      "----------------------------------------\n",
      "df.columns = ['Col One', 'Date', 'DateTime', 'Names', 'hour', 'Temp', 'Time', 'IntegerTemp', 'New Column']\n",
      "=====\n",
      "df.rename(columns={'Col One': 'jupyter_string'}, inplace = True)\n",
      "df\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = pd.to_datetime(df['jupyter_string'])\n",
      "=====\n",
      "df['jupyter_string'].head() \n",
      "----------------------------------------\n",
      "df.head()\n",
      "=====\n",
      "pd.to_datetime(df['jupyter_string'], format='jupyter_string')\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = pd.to_datetime(df['jupyter_string'])\n",
      "=====\n",
      "df.set_index('jupyter_string', inplace=True)\n",
      "----------------------------------------\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(titanic.Fare, bins=np.arange(0, 200, 10), alpha=opacity, label='jupyter_string')\n",
      "plt.hist(titanic_female.Fare, bins=np.arange(0, 200, 10), alpha=opacity, label='jupyter_string')\n",
      "plt.legend()\n",
      "plt.title('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "from scipy.stats import ttest_ind\n",
      "fare_from_q = titanic_data[titanic_data.Embarked == 'jupyter_string']\n",
      "fare_from_c = titanic_data[titanic_data.Embarked == 'jupyter_string']\n",
      "t_stat, p_value = ttest_ind(fare_from_q.Fare, fare_from_c.Fare)\n",
      "\n",
      "print('jupyter_string' % (t_stat, p_value))\n",
      "----------------------------------------\n",
      "titanic_data[['Embarked', 'Fare']].groupby('Embarked').describe()\n",
      "=====\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(fare_from_q.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label='jupyter_string')\n",
      "plt.hist(fare_from_c.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label='jupyter_string')\n",
      "plt.legend()\n",
      "plt.title('jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "titanic_data['Survived'].mean()\n",
      "=====\n",
      "survivors = titanic_data[titanic_data.Survived == 1]\n",
      "survivor_prob = (len(survivors) / len(titanic_data))\n",
      "print('jupyter_string' + str(survivor_prob) + 'jupyter_string')\n",
      "----------------------------------------\n",
      "male_survivors = titanic[titanic['Sex'] == 'jupyter_string']['Survived']\n",
      "female_survivors = titanic[titanic['Sex'] == 'jupyter_string']['Survived']\n",
      "=====\n",
      "from scipy.stats import ttest_ind\n",
      "\n",
      "survivors_male = survivors[(survivors.Sex == 'jupyter_string') & (round(survivors.Age,3) != round(age_mean, 3)) ]\n",
      "survivors_female = survivors[(survivors.Sex == 'jupyter_string') & (round(survivors.Age, 3) != round(age_mean, 3))]\n",
      "t_stat, p_value = ttest_ind(survivors_male.Age, survivors_female.Age)\n",
      "\n",
      "print('jupyter_string' % (t_stat, p_value))\n",
      "----------------------------------------\n",
      "cars1.head()\n",
      "=====\n",
      "cars1 = cars1.iloc[:, :-5].copy()\n",
      "----------------------------------------\n",
      "iris.groupby('jupyter_string').describe()\n",
      "=====\n",
      "iris.plot(kind='jupyter_string')\n",
      "----------------------------------------\n",
      "iris.groupby('jupyter_string').describe()\n",
      "=====\n",
      "iris.boxplot(by='jupyter_string')\n",
      "----------------------------------------\n",
      "results = run_calc()\n",
      "results.head()\n",
      "=====\n",
      "fig = plt.figure(figsize=(20,18))\n",
      "for i,l in enumerate(['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']):\n",
      "    ax = fig.add_subplot(3,2,i+1)\n",
      "    sns.pointplot(x='jupyter_string', y=l, hue='jupyter_string', data=results, ax=ax)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "playerAggDfAllNbaAllStar.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "playerAggDfAllNbaAllStarInitFeatures.head()\n",
      "----------------------------------------\n",
      "train['Age'].fillna(train['Age'].dropna().median(), inplace=True)\n",
      "=====\n",
      "sns.set_style('jupyter_string')\n",
      "----------------------------------------\n",
      "train['Age'].fillna(train['Age'].mean(), inplace=True)\n",
      "=====\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Sex\", palette = 'jupyter_string')\n",
      "----------------------------------------\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Sex\", palette = 'jupyter_string')\n",
      "=====\n",
      "train['Age'].plot.hist(bins = 50)\n",
      "----------------------------------------\n",
      "train['Fare'].plot.hist(bins = 50)\n",
      "=====\n",
      "train.info()\n",
      "----------------------------------------\n",
      "train['Embarked'].value_counts()\n",
      "=====\n",
      "sns.countplot(x = \"SibSp\", data = train)\n",
      "----------------------------------------\n",
      "sns.countplot(x = \"Parch\", data = train)\n",
      "=====\n",
      "train[\"Fare\"].hist(bins =40)\n",
      "----------------------------------------\n",
      "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
      "=====\n",
      "sns.boxplot(x = \"Pclass\", y = \"Age\", data = train)\n",
      "----------------------------------------\n",
      "male = df.temperature[df.gender == 'jupyter_string']\n",
      "female = df.temperature[df.gender == 'jupyter_string']\n",
      "=====\n",
      "mdf.var() \n",
      "----------------------------------------\n",
      "train.drop(['Sex', 'Embarked'], axis = 1, inplace = True)\n",
      "=====\n",
      "train = pd.concat([train, sex, embark], axis = 1)\n",
      "----------------------------------------\n",
      "train.head()\n",
      "=====\n",
      "train.head(1)\n",
      "----------------------------------------\n",
      "test = pd.concat([test, sex, embark], axis = 1)\n",
      "test.head(1)\n",
      "=====\n",
      "train.drop(['Sex', 'Embarked', 'Name', 'Ticket'], axis = 1, inplace = True)\n",
      "----------------------------------------\n",
      "sns.heatmap(subset.corr(),annot=True)\n",
      "plt.show()\n",
      "=====\n",
      "cols = ['TotalConsmp','TempOutSide','Press_mm_hg','H_OutSide','Windspeed','Visibility']\n",
      "cm = np.corrcoef(subset.values.T)\n",
      "sns.set(font_scale=1.5)\n",
      "plt.figure(figsize=(10,10))\n",
      "hm = sns.heatmap(cm,\n",
      "                 cbar=True,\n",
      "                 annot=True,\n",
      "                 square=True,\n",
      "                 fmt='jupyter_string',\n",
      "                 annot_kws= {'jupyter_string': 10},\n",
      "                 yticklabels = cols,\n",
      "                 xticklabels=cols\n",
      "                )\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df = df.groupby(['Postal Code','Borough'])['Neighbourhood'].apply(','.join).reset_index()\n",
      "df.head()\n",
      "=====\n",
      "df['jupyter_string'].nunique()\n",
      "----------------------------------------\n",
      "df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "plt.show()\n",
      "corrmat2 = df.corr()\n",
      "\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis='jupyter_string', colors='jupyter_string')\n",
      "ax.tick_params(axis='jupyter_string', colors='jupyter_string')\n",
      "\n",
      "sns.heatmap(corrmat2,linewidths=.1, annot=True, annot_kws={'jupyter_string': 7})\n",
      "\n",
      "----------------------------------------\n",
      "df_add.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "pd.set_option('jupyter_string', -1)\n",
      "def get_url(row):\n",
      "    return 'jupyter_string'.format(row['jupyter_string'])\n",
      "df_add['jupyter_string'] = df_add.apply(get_url, axis=1)\n",
      "df_add\n",
      "----------------------------------------\n",
      "df_add.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "df_add = df_add.apply(get_towing_info, axis=1).join(df)\n",
      "df_add\n",
      "----------------------------------------\n",
      "sns.distplot(df.temperature)\n",
      "plt.show()\n",
      "=====\n",
      "tdf = df.temperature\n",
      "\n",
      "sns.distplot(tdf, fit=norm,kde=False)\n",
      "plt.title('jupyter_string')\n",
      "----------------------------------------\n",
      "df['jupyter_string'] = df['url'].apply(get_business_name)\n",
      "df['jupyter_string'] = df['url'].apply(get_owner_operator)\n",
      "df['jupyter_string'] = df['url'].apply(get_phone_number)\n",
      "df['jupyter_string'] = df['url'].apply(get_license_status)\n",
      "df['jupyter_string'] = df['url'].apply(get_physical_address)\n",
      "=====\n",
      "new_df = df.apply(get_towing_info, axis=1).join(df)\n",
      "new_df\n",
      "----------------------------------------\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_labels, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "def evaluate(X_data, y_data):\n",
      "    num_examples = len(X_data)\n",
      "    total_accuracy = 0\n",
      "    sess = tf.get_default_session()\n",
      "    for offset in range(0, num_examples, batch_size):\n",
      "        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n",
      "        accuracy = sess.run(accuracy_operation, feed_dict={tf_dataset: batch_x, tf_labels: batch_y, keep_prob: 1.0})\n",
      "        total_accuracy += (accuracy * len(batch_x))\n",
      "    return total_accuracy / num_examples\n",
      "=====\n",
      "num_steps = 1000\n",
      "cost_history = np.empty(shape=[1],dtype=float)\n",
      "loss_history = []\n",
      "pred_valid = []\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    print('jupyter_string')\n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "        \n",
      "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
      "        \n",
      "        \n",
      "        \n",
      "        feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels, keep_prob : 0.8}\n",
      "        _, l, predictions = session.run(\n",
      "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "        loss_history.append(l)\n",
      "        if (step % 100 == 0):\n",
      "          print('jupyter_string' % (step, l))\n",
      "          print('jupyter_string' + str(accuracy(predictions, batch_labels).eval()))\n",
      "          print('jupyter_string' +  str(accuracy(valid_prediction.eval(\n",
      "                    feed_dict = {tf_dataset : valid_dataset, keep_prob : 1}), valid_labels).eval()))\n",
      "    \n",
      "    pred_valid.append(valid_prediction.eval(feed_dict = {tf_dataset : valid_dataset, keep_prob : 1}))\n",
      "----------------------------------------\n",
      "get_ci(mdf, 0.05)\n",
      "=====\n",
      "plt.figure(figsize=(8,5))\n",
      "\n",
      "sample_means = [fdf.mean(),mdf.mean()]\n",
      "intervals = [(98.239932599869292, 98.547759707822962),(97.959962049915021, 98.249268719315779)]\n",
      "plt.errorbar(x=np.arange(1,3,1), y=sample_means, yerr=[(top-bot)/2 for top,bot in intervals], fmt='jupyter_string',capthick=3, capsize=10)\n",
      "plt.hlines(xmin=0, xmax=3, y=fdf.mean(), linewidth=2.0, color='jupyter_string', label='jupyter_string')\n",
      "plt.hlines(xmin=0, xmax=3, y=tdf.mean(), linewidth=2.0, color='jupyter_string', label='jupyter_string')\n",
      "plt.hlines(xmin=0, xmax=3, y=mdf.mean(), linewidth=2.0, color='jupyter_string', label='jupyter_string')\n",
      "\n",
      "plt.title('jupyter_string')\n",
      "plt.legend()\n",
      "locs= [0.0,0.5,1.0,1.5,2.0,2.5,3.0]\n",
      "labels= ['jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string']\n",
      "plt.xticks(locs, labels)    \n",
      "plt.ylabel('jupyter_string')           \n",
      "----------------------------------------\n",
      "regressor = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "print( 'jupyter_string', regressor.max_depth, 'jupyter_string', cross_val_score(regressor, X1, Y3, cv=10))\n",
      "=====\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "----------------------------------------\n",
      "countries.head()\n",
      "=====\n",
      "countries.head(3)\n",
      "----------------------------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries['continent'].value_counts()\n",
      "----------------------------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries['fertility'].cumsum()\n",
      "----------------------------------------\n",
      "countries['fertility'].mean()\n",
      "=====\n",
      "countries.groupby('continent')['population'].sum()\n",
      "----------------------------------------\n",
      "train_df['landmark_id'].value_counts().head(100)\n",
      "=====\n",
      "rank_number = 100 \n",
      "sampling_rate = 0.02 \n",
      "random_state = 17 \n",
      "\n",
      "landmarks=train_df.groupby(by='landmark_id').count().loc[:,'id']\n",
      "l = landmarks.sort_values(ascending=False)\n",
      "\n",
      "\n",
      "lmks = pd.concat([l, l/l.sum(), l.cumsum()/l.sum()], axis=1, ignore_index=True)\n",
      "lmks.columns=['jupyter_string', 'jupyter_string', 'jupyter_string']\n",
      "ranked = lmks[0:rank_number]\n",
      "\n",
      "train_ordered = train_df[train_df.landmark_id.isin(ranked.index)]\n",
      "sample_gby = train_ordered.groupby(by='landmark_id').apply(lambda x: x.sample(frac=sampling_rate, random_state=random_state))\n",
      "sample_idx = sample_gby.index.levels[1]\n",
      "train_sample = train_df.iloc[sample_idx, :]\n",
      "\n",
      "\n",
      "train_sample.to_csv('jupyter_string', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
      "train_sample_df = pd.read_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "train_sample_df['landmark_id'].value_counts()\n",
      "=====\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_train)\n",
      "h.set_title('jupyter_string', fontweight='jupyter_string', fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "prob_correct_number = expected_acc(prob_id)\n",
      "prob_correct_number = np.array(prob_correct_number)\n",
      "prob_correct_number\n",
      "=====\n",
      "import random\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "prob_id = np.array([(y_test == id).sum() / len(y_test) for id in y_test]) \n",
      "seed = [3, 10, 27, 31, 48, 55, 67, 95, 105, 117]\n",
      "\n",
      "expected_val_df = pd.DataFrame(columns=['jupyter_string', 'jupyter_string'])\n",
      "\n",
      "for i in seed:\n",
      "    \n",
      "    random.seed(i)\n",
      "    randsample = random.sample(range(len(y_test)), 20)\n",
      "    \n",
      "    prob_id_montecarlo = prob_id[randsample]  \n",
      "    expected_val_df.loc[len(expected_val_df)] = expected_acc(prob_id_montecarlo)\n",
      "----------------------------------------\n",
      "history = model.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=len(train_generator),\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=len(validation_generator),\n",
      "    epochs=10,\n",
      "    verbose=1\n",
      ")\n",
      "=====\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "epochs = 10\n",
      "\n",
      "checkpointer = ModelCheckpoint(filepath='jupyter_string', \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "hist = model.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "----------------------------------------\n",
      "sns.distplot(fdf)\n",
      "sns.distplot(mdf)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "sns.distplot(fdf, fit=norm,kde=False, label='jupyter_string')\n",
      "sns.distplot(mdf, fit=norm,kde=False, label='jupyter_string')\n",
      "plt.vlines(ymin=0.0, ymax= 0.05,x=mdf.mean(),linewidth=2.0, color='jupyter_string', label='jupyter_string' )\n",
      "plt.vlines(ymin=0.0, ymax= 0.55,x=tdf.mean(),linewidth=2.0, color='jupyter_string', label='jupyter_string' )\n",
      "plt.vlines(ymin=0.0, ymax= 0.05,x=fdf.mean(),linewidth=2.0, color='jupyter_string', label='jupyter_string' )\n",
      "plt.legend()\n",
      "plt.title('jupyter_string')\n",
      "plt.savefig('jupyter_string')\n",
      "----------------------------------------\n",
      "test_datagen = ImageDataGenerator()\n",
      "\n",
      "test_generator = test_datagen.flow(test_tensors, test_target, batch_size= 128)\n",
      "=====\n",
      "model.load_weights('jupyter_string')\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "landmark_prob = [np.amax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print('jupyter_string', test_accuracy)\n",
      "----------------------------------------\n",
      "historyBN = modelBN.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=nb_train_samples // batch_size,\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=nb_validation_samples // batch_size,\n",
      "    epochs=epochs,\n",
      "    verbose=1)\n",
      "=====\n",
      "checkpointer = ModelCheckpoint(filepath='jupyter_string', \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "epochs_batch = 15\n",
      "\n",
      "hist_BN = modelBN.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs_batch, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "----------------------------------------\n",
      "t_critical = scipy.stats.t.ppf(q = 1.0 - 0.05, df = len(tdf) - 1)\n",
      "margin_of_error = t_critical * (tdf.std()/len(tdf)**0.5)\n",
      "confidence_interval = (tdf.mean() - margin_of_error, tdf.mean() + margin_of_error) \n",
      "\n",
      "print(confidence_interval)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "sns.distplot(tdf, fit=norm,kde=False)\n",
      "plt.vlines(ymin=0.0, ymax= 0.55,x=tdf.mean(),linewidth=2.0, color='jupyter_string', label='jupyter_string' )\n",
      "plt.errorbar(x=98.25, y=0.01, xerr=0.106,capthick=2, capsize=5, label='jupyter_string' )\n",
      "plt.legend()\n",
      "plt.title('jupyter_string')\n",
      "----------------------------------------\n",
      "survival = pd.crosstab(df.Survived, df.Sex, df.Pclass)\n",
      "survival\n",
      "=====\n",
      "surv_sex_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                             columns=[titanic[\"Pclass\"],\n",
      "                                      titanic[\"Sex\"]],\n",
      "                             margins=True)   \n",
      "\n",
      "surv_sex_class\n",
      "----------------------------------------\n",
      "X_train.head()\n",
      "=====\n",
      "train_is, test_is = list(problem.get_cv(X_train, y_train))[0]\n",
      "print(len(train_is), len(test_is))\n",
      "----------------------------------------\n",
      "train_X['jupyter_string'] = (train_X['jupyter_string'] - train_X['jupyter_string'].mean()) / train_X['jupyter_string'].std()\n",
      "train_X['jupyter_string'] = (train_X['jupyter_string'] - train_X['jupyter_string'].mean()) / train_X['jupyter_string'].std()\n",
      "=====\n",
      "train_X_anomalies = xr.merge([(train_X[var] - train_X[var].mean(axis=0)) / (train_X[var].std(axis=0)) for var in data_vars])\n",
      "----------------------------------------\n",
      "pca = PCA()\n",
      "pca.fit(X)\n",
      "plt.plot(pca.explained_variance_ratio_)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "coefs = cls.clf.coef_[0]\n",
      "coef_rankings = np.argsort(np.abs(coefs))[::-1]\n",
      "fig, axes = plt.subplots(3, 3, figsize=(16, 9), \n",
      "                         subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=180)))\n",
      "axef = axes.ravel()\n",
      "for c, coef_rank in enumerate(coef_rankings[:9]):\n",
      "    c_var = data_vars[int(np.floor(coef_rank / fe.num_comps))]\n",
      "    c_comp = coef_rank % fe.num_comps\n",
      "    comp_vals = fe.pca[c_var].components_[c_comp]\n",
      "    axef[c].coastlines()\n",
      "    axef[c].contourf(train_X['jupyter_string'] - 180, \n",
      "                     train_X['jupyter_string'], \n",
      "                     fe.pca[c_var].components_[c_comp].reshape(train_X[c_var].shape[1:]),\n",
      "                     np.linspace(-0.04, 0.04, 11), cmap='jupyter_string')\n",
      "    axef[c].set_title('jupyter_string'.format(c_var, c_comp, coefs[coef_rank]))\n",
      "----------------------------------------\n",
      "star_wars = star_wars[star_wars['title'].str.contains('jupyter_string')]\n",
      "toy_story = toy_story[toy_story['title'].str.contains('jupyter_string')]\n",
      "=====\n",
      "df.groupby('jupyter_string')['jupyter_string'].mean()\n",
      "----------------------------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings['jupyter_string'].hist(bins=70)\n",
      "=====\n",
      "sb.jointplot(x='jupyter_string',y='jupyter_string',data=ratings,alpha=0.5)\n",
      "----------------------------------------\n",
      "ratings.sort_values('jupyter_string',ascending=True).head(10)\n",
      "=====\n",
      "starwars_user_rating = filmmat['jupyter_string']\n",
      "toystory_user_rating = filmmat['jupyter_string']\n",
      "\n",
      "starwars_user_rating.head()\n",
      "----------------------------------------\n",
      "train = data.iloc[:-12]\n",
      "test = data.iloc[-12:]\n",
      "=====\n",
      "train_set = milk.head(156)\n",
      "test_set = milk.tail(12)\n",
      "----------------------------------------\n",
      "sns.lmplot(x= 'temperature', y= 'heart_rate', hue='gender',data=df)\n",
      "=====\n",
      "import statsmodels.api as sm\n",
      "Y = df['heart_rate']\n",
      "X = df['temperature']\n",
      "X = sm.add_constant(X)\n",
      "model = sm.OLS(Y,X)\n",
      "results = model.fit()\n",
      "results.params\n",
      "----------------------------------------\n",
      "jnj_2011_df = pd.DataFrame(jnj_2011_dict)\n",
      "jnj_2011_df.head()\n",
      "=====\n",
      "plt.figure(figsize=(12, 14))\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.plot(jnj_2011_dict['date'], jnj_2011_dict['open'], label='jupyter_string')\n",
      "plt.plot(jnj_2011_dict['date'], jnj_2011_dict['high'], label='jupyter_string')\n",
      "plt.plot(jnj_2011_dict['date'], jnj_2011_dict['low'], label='jupyter_string')\n",
      "plt.plot(jnj_2011_dict['date'], jnj_2011_dict['close'], label='jupyter_string')\n",
      "plt.ylim([57, 68])\n",
      "plt.ylabel('jupyter_string', fontsize=14)\n",
      "plt.title('jupyter_string',fontsize=20)\n",
      "plt.yticks(np.arange(57, 69, 1))\n",
      "plt.legend()\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.plot(jnj_2011_dict['date'], jnj_2011['volume'])\n",
      "plt.ylabel('jupyter_string', fontsize=14)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "g = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n",
      "                   data=train, aspect=0.9, size=3.5)\n",
      "=====\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "----------------------------------------\n",
      "train['jupyter_string'] = train[\"Sex\"].map({'jupyter_string':0,'jupyter_string':1}).astype(int)\n",
      "train['jupyter_string'] = train[\"Embarked\"].map({'jupyter_string':0,'jupyter_string':1,'jupyter_string':2}).fillna(0).astype(int)\n",
      "=====\n",
      "mean_age_per_sex_class = train.groupby([\"Sex\",\"Pclass\"]).mean()[\"Age\"]\n",
      "mean_age_per_sex_class.unstack(\"Pclass\").plot.bar()\n",
      "----------------------------------------\n",
      "train[\"Age\"] = train[\"Age\"].fillna(train[\"AgeIsNull\"])\n",
      "=====\n",
      "train['jupyter_string'] = train[\"Age\"]\n",
      "train['jupyter_string'] = pd.isnull(train[\"Age\"]).astype(int)\n",
      "for sex in ['jupyter_string','jupyter_string']:\n",
      "    for pclass in [1,2,3]:\n",
      "        train.loc[train.Age.isnull() & (train.Sex==sex) & (train.Pclass==pclass),'jupyter_string'] = mean_age_per_sex_class[sex][pclass]\n",
      "----------------------------------------\n",
      "test['jupyter_string'] = test[\"Age\"]\n",
      "test['jupyter_string'] = pd.isnull(test[\"Age\"]).astype(int)\n",
      "=====\n",
      "train.describe().round(2)\n",
      "----------------------------------------\n",
      "train.drop(['jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string'],\n",
      "=====\n",
      "text_info_columns = train.dtypes[train.dtypes.map(lambda x: x=='jupyter_string')].index.tolist()\n",
      "train = train.drop(text_info_columns,axis=1)\n",
      "text_info_columns\n",
      "----------------------------------------\n",
      "train.hist()\n",
      "plt.show()\n",
      "=====\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,6));\n",
      "----------------------------------------\n",
      "carmpg = pd.DataFrame(X_filled_mice, columns=carmpg.columns)\n",
      "carmpg.head()\n",
      "=====\n",
      "X_filled_mice = pd.DataFrame(X_filled_mice)\n",
      "X_filled_mice.columns = ['MPG','CYLINDERS','SIZE','HP','WEIGHT','ACCEL','ENG_TYPE']\n",
      "X_filled_mice.insert(0, 'Auto', carmpg['Auto'])\n",
      "\n",
      "X_filled_mice\n",
      "----------------------------------------\n",
      "carmpg = carmpg.dropna()\n",
      "=====\n",
      "carmpg_listwise = carmpg.dropna(axis=0)\n",
      "----------------------------------------\n",
      "X_train = training_set[0:training_set.shape[0],:]\n",
      "y_train = training_set[1:training_set.shape[1],:]\n",
      "=====\n",
      "X_train = training_set[0:1257]\n",
      "y_train = training_set[1:1258]\n",
      "----------------------------------------\n",
      "tests_f = tests_f_max['35 to 44'].values\n",
      "tests_m = tests_m_max['35 to 44'].values\n",
      "diagnoses_f = diagnoses_f_max['35 to 44'].values\n",
      "diagnoses_m = diagnoses_m_max['35 to 44'].values\n",
      "=====\n",
      "import matplotlib\n",
      "matplotlib.use('jupyter_string')\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid.anchored_artists import AnchoredText\n",
      "from matplotlib.patches import Polygon\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "tax = pd.read_csv('jupyter_string',encoding='jupyter_string',usecols=[0,1,2,3,4,5,6])\n",
      "----------------------------------------\n",
      "tax.sort_values(by='Total Income', ascending=False).head(1)\n",
      "=====\n",
      "tax.groupby('Prov/Terr')['Total Income'].sum().sort_values().plot(kind='jupyter_string')\n",
      "----------------------------------------\n",
      "tax[['Total Income', 'Total Tax Filings']].corr()\n",
      "=====\n",
      "tax.plot(kind='jupyter_string',x='Total Income',y='Total')\n",
      "----------------------------------------\n",
      "tax['jupyter_string'] = np.where(tax['Prov/Terr'] == 'jupyter_string', 1, 2)\n",
      "tax['jupyter_string'] = np.where(tax['Prov/Terr'] == 'jupyter_string', 3, 4)\n",
      "tax['jupyter_string'] = np.where(tax['Prov/Terr'] == 'jupyter_string', 5, 6)\n",
      "=====\n",
      "tax['jupyter_string'] = tax['Prov/Terr'].map(lambda code : int(code[0]))\n",
      "----------------------------------------\n",
      "colors = ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']\n",
      "=====\n",
      "tax.plot(kind='jupyter_string',x='Total Income',y='Total',c='jupyter_string',figsize=(15,10),colormap='jupyter_string',s=100,alpha=0.7)\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string', index_col='Date', parse_dates=True)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from datetime import datetime as dt \n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "plt.show()\n",
      "\n",
      "sphist = pd.read_csv('jupyter_string')\n",
      "sphist.shape\n",
      "----------------------------------------\n",
      "sphist.head()\n",
      "=====\n",
      "sphist.head(4)\n",
      "----------------------------------------\n",
      "y_hat = fitted_logreg.predict(predictions_test)\n",
      "\n",
      "print('jupyter_string', accuracy_score(y_test, y_hat))\n",
      "=====\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "for cur_depth in range(1,8):\n",
      "    model = DecisionTreeClassifier(max_depth = cur_depth)\n",
      "    scores = cross_val_score(model, predictions_tune, y_tune, cv=5)\n",
      "    print('jupyter_string'.format(np.mean(scores), np.std(scores)))\n",
      "\n",
      "DecisionTreeClassifier(max_depth=4).fit(predictions_tune, y_tune).score(predictions_test, y_test)\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "df_train = pd.read_csv('jupyter_string', index_col=0)\n",
      "df_test = pd.read_csv('jupyter_string', index_col=0)\n",
      "----------------------------------------\n",
      "plt.plot(x, estgb.predict(x.reshape(-1,1)), label='jupyter_string')\n",
      "plt.plot(x, y, label='jupyter_string')\n",
      "plt.legend();\n",
      "=====\n",
      "display_iters = [0, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 400, 500]\n",
      "\n",
      "\n",
      "\n",
      "import time\n",
      "from IPython import display\n",
      "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
      "ax[0].plot(x, y, 'jupyter_string');\n",
      "ax[0].set_color_cycle([plt.cm.viridis(i) for i in np.linspace(0, 1, len(display_iters))])\n",
      "sleep_time = 2\n",
      "\n",
      "\n",
      "overall_predictions = list(estgb.staged_predict(x.reshape(-1,1)))\n",
      "overall_predictions = [np.mean(y)*np.ones_like(x)] + overall_predictions\n",
      "\n",
      "\n",
      "for i in display_iters:\n",
      "    \n",
      "    \n",
      "    cur_overall_prediction = overall_predictions[i]\n",
      "    ax[0].plot(x, cur_overall_prediction, alpha=0.7, label=str(i), lw=2)\n",
      "    ax[0].legend()\n",
      "    \n",
      "    \n",
      "    resid = y - cur_overall_prediction\n",
      "    ax[1].cla()\n",
      "    ax[1].scatter(x,resid, label='jupyter_string')\n",
      "    ax[1].axhline(0)\n",
      "    \n",
      "    \n",
      "    if i <=5:\n",
      "        cur_est = estgb.estimators_[i,0]\n",
      "        cur_prediction = cur_est.predict(x.reshape(-1,1))\n",
      "        ax[1].plot(x, cur_prediction, color='jupyter_string', label='jupyter_string')\n",
      "    else:\n",
      "        \n",
      "        sleep_time = sleep_time/2\n",
      "    ax[1].legend()\n",
      "    \n",
      "    \n",
      "    display.display(fig)\n",
      "    display.clear_output(wait=True)\n",
      "    time.sleep(sleep_time)\n",
      "----------------------------------------\n",
      "models = np.load('jupyter_string', encoding = 'jupyter_string')\n",
      "models = np.load('jupyter_string', encoding = 'jupyter_string')\n",
      "models = np.load('jupyter_string', encoding = 'jupyter_string')\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegressionCV\n",
      "\n",
      "LR_score = LogisticRegressionCV().fit(x_train, y_train).score(x_test,y_test)\n",
      "\n",
      "scores = []\n",
      "for cur_model in models:\n",
      "    scores.append(cur_model.score(x_test,y_test))\n",
      "    \n",
      "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
      "ax.hist(scores,20, label='jupyter_string');\n",
      "\n",
      "\n",
      "ax.axvline(LR_score, color='jupyter_string',label='jupyter_string')\n",
      "ax.set_xlabel('jupyter_string', fontsize=24) \n",
      "ax.set_ylabel('jupyter_string')\n",
      "ax.legend(loc='jupyter_string', fontsize=24)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "model.compile(loss='jupyter_string', optimizer='jupyter_string', metrics=['jupyter_string'])\n",
      "=====\n",
      "from keras.datasets import mnist\n",
      "from keras.utils import to_categorical\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "\n",
      "\n",
      "y_train_cat = to_categorical(y_train, num_classes=10)\n",
      "y_test_cat  = to_categorical(y_test, num_classes=10)\n",
      "\n",
      "\n",
      "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
      "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
      "\n",
      "\n",
      "scaler = MinMaxScaler().fit(x_train_flat)\n",
      "x_train_scaled = scaler.transform(x_train_flat)\n",
      "x_test_scaled = scaler.transform(x_test_flat)\n",
      "\n",
      "----------------------------------------\n",
      "model.fit(x_train_scaled, y_train_cat, epochs=5, batch_size=32, validation_split = .2)\n",
      "=====\n",
      "model = Sequential([\n",
      "    Dense(500, input_shape=(784,), activation='jupyter_string'),\n",
      "    Dense(100, activation='jupyter_string'),\n",
      "    Dense(50, activation='jupyter_string'),\n",
      "    Dense(10, activation='jupyter_string')\n",
      "])\n",
      "\n",
      "model.compile(loss='jupyter_string', optimizer='jupyter_string', metrics=['jupyter_string'])\n",
      "\n",
      "\n",
      "model.summary()\n",
      "----------------------------------------\n",
      "angle_num.pop(0)\n",
      "angle_num.pop(0)\n",
      "angle_num.pop(0)\n",
      "angle_num.pop(0)\n",
      "=====\n",
      "labels=[]\n",
      "label_num=[]\n",
      "print(angle_num[0],angle_num[100],angle_num[-100])\n",
      "angle_num[0]=20\n",
      "angle_num[100]=angle_num[-100]=0\n",
      "print(angle_num[100],angle_num[-100])\n",
      "sum=0\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "    if label!=0:\n",
      "        sum+=num\n",
      "    \n",
      "print(sum)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,100,10)\n",
      "plt.xticks(x_label,x_label,ha='jupyter_string',rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "x_train_br = np.array(x_train_br)\n",
      "y_train_br = np.array(y_train_br)\n",
      "=====\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "factors = [10,2]\n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)  \n",
      "    val = np.array(x_train[i])\n",
      "    plt.axis('jupyter_string')\n",
      "    img.set_title(y_train[i])    \n",
      "    plt.imshow(val, cmap='jupyter_string')\n",
      "    \n",
      "----------------------------------------\n",
      "image = cv2.imread('jupyter_string')\n",
      "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
      "image = add_random_shadow(image)\n",
      "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
      "plt.imshow(image)\n",
      "=====\n",
      "for i in range(len(x_train)):\n",
      "    x_train[i] = cv2.resize(np.array(x_train[i]),(200,66),interpolation=cv2.INTER_AREA) \n",
      "\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,10))\n",
      "factors = [4,4]\n",
      "    \n",
      "for ind in range(16):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)\n",
      "    val = np.array(x_train[i])\n",
      "    img.set_title(y_train[i])\n",
      "    plt.axis('jupyter_string')\n",
      "    plt.imshow(val, cmap='jupyter_string')\n",
      "----------------------------------------\n",
      "df_clean['a'] = np.log(df_clean['a'])\n",
      "df_clean['d'] = np.log(df_clean['d'])\n",
      "=====\n",
      "plist=list('jupyter_string')\n",
      "logdf=np.log(df_clean[plist])\n",
      "logdf['day']=df_clean['day'].copy()\n",
      "logdf['timestr']=df_clean['timestr'].copy()\n",
      "logdf=logdf[df_clean.columns.tolist()]\n",
      "----------------------------------------\n",
      "r_fta_blk = np.cov(nba[\"fta\"], nba[\"blk\"])[0,1]/(nba['fta'].std()*nba['blk'].std())\n",
      "\n",
      "r_fta_blk\n",
      "=====\n",
      "colunms = [\"fta\", \"blk\"]\n",
      "\n",
      "nba[colunms].corr()\n",
      "----------------------------------------\n",
      "df_clean.columns = ['c', 'timestr', 'e', 'a', 'day', 'f', 'b', 'd']\n",
      "=====\n",
      "df_diff=logdf[['day']+plist].diff(1)\n",
      "normdiff=df_diff[df_diff['day']==0][plist]\n",
      "daydiff=df_diff[df_diff['day']==1][plist]\n",
      "enddiff=df_diff[df_diff['day']>1][plist]\n",
      "----------------------------------------\n",
      "normdiff.loc[:,'a':'f'].plot()\n",
      "=====\n",
      "normdiff[normdiff['c']>-0.6].hist(bins=40);plt.show() \n",
      "daydiff.hist(bins=40);plt.show()\n",
      "enddiff.hist(bins=30);plt.show()\n",
      "----------------------------------------\n",
      "normdiff[normdiff['c']<-0.6].hist(bins=40);plt.show()\n",
      "normdiff[normdiff['c']>0.6].hist(bins=40);plt.show()\n",
      "=====\n",
      "ks_normday=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "ks_normend=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "ks_dayend=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "for ii in plist:\n",
      "    ks_normday[ii]=stats.ks_2samp(normdiff[ii],daydiff[ii])\n",
      "    ks_normend[ii]=stats.ks_2samp(normdiff[ii],enddiff[ii])\n",
      "    ks_dayend[ii]=stats.ks_2samp(daydiff[ii],enddiff[ii])\n",
      "print('jupyter_string')\n",
      "print(ks_normday.iloc[1,:])\n",
      "print('jupyter_string')\n",
      "print(ks_normend.iloc[1,:])\n",
      "print('jupyter_string')\n",
      "print(ks_dayend.iloc[1,:])\n",
      "----------------------------------------\n",
      "plt.plot(logdf_clean['timestr'],logdf_clean['a'])\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "t_d = 391 \n",
      "t_m = t_d*21 \n",
      "R_m = logdf_clean[plist].shift(-t_m) - logdf_clean[plist] \n",
      "\n",
      "R_m[['day','timestr']]=logdf_clean[['day','timestr']].copy();R_m=R_m[['day','timestr']+plist]\n",
      "\n",
      "sigma_m = logdf_clean.rolling(t_d,min_periods=t_d//2).std(ddof=1)\n",
      "sigma_m[['day','timestr']]=R_m[['day','timestr']].copy()\n",
      "sigma_m=sigma_m[['day','timestr']+plist]\n",
      "sigma_m.loc[:,'a':'f'].plot()\n",
      "plt.ylim([0,0.1])\n",
      "----------------------------------------\n",
      "sigma_m.loc[:,'a':'f'].plot()\n",
      "plt.ylim([0,0.1])\n",
      "=====\n",
      "sigma_m_coarse=pd.DataFrame(np.random.randn(len(logdf_clean['day'].unique()),7),columns=[['day']+plist])\n",
      "sigma_m_coarse['day']=logdf_clean['day'].unique().copy()\n",
      "\n",
      "\n",
      "for ii in logdf_clean['day'].unique():\n",
      "    \n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'a']=logdf_clean[logdf_clean['day']==ii]['a'].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'b']=logdf_clean[logdf_clean['day']==ii]['b'].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'c']=logdf_clean[logdf_clean['day']==ii]['c'].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'d']=logdf_clean[logdf_clean['day']==ii]['d'].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'e']=logdf_clean[logdf_clean['day']==ii]['e'].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day']==ii,'f']=logdf_clean[logdf_clean['day']==ii]['f'].std(ddof=1)\n",
      "----------------------------------------\n",
      "a_pred = a_model.predict(start='jupyter_string', end='jupyter_string', dynamic=False)\n",
      "b_pred = b_model.predict(start='jupyter_string', end='jupyter_string', dynamic=False)\n",
      "f_pred = f_model.predict(start='jupyter_string', end='jupyter_string', dynamic=False)\n",
      "=====\n",
      "beg=len(sigma_m_coarse['a'].values)\n",
      "predict_a = a_model.predict(start=beg,end=beg+21)\n",
      "predict_b = b_model.predict(start=beg,end=beg+21)\n",
      "predict_f = f_model.predict(start=beg,end=beg+21)\n",
      "print(predict_a)\n",
      "print(predict_b)\n",
      "print(predict_f)\n",
      "----------------------------------------\n",
      "df.corr()['gross'].sort_values(ascending=False)\n",
      "=====\n",
      "def divide_int_float_strings(df):\n",
      "    list_remove=[]\n",
      "    new_list=[]\n",
      "    for items in df:\n",
      "        if isinstance(df[items][0],str):\n",
      "            list_remove.append(items)\n",
      "        else:\n",
      "            new_list.append(items)\n",
      "    return new_list,list_remove\n",
      "new_list=divide_int_float_strings(df)[0]\n",
      "----------------------------------------\n",
      "plt.scatter(df['jupyter_string'],df['jupyter_string'])\n",
      "plt.show()\n",
      "=====\n",
      "df[new_list].corr()\n",
      "----------------------------------------\n",
      "df['jupyter_string']=np.log(df['jupyter_string'])\n",
      "df['jupyter_string']=np.sqrt(df['jupyter_string'])\n",
      "=====\n",
      "def better_relations(df,new_list):\n",
      "    better_relation_dict={}\n",
      "    better_relation_name_d={}\n",
      "    for items in new_list:\n",
      "        log=df['jupyter_string'].corr(np.log10(df[items]))\n",
      "        square=df['jupyter_string'].corr(np.square(df[items]))\n",
      "        sqrt=df['jupyter_string'].corr(np.sqrt(df[items]))\n",
      "        normal=df['jupyter_string'].corr(df[items])\n",
      "        method=[log,square,sqrt,normal]\n",
      "        max1=np.nanmax(method)\n",
      "        better_relation_dict[items]=max1\n",
      "        if log==max1:\n",
      "            better_relation_name_d[items]='jupyter_string'\n",
      "        if square==max1:\n",
      "            better_relation_name_d[items]='jupyter_string'\n",
      "        if sqrt==max1:\n",
      "            better_relation_name_d[items]='jupyter_string'\n",
      "        if normal==max1:\n",
      "            better_relation_name_d[items]='jupyter_string'       \n",
      "    return better_relation_dict,better_relation_name_d\n",
      "transformer=better_relations(df)[1]\n",
      "----------------------------------------\n",
      "transformed_data.sort_values(by='corr',ascending=False)\n",
      "=====\n",
      "df_corr=pd.DataFrame(better_relations(df)[0])\n",
      "df_corr=df_corr.sort_values('jupyter_string',ascending=False)\n",
      "df_corr=df_corr.drop(df_corr.index[0])\n",
      "df_corr\n",
      "----------------------------------------\n",
      "clf = ExtraTreesClassifier(random_state=361)\n",
      "grid_params = {'jupyter_string': [100, 200, 300],\n",
      "               'jupyter_string': [10, None],\n",
      "               'jupyter_string': [None, {0: 1, 1: 5}, 'jupyter_string']}\n",
      "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=361)\n",
      "grid_search = GridSearchCV(clf, param_grid=grid_params, cv=kf)\n",
      "grid_search.fit(X_train_dict['jupyter_string'], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "X_train_dict = OrderedDict([('jupyter_string', pcaed_X_train),\n",
      "                            ('jupyter_string', subspaced_X_train),\n",
      "                            ('jupyter_string', fs_and_pca_X_train),\n",
      "                            ('jupyter_string', X_train)])\n",
      "X_test_dict = OrderedDict([('jupyter_string', pcaed_X_test),\n",
      "                            ('jupyter_string', subspaced_X_test),\n",
      "                            ('jupyter_string', fs_and_pca_X_test),\n",
      "                            ('jupyter_string', X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "=====\n",
      "y_train_et = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_et = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "----------------------------------------\n",
      "clf = RandomForestClassifier(random_state=361)\n",
      "grid_params = {'jupyter_string': [100, 200, 300],\n",
      "               'jupyter_string': [10, None],\n",
      "               'jupyter_string': [None, {0: 1, 1: 5}, 'jupyter_string']}\n",
      "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=361)\n",
      "X_train_dict = OrderedDict([('jupyter_string', pcaed_X_train),\n",
      "                            ('jupyter_string', subspaced_X_train),\n",
      "                            ('jupyter_string', fs_and_pca_X_train),\n",
      "                            ('jupyter_string', X_train)])\n",
      "X_test_dict = OrderedDict([('jupyter_string', pcaed_X_test),\n",
      "                            ('jupyter_string', subspaced_X_test),\n",
      "                            ('jupyter_string', fs_and_pca_X_test),\n",
      "                            ('jupyter_string', X_test)])\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "clf.fit(X_train_dict[tuner.best_subspace_key_], y_train_rf)\n",
      "y_pred = clf.predict(X_test_dict[tuner.best_subspace_key_])\n",
      "=====\n",
      "_ = joblib.dump(clf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "np.sort(y_test)[-int(np.sum(y_test)):][::-1]\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title('jupyter_string')\n",
      "ax.set_xlabel('jupyter_string')\n",
      "ax.set_ylabel('jupyter_string')\n",
      "_ = ax.hist(y_hat, 100)\n",
      "----------------------------------------\n",
      "df.isnull().sum()\n",
      "=====\n",
      "df.hist(figsize=(10,8));\n",
      "----------------------------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title('jupyter_string')\n",
      "ax.set_xlabel('jupyter_string')\n",
      "ax.set_ylabel('jupyter_string')\n",
      "_ = ax.hist(y_hat, 100)\n",
      "=====\n",
      "n_examples = 3\n",
      "indices = y_hat[border:].argsort()[-n_examples:][::-1].tolist()\n",
      "indices = [x + border for x in indices]\n",
      "indices\n",
      "----------------------------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title('jupyter_string')\n",
      "ax.set_xlabel('jupyter_string')\n",
      "ax.set_ylabel('jupyter_string')\n",
      "_ = ax.hist(y_hat[indices], 100)\n",
      "=====\n",
      "found_candidates = random_smiles.iloc[indices, 0].values.tolist()\n",
      "found_candidates\n",
      "----------------------------------------\n",
      "samples = pd.read_csv('jupyter_string')\n",
      "samples.head()\n",
      "=====\n",
      "positives = np.hstack((geroprotectors, np.ones((geroprotectors.shape[0], 1))))\n",
      "negatives = np.hstack((random_compounds, np.zeros((random_compounds.shape[0], 1))))\n",
      "sample = np.vstack((positives, negatives))\n",
      "sample.shape\n",
      "----------------------------------------\n",
      "df = pd.DataFrame(sample, columns=['Mean', 'Max', 'Name', 'SMILES', 'random_state'])\n",
      "df.head()\n",
      "=====\n",
      "border = positives.shape[0]\n",
      "----------------------------------------\n",
      "predictions.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "np.mean(positives[:, 1152])\n",
      "----------------------------------------\n",
      "df = df.drop_duplicates(['id'], keep='jupyter_string')\n",
      "=====\n",
      "df.drop(labels=2090, axis=0, inplace=True)\n",
      "----------------------------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title('jupyter_string')\n",
      "ax.set_xlabel('jupyter_string')\n",
      "ax.set_ylabel('jupyter_string')\n",
      "_ = ax.plot(evr.cumsum())\n",
      "=====\n",
      "pca = PCA(n_components=20, random_state=361)\n",
      "pcaed_X_train = pca.fit_transform(weighted_X_train)\n",
      "pcaed_X_test = pca.transform(weighted_X_test)\n",
      "----------------------------------------\n",
      "fs_and_pca_X_train = fs_and_pca_X_train[:, fs_and_pca_X_train[:, 0].argsort()[::-1]]\n",
      "fs_and_pca_X_test = fs_and_pca_X_test[:, fs_and_pca_X_test[:, 0].argsort()[::-1]]\n",
      "=====\n",
      "all_and_pca_X_train = np.hstack((X_train, pcaed_X_train))\n",
      "all_and_pca_X_test = np.hstack((X_test, pcaed_X_test))\n",
      "all_and_pca_X_train.shape, all_and_pca_X_test.shape, y_train.shape, y_test.shape\n",
      "----------------------------------------\n",
      "df.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "df.info()\n",
      "----------------------------------------\n",
      "scores = cross_val_score(clf, X_train_dict['jupyter_string'], y_train, cv=kf, scoring='jupyter_string')\n",
      "print('jupyter_string', scores.mean())\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "clf = KNeighborsClassifier()\n",
      "grid_params = {'jupyter_string': [20, 30, 40, 50],\n",
      "               'jupyter_string': ['jupyter_string', 'jupyter_string'],\n",
      "               'jupyter_string': [1, 2]}\n",
      "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=361)\n",
      "grid_search = GridSearchCV(clf, param_grid=grid_params, cv=kf)\n",
      "grid_search.fit(X_train_dict, y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "clf = SVC(random_state=361)\n",
      "grid_params = {'jupyter_string': [0.5, 1, 2],\n",
      "               'jupyter_string': ['jupyter_string', 'jupyter_string', 'jupyter_string'],\n",
      "               'jupyter_string': [None, 'jupyter_string']}\n",
      "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=361)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "clf = GridSearchCV(clf, grid_params, scoring='jupyter_string', cv=kf)\n",
      "clf.fit(X_train_dict['jupyter_string'], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "----------------------------------------\n",
      "X_train_dict = OrderedDict([('jupyter_string', pcaed_X_train),\n",
      "                            ('jupyter_string', fs_and_pca_X_train),\n",
      "                            ('jupyter_string', X_train)])\n",
      "X_test_dict = OrderedDict([('jupyter_string', pcaed_X_test),\n",
      "                            ('jupyter_string', fs_and_pca_X_test),\n",
      "                            ('jupyter_string', X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, 'jupyter_string')\n",
      "=====\n",
      "y_train_gb = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_gb = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "----------------------------------------\n",
      "plt.plot(projected[:,0], projected[:,1], 'jupyter_string')\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "from pylab import rcParams\n",
      "from IPython.display import set_matplotlib_formats\n",
      "set_matplotlib_formats('jupyter_string')\n",
      "rcParams['jupyter_string'] = 13,13\n",
      "\n",
      "plt.scatter(projected[0,:], projected[1, :])\n",
      "\n",
      "for i in range(classes.shape[0]):\n",
      "    x = projected[0, i]\n",
      "    y = projected[1, i]\n",
      "    plt.annotate(classes[i], xy=(x,y), xytext=(x+1,y+1))\n",
      "\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df['release_year'].value_counts()\n",
      "=====\n",
      "all_year_movie = df.groupby('jupyter_string')['jupyter_string'].count()\n",
      "\n",
      "ax3= all_year_movie.plot(figsize=(10,5), y='jupyter_string', x='jupyter_string', color= 'jupyter_string', title='jupyter_string')\n",
      "\n",
      "\n",
      "ax3.grid(False)\n",
      "\n",
      "plt.legend();\n",
      "----------------------------------------\n",
      "n, bins, patches = plt.hist(l, 20, normed=1, facecolor='jupyter_string', alpha=0.75)\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "=====\n",
      "param = norm.fit(l)\n",
      "x = linspace(0,1,100) \n",
      "pdf_fitted = norm.pdf(x, loc=param[0], scale=param[1])\n",
      "\n",
      "title('jupyter_string')\n",
      "text(0.1,1.7, r'jupyter_string')\n",
      "grid()\n",
      "xlabel('jupyter_string')\n",
      "ylabel('jupyter_string')\n",
      "plot(x, pdf_fitted, 'jupyter_string')\n",
      "hist(l, normed=1,alpha=0.5, bins=20)\n",
      "savefig('jupyter_string', bbox_inches='jupyter_string')\n",
      "show()\n",
      "----------------------------------------\n",
      "m = mixture.Mixture(n_components=5, covariance_type='jupyter_string')\n",
      "m.fit(data)\n",
      "=====\n",
      "x = linspace(0,1,100) \n",
      "\n",
      "\n",
      "f1 = norm.pdf(x, 0.769444751466, 0.1)\n",
      "f2 = norm.pdf(x, 0.564284885974, 0.1)\n",
      "f3 = norm.pdf(x, 0.483220658218, 0.146212605803)\n",
      "f4 = norm.pdf(x, 0.769446855025, 0.1)\n",
      "f5 = norm.pdf(x, 0.397788035948, 0.136193314069)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig, ax1 = plt.subplots()\n",
      "title('jupyter_string')\n",
      "grid()\n",
      "xlabel('jupyter_string')\n",
      "ylabel('jupyter_string')\n",
      "\n",
      "\n",
      "n, bins, patches = ax1.hist(l, normed=False,alpha=0.5, bins=20)\n",
      "\n",
      "ax2 = ax1.twinx()\n",
      "ax2.plot(x, f1, 'jupyter_string',\n",
      "     x, f2, 'jupyter_string',\n",
      "     x, f3, 'jupyter_string',\n",
      "     x, f4, 'jupyter_string',\n",
      "     x, f5, 'jupyter_string')\n",
      "\n",
      "\n",
      "\n",
      "ax2.get_yaxis().set_ticks([])\n",
      "savefig('jupyter_string', bbox_inches='jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "data.corr(method='jupyter_string', min_periods=1)\n",
      "=====\n",
      "import seaborn as sns \n",
      "corr = data.corr()\n",
      "plt.subplots(figsize=(16,16))\n",
      "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, \n",
      "            cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df08 = df08[df08['dem_share_2008'] != 1]\n",
      "df08 = df08[df08['gop_share_2008'] != 1]\n",
      "=====\n",
      "filtered_cols = ['jupyter_string', 'jupyter_string',\n",
      "                 'jupyter_string', 'jupyter_string',\n",
      "                 'jupyter_string', 'jupyter_string']\n",
      "is_invalid_filter = (df08[filtered_cols] > 1).any(axis=1)\n",
      "n_rows_to_drop = is_invalid_filter[is_invalid_filter].shape[0]\n",
      "print('jupyter_string'.format(n_rows_to_drop))\n",
      "df08 = df08[~is_invalid_filter]\n",
      "df08.shape[0]\n",
      "----------------------------------------\n",
      "g = sns.FacetGrid(df08, col='jupyter_string')\n",
      "g = g.map(plt.hist, 'jupyter_string')\n",
      "=====\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[0])\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[1])\n",
      "----------------------------------------\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[0])\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[1])\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2016',\n",
      "            scale=alt.Scale(type='jupyter_string')),\n",
      "    y='jupyter_string'\n",
      ")\n",
      "----------------------------------------\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[0])\n",
      "sns.distplot(df08['jupyter_string'], ax=ax[1])\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2012',\n",
      "            scale=alt.Scale(type='jupyter_string')),\n",
      "    y='jupyter_string'\n",
      ")\n",
      "----------------------------------------\n",
      "df08 = pd.melt(df08, id_vars=['variable', 'FIPS', 'gop_share_2008', 'votes_dem_2016', 'year', 'county_name', 'dem_2016', 'fips_code', 'state_abbr', 'gop_2012', 'gop_2016', 'per_gop_2012', 'gop_share_2012', 'gop_2008', 'oth_2016', 'per_dem_2012', 'per_gop_2016', 'votes_gop_2016', 'per_dem_2016', 'total_2012', 'dem_share', 'dem_2008', 'diff_2016', 'per_point_diff_2012', 'dem_2012', 'total_2016', 'total_votes_2016', 'Unnamed: 0', 'dem_share_2016', 'total_2008', 'votes_dem_2012', 'votes_gop_2012', 'dem_share_2012', 'measure_name', 'winner', 'gop_share', 'gop_share_2016', 'oth_2012', 'county_fips', 'diff_2012', 'oth_2008', 'county', 'combined_fips', 'per_point_diff_2016', 'dem_share_2008', 'state_fips', 'dem_margin'], value_vars=['total_votes_2012', 'total_votes_2016', 'total_2008', 'votes_dem_2012', 'votes_gop_2012', 'dem_share_2012', 'measure_name', 'winner', 'gop_share', 'gop_share_2016', 'oth_2012', 'county_fips', 'diff_2012', 'oth_2008', 'county', 'combined_fips', 'per_point_diff_2016', 'dem_share_2008', 'state_fips', 'dem_margin'])\n",
      "=====\n",
      "melted = df08.melt(id_vars=['fips_code', 'county'])\n",
      "\n",
      "new_cols = pd.DataFrame(melted['jupyter_string'].str.rsplit('jupyter_string', 1).tolist(), columns=['jupyter_string', 'jupyter_string'])\n",
      "\n",
      "reshaped = pd.concat([melted, new_cols], axis=1).drop('jupyter_string', axis=1)\n",
      "reshaped.head()\n",
      "----------------------------------------\n",
      "flattened.head()\n",
      "=====\n",
      "alt.data_transformers.enable('jupyter_string', max_rows=1000000)\n",
      "alt.Chart(flattened).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('jupyter_string',\n",
      "            scale=alt.Scale(type='jupyter_string')),\n",
      "    y='jupyter_string',\n",
      "    color='jupyter_string')\n",
      "----------------------------------------\n",
      "flattened.to_csv('jupyter_string')\n",
      "=====\n",
      "flattened.to_csv('jupyter_string', index=False)\n",
      "----------------------------------------\n",
      "test_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\n",
      "=====\n",
      "train_data.describe()\n",
      "----------------------------------------\n",
      "X_train = train_data.drop('Survived', axis=1)\n",
      "y_train = train_data['Survived']\n",
      "X_dev = dev_data.drop('Survived', axis=1)\n",
      "y_dev = dev_data['Survived']\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_set, dev_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
      "dev_set.head()\n",
      "----------------------------------------\n",
      "train_set.drop('Name', axis=1, inplace=True)\n",
      "=====\n",
      "train_set.info()\n",
      "----------------------------------------\n",
      "genre_count.sort_values(ascending=True).plot.pie(legend=False, subplots=True, autopct='jupyter_string', figsize=(8,8), explode=explode)\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string', weight='jupyter_string', fontsize=16)\n",
      "=====\n",
      "df.revenue.astype(float)\n",
      "pd.options.display.float_format = 'jupyter_string'.format\n",
      "----------------------------------------\n",
      "train_data[['jupyter_string','Survived']].groupby(['jupyter_string'],as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
      "=====\n",
      "g = sns.FacetGrid(train_set, col='Sex',row='Survived')\n",
      "g.map(plt.hist, 'Age', bins=20)\n",
      "g.add_legend\n",
      "----------------------------------------\n",
      "train = pd.read_csv('jupyter_string')\n",
      "test = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "train_set=strat_train_set.copy()\n",
      "train_set_labels=train_set[\"Survived\"].copy()\n",
      "train_set=train_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "train_set\n",
      "----------------------------------------\n",
      "test_set=strat_test_set.copy()\n",
      "test_set.info()\n",
      "=====\n",
      "dev_set_labels=dev_set[\"Survived\"].copy()\n",
      "dev_set=dev_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "test_set=test_data.copy()\n",
      "test_set=test_set.drop([\"PassengerId\"],axis=1)\n",
      "----------------------------------------\n",
      "test_set.info()\n",
      "=====\n",
      "train_set_cat=train_set[[\"Name\",\"Embarked\",\"Fare\"]].copy()\n",
      "train_set_num=train_set.drop([\"Name\",\"Embarked\",\"Fare\"],axis=1)\n",
      "cat_attribs=list(train_set_cat)\n",
      "num_attribs=list(train_set_num)\n",
      "num_attribs\n",
      "----------------------------------------\n",
      "train_set_cat.head()\n",
      "=====\n",
      "pd.crosstab(train_set_cat['jupyter_string'], train_set_num['Sex'])\n",
      "----------------------------------------\n",
      "train_set_cat['Embarked'] = train_set_cat['Embarked'].fillna('jupyter_string')\n",
      "=====\n",
      "freq=pd.Series([train_set_cat[c].value_counts().index[0] for c in train_set_cat], index=train_set_cat.columns)\n",
      "\n",
      "freq\n",
      "----------------------------------------\n",
      "forest_clf.fit(X_train,y_train)\n",
      "=====\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = {\n",
      "    'jupyter_string': range(100, 400, 50),\n",
      "    'jupyter_string': range(2, 7, 1),\n",
      "    'jupyter_string':['jupyter_string','jupyter_string']\n",
      "}\n",
      "cross_validation = StratifiedKFold(n_splits=5)\n",
      "grid_clf = GridSearchCV(estimator = forest_clf, param_grid = param_grid, scoring='jupyter_string', cv=cross_validation)\n",
      "grid_clf.fit(X_train, y_train)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "----------------------------------------\n",
      "df.groupby('original_title')['revenue'].mean().sort_values(ascending=False).head(1)\n",
      "=====\n",
      "movie_revenue = df[['jupyter_string', 'revenue']].sort_values('revenue', ascending = False).head(5)\n",
      "movie_revenue.sort_values('jupyter_string',inplace=True)\n",
      "print(movie_revenue)\n",
      "----------------------------------------\n",
      "forest_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
      "forest_clf.fit(X_train, y_train)\n",
      "y_pred = forest_clf.predict(X_dev)\n",
      "=====\n",
      "y_pred=grid_clf.best_estimator_.predict(X_dev)\n",
      "----------------------------------------\n",
      "test_data=test_data.drop(['PassengerId'],axis=1)\n",
      "=====\n",
      "y_full=train_data[\"Survived\"]\n",
      "X_full=train_data.drop(\"Survived\",axis=1)\n",
      "X_full=full_pipeline.fit_transform(X_full)\n",
      "----------------------------------------\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.33, random_state=42)\n",
      "=====\n",
      "grid_clf.fit(X_full, y_full)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "----------------------------------------\n",
      "df.drop(columns_to_drop, axis=1, inplace=True)\n",
      "=====\n",
      "new_df = df.drop(columns_to_drop, axis='jupyter_string')\n",
      "----------------------------------------\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "\n",
      "\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "\n",
      "\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "\n",
      "\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "\n",
      "\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "\n",
      "\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'].astype('jupyter_string')\n",
      "=====\n",
      "df_climate.head()\n",
      "----------------------------------------\n",
      "movie_revenue.to_csv('jupyter_string', index=False)\n",
      "movie_profit.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "fig = plt.figure() \n",
      "\n",
      "ax = fig.add_subplot(111) \n",
      "ax2 = ax.twinx() \n",
      "\n",
      "width = 0.4\n",
      "\n",
      "movie_revenue.plot(figsize = (10,6), kind='jupyter_string', color='jupyter_string', ax=ax, width=width, position=1, title='jupyter_string')\n",
      "movie_profit.plot(figsize = (10,6), kind='jupyter_string', color='jupyter_string', ax=ax2, width=width, position=0)\n",
      "\n",
      "\n",
      "ax.grid(False)\n",
      "\n",
      "\n",
      "ax2.grid(False)\n",
      "\n",
      "ax.set_ylabel('jupyter_string')\n",
      "ax2.set_ylabel('jupyter_string')\n",
      "\n",
      "ax.set_xticklabels(movie_revenue['jupyter_string'], rotation=30)\n",
      "\n",
      "h1, l1 = ax.get_legend_handles_labels()\n",
      "h2, l2 = ax2.get_legend_handles_labels()\n",
      "\n",
      "plt.legend(h1+h2, l1+l2, loc=1)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'] - new_df['jupyter_string']\n",
      "new_df['jupyter_string'] = new_df['jupyter_string'] / new_df['jupyter_string']\n",
      "=====\n",
      "daily_mean_2011 = new_df.resample('jupyter_string').mean()\n",
      "\n",
      "\n",
      "daily_mean_2011.head()\n",
      "----------------------------------------\n",
      "daily_max_2011 = daily_temp_2011.max()\n",
      "daily_min_2011 = daily_temp_2011.min()\n",
      "daily_max_climate = daily_temp_climate.max()\n",
      "daily_min_climate = daily_temp_climate.min()\n",
      "=====\n",
      "sunny = new_df.loc[new_df['jupyter_string'] == 'jupyter_string']\n",
      "\n",
      "\n",
      "overcast = new_df.loc[new_df['jupyter_string'].str.contains('jupyter_string')]\n",
      "----------------------------------------\n",
      "sunny['jupyter_string'] = sunny['jupyter_string'] - sunny['jupyter_string']\n",
      "overcast['jupyter_string'] = overcast['jupyter_string'] - overcast['jupyter_string']\n",
      "=====\n",
      "sunny_daily_max = sunny.resample('jupyter_string').max()\n",
      "overcast_daily_max = overcast.resample('jupyter_string').max()\n",
      "----------------------------------------\n",
      "sns.distplot(august_2011, hist=False)\n",
      "sns.distplot(august_max, hist=False)\n",
      "plt.show()\n",
      "=====\n",
      "august_2011_high = august_2011.loc[august_2011 > august_max]\n",
      "\n",
      "\n",
      "august_2011_high.plot(kind='jupyter_string', normed=True, cumulative=True, bins=25, legend='jupyter_string', grid=True)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "dt = tree.DecisionTreeClassifier()\n",
      "dt = dt.fit(X_train, y_train)\n",
      "dt.score(X_test, y_test)\n",
      "=====\n",
      "ens = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_split=30)\n",
      "ens.fit(X_train,y_train)\n",
      "sk_rf_pred = ens.predict(X_test)\n",
      "----------------------------------------\n",
      "iris = load_iris()\n",
      "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
      "df.head()\n",
      "=====\n",
      "Xy = pd.read_csv('jupyter_string',header=None)\n",
      "Xy[60] = Xy[60].map({'R':0,'jupyter_string':1})\n",
      "X = np.array(Xy.iloc[:,:-1])\n",
      "y = np.array(Xy.iloc[:,-1])\n",
      "Xy = np.array(Xy)\n",
      "----------------------------------------\n",
      "dataframe['name'] = dataframe['name'].replace('jupyter_string', 'jupyter_string')\n",
      "\n",
      "\n",
      "dataframe\n",
      "=====\n",
      "dataframe = dataframe.set_index(\"name\")\n",
      "\n",
      "\n",
      "dataframe\n",
      "----------------------------------------\n",
      "dataframe.plot(x='score_3', y='score_2', kind='jupyter_string')\n",
      "=====\n",
      "dataframe.plot(subplots=True)\n",
      "----------------------------------------\n",
      "kw_count.plot(kind='jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "kw_list = df['keywords']=df.keywords.str.split('jupyter_string', expand=True)[0]\n",
      "kw_list = df['keywords'].value_counts().index.tolist()\n",
      "kw_list\n",
      "----------------------------------------\n",
      "left = df[df['left'] == 1]\n",
      "no_left = df[df['left'] == 0]\n",
      "=====\n",
      "dfleft, dfstay = [x for _, x in df.groupby(df[\"left\"]==0)]\n",
      "dfleft.describe()\n",
      "----------------------------------------\n",
      "dfleft['sales'] = dfleft['sales'].replace({'jupyter_string':1, 'jupyter_string':2, 'jupyter_string':3, 'jupyter_string':4, 'jupyter_string':5, 'jupyter_string':6, 'jupyter_string':7, 'jupyter_string':8, 'jupyter_string':9, 'jupyter_string':10, 'jupyter_string':11, 'jupyter_string':12, 'jupyter_string':13, 'jupyter_string':14, 'jupyter_string':15, 'jupyter_string':16, 'jupyter_string':17, 'jupyter_string':18, 'jupyter_string':19, 'jupyter_string':20, 'jupyter_string':21, 'jupyter_string':22, 'jupyter_string':23, 'jupyter_string':24, 'jupyter_string':25, 'jupyter_string':26, 'jupyter_string':27, 'jupyter_string':28, 'jupyter_string':29, 'jupyter_string':30, 'jupyter_string':31, 'jupyter_string':32, 'jupyter_string':33, 'jupyter_string':34, 'jupyter_string':35, 'jupyter_string':36, 'jupyter_string':37, 'jupyter_string':38, 'jupyter_string':39, 'jupyter_string':40, 'jupyter_string':41, 'jupyter_string':42, 'jupyter_string':43, 'jupyter_string':44, 'jupyter_string':45, 'jupyter_string':46, 'jupyter_string':47, 'jupyter_string':48, 'jupyter_string':49, 'jupyter_string':50})\n",
      "dfstay['sales'] = dfstay['sales'].replace({'jupyter_string':1, 'jupyter_string':2, 'jupyter_string':3, 'jupyter_string':4, 'jupyter_string':5, 'jupyter_string':6, 'jupyter_string':7, 'jupyter_string':8, 'jupyter_string':9, 'jupyter_string':10, 'jupyter_string':11, 'jupyter_string':12, 'jupyter_string':13, 'jupyter_string':14, 'jupyter_string':15, 'jupyter_string':16, 'jupyter_string':17, 'jupyter_string':18, 'jupyter_string':19, 'jupyter_string':20, 'jupyter_string':21, 'jupyter_string':22, 'jupyter_string':23, 'jupyter_string':24, 'jupyter_string':25, 'jupyter_string':26, 'jupyter_string':27, 'jupyter_string':28, 'jupyter_string':29, 'jupyter_string':30, 'jupyter_string':31, 'jupyter_string':32, 'jupyter_string':33, 'jupyter_string':34, 'jupyter_string':35, 'jupyter_string':36, 'jupyter_string':37, 'jupyter_string':38, 'jupyter_string':39, 'jupyter_string':40, 'jupyter_string':41, 'jupyter_string':42, 'jupyter_string':43, 'jupyter_string':44, 'jupyter_string':45, 'jupyter_string':46, 'jupyter_string':47, 'jupyter_string':48, 'jupyter_string':49, 'jupyter_string':50})\n",
      "=====\n",
      "cols_to_transform = ['salary', 'sales']\n",
      "df = pd.get_dummies(df, columns = cols_to_transform )\n",
      "df.head()\n",
      "----------------------------------------\n",
      "cf_recommender = CFRecommender(cf_preds_df, items_df)\n",
      "cf_recommender.train()\n",
      "=====\n",
      "print('jupyter_string')\n",
      "cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\n",
      "print('jupyter_string' % cf_global_metrics)\n",
      "cf_detailed_results_df.head(10)\n",
      "----------------------------------------\n",
      "interactions_df = interactions_df[interactions_df['jupyter_string'] > 5]\n",
      "=====\n",
      "users_interactions_count_df = interactions_df.groupby(['personId', 'contentId']).size().groupby('personId').size()\n",
      "print('jupyter_string' % len(users_interactions_count_df))\n",
      "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]\n",
      "print('jupyter_string' % len(users_with_enough_interactions_df))\n",
      "----------------------------------------\n",
      "interactions_from_selected_users_df['jupyter_string'] = np.log(interactions_from_selected_users_df['jupyter_string'])\n",
      "interactions_from_selected_users_df['jupyter_string'] = np.log(interactions_from_selected_users_df['jupyter_string'])\n",
      "=====\n",
      "def smooth_user_preference(x):\n",
      "    return math.log(1+x, 2)\n",
      "    \n",
      "interactions_full_df = interactions_from_selected_users_df \\\n",
      "                    .groupby(['personId', 'contentId'])['jupyter_string'].sum() \\\n",
      "                    .apply(smooth_user_preference).reset_index()\n",
      "print('jupyter_string' % len(interactions_full_df))\n",
      "interactions_full_df.head(10)\n",
      "----------------------------------------\n",
      "popularity_model = gl.popularity_recommender.create(interactions_full_indexed_df, \n",
      "                                                    user_id='jupyter_string', \n",
      "                                                    item_id='jupyter_string', \n",
      "                                                    target='jupyter_string')\n",
      "=====\n",
      "item_popularity_df = interactions_full_df.groupby('contentId')['jupyter_string'].sum().sort_values(ascending=False).reset_index()\n",
      "item_popularity_df.head(10)\n",
      "----------------------------------------\n",
      "item_popularity_recommender = PopularityRecommender(item_popularity_df)\n",
      "item_popularity_recommender.train()\n",
      "=====\n",
      "print('jupyter_string')\n",
      "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\n",
      "print('jupyter_string' % pop_global_metrics)\n",
      "pop_detailed_results_df.head(10)\n",
      "----------------------------------------\n",
      "users_items_pivot_matrix_df.head(10)\n",
      "=====\n",
      "users_items_pivot_matrix = users_items_pivot_matrix_df.as_matrix()\n",
      "users_items_pivot_matrix[:10]\n",
      "----------------------------------------\n",
      "na_ensemble.describe()\n",
      "=====\n",
      "p_ranges = pd.read_csv('jupyter_string', index_col='name')\n",
      "\n",
      "p_ranges\n",
      "----------------------------------------\n",
      "sns.set_palette(quant_cmap)\n",
      "sns.palplot(sns.color_palette(quant_cmap))\n",
      "sns.palplot(sns.color_palette(quant_cmap))\n",
      "sns.palplot(sns.color_palette(quant_cmap))\n",
      "=====\n",
      "def scatter_misfit_stats(dataset):\n",
      "    \"\"'jupyter_string'\"\"\n",
      "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "    col_pairs = (('RMSD', 'D_mean'),\n",
      "                 ('RMSD', 'misfit'),\n",
      "                 ('D_mean', 'misfit'))\n",
      "\n",
      "    for (x, y), ax in zip(col_pairs, axes):\n",
      "        dataset.plot(kind='jupyter_string', x=x, y=y,\n",
      "                     ax=ax, alpha=0.2, c='jupyter_string')\n",
      "        plt.setp(ax, xlabel=na_labels[x],\n",
      "                 ylabel=na_labels[y])\n",
      "\n",
      "    fig.tight_layout()\n",
      "    return fig, axes\n",
      "\n",
      "fig, axes = scatter_misfit_stats(na_ensemble)\n",
      "\n",
      "fig.savefig('jupyter_string')\n",
      "----------------------------------------\n",
      "scatter_misfit_stats(dataset)\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "for col, ax in zip(['D_mean', 'RMSD', 'misfit'], axes.flatten()):\n",
      "    trace = na_ensemble.get(col)\n",
      "    na_ensemble.plot(kind='jupyter_string', x='NA_iter', y=col, ax=ax,\n",
      "                     s=50, alpha=0.15, color='jupyter_string')\n",
      "    plt.setp(ax, xlim=[-1, na_params['jupyter_string']])\n",
      "    plt.setp(ax, ylabel=na_labels[col], xlabel='jupyter_string')\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig('jupyter_string')\n",
      "----------------------------------------\n",
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "dn = hierarchy.dendrogram(link_matrix, labels=na_subset.index)\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
      "\n",
      "\n",
      "dist_tresh = 0.6  \n",
      "\n",
      "cat_clr = ['jupyter_string'] * 5\n",
      "hierarchy.set_link_color_palette(cat_clr)\n",
      "\n",
      "dendrogram = hierarchy.dendrogram(link_matrix,\n",
      "                                  color_threshold=dist_tresh,\n",
      "                                  labels=na_subset_norm.index,\n",
      "                                  orientation='jupyter_string', ax=axes[1])\n",
      "\n",
      "plt.setp(axes[1], xlabel='jupyter_string',\n",
      "         ylabel='jupyter_string', xticklabels=[])\n",
      "\n",
      "\n",
      "na_clusters_log = log_df(na_clusters)\n",
      "na_clusters_lognorm = norm_df(na_clusters_log)\n",
      "na_centroids_lognorm = na_clusters_lognorm.groupby('cluster').mean()\n",
      "\n",
      "cluster_markers = ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string']\n",
      "cat_clr = ['jupyter_string'] * 5\n",
      " \n",
      "pdplt.parallel_coordinates(\n",
      "    na_clusters_lognorm.sort('cluster'), 'cluster',\n",
      "    alpha=0.4, color=cat_clr, ax=axes[0],\n",
      "    axvlines=False, xticks=range(9)\n",
      ")\n",
      "axes[0].legend_.remove()\n",
      "axes[0].grid(False)\n",
      "\n",
      "lines = []\n",
      "for c, ctr in enumerate(na_centroids_lognorm.values):\n",
      "    l, = axes[0].plot(ctr, linewidth=1.5,\n",
      "                 label='jupyter_string'.format(c + 1),\n",
      "                 marker=cluster_markers[c], markersize=13,\n",
      "                 markerfacecolor='jupyter_string', color='jupyter_string')\n",
      "    lines.append(l)\n",
      "\n",
      "leg = axes[0].legend(handles=lines, loc='jupyter_string', ncol=3,\n",
      "                     frameon=True, labelspacing=1.3)\n",
      "leg.get_frame().set_color('jupyter_string')\n",
      "\n",
      "for x in range(9):\n",
      "    axes[0].axvline(x=x, color='jupyter_string', linestyle='jupyter_string')\n",
      "\n",
      "xlbls = ['jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "         'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string', 'RMSD',]\n",
      "plt.setp(axes[0], ylabel='jupyter_string',\n",
      "         xticklabels=xlbls, ylim=[-1, 1])\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig('jupyter_string')\n",
      "----------------------------------------\n",
      "df_sgd.head()\n",
      "=====\n",
      "df_jpy = df_jpy['jupyter_string':]\n",
      "df_jpy.DEXJPUS = pd.to_numeric(df_jpy.DEXJPUS, errors='jupyter_string')\n",
      "df_jpy = df_jpy.fillna(method='jupyter_string')\n",
      "----------------------------------------\n",
      "df_sgd = df_sgd['jupyter_string':]\n",
      "df_sgd.DEXSIUS = df_sgd.DEXSIUS.shift(-1)\n",
      "df_sgd.DEXJPUS = df_sgd.DEXJPUS.shift(-1)\n",
      "=====\n",
      "df = df_sgd.join(df_jpy, how='jupyter_string')\n",
      "----------------------------------------\n",
      "df[['SGDJPY', 'DEXJPY']].plot(title='jupyter_string')\n",
      "=====\n",
      "df_pc.plot()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "iris = pd.read_csv('jupyter_string')\n",
      "iris.drop('Id',axis=1,inplace=True) \n",
      "\n",
      "def convert2num(df, col):\n",
      "    for idx, val in enumerate(df[col].unique()):\n",
      "        print(val, 'jupyter_string', idx)\n",
      "        df[col] = df[col].replace(val, idx)\n",
      "    return df\n",
      "\n",
      "iris = convert2num(iris, 'Species')\n",
      "----------------------------------------\n",
      "studio_most_profit.head()\n",
      "=====\n",
      "studio_most_profit = df.loc[df['jupyter_string'] == 2015]\n",
      "studio_most_profit = studio_most_profit.groupby('jupyter_string')['jupyter_string'].sum().sort_values(ascending=False).head(10)\n",
      "studio_most_profit\n",
      "----------------------------------------\n",
      "df_pc['jupyter_string'].plot(title='jupyter_string')\n",
      "=====\n",
      "df_pc.describe()\n",
      "----------------------------------------\n",
      "df_pc['jupyter_string'].autocorr(lag=1)\n",
      "=====\n",
      "df_pc.index = pd.to_datetime(df_pc.index)\n",
      "df_pc_weekly = df_pc.dropna().resample(rule='jupyter_string').last()\n",
      "df_pc_weekly.head()\n",
      "----------------------------------------\n",
      "df_pc_monthly['DEXJPUS'].autocorr()\n",
      "=====\n",
      "df_pc_monthly = df_pc.resample('jupyter_string').last()\n",
      "----------------------------------------\n",
      "df.groupby(['release_month','jupyter_string']).count()['jupyter_string'].sort_values(ascending=False)[0:30]\n",
      "=====\n",
      "df['jupyter_string'] = pd.to_datetime(df['jupyter_string'])\n",
      "month = df['jupyter_string'] = df['jupyter_string'].dt.month\n",
      "----------------------------------------\n",
      "sample = np.random.normal(98.6, sigma, size = 10000)\n",
      "\n",
      "\n",
      "x_theor, y_theor = dcst.ecdf(sample)\n",
      "x,y = dcst.ecdf(df.temperature)\n",
      "\n",
      "\n",
      "_ = plt.plot(x_theor, y_theor)\n",
      "_ = plt.plot(x, y, marker='jupyter_string', linestyle='jupyter_string')\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel('jupyter_string')\n",
      "_ = plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "conf_int = np.percentile(df.temperature,[2.5,97.5])\n",
      "conf_int\n",
      "----------------------------------------\n",
      "conf_int = np.percentile(df.temperature,[2.5,97.5])\n",
      "conf_int\n",
      "=====\n",
      "mean = df['temperature'].mean()\n",
      "std = df['temperature'].std()\n",
      "----------------------------------------\n",
      "y_pred = V_classifier.predict(X_test)\n",
      "=====\n",
      "y_blind = V_classifier.fit(X, y).predict(X_blind) \n",
      "blind['Facies'] = y_blind\n",
      "blind.to_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "training_data['Formation'] = training_data['Formation'].astype('jupyter_string')\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "training_data['Well Name'] = training_data['Well Name'].astype('jupyter_string')\n",
      "=====\n",
      "above = []\n",
      "below = []\n",
      "\n",
      "cols = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
      "\n",
      "for i, group in training_data.groupby('Well Name'):\n",
      "    df = group.sort_values('Depth')\n",
      "    dfa = df.shift(-1).fillna(method='jupyter_string')\n",
      "    dfb = df.shift(1).fillna(method='jupyter_string')\n",
      "        \n",
      "    above.append(dfa[cols])\n",
      "    below.append(dfb[cols])\n",
      "\n",
      "above_df = pd.concat(above)\n",
      "below_df = pd.concat(below)\n",
      "\n",
      "above_df.columns = ['jupyter_string'+ column for column in above_df.columns]\n",
      "below_df.columns = ['jupyter_string'+ column for column in below_df.columns]\n",
      "\n",
      "training_data = pd.concat((training_data, above_df, below_df), axis=1)\n",
      "    \n",
      "y = training_data['Facies'].values\n",
      "X = training_data.drop(['Formation', 'Well Name','Facies'], axis=1)\n",
      "scaler = preprocessing.StandardScaler().fit(X)\n",
      "X = scaler.transform(X)\n",
      "\n",
      "----------------------------------------\n",
      "df.groupby(['cast','jupyter_string']).count()['jupyter_string'].sort_values(ascending=False)[0:30]\n",
      "=====\n",
      "import calendar\n",
      "df['jupyter_string'] = df['jupyter_string'].apply(lambda x: calendar.month_abbr[x]).sort_index()\n",
      "----------------------------------------\n",
      "world_firearms.head()\n",
      "=====\n",
      "world_firearms.to_csv('jupyter_string')\n",
      "----------------------------------------\n",
      "gun_ownership['jupyter_string'] = gun_ownership['jupyter_string'].astype(str)\n",
      "gun_ownership['jupyter_string'] = gun_ownership['jupyter_string'].astype(str)\n",
      "gun_ownership['jupyter_string'] = gun_ownership['jupyter_string'].astype(str)\n",
      "gun_ownership['jupyter_string'] = gun_ownership['jupyter_string'].astype(str)\n",
      "=====\n",
      "gun_ownership.head()\n",
      "----------------------------------------\n",
      "df_train.plot.scatter(x='jupyter_string', y='jupyter_string')\n",
      "=====\n",
      "var = 'jupyter_string'\n",
      "data = pd.concat([df_train['jupyter_string'], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(8, 6))\n",
      "fig = sns.boxplot(x=var, y='jupyter_string', data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "----------------------------------------\n",
      "data_allbp.info()\n",
      "=====\n",
      "data_allbp['jupyter_string'] = data_allbp['jupyter_string'].apply( lambda x: x.split('jupyter_string')[0])\n",
      "data_allbp['jupyter_string'].value_counts()\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "sns.set_style('jupyter_string')\n",
      "sns.set_context('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "pal = sns.light_palette('jupyter_string', n_colors=3, reverse=True)\n",
      "g = sns.factorplot(data=results, x='jupyter_string', y='jupyter_string', hue='jupyter_string', col='jupyter_string',\n",
      "                   kind='jupyter_string', palette=pal, aspect=1.2, errwidth=1, capsize=0.04)\n",
      "g.set(ylim=(88, 98))\n",
      "----------------------------------------\n",
      "g = nx.from_pandas_dataframe(df_routes, 'id', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e', 'mfd_a_a', 'mfd_a_ag', 'mfd_a_e')\n",
      "=====\n",
      "df_routesets.label.value_counts()\n",
      "----------------------------------------\n",
      "lambdas = np.array([0,1,2,3,4,5])\n",
      "coef = coefs_OLS(X, Y , lambdas)\n",
      "print(coef)\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Ypred = X.dot(coefs)\n",
      "    erro = Y - Ypred\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv('jupyter_string', header=None, delim_whitespace=True)\n",
      "\n",
      "dfTrain = df[ :30]\n",
      "dfTest = df[30 : ]\n",
      "X = dfTrain.values[ : , : -1]\n",
      "n = np.ones(len(X))\n",
      "X = np.c_[n, X]\n",
      "Y = dfTrain.values[ : , -1]\n",
      "\n",
      "XTest = dfTest.values[ : , : -1]\n",
      "n = np.ones(len(XTest))\n",
      "XTest = np.c_[n, XTest]\n",
      "YTest = dfTest.values[ : , -1]\n",
      "\n",
      "----------------------------------------\n",
      "goldenpercent1['FullAgreement'].value_counts()\n",
      "=====\n",
      "goldenpercent.groupby('jupyter_string').size()\n",
      "----------------------------------------\n",
      "goldenpercent.groupby('jupyter_string').size().plot(kind='jupyter_string')\n",
      "=====\n",
      "sns.countplot('jupyter_string', data=goldenpercent1)\n",
      "----------------------------------------\n",
      "golden = pd.read_csv(goldencsvpath)\n",
      "golden.head()\n",
      "=====\n",
      "usertweetinfo = pd.read_csv('jupyter_string', encoding='jupyter_string')\n",
      "usertweetinfo2 = usertweetinfo[['User','Golden trust','Majority trust']]\n",
      "usertweetinfo2[17:19]\n",
      "----------------------------------------\n",
      "sns.countplot('jupyter_string', data=goldenpercent1)\n",
      "=====\n",
      "goldenpercent1.groupby('jupyter_string').size()\n",
      "----------------------------------------\n",
      "goldenpercent1.groupby('jupyter_string').size().plot(kind='jupyter_string')\n",
      "=====\n",
      "sns.countplot('jupyter_string', data=goldenpercent1)\n",
      "----------------------------------------\n",
      "majority1.FullAgreement.value_counts()\n",
      "=====\n",
      "import seaborn as sns\n",
      "majority1.groupby('jupyter_string').size()\n",
      "----------------------------------------\n",
      "majority1.groupby('jupyter_string').size().plot(kind='jupyter_string')\n",
      "=====\n",
      "sns.countplot('jupyter_string', data=majority1)\n",
      "----------------------------------------\n",
      "sns.countplot('jupyter_string', data=majority1)\n",
      "=====\n",
      "majority1.groupby('jupyter_string').size()\n",
      "----------------------------------------\n",
      "majority1.groupby('jupyter_string').size().plot(kind='jupyter_string')\n",
      "=====\n",
      "sns.countplot('jupyter_string', data=majority1)\n",
      "----------------------------------------\n",
      "golden = pd.read_csv('jupyter_string')\n",
      "golden.head()\n",
      "=====\n",
      "goldenpercent = pd.read_csv('jupyter_string', encoding='jupyter_string')\n",
      "goldenpercent[:5]\n",
      "----------------------------------------\n",
      "X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
      "y = one_hot_matrix(iris['Species'], 3)\n",
      "=====\n",
      "train, test = train_test_split(iris, test_size = 0.3) \n",
      "X_train = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] \n",
      "Y_train=train.Species \n",
      "X_test= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] \n",
      "Y_test =test.Species   \n",
      "\n",
      "X_train = np.array(X_train, dtype=np.float32).T\n",
      "Y_train = one_hot_matrix(np.array(Y_train,dtype=np.float32), C = 3)\n",
      "X_test = np.array(X_test,dtype=np.float32).T\n",
      "Y_test = one_hot_matrix(np.array(Y_test,dtype=np.float32), C = 3)\n",
      "\n",
      "m_train = X_train.shape\n",
      "m_test = X_test.shape\n",
      "\n",
      "print('jupyter_string')\n",
      "print('jupyter_string'.format(m_train))\n",
      "print('jupyter_string'.format(m_test))\n",
      "----------------------------------------\n",
      "goldenpercent.columns = ['NoEmocion', 'Emotion', 'Info', 'User', 'Tristeza', 'id_str', 'CÃĥÂ³lera', 'Felicidad', 'Sorpresa', 'Asco', 'FullAgreement', 'Majority trust', 'Miedo', 'Tweet', 'Golden trust']\n",
      "goldenpercent[:5]\n",
      "=====\n",
      "goldenpercent['jupyter_string']=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "----------------------------------------\n",
      "baseline02042018v2.describe()\n",
      "=====\n",
      "from matplotlib import pyplot\n",
      "import seaborn as sns; sns.set(color_codes=True)\n",
      "----------------------------------------\n",
      "Baseline02042018v2['jupyter_string'] = Baseline02042018v2['jupyter_string'] - Baseline02042018v2['jupyter_string']\n",
      "Baseline02042018v2['jupyter_string'] = Baseline02042018v2['jupyter_string'] - Baseline02042018v2['jupyter_string']\n",
      "Baseline02042018v2['jupyter_string'] = Baseline02042018v2['jupyter_string'] - Baseline02042018v2['jupyter_string']\n",
      "=====\n",
      "print('jupyter_string')\n",
      "print()\n",
      "b1_sample = Baseline02042018v2['jupyter_string']\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Baseline02042018['jupyter_string']\n",
      "print(stats.describe(b2_sample))\n",
      "b3_sample = rad_recording01162018v2['jupyter_string']\n",
      "print(stats.describe(b3_sample))\n",
      "\n",
      "print()\n",
      "print('jupyter_string')\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "print(stats.ttest_ind(b1_sample, b3_sample, equal_var = False) )\n",
      "print(stats.ttest_ind(b2_sample, b3_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print('jupyter_string')\n",
      "print()\n",
      "\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b3_sample, ax=ax, hist=False, rug=True)\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "sns_plot = sns.distplot(co2_mlo['decimal'].dropna())\n",
      "sns_plot = sns.distplot(co2_ice['decimal'].dropna())\n",
      "fig = sns_plot.get_figure()\n",
      "=====\n",
      "import pdvega\n",
      "\n",
      "co2_mlo.vgplot.line(x='decimal', y='ppm', alpha=0.5)\n",
      "----------------------------------------\n",
      "wrist = pd.read_csv('jupyter_string')\n",
      "wrist.columns = ['gamedate', 'pgoalx', 'position', 'vteamcode', 'advantagetypegoalie', 'XNorm', 'lnX', 'hteamcode', 'eventtype', 'advantagetypeshooter', 'ogoals', 'X', 'pgoal', 'isTHome', 'eventnumber', 'pgoaly', 'teamcode', 'isgoal', 'season', 'XNormAdj', 'eventtimefromzero', 'lnY', 'YNorm', 'gamenumber', 'Y', 'otecode', 'playernumber', 'tgoals', 'subseasontype', 'dscore', 'shotType', 'namegoalie', 'period', 'YNormAdj', 'zone']\n",
      "=====\n",
      "plt.show()\n",
      "import os\n",
      "import sys\n",
      "import numpy \n",
      "import pandas \n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import LogNorm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.formula.api import ols\n",
      "    \n",
      "\n",
      "pandas.set_option('jupyter_string', True)\n",
      "pandas.set_option('jupyter_string', 20)\n",
      "pandas.set_option('jupyter_string', 50)\n",
      "\n",
      "from decimal import getcontext, Decimal\n",
      "\n",
      "getcontext().prec = 2\n",
      "----------------------------------------\n",
      "dm[['XNormAdj', 'YNormAdj']].describe()\n",
      "=====\n",
      "dm.head(2)\n",
      "----------------------------------------\n",
      "dm.tail(2)\n",
      "=====\n",
      "dm.describe()\n",
      "----------------------------------------\n",
      "dm[['XNormAdj', 'YNormAdj', 'gamenumber', 'tgoals', 'subseasontype']].corr()\n",
      "=====\n",
      "dm.groupby(['eventtype'])[['XNorm', 'YNorm']].describe()\n",
      "----------------------------------------\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "=====\n",
      "dw.describe()\n",
      "----------------------------------------\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "dw['jupyter_string'] = dw['jupyter_string'] / dw['jupyter_string']\n",
      "=====\n",
      "dw.groupby(['season'])[['jupyter_string', 'jupyter_string']].sum()\n",
      "----------------------------------------\n",
      "dogs = animals[animals['OutcomeType'] == 'jupyter_string']\n",
      "cats = animals[animals['OutcomeType'] == 'jupyter_string']\n",
      "=====\n",
      "animals = pd.get_dummies(animals, columns=['Sex', 'AgeCategory', 'ColorCategory'])\n",
      "animals.head()\n",
      "----------------------------------------\n",
      "dogs_fi.head()\n",
      "=====\n",
      "X_dogs = dogs_fi.drop(['OutcomeType'], axis=1)\n",
      "y_dogs = dogs_fi.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "----------------------------------------\n",
      "cm = confusion_matrix(y_true=dogs_test['OutcomeType'],\n",
      "                      y_pred=dogs_test['jupyter_string'])\n",
      "pd.DataFrame(cm,\n",
      "             index=clf_LR_dogs.classes_,\n",
      "             columns=clf_LR_dogs.classes_)\n",
      "=====\n",
      "dogs_train['OutcomeType'].unique()\n",
      "----------------------------------------\n",
      "X_dogs = dogs.drop('OutcomeType', axis=1)\n",
      "y_dogs = dogs.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "=====\n",
      "clf_KNN_dogs = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_KNN_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_KNN_dogs.score(X_test_dogs, y_test_dogs))\n",
      "----------------------------------------\n",
      "data_m.mean()\n",
      "=====\n",
      "np.mean(data_m)\n",
      "----------------------------------------\n",
      "group_mouse.describe()\n",
      "=====\n",
      "group_mouse.get_group('jupyter_string')\n",
      "----------------------------------------\n",
      "group_mouse.apply(np.mean)\n",
      "=====\n",
      "mousedata.groupby(['Sex']).apply(np.mean)\n",
      "----------------------------------------\n",
      "sns.boxplot(point_stats_matrix.Area)\n",
      "=====\n",
      "sns.boxplot(y='jupyter_string', data=point_stats_matrix)\n",
      "----------------------------------------\n",
      "mask = df['Volume'] < 0.8\n",
      "filtered_points = df[mask]\n",
      "=====\n",
      "mask = point_stats_matrix.Volume <= 0.8\n",
      "filtered_points=point_stats_matrix[mask]\n",
      "filtered_points.head()\n",
      "----------------------------------------\n",
      "plt.scatter(filtered_points.Weight, filtered_points.Value)\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "=====\n",
      "sns.lmplot(x='jupyter_string', y='jupyter_string', fit_reg=False, data=filtered_points)\n",
      "----------------------------------------\n",
      "food_info.columns[food_info.columns.str.endswith('jupyter_string')]\n",
      "=====\n",
      "col_names = food_info.columns.tolist()\n",
      "\n",
      "gram_columns = []\n",
      "for el in col_names:\n",
      "    if el.endswith('jupyter_string'):\n",
      "        gram_columns.append(el)\n",
      "print(gram_columns,'jupyter_string')\n",
      "\n",
      "gram_df = food_info[gram_columns]\n",
      "print(gram_df)\n",
      "----------------------------------------\n",
      "food_info.to_csv('jupyter_string', index=False)\n",
      "=====\n",
      "del food_info['jupyter_string']\n",
      "food_info.head()\n",
      "----------------------------------------\n",
      "data_df = data_df.reindex(np.random.permutation(data_df.index))\n",
      "=====\n",
      "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
      "----------------------------------------\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(accuracy)\n",
      "=====\n",
      "rocscores = roc_curve(y_test, y_pred)\n",
      "plt.plot(rocscores[0], rocscores[1])\n",
      "plt.plot(rocscores[0],rocscores[0])\n",
      "plt.xlabel('jupyter_string')\n",
      "plt.ylabel('jupyter_string')\n",
      "plt.title('jupyter_string')\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "train = pd.read_csv('jupyter_string')\n",
      "test = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pandas import DataFrame,Series\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "MEDIUM_SIZE = 14\n",
      "BIGGER_SIZE = 16\n",
      "\n",
      "plt.rc('jupyter_string', size=MEDIUM_SIZE)          \n",
      "plt.rc('jupyter_string', titlesize=BIGGER_SIZE)     \n",
      "plt.rc('jupyter_string', labelsize=MEDIUM_SIZE)    \n",
      "plt.rc('jupyter_string', labelsize=MEDIUM_SIZE)    \n",
      "plt.rc('jupyter_string', labelsize=MEDIUM_SIZE)    \n",
      "plt.rc('jupyter_string', fontsize=MEDIUM_SIZE)    \n",
      "plt.rc('jupyter_string', titlesize=BIGGER_SIZE)  \n",
      "\n",
      "\n",
      "data = pd.read_csv('jupyter_string')\n",
      "print('jupyter_string',data.shape)\n",
      "----------------------------------------\n",
      "plt.imshow(X_train[random.randint(0, len(X_train))])\n",
      "plt.show()\n",
      "=====\n",
      "fig = plt.figure(figsize=(50, 50))\n",
      "fig.suptitle('jupyter_string', fontsize=40)\n",
      "for id in traffic_db['ClassId']:\n",
      "    fig.add_subplot(11, 4, id+1)\n",
      "    plt.title(str(id)+'jupyter_string'+traffic_db['SignName'][id], fontsize=30)\n",
      "    plt.axis('jupyter_string')\n",
      "    plt.imshow(X_train_clean[random.choice(class_indices[id])])\n",
      "----------------------------------------\n",
      "plt.scatter(simple_features, simple_model_predictions)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(sales.sqft_living, sales.price, color='jupyter_string', s = 2)\n",
      "plt.plot(sales.sqft_living, simple_model_predictions, color='jupyter_string')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel('sqft_living')\n",
      "plt.ylabel('price')\n",
      "plt.ticklabel_format(style = 'jupyter_string')\n",
      "----------------------------------------\n",
      "simple_predictions = simple_model.predict(test_data)\n",
      "other_predictions = other_model.predict(test_data)\n",
      "=====\n",
      "house1 = sales[sales.id== 5309101200]\n",
      "house2 = sales[sales.id == 1925069082]\n",
      "----------------------------------------\n",
      "simple_model = smf.ols(formula='jupyter_string', data=df).fit()\n",
      "simple_model.summary()\n",
      "=====\n",
      "sales[simple_features+['price']].head()\n",
      "----------------------------------------\n",
      "house1_features = ['sqft_basement', 'yr_renovated', 'lat', 'long', 'waterfront', 'grade', 'bedrooms', 'condition', 'zipcode', 'sqft_living', 'view', 'sqft_living15', 'sqft_lot', 'floors', 'price', 'sqft_above', 'date', 'id', 'yr_built', 'bathrooms', 'sqft_lot15']\n",
      "house2_features = ['sqft_basement', 'yr_renovated', 'lat', 'long', 'waterfront', 'grade', 'bedrooms', 'condition', 'zipcode', 'sqft_living', 'view', 'sqft_living15', 'sqft_lot', 'floors', 'price', 'sqft_above', 'date', 'id', 'yr_built', 'bathrooms', 'sqft_lot15']\n",
      "=====\n",
      "house1.head()\n",
      "----------------------------------------\n",
      "house3 = pd.read_csv('jupyter_string')\n",
      "house3.columns = ['sqft_basement', 'yr_renovated', 'lat', 'long', 'waterfront', 'grade', 'bedrooms', 'condition', 'zipcode', 'sqft_living', 'view', 'sqft_lot', 'floors', 'price', 'sqft_above', 'date', 'id', 'yr_built', 'bathrooms', 'sqft_lot15']\n",
      "house3.head()\n",
      "=====\n",
      "house3 = pd.DataFrame(data=bill_gates)\n",
      "----------------------------------------\n",
      "simple_model.fit(X, y)\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(action='jupyter_string', module='jupyter_string', message='jupyter_string')\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "app_store = pd.read_csv('jupyter_string',delimiter='jupyter_string')\n",
      "\n",
      "number_of_apps = len(app_store.id)\n",
      "print('jupyter_string' + str(number_of_apps))\n",
      "----------------------------------------\n",
      "sns.set(rc={'jupyter_string':(20, 15)})\n",
      "sns.countplot(x='genre', data=app_store).set_title('jupyter_string')\n",
      "=====\n",
      "index, count = np.unique(app_store[\"prime_genre\"],return_counts=True)\n",
      "unique = app_store[\"prime_genre\"].nunique()\n",
      "print('jupyter_string'.format(unique))\n",
      "count = count/count.sum()\n",
      "print('jupyter_string'.format(count[7]))\n",
      "\n",
      "\n",
      "pie_chart(count, index, 'jupyter_string')\n",
      "\n",
      "----------------------------------------\n",
      "plot_count('prime_genre', 'jupyter_string', 'jupyter_string', 'jupyter_string', rotation=60)\n",
      "=====\n",
      "stats = app_store.groupby(\"prime_genre\")['user_rating']\n",
      "genres, count = np.unique(app_store[\"prime_genre\"],return_counts=True)\n",
      "describe = stats.describe()\n",
      "describe['jupyter_string']=genres\n",
      "bar_chart(describe['jupyter_string'],index,(20,8),'jupyter_string', 'jupyter_string','jupyter_string' ,60)\n",
      "----------------------------------------\n",
      "plot = sns.regplot(x=app_store['jupyter_string'], y=app_store['price'])\n",
      "plot.set(xlabel='jupyter_string', ylabel='jupyter_string', title='jupyter_string')\n",
      "=====\n",
      "new_prices = []\n",
      "indices = []\n",
      "counted = 0\n",
      "summation = 0\n",
      "prices = app_store_data['price'].tolist()\n",
      "for price in prices:\n",
      "    if price > 0:\n",
      "        new_prices.append(1)\n",
      "        counted = counted + 1\n",
      "        summation = summation + price\n",
      "    else:\n",
      "        new_prices.append(0)\n",
      "        \n",
      "series = pd.Series(new_prices)\n",
      "app_store_data['jupyter_string'] = series.values\n",
      "max = app_store_data[\"price\"].max()\n",
      "min = app_store_data[\"price\"].min()\n",
      "mean = app_store_data[\"price\"].mean()\n",
      "mean_of_paid = summation/counted\n",
      "print(mean_of_paid)\n",
      "print(mean)\n",
      "print(max)\n",
      "----------------------------------------\n",
      "genres = app_store_data['jupyter_string'].value_counts()\n",
      "genres.plot(kind='jupyter_string')\n",
      "plt.title('jupyter_string')\n",
      "plt.show()\n",
      "=====\n",
      "revenue = []\n",
      "\n",
      "for index in range(7197):\n",
      "\n",
      "    revenue.append(app_store_data.loc[[index]]['price'][index]*app_store_data.loc[[index]]['rating_count_tot'][index])\n",
      "    \n",
      "revenue_series = pd.Series(revenue)\n",
      "total_revenue = revenue_series.sum()\n",
      "app_store_data['jupyter_string'] = revenue_series.values\n",
      "\n",
      "print(total_revenue)\n",
      "----------------------------------------\n",
      "app_store_data = app_store_data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1', 'Unnamed: 0.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1.1',\n",
      "=====\n",
      "'''jupyter_string'''\n",
      "app_store_data['user_rating'] = app_store_data['user_rating'].replace([3.5, 3, 2.5, 2, 1.5, 1, 0.5], 0)\n",
      "app_store_data['user_rating'] = app_store_data['user_rating'].replace([4, 4.5, 5], 1)\n",
      "app_store_data.head()\n",
      "----------------------------------------\n",
      "df = pd.read_csv('jupyter_string')\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('jupyter_string', index_col=0)\n",
      "----------------------------------------\n",
      "df = df[df['Inflation']<60]\n",
      "=====\n",
      "df = df.drop(labels=df[df['Inflation']>2500].index, axis=0).reset_index(drop=True)\n",
      "----------------------------------------\n",
      "X = df[['Comm_Tax', 'Region', 'GDP_perCap', 'Elec_Access', 'WB_Code', 'ElecGen_OilGasCoal', 'GDP_Total', 'FossFuel_Cons', 'Imp_San_Access', 'Elec_Consumption', 'Features', 'Adult_Lit', 'School_Enrl', 'Life_Expect', 'Income_Group', 'Inf_Mortality', 'Tech_Exports', 'Inflation', 'Emissions', ''jupyter_string'Metal_exports', 'Corr_Perc_Index', 'Country', 'BusReg']]\n",
      "=====\n",
      "target = df['GDP_perCap']\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "----------------------------------------\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "references = references\n",
    "candidates = hypothesis\n",
    "lang = lang\n",
    "\n",
    "PY_LANGUAGE = Language('/home/v-jipzhang/build/my-languages.so', 'python')\n",
    "parser = Parser()\n",
    "parser.set_language(PY_LANGUAGE)\n",
    "match_count = 0\n",
    "total_count = 0\n",
    "out = 0\n",
    "\n",
    "for i in range(len(candidates)):\n",
    "    references_sample = references[i]\n",
    "    candidate = candidates[i] \n",
    "    for reference in references_sample:\n",
    "        try:\n",
    "            candidate=remove_comments_and_docstrings(candidate,'python')\n",
    "        except:\n",
    "            pass    \n",
    "        try:\n",
    "            reference=remove_comments_and_docstrings(reference,'python')\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        candidate_tree = parser.parse(bytes(candidate,'utf8')).root_node\n",
    "\n",
    "        reference_tree = parser.parse(bytes(reference,'utf8')).root_node\n",
    "\n",
    "        def get_all_sub_trees(root_node):\n",
    "            node_stack = []\n",
    "            sub_tree_sexp_list = []\n",
    "            depth = 1\n",
    "            node_stack.append([root_node, depth])\n",
    "            while len(node_stack) != 0:\n",
    "                cur_node, cur_depth = node_stack.pop()\n",
    "                sub_tree_sexp_list.append([cur_node.sexp(), cur_depth])\n",
    "                for child_node in cur_node.children:\n",
    "                    if len(child_node.children) != 0:\n",
    "                        depth = cur_depth + 1\n",
    "                        node_stack.append([child_node, depth])\n",
    "            return sub_tree_sexp_list\n",
    "        cand_sexps = [x[0] for x in get_all_sub_trees(candidate_tree)]\n",
    "        ref_sexps = get_all_sub_trees(reference_tree)\n",
    "\n",
    "        # print(cand_sexps)\n",
    "        # print(ref_sexps)\n",
    "\n",
    "        t_match_count = 0\n",
    "        for sub_tree, depth in ref_sexps:\n",
    "            if sub_tree in cand_sexps:\n",
    "                match_count += 1\n",
    "                t_match_count += 1\n",
    "        if t_match_count / len(ref_sexps) < 0.3:\n",
    "            if 'import' not in candidate:\n",
    "                print(candidate)\n",
    "                print('='*5)\n",
    "                print(reference)\n",
    "                print('-'*40)\n",
    "            #if 'import' in candidate:\n",
    "            #    out += 1\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a9c4e-7a75-4a5f-a24f-81406f6149c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8656db58-5320-424b-840d-6842b1eca404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:52:15.253691Z",
     "iopub.status.busy": "2021-08-19T14:52:15.253419Z",
     "iopub.status.idle": "2021-08-19T14:52:15.257237Z",
     "shell.execute_reply": "2021-08-19T14:52:15.256592Z",
     "shell.execute_reply.started": "2021-08-19T14:52:15.253663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(hypothesis[1])\n",
    "import ast\n",
    "def syntax(code):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True    \n",
    "    except SyntaxError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0038ff-e441-4a65-aa0f-75f5469d7810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T14:53:24.951893Z",
     "iopub.status.busy": "2021-08-19T14:53:24.951655Z",
     "iopub.status.idle": "2021-08-19T14:53:25.032888Z",
     "shell.execute_reply": "2021-08-19T14:53:25.032288Z",
     "shell.execute_reply.started": "2021-08-19T14:53:24.951867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861849096705633"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [(code, syntax(code),ref) for code,ref in zip(hypothesis,reference)]\n",
    "sum([r[1] for r in results])/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ebeef35-ea70-4f01-b605-297dc733bc62",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-19T14:53:34.606894Z",
     "iopub.status.busy": "2021-08-19T14:53:34.606657Z",
     "iopub.status.idle": "2021-08-19T14:53:34.627066Z",
     "shell.execute_reply": "2021-08-19T14:53:34.626454Z",
     "shell.execute_reply.started": "2021-08-19T14:53:34.606868Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
      "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "\n",
      "model1 = LinearRegression()\n",
      "model2 = Lasso()\n",
      "model3 = Ridge(alpha=0.1)\n",
      "model4 = ElasticNet(random_state=47)\n",
      "model5 = RandomForestRegressor(random_state=47)\n",
      "model6 = BaggingRegressor(max_samples= 0.6, n_estimators= 11, random_state=47)\n",
      "model7 = GradientBoostingRegressor(random_state=47)\n",
      "model8 = KNeighborsRegressor(leaf_size= 20, n_neighbors= 4, p= 1)\n",
      "\n",
      "for i in range(1,17):\n",
      "    print('jupyter_string'.format(i))\n",
      "\n",
      "    model1.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model2.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model3.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model4.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model5.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model6.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model7.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "    model8.fit(pcaX_train.iloc[:,:i], y_train)\n",
      "\n",
      "    \n",
      "    print('jupyter_string'.format(model1.score(pcaX_test.iloc[:,:i], y_test), \n",
      "                                          'jupyter_string'.format(model2.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model3.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model4.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model5.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model6.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model7.score(pcaX_test.iloc[:,:i], y_test),\n",
      "                                          'jupyter_string'.format(model8.score(pcaX_test.iloc[:,:i], y_test))))\n",
      "=====\n",
      "model5.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "model7.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "\n",
      "scoreRF = model5.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseRF = np.sqrt(mean_squared_error(y_test, model5.predict(pcaX_test.iloc[:,:10])))\n",
      "\n",
      "scoreGB = model7.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseGB = np.sqrt(mean_squared_error(y_test, model7.predict(pcaX_test.iloc[:,:10])))\n",
      "               \n",
      "print('jupyter_string'.format(scoreRF,rmseRF))\n",
      "print('jupyter_string'.format(scoreGB,rmseGB))\n",
      "----------------------------------------\n",
      "14\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection\n",
      "=====\n",
      "X_train.head()\n",
      "----------------------------------------\n",
      "201\n",
      "df[df.Quiz 2 == 'jupyter_string'].mean()\n",
      "=====\n",
      "df[df.Grade == 'jupyter_string'].mean()\n",
      "----------------------------------------\n",
      "202\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sk\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings('jupyter_string', category=DeprecationWarning) \n",
      "\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "\n",
      "import keras \n",
      "from keras.models import Sequential \n",
      "from keras.layers import Dense\n",
      "\n",
      "\n",
      "df_train = pd.read_csv('jupyter_string')\n",
      "df_test = pd.read_csv('jupyter_string')\n",
      "df = df_train.append(df_test , ignore_index = True)\n",
      "\n",
      "\n",
      "print(df_train.shape, df_test.shape, df_train.columns.values)\n",
      "----------------------------------------\n",
      "388\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "parameters = [{'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'jupyter_string': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
      "              {'jupyter_string': [0.001, 0.01, 0.\n",
      "=====\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "import warnings\n",
      "warnings.filterwarnings('jupyter_string') \n",
      "\n",
      "\n",
      "\n",
      "grid_params = {\n",
      "    'jupyter_string': [0.01,0.1,1.0,10,100],\n",
      "    'jupyter_string':[False,True],\n",
      "    'jupyter_string':[0.25,0.5,0.75],\n",
      "    'jupyter_string':[500,1000,1500,2000]\n",
      "}\n",
      "\n",
      "\n",
      "grid_eNet = GridSearchCV(eCV,grid_params,cv = 5)\n",
      "grid_eNet.fit(polyX_train,y_train)\n",
      "\n",
      "print(grid_eNet.best_params_)\n",
      "----------------------------------------\n",
      "393\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.manifold import TSNE\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction\n",
      "=====\n",
      "import pandas\n",
      "import matplotlib.pyplot as plot\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "\n",
      "raw_data = pandas.read_csv('jupyter_string') \n",
      "dataset = raw_data[:20]  \n",
      "dataset\n",
      "----------------------------------------\n",
      "529\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "import xgboost as x_gb\n",
      "\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "svm = SVC()\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "\n",
      "print('jupyter_string')\n",
      "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print('jupyter_string')\n",
      "train_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print('jupyter_string')\n",
      "train_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print('jupyter_string')\n",
      "train_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "----------------------------------------\n",
      "645\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sk\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data = pd.read_csv('jupyter_string')\n",
      "data.head()\n",
      "----------------------------------------\n",
      "660\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity, c=leaf_data.class)\n",
      "plt.xlabel('smoothness')\n",
      "plt.ylabel('jupyter_string')\n",
      "=====\n",
      "colors = {1: 'jupyter_string', 2: 'jupyter_string', 3: 'jupyter_string', 4: 'jupyter_string'}\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity, c = leaf_data['class'].map(colors))\n",
      "plt.xlabel('smoothness')\n",
      "plt.ylabel('jupyter_string')\n",
      "\n",
      "'''jupyter_string'''\n",
      "----------------------------------------\n",
      "1055\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn\n",
      "=====\n",
      "playerAggDfAllNbaAllStar.head()\n",
      "----------------------------------------\n",
      "1060\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn\n",
      "=====\n",
      "all_nba_all_year_predicted_df['accolades_all_nba'].value_counts().plot(kind = 'jupyter_string')\n",
      "----------------------------------------\n",
      "1130\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection\n",
      "=====\n",
      "reg = linear_model.LinearRegression()\n",
      "reg1 = reg.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred1 = reg1.predict(X_1_test)\n",
      "\n",
      "\n",
      "print('jupyter_string', pd.DataFrame(reg1.coef_.T, index = X1.columns , columns = Y3.columns))\n",
      "\n",
      "print('jupyter_string'\n",
      "      % mean_squared_error(Y_3_test, y_pred1))\n",
      "\n",
      "print('jupyter_string' % r2_score(Y_3_test, y_pred1))\n",
      "\n",
      "print('jupyter_string' % reg1.score(X_1_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test['jupyter_string'], y_pred1[:,0], 'jupyter_string')\n",
      "plotSummary(Y_3_test['jupyter_string'], y_pred1[:,1], 'jupyter_string')\n",
      "----------------------------------------\n",
      "1153\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from\n",
      "=====\n",
      "scaler = StandardScaler()  \n",
      "scaler.fit(X_1_train)\n",
      "X_1_traina = scaler.transform(X_1_train)  \n",
      "X_1_testa = scaler.transform(X_1_test)\n",
      "\n",
      "clf = MLPRegressor(solver='jupyter_string', alpha=1e-5,  random_state=1)\n",
      "clf = clf.fit(X_1_traina, Y_3_train)\n",
      "\n",
      "pred_nn_1 = clf.predict(X_1_testa)\n",
      "\n",
      "plotSummary(Y_3_test['jupyter_string'], pred_nn_1[:,0], 'jupyter_string')\n",
      "plotSummary(Y_3_test['jupyter_string'], pred_nn_1[:,1], 'jupyter_string')\n",
      "clf.loss_\n",
      "clf.hidden_layer_sizes\n",
      "\n",
      "print('jupyter_string', r2_score(Y_3_test, pred_nn_1))\n",
      "print('jupyter_string', r2_score(Y_3_test['jupyter_string'], pred_nn_1[:,0]))\n",
      "print('jupyter_string', r2_score(Y_3_test['jupyter_string'], pred_nn_1[:,1]))\n",
      "----------------------------------------\n",
      "1189\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations import PReLU\n",
      "from keras.layers.advanced_activations import LeakyReLU\n",
      "from keras.layers.advanced_activations\n",
      "=====\n",
      "modelBN = Sequential()\n",
      "\n",
      "modelBN.add(Conv2D(filters=16, kernel_size=4, padding='jupyter_string', use_bias=False, input_shape=input_shape))\n",
      "modelBN.add(Activation('jupyter_string'))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=32, kernel_size=3, padding='jupyter_string', use_bias=False))\n",
      "modelBN.add(Activation('jupyter_string'))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=64, kernel_size=3, padding='jupyter_string', use_bias=False))\n",
      "modelBN.add(Activation('jupyter_string'))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=128, kernel_size=2, padding='jupyter_string', use_bias=False))\n",
      "modelBN.add(Activation('jupyter_string'))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Flatten())\n",
      "modelBN.add(Dense(512, activation='jupyter_string'))\n",
      "modelBN.add(Dropout(0.3))\n",
      "modelBN.add(Dense(100, activation='jupyter_string'))\n",
      "\n",
      "modelBN.summary()\n",
      "----------------------------------------\n",
      "1265\n",
      "train.drop(['jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string','jupyter_string'],\n",
      "=====\n",
      "text_info_columns = train.dtypes[train.dtypes.map(lambda x: x=='jupyter_string')].index.tolist()\n",
      "train = train.drop(text_info_columns,axis=1)\n",
      "text_info_columns\n",
      "----------------------------------------\n",
      "1270\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "from fancyimpute import MICE\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.model_selection import\n",
      "=====\n",
      "X_filled_mice = fancyimpute.MICE().complete(np.asarray(carmpg.ix[:, carmpg.columns != 'Auto']))\n",
      "----------------------------------------\n",
      "1312\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from keras.utils import np_utils\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from keras.optimizers import SGD\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasRegressor\n",
      "from keras.\n",
      "=====\n",
      "model.compile(loss='jupyter_string', optimizer='jupyter_string', metrics=['jupyter_string'])\n",
      "\n",
      "model.summary()\n",
      "----------------------------------------\n",
      "1378\n",
      "def smiles_to_numpy(smiles, radius=2):\n",
      "    \"\"'jupyter_string'\"\"\n",
      "    molecules = map(MolFromSmiles, smiles.tolist())\n",
      "    to_morgan = partial(rdMolDescriptors.GetMorganFingerprintAsBitVect,\n",
      "                        radius=radius)\n",
      "    fingerprints = map(to_morgan, molecules)\n",
      "    \n",
      "    \n",
      "    np_fingerprints = []\n",
      "    for fingerprint in fingerprints:\n",
      "        arr = np.zeros((1,))\n",
      "        ConvertToNumpyArray(fingerprint, arr)\n",
      "        np_fingerprints.append(arr)\n",
      "  geroprotectors = smiles_to_numpy(df['jupyter_string'])\n",
      "geroprotectors.shape\n",
      "=====\n",
      "path_to_zinc = 'jupyter_string'\n",
      "if os.path.isfile(path_to_zinc):\n",
      "    zinc_smiles = pd.read_csv(path_to_zinc)\n",
      "else:\n",
      "    zinc_smiles = pd.read_csv('jupyter_string' +\n",
      "                              'jupyter_string' +\n",
      "                              'jupyter_string')\n",
      "zinc_smiles.head()\n",
      "----------------------------------------\n",
      "1565\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.ticker as ticker\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings('jupyter_string')\n",
      "\n",
      "plt.show()  \n",
      "----------------------------------------\n",
      "1669\n",
      "goldenpercent['jupyter_string']=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "=====\n",
      "goldenpercent1 = goldenpercent[['Tweet','Felicidad','Tristeza','jupyter_string','Asco','Miedo','Sorpresa','NoEmocion','jupyter_string','jupyter_string']]\n",
      "goldenpercent1[:5]\n",
      "----------------------------------------\n",
      "1677\n",
      "majoritypercent['jupyter_string'] = 0\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.CÃ³lera > majoritypercent.Tristeza) & (majoritypercent.CÃ³lera > majoritypercent.NoEmocion) & (majoritypercent.CÃ³lera > majoritypercent.Felicidad)& (majoritypercent.CÃ³lera > majoritypercent.Sorpresa)& (majoritypercent.CÃ³lera > majoritypercent.Miedo)& (majoritypercent.CÃ³lera > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.Tristeza > majoritypercent.CÃ³lera) & (majoritypercent.Tristeza > majoritypercent.NoEmocion)& (majoritypercent.Tristeza > majoritypercent.Felicidad)& (majoritypercent.Tristeza > majoritypercent.Sorpresa)& (majoritypercent.Tristeza > majoritypercent.Miedo)& (majoritypercent.Tristeza > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.NoEmocion > majoritypercent.CÃ³lera) & (majoritypercent.NoEmocion > majoritypercent.Tristeza)& (majoritypercent.NoEmocion > majoritypercent.Felicidad)& (majoritypercent.NoEmocion > majoritypercent.Sorpresa)& (majoritypercent.NoEmocion > majoritypercent.Miedo)& (majoritypercent.NoEmocion > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.Felicidad > majoritypercent.CÃ³lera) & (majoritypercent.Felicidad > majoritypercent.Tristeza)& (majoritypercent.Felicidad > majoritypercent.NoEmocion)& (majoritypercent.Felicidad > majoritypercent.Sorpresa)& (majoritypercent.Felicidad > majoritypercent.Miedo)& (majoritypercent.Felicidad > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.Sorpresa > majoritypercent.CÃ³lera) & (majoritypercent.Sorpresa > majoritypercent.Tristeza)& (majoritypercent.Sorpresa > majoritypercent.NoEmocion)& (majoritypercent.Sorpresa > majoritypercent.Miedo)& (majoritypercent.Sorpresa > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.Miedo > majoritypercent.CÃ³lera) & (majoritypercent.Miedo > majoritypercent.Tristeza)& (majoritypercent.Miedo > majoritypercent.NoEmocion)& (majoritypercent.Miedo > majoritypercent.Felicidad)& (majoritypercent.Miedo > majoritypercent.Asco)).astype(int)\n",
      "majoritypercent.loc[majoritypercent['jupyter_string'] == 0, 'jupyter_string'] = ((majoritypercent.Asco > majoritypercent.CÃ³lera) & (majoritypercent.Asco > majoritypercent.Tristeza)& (majoritypercent.Asco > majoritypercent.NoEmocion)& (majoritypercent.Asco > majoritypercent.Felicidad)& (majoritypercent.Asco > majoritypercent.Miedo)& (majoritypercent.Asco)).astype(int)\n",
      "=====\n",
      "majoritypercent['jupyter_string']=0\n",
      "majoritypercent.FullAgreement = ((majoritypercent.Tristeza == 1) | (majoritypercent.CÃ³lera == 1) | (majoritypercent.NoEmocion == 1) | (majoritypercent.Asco == 1) | (majoritypercent.Miedo == 1) | (majoritypercent.Sorpresa == 1) | (majoritypercent.Felicidad == 1)).astype(int)\n",
      "majoritypercent[:5]\n",
      "----------------------------------------\n",
      "1684\n",
      "goldenpercent['jupyter_string'] = 0\n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.CÃ³lera > goldenpercent.Tristeza) & (goldenpercent.CÃ³lera > goldenpercent.NoEmocion) & (goldenpercent.CÃ³lera > goldenpercent.Felicidad)& (goldenpercent.CÃ³lera > goldenpercent.Sorpresa)& (goldenpercent.CÃ³lera > goldenpercent.Miedo)& (goldenpercent.CÃ³lera > goldenpercent.Asco)).astype(int)\n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.Tristeza > goldenpercent.CÃ³lera) & (goldenpercent.Tristeza > goldenpercent.NoEmocion)& (goldenpercent.Tristeza > goldenpercent.Felicidad)& (goldenpercent.Tristeza > goldenpercent.Sorpresa)& (goldenpercent.Tristeza > goldenpercent.Miedo)& (goldenpercent.Tristeza > goldenpercent.Asco)).astype(int)+2 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.NoEmocion > goldenpercent.CÃ³lera) & (goldenpercent.NoEmocion > goldenpercent.Tristeza)& (goldenpercent.NoEmocion > goldenpercent.Felicidad)& (goldenpercent.NoEmocion > goldenpercent.Sorpresa)& (goldenpercent.NoEmocion > goldenpercent.Miedo)& (goldenpercent.NoEmocion > goldenpercent.Asco)).astype(int)+4 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.Felicidad > goldenpercent.CÃ³lera) & (goldenpercent.Felicidad > goldenpercent.Tristeza)& (goldenpercent.Felicidad > goldenpercent.NoEmocion)& (goldenpercent.Felicidad > goldenpercent.Sorpresa)& (goldenpercent.Felicidad > goldenpercent.Miedo)& (goldenpercent.Felicidad > goldenpercent.Asco)).astype(int)+6 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.Tristeza > goldenpercent.CÃ³lera) & (goldenpercent.Tristeza > goldenpercent.NoEmocion)& (goldenpercent.Tristeza > goldenpercent.Felicidad)& (goldenpercent.Tristeza > goldenpercent.NoEmocion)& (goldenpercent.Tristeza > goldenpercent.Felicidad)& (goldenpercent.Tristeza > goldenpercent.Asco)).astype(int)+7 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.NoEmocion > goldenpercent.CÃ³lera) & (goldenpercent.NoEmocion > goldenpercent.Tristeza)& (goldenpercent.NoEmocion > goldenpercent.Felicidad)& (goldenpercent.NoEmocion > goldenpercent.Sorpresa)& (goldenpercent.NoEmocion > goldenpercent.Miedo)& (goldenpercent.NoEmocion > goldenpercent.Asco)).astype(int)+8 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.Tristeza > goldenpercent.CÃ³lera) & (goldenpercent.Tristeza > goldenpercent.NoEmocion)& (goldenpercent.Tristeza > goldenpercent.Felicidad)& (goldenpercent.Tristeza > goldenpercent.NoEmocion)& (goldenpercent.Tristeza > goldenpercent.Felicidad)& (goldenpercent.Tristeza > goldenpercent.Asco)).astype(int)+9 \n",
      "goldenpercent.loc[goldenpercent['jupyter_string'] == 0, 'jupyter_string'] = ((goldenpercent.NoEmocion > goldenpercent.CÃ³lera) & (goldenpercent.NoEmocion > goldenpercent.\n",
      "=====\n",
      "goldenpercent['jupyter_string']=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "----------------------------------------\n",
      "1699\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import silhouette_score\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.decomposition import\n",
      "=====\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "    'jupyter_string', header=None,\n",
      "    names=[\n",
      "        'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string',\n",
      "        'jupyter_string', 'jupyter_string', 'jupyter_string'\n",
      "    ])\n",
      "\n",
      "df.head()\n",
      "----------------------------------------\n",
      "1852\n",
      "app_store_data = app_store_data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1', 'Unnamed: 0.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1', 'Unnamed: 0.1.1.1.1.1.1',\n",
      "=====\n",
      "'''jupyter_string'''\n",
      "app_store_data['user_rating'] = app_store_data['user_rating'].replace([3.5, 3, 2.5, 2, 1.5, 1, 0.5], 0)\n",
      "app_store_data['user_rating'] = app_store_data['user_rating'].replace([4, 4.5, 5], 1)\n",
      "app_store_data.head()\n",
      "----------------------------------------\n",
      "1874\n",
      "X = df[['Comm_Tax', 'Region', 'GDP_perCap', 'Elec_Access', 'WB_Code', 'ElecGen_OilGasCoal', 'GDP_Total', 'FossFuel_Cons', 'Imp_San_Access', 'Elec_Consumption', 'Features', 'Adult_Lit', 'School_Enrl', 'Life_Expect', 'Income_Group', 'Inf_Mortality', 'Tech_Exports', 'Inflation', 'Emissions', ''jupyter_string'Metal_exports', 'Corr_Perc_Index', 'Country', 'BusReg']]\n",
      "=====\n",
      "target = df['GDP_perCap']\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "----------------------------------------\n",
      "1875\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import VotingRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import\n",
      "=====\n",
      "rowsToUse = ['Corr_Perc_Index', 'School_Enrl', 'Adult_Lit','BusReg', 'Comm_Tax', 'Elec_Consumption',\n",
      "             'Emissions', 'FossFuel_Cons', 'Inf_Mortality', 'Inflation', 'OreMetal_exports', 'Imp_San_Access',\n",
      "             'Tech_Exports']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target = df['GDP_perCap']\n",
      "\n",
      "\n",
      "X = df.loc[:,rowsToUse]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,r in enumerate(results):\n",
    "    if not r[1]:\n",
    "        print(i)\n",
    "        print(r[0])\n",
    "        print('='*5)\n",
    "        print(r[2])\n",
    "        print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "016549f1-4ebf-4ec0-9eb6-9c1c0ce871dd",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-11T00:45:21.367357Z",
     "iopub.status.busy": "2021-08-11T00:45:21.367129Z",
     "iopub.status.idle": "2021-08-11T00:45:22.129671Z",
     "shell.execute_reply": "2021-08-11T00:45:22.129063Z",
     "shell.execute_reply.started": "2021-08-11T00:45:21.367333Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data3=pd.read_csv(jupyter_string) \n",
      "y=np.asarray(data3.iloc[:,-1]) \n",
      "X=np.asarray(data3.iloc[:,0:-1]) \n",
      "--------------------\n",
      "plt.plot(data.Year, data.Crime_Rate)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(data.index, data[jupyter_string])\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "print(rmse)\n",
      "=====\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "np.sqrt(mean_squared_error(y_test, y_pred))\n",
      "--------------------\n",
      "predict_df.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "submission_filename = jupyter_string\n",
      "predict_df.to_csv(submission_filename, index=False)\n",
      "--------------------\n",
      "train_data = pd.read_csv(jupyter_string)\n",
      "train_labels = pd.read_csv(jupyter_string)\n",
      "test_data = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "X_train = pd.read_csv(jupyter_string)\n",
      "X_train.week_start_date = pd.to_datetime(X_train.week_start_date)\n",
      "print(jupyter_string)\n",
      "\n",
      "y_train = pd.read_csv(jupyter_string, \n",
      "                      usecols=['total_cases' <<unk>>])\n",
      "print(jupyter_string)\n",
      "\n",
      "X_test = pd.read_csv(jupyter_string)\n",
      "X_test.week_start_date = pd.to_datetime(X_test.week_start_date)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "X_train.head()\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "y_train.head()\n",
      "=====\n",
      "y_train.head()\n",
      "--------------------\n",
      "submission_df = pd.read_csv(jupyter_string)\n",
      "submission_df.head()\n",
      "=====\n",
      "df1 = pd.read_csv(jupyter_string, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df2 = pd.read_csv(submission_filename, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df = pd.merge(df1, df2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df[(df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string])]\n",
      "--------------------\n",
      "X_train = pd.concat([X_train, y_train], axis=1)\n",
      "=====\n",
      "Xy_train = pd.concat([y_train, X_train], axis=1) \n",
      "print(jupyter_string)\n",
      "Xy_train.head()\n",
      "--------------------\n",
      "df1 = pd.read_csv(jupyter_string, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df2 = pd.read_csv(submission_filename, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df = pd.merge(df1, df2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df[(df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string])]\n",
      "=====\n",
      "f1 = pd.read_csv(jupyter_string)\n",
      "f2 = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.merge(f1, f2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "=====\n",
      "f2['total_cases' <<unk>>] = ((f1['total_cases' <<unk>>] + f2['total_cases' <<unk>>])/2).astype(int)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "f2.loc[f2['city' <<unk>>] == jupyter_string] = f1.loc[f1['city' <<unk>>] == jupyter_string]\n",
      "\n",
      "f2.tail()\n",
      "f2.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "plt.plot(data[jupyter_string], data[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(data[jupyter_string], data[jupyter_string])\n",
      "--------------------\n",
      "sj_corr = Xy_sj.corr()\n",
      "sj_corr.total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "--------------------\n",
      "Xy_iq.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_iq.corr().total_cases.sort_values(ascending=False)\n",
      "--------------------\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.total_cases.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.groupby(['weekofyear' <<unk>>]).total_cases.sum().plot(kind=jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_iq.total_cases.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "=====\n",
      "Xy_sj.reanalysis_dew_point_temp_k.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_sj.isnull().sum()\n",
      "=====\n",
      "X_train.info()\n",
      "--------------------\n",
      "X_train = X_train.reset_index(drop=True)\n",
      "=====\n",
      "X_train = X_train.reset_index()\n",
      "X_test = X_test.reset_index()\n",
      "--------------------\n",
      "X_test_sj = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "X_test_iq = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "\n",
      "y_test_sj = y_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "y_test_iq = y_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "X_test_sj = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "X_test_iq = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "X_train_sj = normalize(X_train_sj)\n",
      "X_train_iq = normalize(X_train_iq)\n",
      "X_test_sj = normalize(X_test_sj)\n",
      "X_test_iq = normalize(X_test_iq)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "features_to_normalize = features + new_features\n",
      "\n",
      "X_train_sj[features_to_normalize] = X_train_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_train_iq[features_to_normalize] = X_train_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_sj[features_to_normalize] = X_test_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_iq[features_to_normalize] = X_test_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "--------------------\n",
      "fluTrain = pd.read_csv(jupyter_string)\n",
      "fluTrain.head()\n",
      "=====\n",
      "fluTrain=pd.read_csv(jupyter_string,parse_dates=True)\n",
      "--------------------\n",
      "X_train_sj = X_train_sj.reset_index()\n",
      "X_train_iq = X_train_iq.reset_index()\n",
      "X_test_sj = X_test_sj.reset_index()\n",
      "X_test_iq = X_test_iq.reset_index()\n",
      "=====\n",
      "X_train = pd.concat([X_train_sj, X_train_iq], axis=0)\n",
      "X_train.set_index(jupyter_string, inplace=True)\n",
      "X_train.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "=====\n",
      "X_train_iq.head()\n",
      "--------------------\n",
      "test_df = pd.read_csv(jupyter_string)\n",
      "test_df.head()\n",
      "=====\n",
      "predict_sj = X_test_sj[keys].copy()\n",
      "predict_iq = X_test_iq[keys].copy()\n",
      "--------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "y_sj_pred_final = np.array([sum(x)/2.0 for x in zip(y_sj_pred, y_sj_pred_2) ])\n",
      "--------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "predict_sj['total_cases' <<unk>>] = y_sj_pred.round().astype(int)\n",
      "predict_sj.head()\n",
      "--------------------\n",
      "predict_sj.to_csv(jupyter_string, index=False)\n",
      "predict_iq.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "predict_df = pd.concat([predict_sj, predict_iq], axis=0)\n",
      "--------------------\n",
      "nike.head()\n",
      "=====\n",
      "nike.head()\n",
      "--------------------\n",
      "spy.head()\n",
      "=====\n",
      "spy.head()\n",
      "--------------------\n",
      "fluTrain.head()\n",
      "=====\n",
      "fluTrain.head()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].pct_change()\n",
      "\n",
      "\n",
      "df.head()\n",
      "=====\n",
      "returns = df.pct_change()\n",
      "\n",
      "\n",
      "returns.head()\n",
      "--------------------\n",
      "returns = returns.dropna()\n",
      "\n",
      "\n",
      "returns.head()\n",
      "=====\n",
      "returns = returns.dropna(axis = 0)\n",
      "\n",
      "\n",
      "returns.head()\n",
      "--------------------\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "model = smf.ols(formula = jupyter_string, data = returns)\n",
      "results = model.fit()\n",
      "print(results.summary())\n",
      "=====\n",
      "y = returns[jupyter_string]\n",
      "x = returns[jupyter_string]\n",
      "x1 = sm.add_constant(x)\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(x1[:5]) \n",
      "--------------------\n",
      "model = sm.OLS(y, x1)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "linreg = sm.OLS(y, x1)\n",
      "\n",
      "\n",
      "results = linreg.fit()\n",
      "\n",
      "\n",
      "results.summary()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "sns.set_palette(jupyter_string)\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "mental = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "mental.head()\n",
      "=====\n",
      "print(type(mental))\n",
      "mental.head()\n",
      "--------------------\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string], jupyter_string)\n",
      "--------------------\n",
      "employee_health.describe()\n",
      "=====\n",
      "female_health = employee_health.apply(len)\n",
      "\n",
      "\n",
      "print(female_health)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + jupyter_string + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "fluTrain.ILI.hist()\n",
      "=====\n",
      "fluTrain.ILI.hist()\n",
      "--------------------\n",
      "employee_work_locations = work_locations.size()\n",
      "\n",
      "\n",
      "print(employee_work_locations)\n",
      "=====\n",
      "work_space = work_locations.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(work_space)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + str(126 +146) + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "work_locations[jupyter_string].value_counts()\n",
      "=====\n",
      "disorders = {}\n",
      "\n",
      "disorderCounts = dict(mental['If yes, what condition(s) have you been diagnosed with?' <<unk>>].value_counts())\n",
      "for i in disorderCounts:\n",
      "    \n",
      "    disorderList = i.split(jupyter_string)\n",
      "    for j in disorderList:\n",
      "        j = j.split(jupyter_string)[0]\n",
      "        disorders[j] = disorders.get(j, 0) + disorderCounts[i]\n",
      "\n",
      "tmp = pd.DataFrame()\n",
      "for i in disorders:\n",
      "    tmp = tmp.append([i] * disorders[i])\n",
      "    \n",
      "\n",
      "tmp.rename(columns={ tmp.columns[0]: jupyter_string }, inplace= True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tmp.Disorder.value_counts()\n",
      "\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "family_health.describe()\n",
      "=====\n",
      "health_history = family_health.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(health_history)\n",
      "print()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + jupyter_string + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "mental_by_state = mental_by_state.mean()\n",
      "mental_by_state.head()\n",
      "=====\n",
      "mental_state = mental_by_state.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(type(mental_state))\n",
      "print(mental_state.nlargest(10))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "data = pd.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "=====\n",
      "loans_one_hot_enc = pd.get_dummies(loans)\n",
      "--------------------\n",
      "plt.scatter(np.log(df.ILI), np.log(df.Queries))\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import math\n",
      "fluTrain[jupyter_string]= fluTrain.ILI.apply(lambda x:math.log(x))\n",
      "plt.scatter(fluTrain.Queries, fluTrain.ILI_aslog)\n",
      "plt.xlabel(\"Queries\")\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "import json\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn, sklearn.tree\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "loans = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "loans = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_df = pd.DataFrame(train_idx_lst, columns=[jupyter_string])\n",
      "validation_df = pd.DataFrame(validation_idx_lst, columns=[jupyter_string])\n",
      "=====\n",
      "train_data = loans_one_hot_enc.ix[train_idx_lst]\n",
      "validation_data = loans_one_hot_enc.ix[validation_idx_lst]\n",
      "--------------------\n",
      "loans.head()\n",
      "=====\n",
      "loans.head()\n",
      "--------------------\n",
      "predictions = small_model.predict(validation_data.ix[:, validation_data.columns != jupyter_string])\n",
      "predictions\n",
      "=====\n",
      "validation_safe_loans = validation_data[validation_data[target] == 1]\n",
      "validation_risky_loans = validation_data[validation_data[target] == -1]\n",
      "\n",
      "sample_validation_data_risky = validation_risky_loans[0:2]\n",
      "sample_validation_data_safe = validation_safe_loans[0:2]\n",
      "\n",
      "sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)\n",
      "sample_validation_data\n",
      "--------------------\n",
      "sample_validation_data.head()\n",
      "=====\n",
      "samp_vald_data_prob = decision_tree_model.predict_proba(sample_validation_data.ix[:, sample_validation_data.columns != jupyter_string])[:,1]\n",
      "--------------------\n",
      "plt.scatter(fluTrain.Queries, np.log(fluTrain.ILI_aslog))\n",
      "plt.xlabel(\"Queries\")\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(fluTrain.Queries, np.log(fluTrain.ILI))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "loans_data = pd.read_csv(jupyter_string)\n",
      "loans_data.info()\n",
      "=====\n",
      "features = ['grade' <<unk>>,                     \n",
      "            'sub_grade' <<unk>>,                 \n",
      "            'short_emp' <<unk>>,                 \n",
      "            'emp_length_num' <<unk>>,            \n",
      "            'home_ownership' <<unk>>,            \n",
      "            'dti' <<unk>>,                       \n",
      "            'purpose' <<unk>>,                   \n",
      "            'term' <<unk>>,                      \n",
      "            'last_delinq_none' <<unk>>,          \n",
      "            'last_major_derog_none' <<unk>>,     \n",
      "            'revol_util' <<unk>>,                \n",
      "            'total_rec_late_fee' <<unk>>,        \n",
      "           ]\n",
      "\n",
      "target = jupyter_string                   \n",
      "\n",
      "\n",
      "loans = loans[features + [target]]\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "b_train = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string.format(b_train.shape))\n",
      "\n",
      "\n",
      "--------------------\n",
      "b_train.head()\n",
      "=====\n",
      "b_test = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string.format(b_test.shape))\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "print(jupyter_string.format(X_train.shape, y_train.shape))\n",
      "print(jupyter_string.format(X_test.shape, y_test.shape))\n",
      "=====\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "y_test_transformed = np.hstack((1 - y_test.reshape(y_test.size,1),\n",
      "                                y_test.reshape(y_test.size,1)))\n",
      "--------------------\n",
      "xgb_grid_search = GridSearchCV(XGB_Classifier, xgb_param_grid, cv=CV_SSS, scoring=jupyter_string)\n",
      "xgb_grid_search.fit(X_train, y_train)\n",
      "=====\n",
      "random_grid_XGB_CV = RandomizedSearchCV(XGB_Classifier, xgb_param_grid, scoring = jupyter_string, cv = CV_SSS,\n",
      "                                        n_iter = 40)\n",
      "random_grid_XGB_CV.fit(X,y)\n",
      "print(random_grid_XGB_CV.best_score_)\n",
      "print(random_grid_XGB_CV.best_params_)\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test_transformed, y_score)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.grid(True)\n",
      "=====\n",
      "fpr = dict()\n",
      "tpr = dict()\n",
      "roc_auc = dict()\n",
      "for i in range(2):\n",
      "    fpr[i], tpr[i], _ = roc_curve(y_test_transformed[:, i], y_score[:, i])\n",
      "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "lw = 2\n",
      "\n",
      "plt.plot(fpr[1], tpr[1], color=jupyter_string,\n",
      "         lw=lw, label=jupyter_string % roc_auc[1])\n",
      "plt.plot([0, 1], [0, 1], color=jupyter_string, lw=lw, linestyle=jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "submission = pd.DataFrame({\n",
      "        \"PassengerId\": test_df.PassengerId,\n",
      "        \"Survived\": XGB_Classifier.predict_proba(test_df)[:,1]\n",
      "    })\n",
      "submission.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "XGB_Classifier.fit(X, y)\n",
      "submission_prediction = XGB_Classifier.predict_proba(X_submission)[:,1]\n",
      "print(submission_prediction)\n",
      "--------------------\n",
      "model1.fit().summary()\n",
      "=====\n",
      "fitted=model1.fit()\n",
      "\n",
      "fitted.fittedvalues\n",
      "--------------------\n",
      "b_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "b_test[['bidder_id' <<unk>>,jupyter_string]].to_csv(jupyter_string, sep=jupyter_string, header=True, index=False)\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test, bst.predict_proba(test_data)[:,1])\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "test_data = xgb.DMatrix(X_test)\n",
      "y_score = bst.predict(test_data)\n",
      "y_score = np.hstack((1 - y_score.reshape(y_score.size,1),\n",
      "                     y_score.reshape(y_score.size,1)))\n",
      "\n",
      "fpr = {}\n",
      "tpr = {}\n",
      "roc_auc = {}\n",
      "for i in range(2):\n",
      "    fpr[i], tpr[i], _ = roc_curve(y_test_transformed[:, i], y_score[:, i])\n",
      "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
      "\n",
      "plt.figure()\n",
      "lw = 2\n",
      "plt.plot(fpr[1], tpr[1], color=jupyter_string,\n",
      "         lw=lw, label=jupyter_string % roc_auc[1])\n",
      "plt.plot([0, 1], [0, 1], color=jupyter_string, lw=lw, linestyle=jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "b_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "b_test[['bidder_id' <<unk>>,jupyter_string]].to_csv(jupyter_string, sep=jupyter_string, header=True, index=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "ax = sns.barplot(\n",
      "    movies_and_budget.sort_values(by = \"budget_adj\", ascending=False).head(10).original_title, \n",
      "    movies_and_budget.sort_values(by = \"budget_adj\", ascending=False).head(10).budget_adj)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_budget.sort_values(by=\"budget_adj\", ascending=False).head(10).original_title, \n",
      "    movies_and_budget.sort_values(by=\"budget_adj\", ascending=False).head(10).budget_adj)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = sns.barplot(\n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).original_title, \n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).popularity)\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).original_title, \n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).popularity)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel='popularity' <<unk>>, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "data.head(10)\n",
      "=====\n",
      "data.head(7)\n",
      "--------------------\n",
      "plt.scatter(fitted.fittedvalues, fitted.resid)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fluTrain.corr()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head(7)\n",
      "=====\n",
      "data = data[data[\"cast\"].isnull() == False]\n",
      "data = data[data[\"genres\"].isnull() == False]\n",
      "\n",
      "data = data[data.budget_adj != 0]\n",
      "data = data[data.revenue_adj != 0]\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "data.describe()\n",
      "--------------------\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"rating\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.distplot(temp_df.rating)\n",
      "\n",
      "ax = sns.boxplot(x = temp_df.rating)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"release_year\", \"vote_average\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.violinplot(x = temp_df.vote_average, y = temp_df.release_year, orient =jupyter_string)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df.cast.value_counts().head(10)\n",
      "=====\n",
      "actor_dict = {}\n",
      "\n",
      "actors = data[\"cast\"]\n",
      "actors = actors.str.split(jupyter_string)\n",
      "actors = np.array(actors)\n",
      "for actorList in actors:\n",
      "    \n",
      "    for actor in actorList:\n",
      "        actor = actor.lstrip() \n",
      "        if actor not in actor_dict:\n",
      "            actor_dict[actor] = 1\n",
      "        else:\n",
      "            actor_dict[actor] += 1\n",
      "                \n",
      "\n",
      "\n",
      "sorted_actor_dict = sorted(actor_dict.items(), key = operator.itemgetter(1), reverse = True)\n",
      "\n",
      "\n",
      "\n",
      "x_axis = list()\n",
      "y_axis = list()\n",
      "\n",
      "for item in sorted_actor_dict[0:20]:\n",
      "    x_axis.append(item[0])\n",
      "    y_axis.append(item[1])\n",
      "\n",
      "\n",
      "sns.set(rc={jupyter_string:(12,10)}, font_scale=1.4)\n",
      "ax = sns.barplot(x_axis, y_axis, palette=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"adjusted_revenue\", \"adjusted_budget\", \"popularity\", \"vote_average\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.regplot(x = \"adjusted_revenue\", y = \"adjusted_budget\", data = temp_df)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "aux_df = data[['revenue_adj' <<unk>>, 'budget_adj' <<unk>>, 'popularity' <<unk>>, 'vote_average' <<unk>>]]\n",
      "\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3, style=jupyter_string)\n",
      "\n",
      "\n",
      "f1 = sns.jointplot(x = \"budget_adj\", y = \"revenue_adj\", kind = jupyter_string, data = aux_df)\n",
      "f1.fig.suptitle(jupyter_string)\n",
      "\n",
      "f2 = sns.jointplot(x = \"budget_adj\", y = \"popularity\", kind = jupyter_string, data = aux_df)\n",
      "f2.fig.suptitle(jupyter_string)\n",
      "f3 = sns.jointplot(x = \"budget_adj\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f3.fig.suptitle(jupyter_string)\n",
      "\n",
      "f4 = sns.jointplot(x = \"revenue_adj\", y = \"popularity\", kind = jupyter_string, data = aux_df)\n",
      "f4.fig.suptitle(jupyter_string)\n",
      "f5 = sns.jointplot(x = \"revenue_adj\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f5.fig.suptitle(jupyter_string)\n",
      "\n",
      "f6 = sns.jointplot(x = \"popularity\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f6.fig.suptitle(jupyter_string)\n",
      "--------------------\n",
      "df_genres = pd.DataFrame(df.genres.str.split(jupyter_string).tolist(), columns=[jupyter_string, jupyter_string])\n",
      "df_genres = df_genres.set_index(jupyter_string)\n",
      "df_genres.head()\n",
      "=====\n",
      "year_set = set()\n",
      "genre_set = set()\n",
      "genres_and_year = data[[\"genres\", \"release_year\"]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "production_year = genres_and_year[\"release_year\"]\n",
      "production_year = production_year.drop_duplicates()\n",
      "for year in production_year:\n",
      "    if year not in year_set:\n",
      "        year_set.add(year)\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split(jupyter_string)\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_set:\n",
      "                genre_set.add(genre)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gerne_count_per_year_df = pd.DataFrame(index = year_set, columns=genre_set)\n",
      "gerne_count_per_year_df[:] = 0  \n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split(jupyter_string)\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_dict:\n",
      "                genre_dict[genre] = 1\n",
      "            else:\n",
      "                genre_dict[genre] = genre_dict[genre] + 1\n",
      "                    \n",
      "    aux_df = pd.DataFrame(genre_dict, index = [year])\n",
      "    gerne_count_per_year_df.loc[year, aux_df.columns] = gerne_count_per_year_df.loc[year, aux_df.columns] + aux_df.loc[year]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "most_popular_genre_by_year = pd.DataFrame([gerne_count_per_year_df.idxmax(axis = 1).values,\n",
      "                                          gerne_count_per_year_df.apply( max, axis=1 ).values],\n",
      "                                          columns = gerne_count_per_year_df.index,\n",
      "                                         index = [jupyter_string, jupyter_string])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.bar(most_popular_genre_by_year.index, most_popular_genre_by_year[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.bar(most_popular_genre_by_year.index, most_popular_genre_by_year[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,12)}, font_scale=1.3)\n",
      "sns.set_palette(jupyter_string, 20, .65)\n",
      "\n",
      "\n",
      "ax = gerne_count_per_year_df.plot.bar(stacked=True);\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "ax = gerne_count_per_year_df.plot.area(stacked=True);\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_test.head()\n",
      "=====\n",
      "test=pd.read_csv(jupyter_string,parse_dates=True)\n",
      "test.head()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test[jupyter_string]=np.exp(fitted.predict(test))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "query = input(jupyter_string)\n",
      "greet = check_for_greeting(query)\n",
      "while greet != jupyter_string:\n",
      "    new_query = greet + jupyter_string\n",
      "    query = input(new_query)\n",
      "    greet = check_for_greeting(query)\n",
      "question = jupyter_string.join(l for l in query if l not in string.punctuation)\n",
      "city = get_location(question)\n",
      "day_of_week = get_day_of_week(question)\n",
      "weather, time, city = get_weather(city,day_of_week)\n",
      "if city == None:\n",
      "    print(jupyter_string)\n",
      "else:   \n",
      "    output = get_output(query)\n",
      "    \n",
      "    weather_data = output\n",
      "weather_data\n",
      "--------------------\n",
      "surveys = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "surveys_df.isnull()\n",
      "=====\n",
      "pd.isnull(surveys_df)\n",
      "--------------------\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "surveys_df.head()\n",
      "=====\n",
      "true_copy_surveys_df = surveys_df.copy()\n",
      "\n",
      "\n",
      "ref_surveys_df = surveys_df\n",
      "--------------------\n",
      "nosex_df.head()\n",
      "=====\n",
      "sex_df = surveys_df[surveys_df['sex' madeupword0002].isin([jupyter_string, jupyter_string]) \\\n",
      "                    & surveys_df['weight' <<unk>>]]\n",
      "--------------------\n",
      "nosex_df.head()\n",
      "=====\n",
      "grouped = sex_df.groupby(['plot_id' <<unk>>, 'sex' madeupword0002])['weight' <<unk>>]\n",
      "grouped.mean().unstack().plot(kind=jupyter_string, stacked=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "print(len(data3))\n",
      "data3.head()\n",
      "--------------------\n",
      "np.sqrt(mean_squared_error(test.ILI,test.predict_test))\n",
      "=====\n",
      "SSE=((test.predict_test - test.ILI)**2).sum()\n",
      "nrow,ncol=test.shape\n",
      "RMSE=math.sqrt(SSE/nrow)\n",
      "RMSE\n",
      "--------------------\n",
      "surveys_df.head()\n",
      "=====\n",
      "ref_surveys_df.head()\n",
      "--------------------\n",
      "surveys_df.head()\n",
      "=====\n",
      "surveys_df.head()\n",
      "--------------------\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "surveys_df.head()\n",
      "=====\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "lens.groupby(jupyter_string)[jupyter_string].count().sort_values(ascending=False)[:25]\n",
      "=====\n",
      "most_rated=lens.groupby(jupyter_string).size().sort_values(ascending=False)[:25]\n",
      "most_rated\n",
      "--------------------\n",
      "movie_stats.sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "movie_stats.sort_values([(jupyter_string,jupyter_string)],ascending=False).head()\n",
      "--------------------\n",
      "atleast_100=movie_stats[jupyter_string][jupyter_string]>=100\n",
      "movie_stats[atleast_100].sort_values([(jupyter_string,jupyter_string)],ascending=False)[:15]\n",
      "=====\n",
      "movie_stats.sort_values([(jupyter_string,jupyter_string)],ascending=False).head()\n",
      "atleast_100=movie_stats[jupyter_string][jupyter_string]>=100\n",
      "movie_stats[atleast_100].sort_values([(jupyter_string,jupyter_string)],ascending=False)[:15]\n",
      "--------------------\n",
      "most_rated_movies=movie_stats[jupyter_string][jupyter_string]\n",
      "most_rated_movies.head()\n",
      "=====\n",
      "most_50=lens.groupby(jupyter_string).size().sort_values(ascending=False)[:50]\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "users.age.plot.hist(bins=30)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "--------------------\n",
      "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "age_labels = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "users[jupyter_string] = pd.cut(users.age, bins=age_bins, labels=age_labels)\n",
      "=====\n",
      "labels = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "lens[jupyter_string] = pd.cut(lens.age, range(0, 81, 10), right=False, labels=labels)\n",
      "lens[[jupyter_string, jupyter_string]].drop_duplicates()[:10]\n",
      "--------------------\n",
      "train=data.iloc[0:train.shape[0],:]\n",
      "test=data.iloc[train.shape[0]:,:]\n",
      "=====\n",
      "fluTrain[jupyter_string]=fluTrain.ILI.shift(2)\n",
      "fluTrain.ILI_lag2[0:10]\n",
      "--------------------\n",
      "lens[[jupyter_string, jupyter_string]].drop_duplicates()\n",
      "=====\n",
      "lens.groupby(jupyter_string).agg({jupyter_string: [np.size, np.mean]})\n",
      "\n",
      "--------------------\n",
      "lens.groupby(jupyter_string).agg({jupyter_string: [np.size, np.mean]})\n",
      "=====\n",
      "lens.set_index(jupyter_string, inplace=True)\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_pca = pca.fit_transform(X)\n",
      "plt.scatter(X_pca[:,0], X_pca[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(1, 7)\n",
      "plt.plot(x, np.cumsum(pca.explained_variance_ratio_), jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.show()\n",
      "--------------------\n",
      "pca = PCA(n_components=3)\n",
      "pca.fit(X)\n",
      "X_pca = pca.transform(X)\n",
      "=====\n",
      "first_pc = pca.components_[0]\n",
      "second_pc = pca.components_[1]\n",
      "\n",
      "transformed_data = pca.transform(data)\n",
      "plt.close()\n",
      "for ii in transformed_data:\n",
      "    plt.scatter(first_pc[0]*ii[0], first_pc[1]*ii[0], color=jupyter_string)\n",
      "    plt.scatter(second_pc[0]*ii[1], second_pc[1]*ii[1], color=jupyter_string)\n",
      "    plt.scatter(ii[0], ii[1], color=jupyter_string)\n",
      "    \n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(6,9))\n",
      "\n",
      "labels = [ujupyter_string, ujupyter_string, ujupyter_string]\n",
      "\n",
      "sizes = [2698,464,8]\n",
      "colors = [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "explode = (0.05,0,0)\n",
      "\n",
      "patches,l_text,p_text = plt.pie(sizes,explode=explode,labels=labels,colors=colors,\n",
      "                                labeldistance = 1.1,autopct = jupyter_string,shadow = False,\n",
      "                                startangle = 90,pctdistance = 0.6)\n",
      "\n",
      "for t in l_text:\n",
      "    t.set_size=(30)\n",
      "for t in p_text:\n",
      "    t.set_size=(20)\n",
      "\n",
      "plt.axis(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "dcounty.plot(kind=jupyter_string,stacked=True, color=[jupyter_string,jupyter_string])\n",
      "plt.show() \n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "data = pd.read_csv(jupyter_string)\n",
      "print(data.head())\n",
      "--------------------\n",
      "df.plot(kind=jupyter_string,stacked=True, color=[jupyter_string,jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.factorplot('SocioecStatus' <<unk>>,data=data,palette=jupyter_string, kind=jupyter_string,hue=jupyter_string, size=8)\n",
      "plt.show\n",
      "--------------------\n",
      "fluTrain[jupyter_string]=np.log(fluTrain.ILI_lag2)\n",
      "fluTrain[jupyter_string]=np.log(fluTrain.IL1)\n",
      "fluTrain.head()\n",
      "=====\n",
      "plt.scatter(np.log(fluTrain.ILI_lag2),np.log(fluTrain.ILI))\n",
      "plt.xlabel=\"ILI\"\n",
      "plt.ylabel=jupyter_string\n",
      "plt.show()\n",
      "--------------------\n",
      "data[jupyter_string].value_counts().plot(kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data1=dataage.loc[~dataage.Age_estcareer]\n",
      "print(data1.shape[0], jupyter_string)\n",
      "data2=dataage.loc[~dataage.Age_estchild]\n",
      "print(data2.shape[0], jupyter_string)\n",
      "--------------------\n",
      "dmarital=pd.crosstab(data.Married,data.Sex, margins=True)\n",
      "dmarital.sort_values(by=jupyter_string, inplace=True)\n",
      "print(dmarital)\n",
      "=====\n",
      "dmaritalstatus=pd.crosstab(data.MaritalStatus,data.Sex, margins=True)\n",
      "dmaritalstatus.sort_values(by=jupyter_string, inplace=True)\n",
      "print(dmaritalstatus)\n",
      "--------------------\n",
      "firstname=pd.crosstab(data.FirstName,data.Sex, margins=True)\n",
      "firstname.sort_values(by=jupyter_string, inplace=True)\n",
      "print(firstname)\n",
      "=====\n",
      "count=data[\"FirstName\"].value_counts()\n",
      "print(count)\n",
      "--------------------\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "decomposition = seasonal_decompose(week_timeAnalysisKSSuccessRate)\n",
      "\n",
      "trend = decomposition.trend\n",
      "seasonal = decomposition.seasonal\n",
      "residual = decomposition.resid\n",
      "\n",
      "plt.subplot(411)\n",
      "plt.plot(week_timeAnalysisKSSuccessRate, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(412)\n",
      "plt.plot(trend, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(413)\n",
      "plt.plot(seasonal,label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(414)\n",
      "plt.plot(residual, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.tight_layout()\n",
      "=====\n",
      "week_timeAnalysisKS_trend = week_timeAnalysisKS[[jupyter_string]].rolling(60).mean()\n",
      "\n",
      "\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_trend.index,\n",
      "          y=week_timeAnalysisKS_trend.countProj)]\n",
      "\n",
      "fig = dict(data=trend, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "--------------------\n",
      "week_timeAnalysisKS_trend = week_timeAnalysisKS[[jupyter_string,jupyter_string]].rolling(60).mean()\n",
      "\n",
      "\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_trend.index,\n",
      "          y=week_timeAnalysisKS_trend.countProj)]\n",
      "\n",
      "fig = dict(data=trend, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS_trend_success = week_timeAnalysisKSSuccess[[jupyter_string]].rolling(60).mean()\n",
      "week_timeAnalysisKS_trend_fail = week_timeAnalysisKSFail[[jupyter_string]].rolling(60).mean()\n",
      "\n",
      "week_timeAnalysisKS_trend_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_trend_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "ks_raw=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "=====\n",
      "FluTrend2=smf.ols(formula=jupyter_string,data=fluTrain)\n",
      "fitted2=FluTrend2.fit()\n",
      "fitted2.summary()\n",
      "--------------------\n",
      "week_timeAnalysisKS_trend_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_trend_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "=====\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS.index,\n",
      "          y=week_timeAnalysisKS.countProj)]\n",
      "\n",
      "\n",
      "week_timeAnalysisKS_seasonality = week_timeAnalysisKS[[jupyter_string]].diff()\n",
      "seasonality = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_seasonality.index,\n",
      "          y=week_timeAnalysisKS_seasonality.countProj)]\n",
      "\n",
      "fig = dict(data=seasonality, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "--------------------\n",
      "seasonality = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_seasonality.index,\n",
      "          y=week_timeAnalysisKS_seasonality.countProj)]\n",
      "\n",
      "fig = dict(data=seasonality, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS_seasonality_success = week_timeAnalysisKSSuccess[[jupyter_string]].diff()\n",
      "week_timeAnalysisKS_seasonality_fail = week_timeAnalysisKSFail[[jupyter_string]].diff()\n",
      "\n",
      "week_timeAnalysisKS_seasonality_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_seasonality_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "ks[\"launched\"] = pd.to_datetime(ks[\"launched\"])\n",
      "ks[\"deadline\"] = pd.to_datetime(ks[\"deadline\"])\n",
      "=====\n",
      "ks[\"launched\"] = pd.to_datetime(ks[\"launched\"], infer_datetime_format=True)\n",
      "ks[\"deadline\"] = pd.to_datetime(ks[\"deadline\"], infer_datetime_format=True)\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "closed=ks[\"deadline\"]<datetime.now()\n",
      "closed.value_counts()\n",
      "--------------------\n",
      "import pandas_profiling\n",
      "pandas_profiling.ProfileReport(ks)\n",
      "=====\n",
      "pandas_profiling.ProfileReport(ks)\n",
      "--------------------\n",
      "ks.info()\n",
      "=====\n",
      "ks.describe()\n",
      "--------------------\n",
      "ks.corr()\n",
      "=====\n",
      "ks = ks[(ks[\"launched\"].dt.year > 1970)]\n",
      "--------------------\n",
      "countries = pd.read_csv(jupyter_string)\n",
      "countries.head()\n",
      "=====\n",
      "country_mapping=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_test.head()\n",
      "=====\n",
      "test[jupyter_string]=test.ILI.shift(2)\n",
      "test.ILI_lag2.head()\n",
      "--------------------\n",
      "ks = ks.dropna()\n",
      "print(ks.shape)\n",
      "=====\n",
      "ks = ks[(ks[jupyter_string].notna())]\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "ks.describe()\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "ks.head()\n",
      "--------------------\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "=====\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "--------------------\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "=====\n",
      "ks.state=ks[\"state\"].replace({jupyter_string:jupyter_string})\n",
      "\n",
      "--------------------\n",
      "ks.state.value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "ax1=sns.countplot(x=\"state\",data=ks, ax=ax,palette=jupyter_string)\n",
      "\n",
      "for p in ax1.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+3000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "ax1=sns.countplot(x=\"main_category\",data=ks, palette=jupyter_string)\n",
      "\n",
      "for p in ax1.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+3000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax2=sns.countplot(x=\"main_category\",data=ks, ax=ax, order = ks['main_category' <<unk>>].value_counts().index)\n",
      "\n",
      "for p in ax2.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+1000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "--------------------\n",
      "df.groupby(\"main_category\")[\"backers\"].sum()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ksSortbyBacker = ks.groupby([\"main_category\"])['backers' <<unk>>].mean().reset_index().sort_values(\"backers\", ascending=False)\n",
      "ax2=sns.barplot(y=\"main_category\",x=\"backers\",data=ks,ci=None, ax=ax,order=ksSortbyBacker['main_category' <<unk>>])\n",
      "\n",
      "for p in ax2.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "\n",
      "\n",
      "--------------------\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-1)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-2)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-3)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-4)\n",
      "=====\n",
      "test.head()\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "sns.countplot(x=\"main_category\",hue=\"state\",data=ks, ax=ax, palette=jupyter_string)\n",
      "=====\n",
      "mainCategory_byState = ks.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "success_rate_perCategory= mainCategory_byState.successful / (mainCategory_byState.successful+ mainCategory_byState.failed)\n",
      "success_rate_perCategory=success_rate_perCategory.sort_values(ascending=True)\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ax7=success_rate_perCategory.plot(kind=jupyter_string,ax=ax)\n",
      "for p in ax7.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width), va=jupyter_string)\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax5=sns.countplot(y=jupyter_string,data=ks,order = ks[jupyter_string].value_counts().index)\n",
      "\n",
      "\n",
      "for p in ax5.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "=====\n",
      "Country_byState = ks.groupby(jupyter_string).state.value_counts().unstack()\n",
      "success_rate_perCountry= Country_byState.successful / (Country_byState.successful + Country_byState.failed)\n",
      "success_rate_perCountry=success_rate_perCountry.sort_values(ascending=True)\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ax1=success_rate_perCountry.plot(kind=jupyter_string,ax=ax)\n",
      "for p in ax1.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width), va=jupyter_string)\n",
      "--------------------\n",
      "ks.goal_per_day = ks.goal_per_day.astype(float)\n",
      "ks.us_real_goal = ks.us_real_goal.astype(float)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "B= plt.boxplot(ks.goal_per_day, sym=jupyter_string)\n",
      "\n",
      "print([item.get_ydata() for item in B[jupyter_string]])\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "B= plt.boxplot(ks.goal_per_day, sym=jupyter_string)\n",
      "\n",
      "print([item.get_ydata() for item in B[jupyter_string]])\n",
      "=====\n",
      "ks[[\"state\", jupyter_string]].groupby(\"state\").describe()\n",
      "--------------------\n",
      "ks[ks.goal_per_day < 70000].state.value_counts()\n",
      "=====\n",
      "PrjOver70k = ks[(ks[jupyter_string]>70000)].groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "print(PrjOver70k)\n",
      "--------------------\n",
      "plt.figure(figsize=(20,20)) \n",
      "\n",
      "sns.boxplot(x=\"usd_goal_real\", y=jupyter_string,data=ks, palette=jupyter_string,  hue=\"state\")\n",
      "=====\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "ks1 = ks[[jupyter_string,'usd_goal_real' <<unk>>, jupyter_string]]\n",
      "\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[\"usd_goal_real\"]])\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "--------------------\n",
      "fluTest.fillna(method=jupyter_string,inplace=True)\n",
      "=====\n",
      "fluTrain.tail()\n",
      "--------------------\n",
      "ks1 = ks1[ks1.usd_goal_real != 0]\n",
      "ks1 = ks1[ks1.goal_per_day != 0]\n",
      "ks1 = ks1[ks1.project_length != 0]\n",
      "ks1 = ks1[ks1.usd_goal_real != 0]\n",
      "=====\n",
      "mainKS=ks[((ks[jupyter_string]<=1200) & (ks[jupyter_string]<=90))&(ks[\"usd_goal_real\"]<=38000) &(ks[jupyter_string]>0.0002)& (ks[jupyter_string]>=5) & (ks[\"usd_goal_real\"]>0.01)]\n",
      "ksUpperOutliers=ks[((ks[jupyter_string]>1200)| (ks[jupyter_string]>90) | (ks[\"usd_goal_real\"]>38000))]\n",
      "ksLowerOutliers=ks[(ks[jupyter_string]<=0.0002)| (ks[jupyter_string]<5)| (ks[\"usd_goal_real\"]<=0.01)]\n",
      "\n",
      "print(jupyter_string, len(ks.index))\n",
      "print(jupyter_string, len(mainKS.index))\n",
      "print(jupyter_string, len(ksUpperOutliers.index))\n",
      "print(jupyter_string, len(ksLowerOutliers.index))\n",
      "\n",
      "--------------------\n",
      "mainKS.describe()\n",
      "=====\n",
      "mainKS.describe()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.rcParams[jupyter_string] = True\n",
      "sns.distplot(mainKS.goal_per_main, bins=80, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(y=jupyter_string,data=mainKS)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"country\", y=jupyter_string,data=mainKS) \n",
      "=====\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string,data=mainKS)\n",
      "plt.xticks(rotation=90)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string,data=mainKS)\n",
      "plt.xticks(rotation=90)\n",
      "=====\n",
      "mainKS[[\"main_category\", jupyter_string]].groupby(\"main_category\").describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "--------------------\n",
      "mainKS[[\"country\", jupyter_string]].groupby(\"country\").describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "=====\n",
      "mainKS[[jupyter_string, jupyter_string]].groupby(jupyter_string).describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "--------------------\n",
      "ksUpperOutliers[[jupyter_string, jupyter_string]].groupby(jupyter_string).describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "=====\n",
      "a = ksLowerOutliers.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "ksLowerOutliers_successrate_percat= a.successful / (a.successful + a.failed)\n",
      "ksLowerOutliers_successrate_percat=ksLowerOutliers_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "b = mainKS.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "mainKS_successrate_percat= b.successful / (b.successful  + b.failed)\n",
      "mainKS_successrate_percat=mainKS_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "c = ksUpperOutliers.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "ksUpperOutliers_successrate_percat= c.successful / (c.successful  + c.failed)\n",
      "ksUpperOutliers_successrate_percat=ksUpperOutliers_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "plt.figure(figsize=(25,10))\n",
      "plt.subplot(1,3,1)\n",
      "ksLowerOutliers_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "mainKS_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "ksUpperOutliers_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(25,10))\n",
      "plt.subplot(1,3,1)\n",
      "ksLowerOutliers.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "mainKS.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "ksUpperOutliers.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"category\", y=jupyter_string,data=mainKS)\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.set(style=jupyter_string)\n",
      "ax=sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "\n",
      "handles, _ = ax.get_legend_handles_labels()\n",
      "ax.legend(handles, [jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "fluTrain.info()\n",
      "=====\n",
      "test.ILI_lag2[0]=fluTrain.ILI[415]\n",
      "test.ILI_lag2[1]=fluTrain.ILI[416]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.set(style=jupyter_string)\n",
      "ax=sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "\n",
      "handles, _ = ax.get_legend_handles_labels()\n",
      "ax.legend(handles, [jupyter_string, jupyter_string])\n",
      "=====\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.distplot(mainKS.project_length, bins=25, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.distplot(mainKS.project_status, bins=25, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "=====\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True, label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True,label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True, label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True,label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "=====\n",
      "plt.figure(figsize=(15,8))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "sns.boxplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "plt.subplot(1,2,2)\n",
      "sns.violinplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "=====\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.set(style=jupyter_string)\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "sns.despine(offset=10, trim=True)\n",
      "\n",
      "--------------------\n",
      "Bottom5=ks.sort_values(by=jupyter_string)\n",
      "Bottom5.head()\n",
      "=====\n",
      "Bottom5=ks.sort_values(by=jupyter_string,ascending=False)\n",
      "Bottom5.tail()\n",
      "--------------------\n",
      "mainKS.head()\n",
      "=====\n",
      "timeAnalysisKS=mainKS.groupby(\"launched\")[\"usd_pledged_real\",jupyter_string, \"backers\"].sum()\n",
      "\n",
      "timeAnalysisKSSuccess=mainKS[(mainKS[\"state\"]==jupyter_string)].groupby(\"launched\")[\"usd_pledged_real\",jupyter_string,\"backers\"].sum()\n",
      "timeAnalysisKSFail=mainKS[(mainKS[\"state\"]==jupyter_string)].groupby(\"launched\")[\"usd_pledged_real\",jupyter_string,\"backers\"].sum()\n",
      "--------------------\n",
      "timeAnalysisKSSuccess.resample(jupyter_string).countProj.plot(title=jupyter_string, figsize=(30,20), label=jupyter_string)\n",
      "timeAnalysisKSFail.resample(jupyter_string).countProj.plot(title=jupyter_string, figsize=(30,20), label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS = pd.DataFrame()\n",
      "week_timeAnalysisKS[jupyter_string] = timeAnalysisKS.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKS[\"usd_pledged_real\"] = timeAnalysisKS.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKS[\"backers\"] = timeAnalysisKS.backers.resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "week_timeAnalysisKSSuccess = pd.DataFrame()\n",
      "week_timeAnalysisKSSuccess[jupyter_string] = timeAnalysisKSSuccess.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSSuccess[\"usd_pledged_real\"] = timeAnalysisKSSuccess.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSSuccess[\"backers\"] = timeAnalysisKSSuccess.backers.resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "week_timeAnalysisKSFail = pd.DataFrame()\n",
      "week_timeAnalysisKSFail[jupyter_string] = timeAnalysisKSFail.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSFail[\"usd_pledged_real\"] = timeAnalysisKSFail.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSFail[\"backers\"] = timeAnalysisKSFail.backers.resample(jupyter_string).mean()\n",
      "--------------------\n",
      "students = [75,80,79,60]\n",
      "id = [1,2,3,4]\n",
      "marks = [75,80,79,60]\n",
      "df = pd.DataFrame({jupyter_string:students,jupyter_string:id,jupyter_string:marks})\n",
      "df\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df = df.fillna(0)\n",
      "df.head()\n",
      "=====\n",
      "df.fillna(value=0, inplace=True)\n",
      "df.head()\n",
      "--------------------\n",
      "df.head(10)\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.tail(10)\n",
      "=====\n",
      "df.tail(10)\n",
      "--------------------\n",
      "df[jupyter_string].value_counts()\n",
      "=====\n",
      "df[[jupyter_string,\"Grade\"]][df.Grade == jupyter_string].count()\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "df.groupby([jupyter_string])[jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "\n",
      "--------------------\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "--------------------\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "--------------------\n",
      "test[jupyter_string]=np.exp(A0+Q0*(test.Queries)+Q1*(np.log(test.ILI_lag2)))\n",
      "=====\n",
      "test.head()\n",
      "RMSE_test=math.sqrt(((test.predict_test-test.ILI)**2).mean())\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string, category=DeprecationWarning) \n",
      "\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "\n",
      "import keras \n",
      "from keras.models import Sequential \n",
      "from keras.layers import Dense\n",
      "\n",
      "\n",
      "df_train = pd.read_csv(jupyter_string)\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df = df_train.append(df_test , ignore_index = True)\n",
      "\n",
      "\n",
      "print(df_train.shape, df_test.shape, df_train.columns.values)\n",
      "--------------------\n",
      "df[jupyter_string] = df.Name.str.extract(jupyter_string, expand=False)\n",
      "=====\n",
      "df[jupyter_string] = df.Name.map( lambda x: x.split(jupyter_string)[1].split( jupyter_string )[0].strip())\n",
      "\n",
      "\n",
      "df[jupyter_string].value_counts()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].map({jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string})\n",
      "\n",
      "\n",
      "df[jupyter_string].value_counts()\n",
      "=====\n",
      "df[jupyter_string] = df[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "df[jupyter_string] = df[jupyter_string].replace([jupyter_string,jupyter_string,jupyter_string], jupyter_string)\n",
      "df.Title.loc[ (df.Title !=  jupyter_string) & (df.Title !=  jupyter_string) & (df.Title !=  jupyter_string) \n",
      "             & (df.Title !=  jupyter_string)] = jupyter_string\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "title_dummies = pd.get_dummies(df[jupyter_string], prefix=jupyter_string)\n",
      "df = pd.concat([df, title_dummies], axis=1)\n",
      "df = df.drop([jupyter_string], axis=1)\n",
      "=====\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string])], axis=1).drop(labels=['Name' <<unk>>], axis=1)\n",
      "--------------------\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string])], axis=1)\n",
      "=====\n",
      "df.Sex.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.Sex.value_counts()\n",
      "=====\n",
      "df[['Sex' <<unk>>, 'Survived' <<unk>>]].groupby(['Sex' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
      "=====\n",
      "train_index=[22,15,31,18,34,32,12,26,19,0,44,5,41,4,2,21,11,28,14,46,8,25,20,33,35,39,10,27,7,42]\n",
      "test_index=[x for x in list(range(len(X))) if x not in train_index]\n",
      "\n",
      "X_train=X[train_index,:] \n",
      "y_train=y[train_index] \n",
      "X_test=X[test_index,:] \n",
      "y_test=y[test_index] \n",
      "print(jupyter_string,len(train_index),len(test_index))\n",
      "--------------------\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "iris.head()\n",
      "=====\n",
      "iris_cols = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "iris = pd.read_csv(jupyter_string, header=None, names=iris_cols)\n",
      "--------------------\n",
      "df.Age.fillna(df.Age.median(), inplace=True)\n",
      "=====\n",
      "df.Age.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.SibSp.fillna(0, inplace=True)\n",
      "df.Parch.fillna(0, inplace=True)\n",
      "=====\n",
      "df[jupyter_string] = df['SibSp' <<unk>>] + df['Parch' <<unk>>] + 1\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "df.Ticket.value_counts()\n",
      "=====\n",
      "df.Ticket.head(20)\n",
      "--------------------\n",
      "df.Ticket = df.Ticket.apply(lambda x: x[0])\n",
      "=====\n",
      "df.Ticket = df.Ticket.map(lambda x: x[0])\n",
      "\n",
      "\n",
      "df[['Ticket' <<unk>>, 'Survived' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.head()\n",
      "--------------------\n",
      "df.Ticket.head(20)\n",
      "=====\n",
      "df['Ticket' <<unk>>].value_counts()\n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.describe()\n",
      "--------------------\n",
      "df.Cabin.fillna(jupyter_string,inplace=True)\n",
      "df.Cabin.isnull().sum(axis=0)\n",
      "=====\n",
      "df = df.drop(labels=['Cabin' <<unk>>], axis=1)\n",
      "--------------------\n",
      "df.Embarked.value_counts()\n",
      "=====\n",
      "df.describe(include=[jupyter_string]) \n",
      "--------------------\n",
      "df.describe(include=[jupyter_string]) \n",
      "=====\n",
      "df.Embarked.fillna(jupyter_string , inplace=True )\n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.isnull().sum()\n",
      "--------------------\n",
      "df.Ticket.value_counts()\n",
      "=====\n",
      "df[['Ticket' <<unk>>, 'Survived' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import Activation\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import Convolution2D\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "\n",
      "=====\n",
      "model = Sequential()\n",
      "\n",
      "\n",
      "model.add(Dense(9, kernel_initializer = jupyter_string, activation = jupyter_string, input_dim = 17))\n",
      "model.add(Dense(9, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "model.add(Dense(5, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "model.add(Dense(1, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(optimizer = jupyter_string, loss = jupyter_string, metrics = [jupyter_string])\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train, batch_size = 32, epochs = 200)\n",
      "--------------------\n",
      "y_pred = model.predict(X_test)\n",
      "y_pred = (y_pred > 0.5)\n",
      "=====\n",
      "y_pred = model.predict(X_test)\n",
      "y_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n",
      "\n",
      "output = pd.DataFrame({'PassengerId' <<unk>>: df_test['PassengerId' <<unk>>], 'Survived' <<unk>>: y_final})\n",
      "output.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "test_all = pd.read_csv(jupyter_string)\n",
      "test_x = test_all.loc[:, \"Class\" : \"Fare\"]\n",
      "\n",
      "test_x.head()\n",
      "=====\n",
      "test_all = pd.read_csv(jupyter_string)\n",
      "test_x = test_all.loc[:, \"Class\" : \"Fare\"]\n",
      "test_y = test_all.loc[:, \"Survived\"]\n",
      "--------------------\n",
      "train_x = train_x.as_matrix()\n",
      "test_x = test_x.as_matrix()\n",
      "train_y = train_y.as_matrix()\n",
      "=====\n",
      "validate_all = pd.read_csv(jupyter_string)\n",
      "validate_x = validate_all.loc[:, \"Class\" : \"Fare\"]\n",
      "validate_y = validate_all.loc[:, \"Survived\"]\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(train_x)\n",
      "train_x = scaler.transform(train_x)\n",
      "validate_x = scaler.transform(validate_x)\n",
      "test_x = scaler.transform(test_x)\n",
      "=====\n",
      "df = df.drop(['Id' <<unk>>, 'Name' <<unk>>], axis=1)\n",
      "\n",
      "\n",
      "df = df.dropna(how=jupyter_string, axis=0)\n",
      "\n",
      "df = df.replace([jupyter_string,jupyter_string], [0,1])\n",
      "\n",
      "print(df.shape)\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_x, validate_x, train_y, validate_y = train_test_split(df.loc[:, \"Class\" : \"Fare\"], df.loc[:, \"Survived\"])\n",
      "\n",
      "print(train_x.shape, validate_x.shape, train_y.shape, validate_y.shape)\n",
      "=====\n",
      "train_all = df.iloc[0:300, :]\n",
      "export = pd.DataFrame(train_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "--------------------\n",
      "test_all = df.iloc[300:, :]\n",
      "export = pd.DataFrame(test_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "=====\n",
      "test_all = df.iloc[300:500, :]\n",
      "export = pd.DataFrame(test_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "churn_df = pd.read_csv(jupyter_string)\n",
      "col_names = churn_df.columns.tolist()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(col_names)\n",
      "\n",
      "to_show = col_names[:6] + col_names[-6:]\n",
      "\n",
      "print(jupyter_string)\n",
      "churn_df[to_show].head(6)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "plt.scatter(X_train, y_train, color=jupyter_string)\n",
      "plt.plot(X_train, lr.predict_proba(X_train)[:, 1], color=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "x1 = np.linspace(-5,5,50)\n",
      "fx = 1/(1 + np.exp(-x1))\n",
      "\n",
      "plt.plot(x1, fx, c=jupyter_string, linewidth=2)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "iris.groupby([jupyter_string, jupyter_string]).mean()\n",
      "=====\n",
      "iris.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "plt.plot(x2, y2, jupyter_string)\n",
      "plt.plot(x_bound, y_pred, jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(x2, y2)\n",
      "plt.plot(x2, hx, color=jupyter_string)\n",
      "plt.plot([x_bound,x_bound], [-0.2,1.2], color=jupyter_string)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel('y' <<unk>>)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "=====\n",
      "research = pd.read_csv(jupyter_string, sep=jupyter_string, header=0)\n",
      "research\n",
      "\n",
      "--------------------\n",
      "exprs = pd.read_csv(jupyter_string, index_col=0)\n",
      "sampleinfo = pd.read_csv(jupyter_string, index_col=0)\n",
      "=====\n",
      "exprs = pd.read_csv(jupyter_string,index_col=0)\n",
      "exprs.head()\n",
      "    \n",
      "--------------------\n",
      "sampleinfo = pd.read_csv(jupyter_string)\n",
      "sampleinfo.head()\n",
      "sampleinfo.shape\n",
      "=====\n",
      "sampleinfo = pd.read_csv(jupyter_string)\n",
      "sampleinfo.head()\n",
      "\n",
      "--------------------\n",
      "(exprs.columns == sampleinfo.filename).all()\n",
      "=====\n",
      "(exprs.columns == sampleinfo.filename).all()\n",
      "--------------------\n",
      "dfs[jupyter_string].head()\n",
      "=====\n",
      "dfs[jupyter_string].head()\n",
      "--------------------\n",
      "exprs.head()\n",
      "=====\n",
      "sampleinfo.head()\n",
      "--------------------\n",
      "sampleinfo[jupyter_string] = pd.to_datetime(sampleinfo[jupyter_string])\n",
      "sampleinfo.head()\n",
      "=====\n",
      "sampleinfo[jupyter_string]=pd.to_datetime(sampleinfo['date' <<unk>>])\n",
      "--------------------\n",
      "sampleinfo[jupyter_string]=sampleinfo[jupyter_string].dt.month\n",
      "sampleinfo[jupyter_string]=sampleinfo[jupyter_string].dt.year\n",
      "=====\n",
      "sampleinfo[jupyter_string] = sampleinfo[jupyter_string].apply(lambda x: x.year)\n",
      "sampleinfo[jupyter_string] = sampleinfo[jupyter_string].apply(lambda x: x.month)\n",
      "--------------------\n",
      "iris.plot(kind=jupyter_string, subplots=True)\n",
      "=====\n",
      "iris.boxplot(by=jupyter_string)\n",
      "--------------------\n",
      "sampleinfo.head()\n",
      "=====\n",
      "sampleinfo[jupyter_string].head()\n",
      "--------------------\n",
      "sampleinfoCEU = sampleinfo[sampleinfo.Ethnicity == jupyter_string]\n",
      "sampleinfoCEU.head()\n",
      "=====\n",
      "sampleinfoCEU = sampleinfo.loc[sampleinfo.ethnicity == jupyter_string]\n",
      "sampleinfoCEU.head()\n",
      "--------------------\n",
      "exprsCEU = exprs.loc[exprs.ethnicity == jupyter_string]\n",
      "exprsCEU.head()\n",
      "=====\n",
      "exprsCEU = exprs[sampleinfoCEU.filename]\n",
      "exprsCEU.head()\n",
      "--------------------\n",
      "exprsCEU = exprsCEU - exprsCEU.mean()\n",
      "exprsCEU.head()\n",
      "=====\n",
      "exprsCEU.mean(axis=1).head()\n",
      "--------------------\n",
      "exprsCEU.std(axis=1).head()\n",
      "=====\n",
      "exprsCEU_mn = exprsCEU.apply(lambda x: x - exprsCEU.mean(axis=1))\n",
      "exprsCEU_mn.head()\n",
      "--------------------\n",
      "plt.hist(PC1, bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(Vh[0,:],bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "plt.hist(Vh[1,:],bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(projection,bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "plt.scatter(Vh[0,:],projection)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh[0,:])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh[0,:])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlim(0,170)\n",
      "plt.axvline(x=100,color=jupyter_string)\n",
      "--------------------\n",
      "election = pd.read_csv(jupyter_string)\n",
      "election.head()\n",
      "=====\n",
      "election = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "Data_train = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "Data = Data_train[Data_train['Subject' madeupword0412]==1]\n",
      "Data.head()\n",
      "--------------------\n",
      "election.head()\n",
      "=====\n",
      "election.head()\n",
      "--------------------\n",
      "N = np.median([1,2,3,4,5])\n",
      "N\n",
      "=====\n",
      "N = election.loc[election[\"Start Date\"].apply(lambda x: x.month==11 and x.year == 2012),\"Number of Observations\"].median()\n",
      "--------------------\n",
      "np.random.seed(42)\n",
      "N = 1000\n",
      "p = 0.53\n",
      "samples = np.random.binomial(N, p, size=10000)\n",
      "samples.mean()\n",
      "=====\n",
      "simu_poll_sample= np.random.binomial(N,0.53,1000)\n",
      "\n",
      "--------------------\n",
      "plt.hist(simu_poll_sample)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "simu_result = pd.DataFrame(simu_poll_sample/N)\n",
      "simu_result.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "np.std(simu_poll_sample)\n",
      "=====\n",
      "std_single = simu_result.std()\n",
      "--------------------\n",
      "plt.hist(std_single)\n",
      "plt.show()\n",
      "=====\n",
      "import scipy.stats as stats\n",
      "stats.probplot(((simu_poll_sample - (simu_poll_sample).mean()) / simu_poll_sample.std()), dist=jupyter_string, plot = plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "simu_result = np.zeros(10000)\n",
      "simu_poll_sample = np.zeros(10000)\n",
      "for i in range(10000):\n",
      "    simu_poll_sample[i] = np.random.binomial(1, 0.5)\n",
      "    simu_result[i] = (simu_poll_sample[i] == 1).mean()\n",
      "=====\n",
      "simu_multi_poll_sample= np.random.binomial(N,0.53,(M,1000))\n",
      "--------------------\n",
      "sns.distplot(simu_multi_poll_mean)\n",
      "=====\n",
      "pd.DataFrame(simu_multi_poll_mean).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string % N)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.show\n",
      "--------------------\n",
      "simu_multi_poll_std = np.std(simu_multi_poll_mean)\n",
      "simu_multi_poll_std\n",
      "=====\n",
      "std_multi = simu_multi_poll_mean.std()\n",
      "--------------------\n",
      "X = Data.iloc[:,1:]\n",
      "y = Data.iloc[:,0]\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "X, y = Data.iloc[:,2:], Data.iloc[:,0]\n",
      "y=le.fit_transform(y)\n",
      "\n",
      "np.unique(y)\n",
      "--------------------\n",
      "np.sqrt(std_multi)\n",
      "=====\n",
      "stats.probplot((simu_multi_poll_mean - simu_multi_poll_mean.mean())/std_multi,dist=jupyter_string,plot=plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "std_cross_pull = (simu_multi_poll_sample/N).std(axis=0)\n",
      "--------------------\n",
      "plt.hist(std_cross_pull)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "pd.DataFrame(std_cross_pull).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "--------------------\n",
      "pd.DataFrame(std_cross_pull).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "stats.probplot((std_cross_pull-std_cross_pull.mean())/std_cross_pull.std(),dist=jupyter_string,plot = plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "subset_ratings.mean()\n",
      "=====\n",
      "subset_ratings.std()\n",
      "--------------------\n",
      "subset_ratings.mean()\n",
      "=====\n",
      "subset_ratings.mean()\n",
      "--------------------\n",
      "subset_ratings.std()\n",
      "=====\n",
      "np.mean(subset_ratings.std()<std_cross_pull)\n",
      "--------------------\n",
      "subset_election.head()\n",
      "=====\n",
      "subset_election[jupyter_string].head()\n",
      "--------------------\n",
      "plt.plot(dates.date2num(election[jupyter_string]), election[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax = plt.subplot(111)\n",
      "ax.plot_date(subset_election[\"Start Date\"],subset_election[jupyter_string])\n",
      "plt.axhline(y=0.039, linewidth=2, color = jupyter_string)\n",
      "ax.set_xlim([dt.datetime(2012,10,31,0,0),dt.datetime(2012,11,4,0,0)])\n",
      "ax.xaxis.set_major_formatter(dates.DateFormatter(jupyter_string))\n",
      "ax.set_ylim([-0.03,0.05])\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = plt.subplot(111)\n",
      "ax.plot_date(subset_election[\"Start Date\"],subset_election[jupyter_string])\n",
      "ax.set_xlim([dt.datetime(2012,10,31,0,0),dt.datetime(2012,11,4,0,0)])\n",
      "ax.xaxis.set_major_formatter(dates.DateFormatter(jupyter_string))\n",
      "ax.set_ylim([-0.03,0.05])\n",
      "plt.show()\n",
      "=====\n",
      "set_Poll = list(set(subset_election[\"Pollster\"]))\n",
      "index_map = {}\n",
      "for i,p in enumerate(set_Poll):\n",
      "    index_map[p] = i\n",
      "\n",
      "plt.scatter(map(lambda x: index_map[x], subset_election[\"Pollster\"]),subset_election[jupyter_string],c=map(lambda x: index_map[x], subset_election[\"Pollster\"]))\n",
      "plt.xticks(range(len(set_Poll)),set_Poll,rotation = 90)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "cov_mat = np.cov(X_train_std.T)\n",
      "eigen_vals,eigen_vecs = np.linalg.eig(cov_mat)\n",
      "eigen_vecs\n",
      "=====\n",
      "import numpy as np\n",
      "covariance_matrix = np.cov(X_train_std.T)\n",
      "eigen_vals,eigen_vecs = np.linalg.eig(covariance_matrix)\n",
      "\n",
      "sum_eigenvals = np.sum(eigen_vals)\n",
      "var_exp = np.real(eigen_vals)/np.real(sum_eigenvals)\n",
      "cumsum = np.cumsum(var_exp)\n",
      "n=var_exp.shape[0]\n",
      "--------------------\n",
      "obama_avg = np.mean(obama_polls)\n",
      "obama_std = np.std(obama_polls)\n",
      "print(obama_avg, obama_std)\n",
      "=====\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().mean()\n",
      "--------------------\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().std()\n",
      "=====\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().std()\n",
      "--------------------\n",
      "model = Sequential()\n",
      "model.add(embedding_layer)\n",
      "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
      "model.add(Dense(1, activation=jupyter_string))\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model=Sequential()\n",
      "embedding_layer = pretrained_embedding_layer(words_to_vec,word_to_index)\n",
      "model.add(embedding_layer)\n",
      "model.add(LSTM(128, return_sequences=True))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(LSTM(128))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(5,activation=jupyter_string))\n",
      "model.add(Activation(jupyter_string))\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string,optimizer=jupyter_string,metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "train_dataframe=pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string])\n",
      "test_dataframe=pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string])\n",
      "--------------------\n",
      "from keras.utils import to_categorical\n",
      "Y_train=to_categorical(Y_train,5)\n",
      "Y_test=to_categorical(Y_test,5)\n",
      "=====\n",
      "Y_oh_train=to_categorical(Y_train,num_classes=5).astype(jupyter_string)\n",
      "Y_oh_test=to_categorical(Y_test,num_classes=5).astype(jupyter_string)\n",
      "--------------------\n",
      "plt.plot(k_range, k_scores)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "plt.plot(k_range, k_scores)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "X = pd.read_csv(jupyter_string, sep=jupyter_string, header=0, usecols=range(13), thousands=jupyter_string)\n",
      "Y = pd.read_csv(jupyter_string, sep=jupyter_string, header=0, usecols=[13], encoding=jupyter_string)\n",
      "Y = Y.replace({jupyter_string: 0}, regex=True)\n",
      "Y = Y.replace({jupyter_string: 1}, regex=True)\n",
      "Y = Y.replace({jupyter_string: 2}, regex=True)\n",
      "\n",
      "X = X.values.astype(np.float64)\n",
      "Y = Y.values[:,0].astype(np.float64)\n",
      "\n",
      "\n",
      "from sklearn import preprocessing\n",
      "\n",
      "X_NORMAL = preprocessing.normalize(X)\n",
      "X_STD = preprocessing.scale(X)\n",
      "X_NORMAL_STD = preprocessing.scale(X_NORMAL)\n",
      "\n",
      "--------------------\n",
      "U = U[:, :-1]\n",
      "V = V[:, :-1]\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(U)\n",
      "print(jupyter_string)\n",
      "print(V)\n",
      "=====\n",
      "s_rank = 12\n",
      "S = np.zeros(X.shape, dtype=complex)\n",
      "S[:s_rank, :s_rank] = np.diag(s)[:s_rank, :s_rank]\n",
      "\n",
      "X_SVD = np.dot(U, np.dot(S, V));\n",
      "X_SVD = np.real(X_SVD);\n",
      "--------------------\n",
      "X_ED = X_ED.drop([jupyter_string, jupyter_string, jupyter_string, jupyter_string], axis=1)\n",
      "=====\n",
      "used_columns = [1,6,10,11,12]\n",
      "X_EDITED = X[:, used_columns]\n",
      "--------------------\n",
      "model=sm.OLS(y=y_train,x=pd.DataFrame(X_train))\n",
      "result=model.fit()\n",
      "result.summary()\n",
      "=====\n",
      "result=ols(y=y_train,x=pd.DataFrame(X_train))\n",
      "\n",
      "R_2_IS=result.r2  \n",
      "OLS_coef=result.beta\n",
      "\n",
      "\n",
      "a=np.array(X_test)  \n",
      "b=np.array(result.beta) \n",
      "print(jupyter_string.format(b))\n",
      "c=np.sum(a*b[0:-1],axis=1)+b[-1] \n",
      "error=y_test-c \n",
      "R_2_OS=1-error.var()/y_test.var() \n",
      "print(jupyter_string.format(R_2_IS))\n",
      "print(jupyter_string.format(R_2_OS))\n",
      "--------------------\n",
      "tot = sum(var_exp)\n",
      "exp_var_exp = [(i/tot)*100 for i in sorted(var_exp, reverse=True)]\n",
      "cum_exp_var_exp = np.cumsum(exp_var_exp)\n",
      "=====\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set(style=jupyter_string)\n",
      "\n",
      "plt.figure(figsize=(5,4))\n",
      "sns.barplot(x=np.arange(1,21),y=var_exp[:20],palette=jupyter_string)\n",
      "\n",
      "plt.step(np.arange(0,20),cumsum[:20],where=jupyter_string,color=jupyter_string,label=jupyter_string)\n",
      "labels= plt.yticks()\n",
      "new_labels = [str(int(label*100))+jupyter_string for label in labels[0]]\n",
      "plt.yticks(labels[0], new_labels)\n",
      "plt.xlim(-1,20)\n",
      "plt.ylim(0,0.9)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.grid(axis=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "--------------------\n",
      "classifier = svm.SVC(kernel=jupyter_string, degree=3)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "y_pred = classifier.predict(X_test)\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "print(cm)\n",
      "=====\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=3, C=Penalty_C)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X_KPCA, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "classifier = SVC(kernel = jupyter_string, random_state = 0)\n",
      "classifier.fit(X_train, y_train)\n",
      "y_pred = classifier.predict(X_test)\n",
      "from sklearn.metrics import confusion_matrix\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "print(cm)\n",
      "=====\n",
      "svc = svm.SVC(kernel=jupyter_string, C=Penalty_C)\n",
      "accuracy_svc = cross_val_score(svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_svc)\n",
      "--------------------\n",
      "svc = svm.SVC(kernel=jupyter_string, C=Penalty_C)\n",
      "accuracy_svc = cross_val_score(svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_svc)\n",
      "=====\n",
      "rbf_svc = svm.SVC(kernel=jupyter_string, gamma=jupyter_string, C=Penalty_C)\n",
      "accuracy_rbf_svc = cross_val_score(rbf_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_rbf_svc)\n",
      "--------------------\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=2)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "=====\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=2, C=Penalty_C)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "--------------------\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "=====\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "model = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=1, random_state=0)\n",
      "accuracy_boost = cross_val_score(model, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_boost)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "data = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "data.head()\n",
      "--------------------\n",
      "data[jupyter_string] = data[jupyter_string].apply(lambda x: 1 if x == jupyter_string else 0)\n",
      "data.head()\n",
      "=====\n",
      "data['Class' <<unk>>] = data.apply(lambda x: 1 if x['Class' <<unk>>]==jupyter_string else 0, axis=1)\n",
      "data.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_binary, y_train)\n",
      "y_pred_lr = lr.predict(X_test_binary)\n",
      "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_lr)\n",
      "\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(X_train_tfidf, y_train)\n",
      "y_pred_bnb = bnb.predict(X_test_tfidf)\n",
      "fpr_bnb, tpr_bnb, thresholds_bnb = roc_curve(y_test, y_pred_bnb)\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "clf = LogisticRegression()\n",
      "for train,test,feature_type in [(X_train_binary, X_test_binary, jupyter_string), (X_train_tfidf, X_test_tfidf, jupyter_string)]:\n",
      "    clf.fit(train, Y_train)\n",
      "    \n",
      "    predictions = clf.predict_proba(test)[:,1]\n",
      "    fpr, tpr, threshold = metrics.roc_curve(Y_test, predictions)\n",
      "    score = metrics.roc_auc_score(Y_test, predictions)\n",
      "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
      "    plt.plot(fpr, tpr, color=c, label=jupyter_string + feature_type + jupyter_string+ str(score))\n",
      "\n",
      "clf = BernoulliNB()\n",
      "for train, test, feature_type in [(X_train_binary, X_test_binary, jupyter_string), (X_train_tfidf, X_test_tfidf, jupyter_string)]:\n",
      "    clf.fit(train, Y_train)\n",
      "    \n",
      "    predictions = clf.predict_proba(test)[:,1]\n",
      "    fpr, tpr, threshold = metrics.roc_curve(Y_test, predictions)\n",
      "    score = metrics.roc_auc_score(Y_test, predictions)\n",
      "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
      "    plt.plot(fpr, tpr, color=c, label=jupyter_string + feature_type + jupyter_string+ str(score))\n",
      "\n",
      "plt.legend(loc=jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "import pprint as pprint\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "plt.style.use(jupyter_string)\n",
      "--------------------\n",
      "tot = sum(var_exp)\n",
      "exp_var_exp = [(i/tot)*100 for i in sorted(var_exp, reverse=True)]\n",
      "cum_exp_var_exp = np.cumsum(exp_var_exp)\n",
      "=====\n",
      "Pca_w = np.real(eigen_vecs[:,0:20])\n",
      "X_train_pca = X_train_std.dot(Pca_w)\n",
      "X_test_pca = X_test_std.dot(Pca_w)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "bb_df = pd.read_csv(jupyter_string)\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df.info()\n",
      "=====\n",
      "bb_df.describe()\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "bb_df[jupyter_string] = bb_df['time' <<unk>>].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df[jupyter_string] = bb_df[jupyter_string].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "=====\n",
      "bb_df[jupyter_string] = bb_df['date.entered' <<unk>>].dt.month\n",
      "bb_df[jupyter_string] = bb_df['date.entered' <<unk>>].dt.year\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "sns.distplot(bb_small_df[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "bb_small_df.hist(figsize=(15,15));\n",
      "--------------------\n",
      "small_df.hist(figsize=(15,15));\n",
      "=====\n",
      "bb_small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                     bins = 50,\\\n",
      "                                    fontsize = 15\\\n",
      "                                    ).set_title(jupyter_string, \\\n",
      "                                                fontsize = 20, y = 1.01)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                     bins = 50,\\\n",
      "                                    fontsize = 15\\\n",
      "                                    ).set_title(jupyter_string, \\\n",
      "                                                fontsize = 20, y = 1.01)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "bb_small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                 bins = 40, fontsize = 15\\\n",
      "                                ).set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "pca = PCA(n_components=20)\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "=====\n",
      "from sklearn.learning_curve import validation_curve\n",
      "param_range = np.logspace(-4,2,num=10)\n",
      "train_scores, test_scores = validation_curve(estimator=clf,X=X_train_pca,y=y_train,param_name=jupyter_string,\n",
      "                                             param_range=param_range,cv=10)\n",
      "\n",
      "train_mean = np.mean(train_scores,axis=1)\n",
      "train_std  = np.std(train_scores,axis=1)\n",
      "test_mean  = np.mean(test_scores,axis=1)\n",
      "test_std   = np.std(test_scores,axis=1)\n",
      "\n",
      "plt.plot(param_range,train_mean,color=jupyter_string,marker=jupyter_string,markersize=5,label=jupyter_string)\n",
      "plt.fill_between(param_range,train_mean+train_std,train_mean-train_std,color=jupyter_string,alpha=0.15)\n",
      "plt.plot(param_range,test_mean,color=jupyter_string,marker=jupyter_string,markersize=5,label=jupyter_string)\n",
      "plt.fill_between(param_range,test_mean+test_std,test_mean-test_std,color=jupyter_string,alpha=0.15)\n",
      "plt.grid()\n",
      "plt.xscale(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.ylim((0.9,1.02));\n",
      "--------------------\n",
      "plt.figure(figsize = (15,7))\n",
      "plt.title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.xlabel(jupyter_string, fontsize = 15)\n",
      "plt.ylabel(jupyter_string, fontsize = 15)\n",
      "plt.scatter(small_df[jupyter_string], small_df[jupyter_string])\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "sns.pairplot(data=bb_small_df)\n",
      "plt.rcParams[jupyter_string]=(15,15);\n",
      "--------------------\n",
      "plt.scatter(x=bb_small_df[jupyter_string], y=bb_small_df[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 22, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 22, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 0.5,  linestyle=jupyter_string, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14);\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 22, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 22, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "unicorn = bb_df[(bb_df[jupyter_string] < 38) & (bb_df[jupyter_string] > 32)]\\\n",
      "[['artist.inverted' <<unk>>, 'genre' <<unk>>,'track' <<unk>>, jupyter_string, jupyter_string]]\n",
      "unicorn.sort_values(jupyter_string, inplace = True)\n",
      "\n",
      "\n",
      "new_col = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "unicorn.columns = new_col\n",
      "unicorn\n",
      "--------------------\n",
      "pd.pivot_table(bb_df, values = jupyter_string, index = [jupyter_string], columns = [jupyter_string], aggfunc = np.sum)\n",
      "=====\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.median, np.std, max, min])\n",
      "--------------------\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.median, np.std, max, min])\n",
      "=====\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.mean, np.median, np.std, max, min])\n",
      "--------------------\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.mean, np.median, np.std, max, min])\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.1)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.1)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (10,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings; warnings.simplefilter(jupyter_string)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (10,8))\n",
      "\n",
      "ax = sns.violinplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "import matplotlib.cm as cm\n",
      "from sklearn import linear_model\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "\n",
      "lm = linear_model.LinearRegression()\n",
      "\n",
      "X = bb_small_df[[jupyter_string]]\n",
      "y = bb_small_df[[jupyter_string]]\n",
      "\n",
      "lm.fit(X, y)\n",
      "predictions = lm.predict(X)\n",
      "\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "plt.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.9,\\\n",
      "          c = bb_small_df[jupyter_string], cmap=jupyter_string)\n",
      "\n",
      "plt.plot(X, predictions, c= jupyter_string, marker = jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 16)\n",
      "ax.set_xticklabels([0,20,40,60,80,100,120],fontsize = 14)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 16)\n",
      "ax.set_yticklabels([-10,0,10,20,30,40,50,60], fontsize = 14)\n",
      "\n",
      "cb = plt.colorbar()\n",
      "cb.set_label(jupyter_string, rotation = 90, fontsize = 14)\n",
      "cb.set_ticks([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
      "cb.ax.tick_params(labelsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "lm = linear_model.LinearRegression()\n",
      "\n",
      "X = bb_small_df[[jupyter_string]]\n",
      "y = bb_small_df[[jupyter_string]]\n",
      "\n",
      "lm.fit(X, y)\n",
      "predictions = lm.predict(X)\n",
      "\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,10))\n",
      "\n",
      "plt.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.5,\\\n",
      "          c = bb_small_df[jupyter_string], cmap=jupyter_string)\n",
      "\n",
      "plt.plot(X, predictions, c= jupyter_string, marker = jupyter_string)\n",
      "\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 16)\n",
      "ax.set_xticklabels([0,20,40,60,80,100,120],fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, fontsize = 16)\n",
      "ax.set_yticklabels([-10,0,10,20,30,40,50,60], fontsize = 14)\n",
      "\n",
      "cb = plt.colorbar()\n",
      "cb.set_label(jupyter_string, rotation = 90, fontsize = 14)\n",
      "cb.set_ticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
      "cb.ax.tick_params(labelsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "shuf_season_df = bb_small_df[[jupyter_string, jupyter_string]]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "shuf_peak_df = bb_small_df[[jupyter_string, jupyter_string]]\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "rs = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "rs.head()\n",
      "=====\n",
      "pd.set_option(jupyter_string, 500)\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arima\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "=====\n",
      "rs['Borough' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "fig = plt.figure(figsize=(20,18))\n",
      "for i,l in enumerate([jupyter_string, jupyter_string, jupyter_string, jupyter_string]):\n",
      "    ax = fig.add_subplot(3,2,i+1)\n",
      "    sns.pointplot(x=jupyter_string, y=l, hue=jupyter_string, data=results, ax=ax)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn import linear_model, preprocessing\n",
      "\n",
      "sstat = pd.read_csv(jupyter_string,index_col=1)\n",
      "ss = sstat.fillna(0)\n",
      "print(ss)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "=====\n",
      "ytr=ss['Pos' <<unk>>]\n",
      "xtr=ss[['FG' <<unk>>,'FGA' <<unk>>,'FG%' <<unk>>,'3P' <<unk>>,'3PA' <<unk>>,'3P%' <<unk>>,'2P' <<unk>>,'2PA' <<unk>>,'2P%' madeupword0002,'eFG%' <<unk>>,'FT' <<unk>>,'FTA' <<unk>>,'FT%' <<unk>>,'ORB' <<unk>>,'DRB' <<unk>>,'TRB' <<unk>>,'AST' <<unk>>,'STL' <<unk>>,'BLK' <<unk>>,'TOV' <<unk>>,'PF' <<unk>>,jupyter_string]]\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(xtr,ytr,test_size=0.2,random_state=42)\n",
      "=====\n",
      "logreg = linear_model.LogisticRegression(verbose=5, \n",
      "                                         solver=jupyter_string, max_iter=100)\n",
      "logreg.fit(xtr,ytr)\n",
      "--------------------\n",
      "ss8[jupyter_string] = ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string]\n",
      "ss8\n",
      "=====\n",
      "yts=ss8['Pos' <<unk>>]\n",
      "xts=ss8[['FG' <<unk>>,'FGA' <<unk>>,'FG%' <<unk>>,'3P' <<unk>>,'3PA' <<unk>>,'3P%' <<unk>>,'2P' <<unk>>,'2PA' <<unk>>,'2P%' madeupword0002,'eFG%' <<unk>>,'FT' <<unk>>,'FTA' <<unk>>,'FT%' <<unk>>,'ORB' <<unk>>,'DRB' <<unk>>,'TRB' <<unk>>,'AST' <<unk>>,'STL' <<unk>>,'BLK' <<unk>>,'TOV' <<unk>>,'PF' <<unk>>,jupyter_string]]\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "xts_train, xts_test, yts_train, yts_test = train_test_split(xts, yts, test_size=0.33, random_state=42)\n",
      "=====\n",
      "yhat = logreg.predict(xtr)\n",
      "acc = np.mean(yhat == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "yhat = logreg.predict(xts)\n",
      "acc = np.mean(yhat == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "=====\n",
      "nprt = 10\n",
      "Ierr = np.where(ytr != yhat)[0]\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = jupyter_string.format(ytr[ind], yhat[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "plt.scatter(ytr, yhat)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "from sklearn.metrics import confusion_matrix\n",
      "C = confusion_matrix(ytr,yhat)\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='PF' <<unk>>)&(yhat=='PF' <<unk>>))[0]\n",
      "b=np.where((ytr=='PF' <<unk>>))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "\n",
      "Csum = np.sum(C,1)\n",
      "C = C / Csum[None,:]\n",
      "\n",
      "\n",
      "print(np.array_str(C, precision=3, suppress_small=True))\n",
      "plt.imshow(C, interpolation=jupyter_string)\n",
      "x=[0,1,2,3,4,5]\n",
      "y=[0,1,2,3,4,5]\n",
      "ygroup_labels = [jupyter_string, 'PF' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string]  \n",
      "xgroup_labels = [jupyter_string, 'PF' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string] \n",
      "plt.xticks(x, xgroup_labels, rotation=90)  \n",
      "plt.yticks(y, ygroup_labels, rotation=0)\n",
      "plt.colorbar()\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "--------------------\n",
      "svc.fit(X_train,y_train)\n",
      "=====\n",
      "svc.fit(xtr,ytr)\n",
      "yhat_tr = svc.predict(xtr)\n",
      "acc = np.mean(yhat_tr == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "N = 60  \n",
      "\n",
      "data = pd.read_csv(jupyter_string, index_col='Date' <<unk>>, parse_dates=['Date' <<unk>>]).iloc[:,:N]\n",
      "stock_list = data.columns\n",
      "\n",
      "print( jupyter_string.format(data.isna().sum().sum()) )\n",
      "print( jupyter_string.format(data.shape) )\n",
      "display(data.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if N <= 30:\n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "\n",
      "    ax = fig.add_subplot(121)\n",
      "    np.cumprod(1+data, axis=0).plot(ax=ax, title=jupyter_string)\n",
      "    np.cumprod(1+data.mean(axis=1)).plot(ax=ax, label=jupyter_string, color=jupyter_string)\n",
      "    ax.legend()\n",
      "\n",
      "    ax = fig.add_subplot(122)\n",
      "    data.plot(ax=ax, title=jupyter_string)\n",
      "\n",
      "    plt.show()\n",
      "--------------------\n",
      "yhat_te = svc.predict(xtest)\n",
      "acc = np.mean(yhat_te == ytest)\n",
      "print(jupyter_string.format(acc))\n",
      "=====\n",
      "Ierr = np.where(yhat_tr != ytr)[0]\n",
      "nprt = 2\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = jupyter_string.format(ytr[ind], yhat_tr[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "plt.scatter(ytr, yhat_tr)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "yhat_ts = svc.predict(xts)\n",
      "acc = np.mean(yhat_ts == yts)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(yts, yhat_ts))\n",
      "=====\n",
      "Ierr = np.where(yhat_ts != yts)[0]\n",
      "nprt = 324\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xts.index[ind])  \n",
      "    title = jupyter_string.format(yts[ind], yhat_ts[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data_df.head()\n",
      "=====\n",
      "data_df.head()\n",
      "--------------------\n",
      "corrmat = data_df.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat)\n",
      "=====\n",
      "feature_df = data_df.drop(['Y1' <<unk>>, 'Y2' <<unk>>], axis=1)\n",
      "--------------------\n",
      "feature_df.head()\n",
      "=====\n",
      "y1_corrs = feature_df.corrwith(data_df.Y1)\n",
      "y1_corrs.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "y2_corrs = feature_df.corr()\n",
      "y2_corrs.plot(kind=jupyter_string)\n",
      "=====\n",
      "f_corrs = feature_df.corr()\n",
      "sns.heatmap(f_corrs, annot=True)\n",
      "--------------------\n",
      "model = sm.ols(formula=jupyter_string, data=feature_df)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "y1_model = sm.ols(data=data_df, \n",
      "                  formula=jupyter_string)\n",
      "y1_result = y1_model.fit()\n",
      "y1_result.summary()\n",
      "--------------------\n",
      "y2_model = sm.ols(data=data_df, \n",
      "                  formula=jupyter_string)\n",
      "y2_result = y2_model.fit()\n",
      "=====\n",
      "y2_corrs = feature_df.corrwith(data_df.Y2)\n",
      "y2_corrs.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd, numpy as np\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "model = KNeighborsClassifier(n_neighbors=5) \n",
      "model.fit(train_X, train_y)\n",
      "prediction = model.predict(test_X)\n",
      "print(jupyter_string, metrics.accuracy_score(prediction, test_y))\n",
      "=====\n",
      "a_index = list(range(1,11))\n",
      "a = pd.Series()\n",
      "for i in list(range(1,11)):\n",
      "    model = KNeighborsClassifier(n_neighbors=i)\n",
      "    model.fit(train_X, train_y)\n",
      "    prediction = model.predict(test_X)\n",
      "    a = a.append(pd.Series(metrics.accuracy_score(prediction, test_y)))\n",
      "plt.plot(a_index, a)\n",
      "x = [1,2,3,4,5,6,7,8,9,10]\n",
      "plt.xticks(x)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "petal = iris[['PetalLengthCm' madeupword0002,'PetalWidthCm' <<unk>>,'Species' <<unk>>]]\n",
      "sepal = iris[['SepalLengthCm' <<unk>>,'SepalWidthCm' <<unk>>,'Species' <<unk>>]]\n",
      "--------------------\n",
      "iris.head()\n",
      "=====\n",
      "fig = iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "\n",
      "fig.set_xlabel(jupyter_string)\n",
      "fig.set_ylabel(jupyter_string)\n",
      "fig.set_title(jupyter_string)\n",
      "\n",
      "fig=plt.gcf()\n",
      "fig.set_size_inches(10, 7)\n",
      "plt.show()\n",
      "--------------------\n",
      "clf = tree.DecisionTreeClassifier(criterion=jupyter_string)\n",
      "train = TrainModel(clf, df, target=df.columns[-1])\n",
      "train.run()\n",
      "train.predict()\n",
      "display_clf(clf, feature_names=train.get_train_x_names(), class_names=train.get_train_y_names())\n",
      "save_png_clf(clf, clf.criterion)\n",
      "=====\n",
      "clf = tree.DecisionTreeClassifier(criterion=jupyter_string)\n",
      "train = TrainModel(clf, df, target=df.columns[-1])\n",
      "train.run()\n",
      "train.predict()\n",
      "display_clf(clf, feature_names=train.get_train_x_names(), class_names=train.get_train_y_names())\n",
      "save_png_clf(clf, clf.criterion)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier \n",
      "knn = KNeighborsClassifier(n_neighbors=1) \n",
      "knn.fit(train_X, train_y) \n",
      "predictions = knn.predict(test_X) \n",
      "=====\n",
      "train_X.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "display(data.head())\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "con_mean = data['Congruent' <<unk>>].mean()\n",
      "con_median = data['Congruent' <<unk>>].median()\n",
      "con_std = data['Congruent' <<unk>>].std()\n",
      "incon_mean = data['Incongruent' <<unk>>].mean()\n",
      "incon_median = data['Incongruent' <<unk>>].median()\n",
      "incon_std = data['Incongruent' <<unk>>].std()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "=====\n",
      "df_pop1 = df.copy(deep=True)\n",
      "df_pop1 = df_pop1[df.A_Se != jupyter_string]\n",
      "df_pop1 = df_pop1[df.A_As != jupyter_string]\n",
      "\n",
      "\n",
      "df_pop2 = df.copy(deep=True)\n",
      "df_pop2 = df_pop2[df.A_As != jupyter_string]\n",
      "df_pop2 = df_pop2[df.A_Pb != jupyter_string]\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "=====\n",
      "df2 = df.replace(jupyter_string, 0)\n",
      "--------------------\n",
      "df_pop3 = df.copy(deep=True)\n",
      "df_pop3 = df_pop3[df_pop3.A_Se != jupyter_string]\n",
      "df_pop3 = df_pop3[df_pop3.A_As != jupyter_string]\n",
      "df_pop3 = df_pop3[df_pop3.A_Pb != jupyter_string]\n",
      "=====\n",
      "df2.head()\n",
      "--------------------\n",
      "df6 = df5.copy(deep=True)\n",
      "df6.head()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "df_pop2['A_As' madeupword0002] = df_pop2['A_As' madeupword0002].astype(float)\n",
      "print(df_pop2['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop2['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df3['A_As' madeupword0002] = df3['A_As' madeupword0002].astype(float)\n",
      "print(df3['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df4['A_As' madeupword0002] = df4['A_As' madeupword0002].astype(float)\n",
      "print(df4['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df4['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df5['A_As' madeupword0002] = df5['A_As' madeupword0002].astype(float)\n",
      "print(df5['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df5['A_As' madeupword0002].median())\n",
      "--------------------\n",
      "kmf.fit(df[jupyter_string], event_observed=df[jupyter_string])\n",
      "kmf.plot()\n",
      "plt.show()\n",
      "=====\n",
      "T = df_pop1[jupyter_string]\n",
      "C = df_pop1['A_Se' <<unk>>]\n",
      "kmf.fit(T, event_observed=C)\n",
      "--------------------\n",
      "import gmaps\n",
      "import gmaps.datasets\n",
      "\n",
      "gmaps.configure(api_key=jupyter_string)\n",
      "=====\n",
      "import os\n",
      "import gmaps\n",
      "import gmaps.datasets\n",
      "\n",
      "\n",
      "gmaps.configure(api_key=jupyter_string) \n",
      "--------------------\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop1[[\"Latitude\", \"Longitude\"]], weights=df_pop1[\"A_Se\"],\n",
      "    \n",
      "    max_intensity=5, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "from gmaps.display import Image\n",
      "Image(filename=jupyter_string)\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop2[[\"Latitude\", \"Longitude\"]], weights=df_pop2[\"A_As\"],\n",
      "    max_intensity=80, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "df_pop3 = pd.read_csv(jupyter_string)\n",
      "df_pop3.head()\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop2[[\"Latitude\", \"Longitude\"]], weights=df_pop2[\"A_Pb\"],\n",
      "    \n",
      "    max_intensity=110, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "cars = pd.read_csv(jupyter_string)\n",
      "cars.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "cars = pd.read_csv(jupyter_string)\n",
      "cars = cars.dropna()\n",
      "cars.shape\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "=====\n",
      "cars.head()\n",
      "--------------------\n",
      "from pandas.plotting import scatter_matrix\n",
      "scatter_matrix(cars)\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams \n",
      "from pandas.tools.plotting import scatter_matrix \n",
      "\n",
      "\n",
      "\n",
      "def MatplotLib_Params():\n",
      "    \n",
      "    rcParams[jupyter_string] = (8, 6)\n",
      "    \n",
      "    rcParams[jupyter_string] = 150\n",
      "    \n",
      "    rcParams[jupyter_string] = 14\n",
      "    \n",
      "MatplotLib_Params()\n",
      "cont = cars[[\"mpg\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\"]]\n",
      "ax = scatter_matrix(cont)\n",
      "plt.show()\n",
      "--------------------\n",
      "grid_eNet.best_estimator_\n",
      "=====\n",
      "plot_learning_curve(grid_eNet, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.scatter(cars[\"weight\"],cars[\"mpg\"])\n",
      "plt.xlabel(\"weight\")\n",
      "plt.ylabel(\"mpg\")\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(cars.weight,cars.mpg)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "random_params = {\n",
      "    jupyter_string:st.expon(scale=0.01),\n",
      "    jupyter_string:[False,True],\n",
      "    jupyter_string:[500,1000,1500,2000]\n",
      "}\n",
      "\n",
      "\n",
      "random_eNet = RandomizedSearchCV(eCV,random_params,cv = 5, n_iter = 50)\n",
      "random_eNet.fit(polyX_train,y_train)\n",
      "\n",
      "print(random_eNet.best_params_)\n",
      "=====\n",
      "plot_learning_curve(random_eNet, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "result=ols(y=y_train,x=pd.DataFrame(X_train))\n",
      "\n",
      "R_2_IS=result.r2  \n",
      "OLS_coef=result.beta\n",
      "\n",
      "\n",
      "a=np.array(X_test)  \n",
      "b=np.array(result.beta) \n",
      "print(jupyter_string.format(b))\n",
      "c=np.sum(a*b[0:-1],axis=1)+b[-1] \n",
      "error=y_test-c \n",
      "R_2_OS=1-error.var()/y_test.var() \n",
      "print(jupyter_string.format(R_2_OS))\n",
      "=====\n",
      "Number_variables=[]\n",
      "\n",
      "OLS_R_2_OS_F=[]\n",
      "OLS_R_2_IS_F=[]\n",
      "\n",
      "t=0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for j in range(len(X_train.T)): \n",
      "    \n",
      "    t+=1\n",
      "    Number_variables.append(t)\n",
      "\n",
      "    result=ols(y=y_train,x=pd.DataFrame(X_train[:,0:j+1]))\n",
      "    temp=X_test[:,0:j+1]\n",
      "\n",
      "    a=np.array(temp)\n",
      "    b=np.array(result.beta)\n",
      "    c=np.sum(a*b[0:-1],axis=1)+b[-1]\n",
      "\n",
      "    error=y_test-c\n",
      "    R_2=1-error.var()/y_test.var()\n",
      "    if R_2>0:\n",
      "        OLS_R_2_OS_F.append(R_2)\n",
      "    else:\n",
      "        OLS_R_2_OS_F.append(0)\n",
      "    \n",
      "    OLS_R_2_IS_F.append(result.r2)\n",
      "\n",
      "pylab.title(jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_OS_F,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_IS_F,jupyter_string,label=jupyter_string)\n",
      "\n",
      "\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.xlabel(jupyter_string)\n",
      "pylab.ylabel(jupyter_string)\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "selectedCols = [\n",
      "    'perGameStats_Age' <<unk>>,\n",
      "    'perGameStats_G' <<unk>>,\n",
      "    'perGameStats_GS' <<unk>>,\n",
      "    'perGameStats_MP' <<unk>>,\n",
      "    'per100Stats_FG' <<unk>>,\n",
      "    'per100Stats_FGA' <<unk>>,\n",
      "    'per100Stats_FGPerc' <<unk>>,\n",
      "    'per100Stats_3P' <<unk>>,\n",
      "    'per100Stats_3PA' <<unk>>,\n",
      "    'per100Stats_3PPerc' <<unk>>,\n",
      "    'per100Stats_2P' <<unk>>,\n",
      "    'per100Stats_2PA' madeupword0002,\n",
      "    'per100Stats_2PPerc' <<unk>>,\n",
      "    'per100Stats_FT' <<unk>>,\n",
      "    'per100Stats_FTA' <<unk>>,\n",
      "    'per100Stats_FTPerc' <<unk>>,\n",
      "    'per100Stats_ORB' <<unk>>,\n",
      "    'per100Stats_DRB' <<unk>>,\n",
      "    'per100Stats_TRB' <<unk>>,\n",
      "    'per100Stats_AST' <<unk>>,\n",
      "    'per100Stats_STL' <<unk>>,\n",
      "    'per100Stats_BLK' <<unk>>,\n",
      "    'per100Stats_TOV' <<unk>>,\n",
      "    'per100Stats_PF' <<unk>>,\n",
      "    'per100Stats_PTS' <<unk>>,\n",
      "    'per100Stats_ORtg' <<unk>>,\n",
      "    'per100Stats_DRtg' <<unk>>,\n",
      "    'advancedStats_PER' <<unk>>,\n",
      "    'advancedStats_TSPerc' <<unk>>,\n",
      "    'advancedStats_3PAr' <<unk>>,\n",
      "    'advancedStats_FTr' <<unk>>,\n",
      "    'advancedStats_ORBPerc' <<unk>>,\n",
      "    'advancedStats_DRBPerc' <<unk>>,\n",
      "    'advancedStats_TRBPerc' <<unk>>,\n",
      "    'advancedStats_ASTPerc' <<unk>>,\n",
      "    'advancedStats_STLPerc' <<unk>>,\n",
      "    'advancedStats_BLKPerc' <<unk>>,\n",
      "    'advancedStats_TOVPerc' <<unk>>,\n",
      "    'advancedStats_USGPerc' <<unk>>,\n",
      "    'advancedStats_OWS' <<unk>>,\n",
      "    'advancedStats_DWS' <<unk>>,\n",
      "    'advancedStats_WS' <<unk>>,\n",
      "    'advancedStats_WS48' <<unk>>,\n",
      "    'advancedStats_OBPM' <<unk>>,\n",
      "    'advancedStats_DBPM' <<unk>>,\n",
      "    'advancedStats_BPM' <<unk>>,\n",
      "    'advancedStats_VORP' <<unk>>,\n",
      "    'accolades_all_nba' <<unk>>\n",
      "]\n",
      "\n",
      "playerAggDfAllNbaAllStarInitFeatures = playerAggDfAllNbaAllStar[selectedCols]\n",
      "--------------------\n",
      "sns.regplot(cars.weight,cars.mpg)\n",
      "plt.show()\n",
      "=====\n",
      "import  seaborn as sns\n",
      "\n",
      "ax = sns.regplot(x = \"weight\", y = \"mpg\", data = cars)\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = sns.regplot(x = \"weight\", y = \"mpg\", data = cars, order = 2)\n",
      "plt.show()\n",
      "=====\n",
      "import numpy as np\n",
      "\n",
      "z = np.polyfit(cars.weight,cars.mpg,12)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "--------------------\n",
      "z = np.polyfit(cars.weight,cars.mpg,12)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "=====\n",
      "z = np.polyfit(cars.weight,cars.mpg,2)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X = cars.weight.values.reshape(-1,1)\n",
      "y = cars.mpg\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "print(y_train.shape)\n",
      "print(y_test.shape)\n",
      "=====\n",
      "def Normalize(cont):\n",
      "    mean = cont.mean()\n",
      "    std = cont.std()\n",
      "    return (cont - mean)/std\n",
      "\n",
      "\n",
      "cars_cont = cars.drop([\"model_year\",\"name\",\"origin\",\"cylinders\"],axis = 1)\n",
      "\n",
      "\n",
      "for column in cars_cont.columns:\n",
      "    cars_cont[column] = Normalize(cars_cont[column])\n",
      "\n",
      "\n",
      "cars[cars_cont.columns] = cars_cont\n",
      "cars.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X = cars_cont\n",
      "y = cars.mpg\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "print(y_train.shape)\n",
      "print(y_test.shape)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(cars.drop([\"mpg\"],axis = 1),cars.mpg, \n",
      "                                                 test_size = 0.23, random_state = 42 )\n",
      "\n",
      "\n",
      "X_train.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "model = LinearRegression()\n",
      "\n",
      "\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "\n",
      "y_pred = model.predict(X_test)\n",
      "=====\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "\n",
      "right_model = LinearRegression(fit_intercept = False,normalize = True)\n",
      "right_model.fit(X_train,y_train)\n",
      "\n",
      "print(jupyter_string + str(right_model.score(X_train,y_train)))\n",
      "print(jupyter_string + str(right_model.score(X_test,y_test)))\n",
      "\n",
      "\n",
      "over_model = LinearRegression(fit_intercept = False)\n",
      "Poly = PolynomialFeatures(4)\n",
      "polyX_train = Poly.fit_transform(X_train.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "polyX_test = Poly.fit_transform(X_test.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "\n",
      "polyX_train = np.c_[polyX_train,X_train[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "polyX_test= np.c_[polyX_test,X_test[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "\n",
      "over_model.fit(polyX_train,y_train)\n",
      "print(jupyter_string + str(over_model.score(polyX_train,y_train)))\n",
      "print(jupyter_string + str(over_model.score(polyX_test,y_test)))\n",
      "\n",
      "\n",
      "almost_model = LinearRegression(fit_intercept = False)\n",
      "Poly = PolynomialFeatures(2)\n",
      "polyX_train = Poly.fit_transform(X_train.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "polyX_test = Poly.fit_transform(X_test.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "\n",
      "polyX_train = np.c_[polyX_train,X_train[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "polyX_test= np.c_[polyX_test,X_test[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "\n",
      "almost_model.fit(polyX_train,y_train)\n",
      "print(jupyter_string + str(almost_model.score(polyX_train,y_train)))\n",
      "print(jupyter_string + str(almost_model.score(polyX_test,y_test)))\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.model_selection import learning_curve\n",
      "\n",
      "title = jupyter_string\n",
      "\n",
      "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
      "\n",
      "estimator = LinearRegression()\n",
      "plot_learning_curve(estimator, title, X_train, y_train, (0.0, 1.01), cv=cv, n_jobs=4)\n",
      "\n",
      "plt.show()\n",
      "=====\n",
      "plot_learning_curve(right_model, jupyter_string, X_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "y_pred_lasso = lasso.predict(polyX_test)\n",
      "y_pred_ridge = ridge.predict(polyX_test)\n",
      "y_pred_eNet = eNet.predict(polyX_test)\n",
      "\n",
      "mse_lasso = mean_squared_error(y_test,y_pred_lasso)\n",
      "mse_ridge = mean_squared_error(y_test,y_pred_ridge)\n",
      "mse_eNet = mean_squared_error(y_test,y_pred_eNet)\n",
      "\n",
      "print(jupyter_string,mse_lasso)\n",
      "print(jupyter_string,mse_ridge)\n",
      "print(jupyter_string,mse_eNet)\n",
      "=====\n",
      "plot_learning_curve(lasso, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "plot_learning_curve(ridge, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "=====\n",
      "plot_learning_curve(ridge, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "x_train = playerAggDfAllNbaAllStarInitFeatures.ix[:, playerAggDfAllNbaAllStarInitFeatures.columns != 'accolades_all_nba' <<unk>>]\n",
      "y_train = playerAggDfAllNbaAllStarInitFeatures['accolades_all_nba' <<unk>>]\n",
      "--------------------\n",
      "stack.head()\n",
      "=====\n",
      "SES = pd.read_csv(jupyter_string)\n",
      "SES.set_index(jupyter_string, inplace = True)\n",
      "SES = SES.iloc[:, 2:12]\n",
      "SES.dropna(thresh = 10, inplace = True)\n",
      "ColNames = SES.iloc[0]\n",
      "listnames = ColNames.str.extract(jupyter_string).astype(int)\n",
      "SES.columns = listnames\n",
      "SES[listnames] = SES[listnames].replace({jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string:jupyter_string}, regex=True)\n",
      "\n",
      "--------------------\n",
      "stack.head()\n",
      "=====\n",
      "stack[jupyter_string].value_counts()\n",
      "--------------------\n",
      "crimes = pd.read_csv(jupyter_string)\n",
      "crimes.head()\n",
      "=====\n",
      "AAnn = pd.read_csv(jupyter_string)\n",
      "AUF2015= pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "SES.index = SES.swapaxes(1,0).columns.str.replace(jupyter_string,jupyter_string) \n",
      "\n",
      "\n",
      "crime_table = pd.merge(SES.swapaxes(1,0), AAnn, left_index = True, right_on = 'Council District' <<unk>>, how = jupyter_string)\n",
      "\n",
      "\n",
      "crime_agg = pd.DataFrame(crime_table.groupby('Council District' <<unk>>).count().loc[:, 'GO Primary Key' <<unk>>]).rename(columns = {'GO Primary Key' <<unk>> : jupyter_string})\n",
      "\n",
      "\n",
      "crime_table = pd.merge(crime_table, crime_agg, left_on = 'Council District' <<unk>>, right_index = True, how = jupyter_string)\n",
      "--------------------\n",
      "import statsmodels.formula.api as smf\n",
      "model = smf.ols(formula=jupyter_string, data=stack)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "sesmodel = smf.ols(formula = jupyter_string, data = stack).fit()\n",
      "sesmodel.summary()\n",
      "--------------------\n",
      "model = smf.ols(formula = jupyter_string, data = stack).fit()\n",
      "model.summary()\n",
      "=====\n",
      "model = smf.probit(formula = jupyter_string , data = stack).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "=====\n",
      "crime_types = pd.merge(pd.get_dummies(crime_table.loc[:,  'Highest NIBRS/UCR Offense Description' <<unk>>]), crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "--------------------\n",
      "crime_types = pd.merge(crime_types, crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "=====\n",
      "crime_types_agg = pd.DataFrame(crime_types.groupby('Council District' <<unk>>)[jupyter_string].transform(jupyter_string))\n",
      "for item in [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]:\n",
      "    crime_types_agg = pd.merge(pd.DataFrame(crime_types.groupby('Council District' <<unk>>)[item].transform(jupyter_string)), crime_types_agg, left_index = True, right_index = True, how = jupyter_string)\n",
      "\n",
      "\n",
      "crime_table_types = pd.merge(crime_types_agg, crime_table, left_index = True, right_index = True)\n",
      "--------------------\n",
      "crime_table_types = pd.merge(crime_table_types, crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "=====\n",
      "stack_types = pd.merge(crime_table_types, AUF2015, left_on='GO Primary Key' <<unk>>, right_on=' Primary Key' <<unk>>, how=jupyter_string, indicator = True)\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "\n",
      "stack_types.loc[:, jupyter_string] = 0\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "\n",
      "stack_types.loc[:, jupyter_string] = 0\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "--------------------\n",
      "sns.heatmap(stack_types.corr(), annot = True, fmt = jupyter_string)\n",
      "=====\n",
      "stack_types[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]].corr()\n",
      "--------------------\n",
      "stack_types[[jupyter_string, jupyter_string, jupyter_string]].corr()\n",
      "=====\n",
      "pd.DataFrame(stack_types.loc[:, [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]].corrwith(stack_types.loc[:, jupyter_string]), columns = [jupyter_string]             )\n",
      "--------------------\n",
      "model_types[jupyter_string].summary()\n",
      "=====\n",
      "ENTER_CRIME_STRING_HERE = jupyter_string\n",
      "model_types[ENTER_CRIME_STRING_HERE].summary()\n",
      "--------------------\n",
      "districts = pd.read_csv(jupyter_string)\n",
      "districts.head()\n",
      "=====\n",
      "AAnn = pd.read_csv(jupyter_string)\n",
      "AUF2015= pd.read_csv(jupyter_string)\n",
      "districts = gpd.read_file(jupyter_string)\n",
      "--------------------\n",
      "districts.head()\n",
      "=====\n",
      "AUF2015.rename(columns={' Primary Key' <<unk>>: jupyter_string, ' Effect on Officer' <<unk>>: jupyter_string, jupyter_string:jupyter_string, 'Officer Yrs of Service' <<unk>>: jupyter_string}, inplace=True)\n",
      "AAnn.rename(columns={jupyter_string:jupyter_string, 'Council District' <<unk>>: jupyter_string}, inplace=True)\n",
      "\n",
      "\n",
      "AUF2015.columns = AUF2015.columns.str.replace(jupyter_string,jupyter_string)\n",
      "AAnn.columns = AAnn.columns.str.replace(jupyter_string,jupyter_string)\n",
      "\n",
      "\n",
      "AUF2015 = AUF2015.drop_duplicates(subset=jupyter_string, keep=jupyter_string, inplace = False)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(10,10))\n",
      "districts.plot(ax=ax, color=jupyter_string)\n",
      "AUF2015.plot(ax=ax, color=jupyter_string)\n",
      "AAnn.plot(ax=ax, color=jupyter_string)\n",
      "=====\n",
      "AAnnCount = AAnn.groupby(jupyter_string).count()\n",
      "\n",
      "\n",
      "AAnnCount.index = AAnnCount.index.map(lambda x: int(x))\n",
      "\n",
      "districts.set_index(districts[jupyter_string].astype(int), inplace = True)\n",
      "districts.index = districts.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "geo_merge = pd.merge(districts, pd.DataFrame(AAnnCount.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string )\n",
      "\n",
      "\n",
      "AAnnMap = geo_merge.plot(column = jupyter_string,legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "geo_merge[jupyter_string] = geo_merge[jupyter_string].apply(lambda x: x.representative_point().coords[:])\n",
      "geo_merge[jupyter_string] = [coords[0] for coords in geo_merge[jupyter_string]]\n",
      "\n",
      "for idx, row in geo_merge.iterrows():\n",
      "    AAnnMap.annotate(s = row[jupyter_string], xy = row[jupyter_string], horizontalalignment= jupyter_string, fontsize = 30, color = jupyter_string)\n",
      "\n",
      "AAnnMap.set_title(jupyter_string, fontsize = 30, loc = jupyter_string)\n",
      "AAnnMap.set_axis_off()\n",
      "--------------------\n",
      "force_count = force.groupby(jupyter_string).count()\n",
      "\n",
      "\n",
      "force_count.index = force_count.index.map(lambda x: int(x))\n",
      "\n",
      "force_count.set_index(force_count[jupyter_string].astype(int), inplace = True)\n",
      "force_count.index = force_count.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "geo_merge = pd.merge(districts, pd.DataFrame(force_count.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string )\n",
      "\n",
      "\n",
      "forceMap = geo_merge.plot(column = jupyter_string,legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "geo_merge[jupyter_string]\n",
      "=====\n",
      "AUF2015Count = AUF2015.groupby('CouncilDistrict' <<unk>>).count()\n",
      "\n",
      "\n",
      "AUF2015Count.index = AUF2015Count.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "AUF_merge = pd.merge(districts, pd.DataFrame(AUF2015Count.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string)\n",
      "\n",
      "\n",
      "AUFMap = AUF_merge.plot(column = jupyter_string, legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "AUF_merge[jupyter_string] = AUF_merge[jupyter_string].apply(lambda x: x.representative_point().coords[:])\n",
      "AUF_merge[jupyter_string] = [coords[0] for coords in AUF_merge[jupyter_string]]\n",
      "\n",
      "for idx, row in AUF_merge.iterrows():\n",
      "    AUFMap.annotate(s = row[jupyter_string], xy = row[jupyter_string], horizontalalignment= jupyter_string, fontsize = 30, color = jupyter_string)\n",
      "\n",
      "AUFMap.set_title(jupyter_string, fontsize = 30, loc = jupyter_string)\n",
      "AUFMap.set_axis_off()\n",
      "\n",
      "AUFMap.figure.savefig(jupyter_string)\n",
      "--------------------\n",
      "varnames = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "          jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "x_lim = 60\n",
      "burnin = 50000\n",
      "\n",
      "y_pred = trace[burnin:].get_values(jupyter_string)\n",
      "mu_mean = trace[burnin:].get_values(jupyter_string).mean()\n",
      "\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype=jupyter_string, color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds' <<unk>>].values, range=[0, x_lim], bins=x_lim, histtype=jupyter_string)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import LeaveOneYearOut\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "=====\n",
      "playerAggDfAllNbaAllStar.head()\n",
      "--------------------\n",
      "_ = pm.plot_posterior(trace[burnin:], varnames=[jupyter_string, jupyter_string])\n",
      "=====\n",
      "x_lim = 60\n",
      "y_pred = trace[burnin:].get_values(jupyter_string)\n",
      "\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype=jupyter_string, color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds' <<unk>>].values, range=[0, x_lim], bins=x_lim, histtype=jupyter_string)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "pm.forestplot(trace[burnin:], varnames=[jupyter_string])\n",
      "=====\n",
      "prob_pois = trace[burnin:][jupyter_string].mean()\n",
      "prob_nb = 1 - prob_pois\n",
      "BF = (prob_nb/prob_pois)*(prior_model_prob/(1-prior_model_prob))\n",
      "print(jupyter_string % BF)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import re\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk import word_tokenize\n",
      "import matplotlib.pyplot as plt\n",
      "import warnings\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "conversation_df = pd.read_csv(jupyter_string,encoding=jupyter_string, sep=jupyter_string,warn_bad_lines =False,header=None)\n",
      "lines_df = pd.read_csv(jupyter_string,sep=jupyter_string,error_bad_lines=False,warn_bad_lines =False,header=None)\n",
      "characters_df = pd.read_csv(jupyter_string,sep=jupyter_string,warn_bad_lines =False,error_bad_lines=False,header=None)\n",
      "\n",
      "--------------------\n",
      "dialogues.info()\n",
      "=====\n",
      "cul_sentence = jupyter_string.join(dialogues)\n",
      "nltk.download(jupyter_string)\n",
      "--------------------\n",
      "characters_df.head()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "characters_df.columns=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "characters_df.head(5)\n",
      "--------------------\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk import word_tokenize\n",
      "=====\n",
      "tknzr = TweetTokenizer()\n",
      "final_tokens = tknzr.tokenize(cul_sentence)\n",
      "fdist = nltk.FreqDist(final_tokens)\n",
      "fdist.plot(20,cumulative=False)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "conversations_df.columns=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "conversations_df.head(5)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "conversation_df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "conversation_df.head(5)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "lines_df = pd.read_csv(jupyter_string)\n",
      "lines_df.columns = [jupyter_string, jupyter_string]\n",
      "lines_df.head(5)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "lines_df.columns = [jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "lines_df.head(5)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf = TfidfVectorizer(ngram_range=(2,2))\n",
      "tfidf_matrix = tfidf.fit_transform(df[jupyter_string])\n",
      "tfidf_matrix.shape\n",
      "=====\n",
      "biCharWords = nltk.bigrams(char_final_tokens)\n",
      "biFdist = nltk.FreqDist(biCharWords)\n",
      "print(len(biFdist))\n",
      "biFdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "playerAggDfAllNbaAllStar = playerAggDfAllNbaAllStar.reset_index()\n",
      "playerAggDfAllNbaAllStar.head()\n",
      "=====\n",
      "playerAggDfAllNbaAllStarInitFeatures.head()\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "final_processable_df.head(5)\n",
      "=====\n",
      "merged_df = pd.merge(lines_df, characters_df, how=jupyter_string, on=[jupyter_string,jupyter_string,jupyter_string],\n",
      "         left_index=False, right_index=False, sort=True,\n",
      "         suffixes=(jupyter_string, jupyter_string), copy=True, indicator=False)\n",
      "merged_df.head()\n",
      "--------------------\n",
      "merged_df.info()\n",
      "=====\n",
      "merged_df.dropna(how=jupyter_string, inplace=True)\n",
      "--------------------\n",
      "triFdist = nltk.FreqDist(triCharWords)\n",
      "print(len(triFdist))\n",
      "triFdist.plot(20, cumulative=False)\n",
      "triFdist.most_common()[:5]\n",
      "=====\n",
      "triCharWords = nltk.trigrams(char_final_tokens)\n",
      "triFdist = nltk.FreqDist(triCharWords)\n",
      "triFdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "dialogues[dialogues.dialogue == jupyter_string].dialogue.iloc[0]\n",
      "=====\n",
      "char_cul_sentence = jupyter_string.join(char_dialogues)\n",
      "--------------------\n",
      "char_cul_sentence.split(jupyter_string)\n",
      "=====\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "tknzr = TweetTokenizer()\n",
      "char_final_tokens = tknzr.tokenize(char_cul_sentence)\n",
      "--------------------\n",
      "df = pd.DataFrame(ng, columns=[jupyter_string, jupyter_string])\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: len(x))\n",
      "df.head()\n",
      "=====\n",
      "fdist = nltk.FreqDist(ng)\n",
      "fdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "reg_dftrain=pd.read_csv(jupyter_string)\n",
      "reg_dftest=pd.read_csv(jupyter_string)\n",
      "dftrain = pd.read_csv(jupyter_string)\n",
      "dftest = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(train_values[:,np.newaxis], np.array(Ytrain_actual), jupyter_string)\n",
      "ax.plot(train_values[:,np.newaxis], model.predict(train_values[:,np.newaxis]), jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(train_values, Ytrain_actual, color=jupyter_string)\n",
      "plt.plot(train_values, Ytrain_pred)\n",
      "--------------------\n",
      "plt.scatter(test_values, Ytest_actual, color=jupyter_string)\n",
      "plt.plot(test_values, Ytest_pred)\n",
      "=====\n",
      "plt.scatter(test_values, Ytest_actual, color=jupyter_string)\n",
      "plt.plot(test_values, Ytest_pred)\n",
      "--------------------\n",
      "base_dict_train.to_csv(jupyter_string)\n",
      "base_dict_test.to_csv(jupyter_string)\n",
      "=====\n",
      "train_base = pd.DataFrame.from_dict(base_dict_train)\n",
      "test_base = pd.DataFrame.from_dict(base_dict_test)\n",
      "train_base.to_csv(jupyter_string)\n",
      "test_base.to_csv(jupyter_string)\n",
      "--------------------\n",
      "playerAggDfAllNbaAllStarInitFeatures = playerAggDfAllNbaAllStarInitFeatures.reset_index(drop=True)\n",
      "=====\n",
      "playerAggDfAllNbaAllStarInitFeaturesFullSet = playerAggDfAllNbaAllStarInitFeatures.merge(\n",
      "    playerAggDfAllNbaAllStar[['season_start_year' <<unk>>, 'perGameStats_Player' <<unk>>, 'perGameStats_Pos' <<unk>>]],\n",
      "    how = jupyter_string,\n",
      "    left_index = True,\n",
      "    right_index = True\n",
      ")\n",
      "\n",
      "\n",
      "playerAggDfAllNbaAllStarInitFeaturesFullSet.tail()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "=====\n",
      "dftrain = reg_dftrain\n",
      "dftest = reg_dftest\n",
      "Ytrain_actual = dftrain['like_x' <<unk>>]\n",
      "Ytest_actual = dftest['like_x' <<unk>>]\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "=====\n",
      "mergedf = pd.concat([dftrain, dftest])\n",
      "avg_like = {}\n",
      "for iid in set(list(mergedf['iid' madeupword0186].values)):\n",
      "    avg_like[iid] = np.mean(mergedf[mergedf['iid' madeupword0186] == iid]['like_x' <<unk>>])\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.info()\n",
      "\n",
      "\n",
      "\n",
      "df2 = df[['Transaction_Id' <<unk>>,'Product_Name' <<unk>>]]\n",
      "df2.head(5)\n",
      "df = df2.drop_duplicates()\n",
      "df.head(5)\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "result_df.sort_values(jupyter_string, ascending=False).head(20)\n",
      "=====\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=False)\n",
      "print(result_df.head(10))\n",
      "--------------------\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=True)\n",
      "print(result_df.head(10))\n",
      "=====\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=False)\n",
      "print(result_df.head(10))\n",
      "--------------------\n",
      "my_dataframe = pd.read_csv(jupyter_string)\n",
      "test_dataframe = pd.read_csv(jupyter_string)\n",
      "my_head = my_dataframe.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "from pandas import Series,DataFrame\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import statsmodels.formula.api as sm\n",
      "\n",
      "my_dataframe=pd.read_csv(jupyter_string)\n",
      "test_dataframe=pd.read_csv(jupyter_string)\n",
      "my_head=my_dataframe.head()\n",
      "\n",
      "\n",
      "--------------------\n",
      "my_head.head()\n",
      "=====\n",
      "bitcoin_price_analysis = pd.read_csv(jupyter_string)\n",
      "bcktstdf=pd.read_csv(jupyter_string)\n",
      "tmp=bitcoin_price_analysis.head()\n",
      "--------------------\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "=====\n",
      "x_test_full.head()\n",
      "--------------------\n",
      "my_plt = tmp.plot(kind=jupyter_string)\n",
      "my_inf = jupyter_string\n",
      "my_inf\n",
      "=====\n",
      "my_inf=jupyter_string\n",
      "my_plt=sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "my_inf=jupyter_string\n",
      "my_plt=sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "=====\n",
      "g = sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "my_plt = sns.jointplot( \"btc_total_bitcoins\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "=====\n",
      "my_plt=sns.jointplot( \"btc_total_bitcoins\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "my_inf=jupyter_string\n",
      "--------------------\n",
      "my_plt=sns.jointplot( \"btc_market_cap\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "my_inf=jupyter_string\n",
      "=====\n",
      "g = sns.jointplot( \"btc_total_bitcoins\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "ms_columns = bitcoin_price_analysis.columns[bitcoin_price_analysis.isnull().any()]\n",
      "ms_columns\n",
      "=====\n",
      "ms_columns=my_dataframe.columns[df1.isnull().any()].tolist()\n",
      "--------------------\n",
      "ms_columns=dataframe.columns[dataframe.isnull().any()].tolist()\n",
      "ms_columns\n",
      "=====\n",
      "df1=bitcoin_price_analysis\n",
      "missing_columns=df1.columns[df1.isnull().any()].tolist()\n",
      "--------------------\n",
      "btc_total_bitcoins = btc_total_bitcoins.fillna(method=jupyter_string)\n",
      "btc_total_bitcoins.head()\n",
      "=====\n",
      "my_dataframe['btc_total_bitcoins' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "--------------------\n",
      "my_dataframe.head()\n",
      "=====\n",
      "df1['btc_total_bitcoins' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "=====\n",
      "all_nba_all_year_predicted_df['accolades_all_nba' <<unk>>].value_counts().plot(kind = jupyter_string)\n",
      "--------------------\n",
      "my_dataframe.head()\n",
      "=====\n",
      "df1['btc_trade_volume' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_transaction_fees' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_blocks_size' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_median_confirmation_time' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_difficulty' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "\n",
      "--------------------\n",
      "sns.heatmap(my_dataframe.corr())\n",
      "=====\n",
      "df1.corr()\n",
      "--------------------\n",
      "titanic = pd.read_csv(jupyter_string)\n",
      "titanic.head()\n",
      "=====\n",
      "titanic_df = pd.read_csv(jupyter_string)\n",
      "titanic_df.head()\n",
      "--------------------\n",
      "titanic_df.head()\n",
      "=====\n",
      "titanic_df.head()\n",
      "--------------------\n",
      "groupedby_Survived.head()\n",
      "=====\n",
      "percentage_survived = (survival_count.loc[1,'PassengerId' <<unk>>]/titanic_df.shape[0])*100\n",
      "percentage_not_survived = (survival_count.loc[0,'PassengerId' <<unk>>]/titanic_df.shape[0])*100\n",
      "sizes = [percentage_survived,percentage_not_survived]\n",
      "labels = ['Survived' <<unk>>,jupyter_string]\n",
      "plt.pie(sizes, labels=labels, autopct=jupyter_string, startangle=90)\n",
      "x = plt.title(jupyter_string)\n",
      "--------------------\n",
      "all_nba_all_year_predicted_df.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "all_nba_all_year_predicted_df[all_nba_all_year_predicted_df['accolades_all_nba' <<unk>>] == jupyter_string][jupyter_string].hist()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "customer_df.to_csv(jupyter_string)\n",
      "=====\n",
      "customer_df.to_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head(10)\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.isnull().sum()\n",
      "=====\n",
      "df.isnull().sum()\n",
      "--------------------\n",
      "df.CustomerID = df.CustomerID.astype(int)\n",
      "df.CustomerID.head()\n",
      "=====\n",
      "df['CustomerID' <<unk>>] = df.CustomerID.astype(int)\n",
      "\n",
      "\n",
      "df.CustomerID.head()\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, index_col=None)\n",
      "\n",
      "\n",
      "df.head()\n",
      "=====\n",
      "df.to_csv(jupyter_string,index=None)\n",
      "--------------------\n",
      "invoice_data = pd.read_csv(jupyter_string)\n",
      "invoice_data.head()\n",
      "=====\n",
      "invoice_data = df.groupby('CustomerID' <<unk>>).InvoiceNo.agg({ jupyter_string : jupyter_string })\n",
      "\n",
      "\n",
      "invoice_data.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "full_df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "full_df.head()\n",
      "--------------------\n",
      "full_df.info()\n",
      "=====\n",
      "df = full_df[['Survived' <<unk>>, 'Sex' madeupword0002, 'Pclass' <<unk>>, 'Age' <<unk>>]]\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0,100)\n",
      "plt.xlim(0,100)\n",
      "plt.scatter(df[jupyter_string],df[jupyter_string])\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0,100)\n",
      "plt.xlim(0,100)\n",
      "plt.scatter(df[jupyter_string],df[jupyter_string])\n",
      "=====\n",
      "survivor_counts = df['Survived' <<unk>>].value_counts()\n",
      "survivor_counts.index = [jupyter_string, jupyter_string]\n",
      "survivor_counts.name = jupyter_string\n",
      "survivor_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from IPython.display import display\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string, nrows=5, usecols=['LA1' <<unk>>,'LO1' <<unk>>,'EL1' <<unk>>,'LO1.1' <<unk>>,'LA1.1' <<unk>>,'EL1.1' <<unk>>,'VISIBLE' madeupword0002]).head()\n",
      "df['EL1' <<unk>>] = df['EL1' <<unk>>].round()\n",
      "df['EL1.1' <<unk>>] = df['EL1.1' <<unk>>].round()\n",
      "df['VISIBLE' madeupword0002] = df['VISIBLE' madeupword0002].round()\n",
      "display(df)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "SPX_data_dump = pd.read_csv(jupyter_string)\n",
      "print (jupyter_string.format(*SPX_data_dump.shape))\n",
      "print (jupyter_string)\n",
      "display(SPX_data_dump.head(n=5))\n",
      "--------------------\n",
      "SPX_data_dump.head()\n",
      "=====\n",
      "SPX_data_dump['Date' <<unk>>] = pd.to_datetime(SPX_data_dump.Date, format=jupyter_string)\n",
      "SPX_data_dump = SPX_data_dump.sort_values(by='Date' <<unk>>, ascending=1)\n",
      "SPX_data_dump.index = np.arange(rows)[::-1]\n",
      "display(SPX_data_dump.head(n=5))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train = pd.read_csv(jupyter_string)\n",
      "train.head(3)\n",
      "--------------------\n",
      "SPX_data_modified.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "SPX_data_modified.to_csv(jupyter_string)\n",
      "--------------------\n",
      "SPX_data_modified = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "X = SPX_data_modified.iloc[:,0:(cols_new-1)]\n",
      "Y = SPX_data_modified.iloc[:,(cols_new-1)]\n",
      "print(jupyter_string)\n",
      "display(X.head(n=5))\n",
      "print(jupyter_string)\n",
      "display(Y.head(n=5))\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "a = pd.concat([X,Y], axis=1)\n",
      "if pd.DataFrame.equals(a,SPX_data_modified):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
      "print(jupyter_string)\n",
      "print(X_train.shape)\n",
      "print(jupyter_string)\n",
      "print(X_test.shape)\n",
      "print(jupyter_string)\n",
      "print(Y_train.shape)\n",
      "print(jupyter_string)\n",
      "print(Y_test.shape)\n",
      "=====\n",
      "n = round(0.2*rows_new)\n",
      "X_test = X.iloc[0:n,:]\n",
      "Y_test = Y[0:n]\n",
      "\n",
      "X_cv = X.iloc[n:(2*n+1),:]\n",
      "Y_cv = Y[n:(2*n+1)]\n",
      "\n",
      "X_train = X.iloc[(2*n+1):,:]\n",
      "Y_train = Y[(2*n+1):]\n",
      "\n",
      "print(X_test.shape[0], X_cv.shape[0], X_train.shape[0])\n",
      "--------------------\n",
      "X_train = X_train.reset_index(drop=True)\n",
      "Y_train = Y_train.reset_index(drop=True)\n",
      "X_cv = X_cv.reset_index(drop=True)\n",
      "Y_cv = Y_cv.reset_index(drop=True)\n",
      "=====\n",
      "a = pd.concat([Y_test,Y_cv,Y_train])\n",
      "if pd.DataFrame.equals(a,Y):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "\n",
      "b = pd.concat([X_test,X_cv,X_train])\n",
      "if pd.DataFrame.equals(b,X):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "=====\n",
      "from sklearn.metrics import f1_score, accuracy_score\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX' <<unk>>)\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX' <<unk>>)\n",
      "\n",
      "benchmark_Y_pred_train = np.empty(Y_train.shape[0])\n",
      "benchmark_Y_pred_train.fill(1)\n",
      "cashflows_train = (X_train.iloc[0,0],X_train.iloc[0,int(close_index[0])]),\\\n",
      "                  (X_train.iloc[X_train.shape[0]-1,0],-X_train.iloc[X_train.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.\\\n",
      "      format(accuracy_score(Y_train, benchmark_Y_pred_train)))\n",
      "print(jupyter_string.format(f1_score(Y_train, benchmark_Y_pred_train)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_train)))\n",
      "\n",
      "benchmark_Y_pred_cv = np.empty(Y_cv.shape[0])\n",
      "benchmark_Y_pred_cv.fill(1)\n",
      "cashflows_cv = (X_cv.iloc[0,0],X_cv.iloc[0,int(close_index[0])]),\\\n",
      "               (X_cv.iloc[X_cv.shape[0]-1,0],-X_cv.iloc[X_cv.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.\\\n",
      "      format(accuracy_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print(jupyter_string.format(f1_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_cv)))\n",
      "\n",
      "benchmark_Y_pred_test = np.empty(Y_test.shape[0])\n",
      "benchmark_Y_pred_test.fill(1)\n",
      "cashflows_test = (X_test.iloc[0,0],X_test.iloc[0,int(close_index[0])]),\\\n",
      "                 (X_test.iloc[X_test.shape[0]-1,0],-X_test.iloc[X_test.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.format(accuracy_score(Y_test, benchmark_Y_pred_test)))\n",
      "print(jupyter_string.format(f1_score(Y_test, benchmark_Y_pred_test)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_test)))\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "svm = SVC()\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "\n",
      "dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "=====\n",
      "max_depth_list = np.arange(1,11)\n",
      "min_samples_leaf_list = np.arange(1,11)\n",
      "criterion_list = (jupyter_string,jupyter_string)\n",
      "splitter_list = (jupyter_string,jupyter_string)\n",
      "\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "best_dt = dt\n",
      "best_dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_dt_Y_cv_pred = best_dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_dt_F1 = f1_score(Y_cv, best_dt_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for min_samples_leaf in min_samples_leaf_list:\n",
      "        for criterion in criterion_list:\n",
      "            for splitter in splitter_list:\n",
      "                dt = DecisionTreeClassifier(random_state=42, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
      "                dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
      "                if dt_F1 > best_dt_F1:\n",
      "                    best_dt = dt\n",
      "                    best_dt_F1 = dt_F1\n",
      "                    print(jupyter_string.\\\n",
      "                          format(max_depth, min_samples_leaf, criterion, splitter, dt_F1))\n",
      "\n",
      "print(best_dt)\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
      "=====\n",
      "best_dt_feature_importance = pd.Series(best_dt.feature_importances_, index=\\\n",
      "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
      "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
      "print(best_dt_feature_importance.sort_values(ascending=False))\n",
      "--------------------\n",
      "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=jupyter_string)\n",
      "=====\n",
      "sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap = jupyter_string)\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "=====\n",
      "kernel_list = (jupyter_string, jupyter_string, jupyter_string, jupyter_string)\n",
      "C_list = np.arange(1,16)\n",
      "gamma_list = [1e-7*10**i for i in np.arange(1,7)]\n",
      "degree_list = np.arange(2,6)\n",
      "\n",
      "svm = SVC()\n",
      "best_svm = svm\n",
      "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_svm_Y_cv_pred = best_svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_svm_F1 = f1_score(Y_cv, best_svm_Y_cv_pred)\n",
      "for kernel in kernel_list:\n",
      "    for C in C_list:\n",
      "        for gamma in gamma_list:\n",
      "            for degree in degree_list:\n",
      "                svm = SVC(C=C, gamma=gamma, kernel=kernel, degree=degree)\n",
      "                svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                svm_Y_cv_pred = svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                svm_F1 = f1_score(Y_cv, svm_Y_cv_pred)\n",
      "                if svm_F1 > best_svm_F1:\n",
      "                    best_svm = svm\n",
      "                    best_svm_F1 = svm_F1\n",
      "                    print(jupyter_string.format(kernel, C, gamma, degree, svm_F1))\n",
      "\n",
      "print(best_svm)\n",
      "--------------------\n",
      "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_svm_Y_test_pred = best_svm.predict(X_test.iloc[:,engineered_features_start_index:X_test.shape[1]])\n",
      "=====\n",
      "n_estimators_list = [1] + [i*10 for i in np.arange(1,11)]\n",
      "criterion_list = (jupyter_string,jupyter_string)\n",
      "max_depth_list = np.arange(1,7)\n",
      "min_samples_leaf_list = np.arange(1, 10, 3)\n",
      "max_features_list = (jupyter_string,jupyter_string,None)\n",
      "\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "best_rf = rf\n",
      "best_rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for min_samples_leaf in min_samples_leaf_list:\n",
      "        for criterion in criterion_list:\n",
      "            for n_estimators in n_estimators_list:\n",
      "                for max_features in max_features_list:\n",
      "                    rf = RandomForestClassifier(random_state=42,max_depth=max_depth, min_samples_leaf=min_samples_leaf,\\\n",
      "                                                criterion=criterion, n_estimators=n_estimators, max_features=max_features)\n",
      "                    rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                    rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                    rf_F1 = f1_score(Y_cv, rf_Y_cv_pred)\n",
      "                    if rf_F1 > best_rf_F1:\n",
      "                        best_rf = rf\n",
      "                        best_rf_F1 = rf_F1\n",
      "                        print(jupyter_string.\\\n",
      "                              format(max_depth, min_samples_leaf, criterion, n_estimators, max_features, rf_F1))\n",
      "\n",
      "print(best_rf)\n",
      "--------------------\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "rf_Y_test_pred = rf.predict(X_test.iloc[:,engineered_features_start_index:X_test.shape[1]])\n",
      "=====\n",
      "best_rf_feature_importance = pd.Series(best_rf.feature_importances_, index=\\\n",
      "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
      "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
      "print(best_rf_feature_importance.sort_values(ascending=False))\n",
      "--------------------\n",
      "best_rf_feature_importance.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
      "max_depth_list = np.arange(1,11)\n",
      "reg_lambda_list = [0, 0.1, 0.5, 1, 2, 5]\n",
      "gamma_list = [0] + [10**i for i in np.arange(0,4)]\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "best_xgb = xgb\n",
      "best_xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_xgb_Y_cv_pred = best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for n_estimators in n_estimators_list:\n",
      "        for reg_lambda in reg_lambda_list:\n",
      "            for gamma in gamma_list:\n",
      "                xgb = x_gb.XGBClassifier(max_depth=max_depth, n_estimators = n_estimators, reg_lambda=reg_lambda, gamma=gamma)\n",
      "                xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                xgb_Y_cv_pred = xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                xgb_F1 = f1_score(Y_cv, xgb_Y_cv_pred)\n",
      "                if xgb_F1 > best_xgb_F1:\n",
      "                    best_xgb = xgb\n",
      "                    best_xgb_F1 = xgb_F1\n",
      "                    print(jupyter_string.\\\n",
      "                           format(max_depth, n_estimators, reg_lambda, gamma, xgb_F1))\n",
      "\n",
      "print(best_xgb)\n",
      "--------------------\n",
      "best_xgb_Y_cv_pred = best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
      "=====\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX' <<unk>>)\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX' <<unk>>)\n",
      "date_index = np.where(SPX_data_modified.columns.values == 'Date' <<unk>>)\n",
      "\n",
      "def convert_label_to_cash_sign(label):\n",
      "    if label == 1:\n",
      "        cash_sign = -1\n",
      "    else:\n",
      "        cash_sign = 1\n",
      "    \n",
      "    return(cash_sign)\n",
      "\n",
      "def get_cashflows(best_clf, X):\n",
      "    Y_pred = best_clf.predict(X.iloc[:,engineered_features_start_index:X.shape[1]])\n",
      "    \n",
      "    \n",
      "    \n",
      "    length = len(Y_pred)\n",
      "    prev_cash_sign = convert_label_to_cash_sign(Y_pred[length-1])\n",
      "    net_position = -prev_cash_sign\n",
      "    cashflows = ((X.iloc[length-1, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                  X.iloc[length-1, int(open_index[0])]*prev_cash_sign),)\n",
      "\n",
      "    for i in np.arange(length-2, 1, -1):\n",
      "        cur_cash_sign = convert_label_to_cash_sign(Y_pred[i])\n",
      "        if cur_cash_sign != prev_cash_sign or net_position == 0:\n",
      "            net_position = net_position - cur_cash_sign\n",
      "            cashflows = (cashflows) + ((X.iloc[i, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                         X.iloc[i, int(open_index[0])]*cur_cash_sign),)\n",
      "        prev_cash_sign = cur_cash_sign\n",
      "\n",
      "    cur_cash_sign = convert_label_to_cash_sign(Y_pred[0])\n",
      "    \n",
      "    \n",
      "    \n",
      "    if net_position == 0:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),) + \\\n",
      "                                ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*cur_cash_sign),)\n",
      "    elif cur_cash_sign != prev_cash_sign:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),)\n",
      "        net_position = net_position - cur_cash_sign\n",
      "    else:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*prev_cash_sign),)\n",
      "        net_position = net_position + prev_cash_sign\n",
      "    \n",
      "    return(cashflows)\n",
      "\n",
      "best_dt_cv_cashflows = get_cashflows(best_dt, X_cv)\n",
      "best_svm_cv_cashflows = get_cashflows(best_svm, X_cv)\n",
      "best_rf_cv_cashflows = get_cashflows(best_rf, X_cv)\n",
      "best_xgb_cv_cashflows = get_cashflows(best_xgb, X_cv)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "voting_list = (jupyter_string,jupyter_string)\n",
      "weights_list = [[2,1,3],[2,3,1],[1,2,3],[1,3,2],[3,1,2],[3,2,1],None]\n",
      "vot = VotingClassifier(estimators=[(jupyter_string, best_dt), (jupyter_string, best_xgb), (jupyter_string, best_rf)])\n",
      "best_vot = vot\n",
      "best_vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_vot_Y_cv_pred = best_vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_vot_F1 = f1_score(Y_cv, best_vot_Y_cv_pred)\n",
      "for voting in voting_list:\n",
      "    for weights in weights_list:        \n",
      "        vot = VotingClassifier(estimators=[(jupyter_string, best_dt), (jupyter_string, best_xgb), (jupyter_string, best_rf)], voting=voting, weights=weights)\n",
      "        vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "        vot_Y_cv_pred = vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "        vot_F1 = f1_score(Y_cv, vot_Y_cv_pred)        \n",
      "        if vot_F1 > best_vot_F1:\n",
      "            best_vot = vot\n",
      "            best_vot_F1 = vot_F1\n",
      "            print(jupyter_string.format(voting, weights, vot_F1))\n",
      "\n",
      "print(best_vot)\n",
      "--------------------\n",
      "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_rf_accuracy = accuracy_score(Y_cv, best_rf_Y_cv_pred)\n",
      "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
      "best_rf_cashflows = get_cashflows(best_rf, X_cv)\n",
      "best_rf_XIRR = xirr(best_rf_cashflows)\n",
      "print(jupyter_string.format(best_rf_accuracy, \\\n",
      "                                                                                               best_rf_F1, 100*best_rf_XIRR))\n",
      "=====\n",
      "print(jupyter_string.format(best_rf))\n",
      "print((jupyter_string)*100)\n",
      "\n",
      "random_state_list = np.arange(1,101)\n",
      "F1_list = []\n",
      "XIRR_list = []\n",
      "for random_state in random_state_list:\n",
      "    rf_modified_random_state = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=jupyter_string,\n",
      "                            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                            min_samples_leaf=7, min_samples_split=2,\n",
      "                            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
      "                            oob_score=False, random_state=random_state, verbose=0, warm_start=False)\n",
      "    rf_modified_random_state.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "    rfm_Y_cv_pred = rf_modified_random_state.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "    rfm_F1 = f1_score(Y_cv, rfm_Y_cv_pred)\n",
      "    F1_list = F1_list + [rfm_F1]\n",
      "    rfm_cv_cashflows = get_cashflows(rf_modified_random_state, X_cv)\n",
      "    try:\n",
      "        rfm_XIRR = xirr(rfm_cv_cashflows)\n",
      "    except:\n",
      "        rfm_XIRR = np.nan\n",
      "    XIRR_list = XIRR_list + [rfm_XIRR]\n",
      "\n",
      "\n",
      "\n",
      "from itertools import compress\n",
      "XIRR_list = list(compress(XIRR_list, np.logical_not(np.isnan(XIRR_list))))\n",
      "\n",
      "f, axes = plt.subplots(2,1)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string.format(np.mean(F1_list), np.std(F1_list)))\n",
      "print(jupyter_string.\\\n",
      "      format(np.max(F1_list), int(np.where(F1_list == np.max(F1_list))[0]+1)))\n",
      "print(jupyter_string.format(np.min(F1_list), int(np.where(F1_list == np.min(F1_list))[0]+1)))\n",
      "print(jupyter_string)\n",
      "print(jupyter_string.format(100*np.mean(XIRR_list), 100*np.std(XIRR_list)))\n",
      "print(jupyter_string.\\\n",
      "      format(100*np.max(XIRR_list), int(np.where(XIRR_list == np.max(XIRR_list))[0]+1)))\n",
      "print(jupyter_string.\\\n",
      "      format(100*np.min(XIRR_list), int(np.where(XIRR_list == np.min(XIRR_list))[0]+1)))\n",
      "\n",
      "axes[0].axis([0, 100, 0.5, 0.7])\n",
      "axes[0].plot(F1_list)\n",
      "axes[0].set_ylabel(jupyter_string)\n",
      "\n",
      "axes[1].axis([0, 100, 0.18, 7.5])\n",
      "axes[1].plot(XIRR_list)\n",
      "axes[1].set_ylabel(jupyter_string)\n",
      "\n",
      "axes[1].set_xlabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "sns.distplot(F1_list)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import plotly\n",
      "plotly.tools.set_credentials_file(username=jupyter_string, api_key=jupyter_string)\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.tools import FigureFactory as FF\n",
      "import scipy\n",
      "\n",
      "x = F1_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "--------------------\n",
      "x = XIRR_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "=====\n",
      "x = XIRR_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.05,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "--------------------\n",
      "x = F1_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "=====\n",
      "X_train_new = pd.concat([X_cv, X_train])\n",
      "Y_train_new = pd.concat([Y_cv, Y_train])\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(best_rf, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
      "score_classifier(best_rf, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
      "best_rf_test_cashflows = get_cashflows(best_rf, X_test)\n",
      "print(jupyter_string.format(100*xirr(best_rf_test_cashflows)))\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(best_vot, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
      "score_classifier(best_vot, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
      "best_vot_test_cashflows = get_cashflows(best_vot, X_test)\n",
      "print(jupyter_string.format(100*xirr(best_vot_test_cashflows)))\n",
      "--------------------\n",
      "data = pandas.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pandas.DataFrame()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.DataFrame()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "data.tail()\n",
      "=====\n",
      "data.tail()\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "pd.date_range(jupyter_string, jupyter_string, freq=jupyter_string)\n",
      "=====\n",
      "times = pd.DatetimeIndex(data['starttime' <<unk>>])\n",
      "--------------------\n",
      "days = np.array([1,2,3,4,5,6])\n",
      "days\n",
      "=====\n",
      "import numpy as np\n",
      "np.exp(data[jupyter_string])\n",
      "--------------------\n",
      "rides.gender.value_counts()\n",
      "=====\n",
      "pd.value_counts(data['gender' <<unk>>])\n",
      "--------------------\n",
      "pd.value_counts(times.dayofweek).sort_index()\n",
      "=====\n",
      "pd.value_counts(times.dayofweek, sort=False)\n",
      "--------------------\n",
      "pd.value_counts(times.dayofweek, sort=True)\n",
      "=====\n",
      "pd.value_counts(times.month)\n",
      "--------------------\n",
      "pd.value_counts(times.year)\n",
      "=====\n",
      "pd.value_counts(times.month, sort=False)\n",
      "--------------------\n",
      "df.groupby([df.index.hour, df.index.dayofweek]).size()\n",
      "=====\n",
      "pd.value_counts(times.hour)\n",
      "--------------------\n",
      "data.groupby(times.hour).mean()\n",
      "=====\n",
      "data.groupby(times.hour)[jupyter_string].mean()\n",
      "--------------------\n",
      "data.groupby(times.hour)[jupyter_string].mean()\n",
      "=====\n",
      "data.groupby(['gender' <<unk>>])[jupyter_string].mean()\n",
      "--------------------\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Pclass\", palette = jupyter_string)\n",
      "=====\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Pclass\", palette = jupyter_string)\n",
      "--------------------\n",
      "grouped.unstack()\n",
      "=====\n",
      "grouped.unstack()\n",
      "--------------------\n",
      "df.plot()\n",
      "=====\n",
      "data.groupby([times.hour, 'usertype' <<unk>>])[jupyter_string].mean().unstack().plot()\n",
      "--------------------\n",
      "data.plot.hist()\n",
      "=====\n",
      "data[jupyter_string].plot.hist(bins=100)\n",
      "--------------------\n",
      "data[jupyter_string].plot.hist(bins=100)\n",
      "=====\n",
      "plot = data[jupyter_string].plot.hist(bins=500)\n",
      "plot.set_xlim(0, 50)\n",
      "\n",
      "--------------------\n",
      "plt.scatter(Xtrain.iloc[:,0],Xtrain.iloc[:,1],c=ypred_train)\n",
      "plt.scatter(Xtest.iloc[:,0],Xtest.iloc[:,1],c=ypred_test)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "sns.lmplot(data=train,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "--------------------\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "=====\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.datasets import samples_generator\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "\n",
      "X,y = samples_generator.make_classification(n_informative=4,n_features=20,\n",
      "                                           n_redundant=0,random_state=5)\n",
      "X = pd.DataFrame(X); y = pd.DataFrame(y)\n",
      "X.head()\n",
      "--------------------\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "plt.show()\n",
      "=====\n",
      "distances, indices = clf.kneighbors(Xtest)\n",
      "neighborpoints = train.iloc[np.unique(indices.ravel())]\n",
      "sns.lmplot(data=neighborpoints,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "plt.title(jupyter_string)\n",
      "--------------------\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Sex\", palette = jupyter_string)\n",
      "=====\n",
      "train['Age' <<unk>>].plot.hist(bins = 50)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "knn = KNeighborsRegressor(n_neighbors=3)\n",
      "knn.fit(Xtrain,ytrain)\n",
      "ypred_test = knn.predict(Xtest)\n",
      "print(jupyter_string,accuracy_score(ytest,ypred_test))\n",
      "=====\n",
      "amplitude = 10.; num_points=100\n",
      "X = amplitude*np.random.rand(num_points,1)-0.5*amplitude\n",
      "\n",
      "\n",
      "y = np.sinc(X).ravel(); y+=0.2*(0.5 - np.random.rand(y.size))\n",
      "X.shape\n",
      "y.shape\n",
      "data = pd.DataFrame({jupyter_string:X.ravel(),jupyter_string:y})\n",
      "\n",
      "\n",
      "sns.lmplot(data=data,x=jupyter_string,y=jupyter_string,fit_reg=False)\n",
      "--------------------\n",
      "plt.scatter(X,y)\n",
      "plt.plot(xgrid,yvals)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "plt.scatter(X,y,marker=jupyter_string,facecolors=jupyter_string,edgecolors=jupyter_string,s=50)\n",
      "plt.plot(xgrid,yvals,jupyter_string,linewidth=3.0)\n",
      "--------------------\n",
      "ratings = pd.read_json(jupyter_string)\n",
      "ratings.head()\n",
      "=====\n",
      "ratingsdf = pd.read_json(jupyter_string)\n",
      "ratingsdf.head(10)\n",
      "--------------------\n",
      "importances = pipe.feature_importances_\n",
      "std = np.std([tree.feature_importances_ for tree in pipe.estimators_],\n",
      "             axis=0)\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "for f in range(X.shape[1]):\n",
      "    print(jupyter_string % (f + 1, indices[f], importances[indices[f]]))\n",
      "\n",
      "plt.figure()\n",
      "plt.title(jupyter_string)\n",
      "plt.bar(range(X.shape[1]), importances[indices],\n",
      "       color=jupyter_string, yerr=std[indices], align=jupyter_string)\n",
      "plt.xticks(range(X.shape[1]), indices)\n",
      "plt.xlim([-1, X.shape[1]])\n",
      "plt.show()\n",
      "=====\n",
      "features_status = pipe.named_steps[jupyter_string].get_support()\n",
      "features = np.arange(1,21)\n",
      "print(jupyter_string,features[features_status])\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "import pandas as pd\n",
      "=====\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "X = pd.DataFrame( np.array([[1,1],[1,3],[2,2],[2.5,5],[3,1],[4,2],[2.,3.5],\n",
      "                            [3,3],[3.5,4] ]),columns=[jupyter_string,jupyter_string])\n",
      "X.head()\n",
      "--------------------\n",
      "neigh = NearestNeighbors(n_neighbors=3)\n",
      "neigh.fit(X)\n",
      "distances, indices = neigh.kneighbors(X)\n",
      "=====\n",
      "num_neighbors= 3\n",
      "\n",
      "\n",
      "input=[2.6,1.7]\n",
      "\n",
      "\n",
      "sns.lmplot(x=jupyter_string,y=jupyter_string,data=X,fit_reg=False)\n",
      "plt.plot(input[0],input[1],jupyter_string)\n",
      "--------------------\n",
      "user1=jupyter_string\n",
      "user2=jupyter_string\n",
      "print(jupyter_string, pearson_score(ratingsdf,user1,user2))\n",
      "=====\n",
      "ratingsdf = pd.read_json(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "\n",
      "knn = KNeighborsRegressor(n_neighbors=num_neighbors)\n",
      "\n",
      "\n",
      "knn.fit(X,y)\n",
      "=====\n",
      "knn = NearestNeighbors(n_neighbors=num_neighbors,algorithm=jupyter_string).fit(X)\n",
      "\n",
      "\n",
      "distances,indices = knn.kneighbors(input)\n",
      "print( indices)\n",
      "print(jupyter_string)\n",
      "knnX = X.iloc[indices[0]]\n",
      "knnX.head()\n",
      "--------------------\n",
      "plt.scatter(knnX[jupyter_string],knnX[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "sns.lmplot(x=jupyter_string,y=jupyter_string,data=X,fit_reg=False)\n",
      "plt.plot(input[0],input[1],jupyter_string)\n",
      "plt.scatter(knnX.x1,knnX.x2,marker=jupyter_string,s=200,color=jupyter_string,facecolors=jupyter_string,linewidth=3.0)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "data = pd.read_csv(jupyter_string,header=None)\n",
      "X = data.iloc[:,0:2].values\n",
      "y = data.iloc[:,2].values\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X,y)\n",
      "=====\n",
      "from sklearn import neighbors,datasets\n",
      "data = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string])\n",
      "display( data.head() )\n",
      "sns.lmplot(data=data,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "--------------------\n",
      "similar_users = find_similar_users(ratings,jupyter_string,3)\n",
      "similar_users.head()\n",
      "=====\n",
      "user=jupyter_string\n",
      "scores = find_similar_users(ratingsdf,user,3)\n",
      "print(jupyter_string, user)\n",
      "scores.head()\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "n_neighbors = 10\n",
      "\n",
      "\n",
      "h = 0.01\n",
      "\n",
      "\n",
      "train,test = train_test_split(data,test_size=0.1, random_state=42)\n",
      "Xtrain = train.drop([jupyter_string],axis=1); ytrain=train.y\n",
      "Xtest  = test.drop([jupyter_string],axis=1); ytest = test.y\n",
      "clf = neighbors.KNeighborsClassifier(n_neighbors,weights=jupyter_string)\n",
      "clf.fit(Xtrain,ytrain)\n",
      "\n",
      "ypred_test = clf.predict(Xtest)\n",
      "ypred_train= clf.predict(Xtrain)\n",
      "\n",
      "x1max,x2max = Xtrain.max()\n",
      "x1min,x2min = Xtrain.min()\n",
      "x1grid,x2grid = np.meshgrid(np.arange(x1min,x1max,h),np.arange(x2min,x2max,h))\n",
      "output = clf.predict(np.c_[x1grid.ravel(),x2grid.ravel()])\n",
      "output = output.reshape(x1grid.shape)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.info()\n",
      "=====\n",
      "pd_data = pd.read_csv(jupyter_string, dtype={'Survived' <<unk>>: bool, 'Pclass' <<unk>>: int, 'Age' <<unk>>: float, 'SibSp' <<unk>>: int, 'Parch' <<unk>>: int, 'Fare' <<unk>>: float}, na_values=[jupyter_string])\n",
      "\n",
      "\n",
      "pd_data.describe()\n",
      "--------------------\n",
      "pd_data.info()\n",
      "=====\n",
      "pd_data.count()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn import svm\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "=====\n",
      "import numpy as np\n",
      "from sklearn import preprocessing, svm\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "df.drop(['Id' <<unk>>], 1, inplace=True)\n",
      "print(df.head())\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "predict = np.array([2.2, 4.4, 1.3, 0.2])\n",
      "predict = predict.reshape(1, -1)\n",
      "prediction = clf.predict(predict)\n",
      "print(prediction)\n",
      "--------------------\n",
      "polykernel = svm.SVC(kernel=jupyter_string, degree=2)\n",
      "polykernel.fit(X_train, y_train)\n",
      "print(polykernel.score(X_test, y_test))\n",
      "=====\n",
      "polykernel = svm.SVC(kernel=jupyter_string, decision_function_shape=jupyter_string)\n",
      "polykernel.fit(X_train, y_train)\n",
      "\n",
      "polyaccuracy = clf.score(X_test, y_test)\n",
      "print(polyaccuracy)\n",
      "--------------------\n",
      "airports = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "airport = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "weather = pd.read_csv(jupyter_string)\n",
      "weather.head()\n",
      "=====\n",
      "weather = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "lambdas=[0.001,0.01,0.1,1,10,100]\n",
      "R_2_OS_Ridge=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "plt.plot(lambdas,R_2_OS_Ridge)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "lambdas = np.linspace(-5,13,200)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "sns.countplot(x = \"Parch\", data = train)\n",
      "=====\n",
      "train[\"Fare\"].hist(bins =40)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "def ecdf(data):\n",
      "    x = np.sort(data)\n",
      "    y = 1.* np.arange(1,len(data)+1) / (len(data))\n",
      "    return x, y\n",
      "\n",
      "temp = np.array(df['temperature' <<unk>>])\n",
      "mu = np.mean(temp)\n",
      "sigma = np.std(temp)\n",
      "bs_samples = np.random.normal(mu,sigma,10000)\n",
      "\n",
      "x_theo, y_theo = ecdf(bs_samples)\n",
      "x ,y = ecdf(temp)\n",
      "\n",
      "\n",
      "_ = plt.plot(x_theo, y_theo)\n",
      "_ = plt.plot(x, y, marker=jupyter_string, linestyle=jupyter_string)\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "dc_listings = pd.read_csv(jupyter_string)\n",
      "dc_listings.info()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "np.random.seed(1)\n",
      "\n",
      "dc_listings = pd.read_csv(jupyter_string)\n",
      "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
      "stripped_commas = dc_listings['price' <<unk>>].str.replace(jupyter_string, jupyter_string)\n",
      "stripped_dollars = stripped_commas.str.replace(jupyter_string, jupyter_string)\n",
      "dc_listings['price' <<unk>>] = stripped_dollars.astype(jupyter_string)\n",
      "--------------------\n",
      "dc_listings.info()\n",
      "=====\n",
      "dc_listings.head(1)\n",
      "--------------------\n",
      "dc_listings.info()\n",
      "=====\n",
      "dc_listings.info()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head()\n",
      "--------------------\n",
      "titanic_data = titanic_data.dropna()\n",
      "=====\n",
      "titanic_data = titanic_data.dropna()\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data = titanic_data.drop('PassengerId' <<unk>>, axis=1)\n",
      "--------------------\n",
      "titanic_data.head()\n",
      "=====\n",
      "titanic_data.head()\n",
      "--------------------\n",
      "sns.factorplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data_by_class, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "def group_by_age(i):\n",
      "    return str(int(i/10)*10)+jupyter_string+str(int(i/10)*10+10)\n",
      "titanic_data[jupyter_string] = titanic_data['Age' <<unk>>].apply(group_by_age)\n",
      "--------------------\n",
      "sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=titanic_data, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data_by_age = titanic_data.groupby(['Pclass' <<unk>>,jupyter_string],as_index=False)[['Survived' <<unk>>]].mean()\n",
      "data_by_age\n",
      "--------------------\n",
      "sns.factorplot(x=\"Sex\", y=\"Survived\", hue=jupyter_string, data=data_by_age, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sib_count_survived_female = survived_female['SibSp' <<unk>>].value_counts()\n",
      "sib_count_no_survived_female = non_survived_female['SibSp' <<unk>>].value_counts()\n",
      "--------------------\n",
      "train.drop(\"Cabin\", axis = 1, inplace = True)\n",
      "=====\n",
      "train.drop('Cabin' <<unk>>, axis = 1, inplace = True)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "--------------------\n",
      "y_pred = nb_model.predict(X_test)\n",
      "=====\n",
      "y_pred = nb_model.predict(X_test)\n",
      "pred_summary = X_test.copy()\n",
      "pred_summary[y.name] = y_test\n",
      "pred_summary[jupyter_string] = y_pred\n",
      "pred_summary.head()\n",
      "--------------------\n",
      "X = data[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "y = data[jupyter_string]\n",
      "=====\n",
      "X = data[['Gender' <<unk>>, 'Age' <<unk>>, 'EstimatedSalary' <<unk>>]]\n",
      "y = data[\"Purchased\"]\n",
      "--------------------\n",
      "X = pd.get_dummies(X, columns=[\"Gender\"])\n",
      "=====\n",
      "X_dummies = pd.get_dummies(X[\"Gender\"], drop_first = True)\n",
      "X = pd.concat([X, X_dummies], axis = 1)\n",
      "X = X.drop([\"Gender\"], axis = 1)\n",
      "--------------------\n",
      "plt.hist(leaf_data.average-contrast)\n",
      "plt.show()\n",
      "=====\n",
      "leaf_data.average_contrast.plot(kind = jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "adult_data = pd.read_csv(jupyter_string, skipinitialspace=True)\n",
      "\n",
      "\n",
      "adult_data.columns = [c.replace(jupyter_string, jupyter_string) for c in adult_data.columns]\n",
      "\n",
      "\n",
      "\n",
      "adult_data.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.hist(adult_data.education)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "adult_data['education' <<unk>>].value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "adult_data_copy = adult_data.copy()\n",
      "\n",
      "adult_data_copy[jupyter_string] = adult_data_copy[jupyter_string].map(continent_dict)\n",
      "adult_data_copy.head()\n",
      "=====\n",
      "adult_data_copy.groupby(jupyter_string).age.mean().plot(kind = jupyter_string, color = jupyter_string, position = 1, width = 0.2, figsize = (7,7))\n",
      "adult_data_copy.groupby(jupyter_string).age.std().plot(kind = jupyter_string, color = jupyter_string, position = 0, width = 0.2)\n",
      "plt.legend([jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "leaf_data = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "leaf_data.columns = [c.replace(jupyter_string, jupyter_string) for c in leaf_data.columns]\n",
      "\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "leaf_data.eccentricity.fillna(leaf_data.eccentricity.median(), inplace=True)\n",
      "leaf_data.head()\n",
      "=====\n",
      "replace_val = leaf_data.eccentricity.dropna().mean()\n",
      "leaf_data['eccentricity' <<unk>>] = leaf_data.eccentricity.fillna(replace_val)\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "leaf_data[jupyter_string] = (leaf_data.eccentricity - leaf_data.eccentricity.mean()) / leaf_data.eccentricity.std()\n",
      "leaf_data.head()\n",
      "=====\n",
      "me = leaf_data.eccentricity.mean()\n",
      "sd = leaf_data.eccentricity.std()\n",
      "leaf_data['eccentricity' <<unk>>] = leaf_data['eccentricity' <<unk>>].apply(lambda x: (x - me) / (sd))\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity)\n",
      "plt.xlabel('smoothness' <<unk>>)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_mur=pd.read_csv(jupyter_string) \n",
      "df_ms=pd.read_csv(jupyter_string)  \n",
      "df_pop=pd.read_csv(jupyter_string, skiprows=(0,2)) \n",
      "                                                                        \n",
      "df_gdp=pd.read_csv(jupyter_string, skiprows=(0,2))  \n",
      "--------------------\n",
      "df_mur.head()\n",
      "=====\n",
      "df_mur=df_mur[[\"Country/Territory\",\"Number of homicides by firearm\",\"Average total all civilian firearms\"]]\n",
      "df_ms\n",
      "df_pop=df_pop[[jupyter_string,jupyter_string]]\n",
      "df_gdp=df_gdp[[jupyter_string,jupyter_string]]\n",
      "\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(df[jupyter_string], df[jupyter_string], \n",
      "            yerr = np.sqrt(df[jupyter_string]), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.GDP / 1e9, df['Number of mass shootings' <<unk>>])\n",
      "ax.errorbar(df.GDP / 1e9, df['Number of mass shootings' <<unk>>], \n",
      "            yerr = np.sqrt(df['Number of mass shootings' <<unk>>] * 1.0), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "\n",
      "--------------------\n",
      "train.dropna(inplace = True)\n",
      "=====\n",
      "train.dropna(inplace = True)\n",
      "--------------------\n",
      "import statsmodels.formula.api as st\n",
      "import statsmodels as sm\n",
      "import seaborn as sns\n",
      "import statsmodels.graphics.regressionplots as plots\n",
      "plots.style.use(jupyter_string)\n",
      "=====\n",
      "df[\"Average total all civilian firearms\"].mean()\n",
      "--------------------\n",
      "model2.summary()\n",
      "=====\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "=====\n",
      "W1 = model.params[1]\n",
      "W2 = model.params[0] \n",
      "\n",
      "wmodel2 = model2.params[0]\n",
      "\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], W1*df[jupyter_string] + W2, c=jupyter_string,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], wmodel2*df[jupyter_string], c=jupyter_string,label=jupyter_string)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], W1*df[jupyter_string] + W2, c=jupyter_string,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], wmodel2*df[jupyter_string], c=jupyter_string,label=jupyter_string)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "=====\n",
      "sns.regplot(x=df.firearmspp, y=df.masshootingpp, data=df)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "import statsmodels.graphics.influence as influence\n",
      "influence.plot_influence(model)\n",
      "plt.show()\n",
      "=====\n",
      "statsmodels.graphics.regressionplots.influence_plot(model, alpha  = 0.05, criterion=jupyter_string);\n",
      "--------------------\n",
      "plt.scatter(X[:,0], X[:,1], c=y)\n",
      "plt.show()\n",
      "=====\n",
      "plot_points(X, y)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plot_line(-2.86, 2.0, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, header=None) \n",
      "X = np.array(data[[0, 1]])\n",
      "y = np.array(data[2])\n",
      "plot_points(X, y)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train_set = pd.read_csv(jupyter_string, header = None)\n",
      "test_set = pd.read_csv(jupyter_string , skiprows = 1, header = None)\n",
      "--------------------\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.head()\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.head()\n",
      "--------------------\n",
      "df_set = df_set.replace(jupyter_string, np.nan)\n",
      "df_set.head()\n",
      "=====\n",
      "train_set=train_set.replace(regex=jupyter_string,value=pd.np.nan).dropna(how=jupyter_string)\n",
      "train_set.head()\n",
      "--------------------\n",
      "train_set[jupyter_string] = train_set.hours_per_week.apply(lambda x: 1 if x>=40 else 2 if x>=45 else 3 if x>=60 else 4 if x>=80 else 5)\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.hours_per_week=train_set.hours_per_week.astype(int)\n",
      "train_set.loc[train_set.hours_per_week < 40,jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week >= 40) & (train_set.hours_per_week <= 45),jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week > 45) & (train_set.hours_per_week <= 60),jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week > 60) & (train_set.hours_per_week <= 80),jupyter_string] = jupyter_string\n",
      "train_set.loc[train_set.hours_per_week > 80,jupyter_string] = jupyter_string\n",
      "--------------------\n",
      "train_set.hours_per_week.value_counts()\n",
      "=====\n",
      "train_set[jupyter_string].head()\n",
      "--------------------\n",
      "train_set.native_country.unique()\n",
      "=====\n",
      "train_set.native_country.unique()\n",
      "\n",
      "--------------------\n",
      "train_set.native_county.unique()\n",
      "=====\n",
      "Asia_East = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string])\n",
      "Asia_Central = np.array([jupyter_string, jupyter_string])\n",
      "Central_America =  np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,  jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "South_America = np.array([jupyter_string, jupyter_string, jupyter_string,jupyter_string])\n",
      "Europe_West = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "Europe_East = np.array([jupyter_string, jupyter_string, jupyter_string])\n",
      "North_America = np.array([jupyter_string, jupyter_string,jupyter_string])\n",
      "--------------------\n",
      "train_set.native_region.unique()\n",
      "=====\n",
      "train_set.native_region.unique()\n",
      "\n",
      "--------------------\n",
      "train_set.capital_gain.unique()\n",
      "=====\n",
      "sns.kdeplot(train_set.capital_gain,shade=True)\n",
      "train_set.capital_gain.describe()\n",
      "--------------------\n",
      "train_set.capital_loss.describe()\n",
      "=====\n",
      "sns.kdeplot(train_set.capital_loss,shade=True)\n",
      "train_set.capital_loss.describe()\n",
      "--------------------\n",
      "train_set.capital_gain.fillna(train_set.capital_gain.mean(),inplace=True)\n",
      "train_set.capital_loss.fillna(train_set.capital_loss.mean(),inplace=True)\n",
      "=====\n",
      "capital_gain_mean = train_set.capital_gain.mean()\n",
      "capital_loss_mean = train_set.capital_loss.mean()\n",
      "\n",
      "--------------------\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.capital_gain.describe()\n",
      "--------------------\n",
      "train_set.workclass.unique()\n",
      "=====\n",
      "train_set.workclass.describe()\n",
      "--------------------\n",
      "train_set.workclass = train_set.workclass.fillna(jupyter_string)\n",
      "=====\n",
      "train_set.workclass.unique()\n",
      "--------------------\n",
      "train_set.education.unique()\n",
      "=====\n",
      "workclass = pd.factorize(train_set.workclass)\n",
      "education = pd.factorize(train_set.education)\n",
      "hours_per_week_group = pd.factorize(train_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(train_set.marital_status)\n",
      "occupation = pd.factorize(train_set.occupation)\n",
      "relationship = pd.factorize(train_set.relationship)\n",
      "race = pd.factorize(train_set.race)\n",
      "sex = pd.factorize(train_set.sex)\n",
      "native_region = pd.factorize(train_set.native_region) \n",
      "wage_class = pd.factorize(train_set.wage_class)\n",
      "\n",
      "--------------------\n",
      "train = pd.concat([train,sex,embark], axis = 1)\n",
      "=====\n",
      "train = pd.concat([train, sex, embark], axis = 1)\n",
      "--------------------\n",
      "train_set = pd.concat([train_set,workclass,education,hours_per_week_group,marital_status,occupation,relationship,race,sex,native_region,wage_class],axis=1)\n",
      "=====\n",
      "X_train = pd.DataFrame({jupyter_string:train_set.age,jupyter_string:workclass[0],jupyter_string:train_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:train_set.capital_gain,jupyter_string:train_set.capital_loss,jupyter_string:hours_per_week_group[0],jupyter_string:native_region[0]})\n",
      "Y_train = wage_class[0]\n",
      "X_train.head()\n",
      "--------------------\n",
      "X_test = pd.DataFrame({jupyter_string:test_set.age,jupyter_string:test_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:native_region[0]})\n",
      "Y_test = wage_class[0]\n",
      "X_test.head()\n",
      "=====\n",
      "test_set=test_set.replace(regex=jupyter_string,value=pd.np.nan).dropna(how=jupyter_string) \n",
      "test_set.head()\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.native_region.unique()\n",
      "--------------------\n",
      "test_set.capital_gain.fillna(0,inplace=True)\n",
      "test_set.capital_loss.fillna(0,inplace=True)\n",
      "=====\n",
      "capital_gain_mean = test_set.capital_gain.mean()\n",
      "capital_loss_mean = test_set.capital_loss.mean()\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.head()\n",
      "--------------------\n",
      "test_set.wage_class.unique()\n",
      "=====\n",
      "test_set.wage_class.head()\n",
      "--------------------\n",
      "test_set.wage_class = test_set.wage_class.str.replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "test_set.loc[test_set.wage_class == jupyter_string ,jupyter_string] = jupyter_string\n",
      "test_set.loc[test_set.wage_class == jupyter_string ,jupyter_string] = jupyter_string\n",
      "test_set.wage_class.head()\n",
      "--------------------\n",
      "train_set = pd.get_dummies(train_set)\n",
      "test_set = pd.get_dummies(test_set)\n",
      "=====\n",
      "workclass = pd.factorize(test_set.workclass)\n",
      "education = pd.factorize(test_set.education)\n",
      "hours_per_week_group = pd.factorize(test_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(test_set.marital_status)\n",
      "occupation = pd.factorize(test_set.occupation)\n",
      "relationship = pd.factorize(test_set.relationship)\n",
      "race = pd.factorize(test_set.race)\n",
      "sex = pd.factorize(test_set.sex)\n",
      "native_region = pd.factorize(test_set.native_region) \n",
      "wage_class = pd.factorize(test_set.wage_class)\n",
      "--------------------\n",
      "test_set = pd.concat([test_set,workclass,education,hours_per_week_group,marital_status,occupation,relationship,race,sex,native_region,wage_class],axis=1)\n",
      "test_set.head()\n",
      "=====\n",
      "X_test = pd.DataFrame({jupyter_string:test_set.age,jupyter_string:workclass[0],jupyter_string:test_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:test_set.capital_gain,jupyter_string:test_set.capital_loss,jupyter_string:hours_per_week_group[0],jupyter_string:native_region[0]})\n",
      "Y_test = wage_class[0]\n",
      "X_test.head()\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.head(1)\n",
      "--------------------\n",
      "param_test1 = {\n",
      "    jupyter_string:range(3,10,2),\n",
      "    jupyter_string:range(2,10,2)\n",
      "}\n",
      "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
      " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
      " objective=jupyter_string, nthread=4, scale_pos_weight=1, seed=27), \n",
      " param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train,y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:range(3,10,2),\n",
      " jupyter_string:range(1,6,2)\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:range(3,10,2),\n",
      " jupyter_string:range(1,6,2)\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[4,5,6],\n",
      " jupyter_string:[1,2]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(max_depth=5, min_child_weight=2)\n",
      "model.fit(X_train, Y_train)\n",
      "Y_pred = model.predict(X_test)\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.8,\n",
      " colsample_bytree=0.8,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[i/10.0 for i in range(3,10)],\n",
      " jupyter_string:[i/10.0 for i in range(3,10)]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[i/10.0 for i in range(6,10)],\n",
      " jupyter_string:[i/10.0 for i in range(6,10)]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[i/10.0 for i in range(6,10)],\n",
      " jupyter_string:[i/10.0 for i in range(6,10)]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[i/100 for i in range(55,70,5)],\n",
      " jupyter_string:[i/100 for i in range(85,100,5)]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(colsample_bytree=0.85, subsample=0.65)\n",
      "model.fit(X_train, Y_train)\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.85,\n",
      " colsample_bytree=0.85,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      "jupyter_string:[0, 0.001, 0.005, 0.01, 0.05]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test1 = {\n",
      " jupyter_string:[1e-5, 1e-2, 0.1, 1, 100]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.01,\n",
      " n_estimators=5000,\n",
      " reg_alpha= 0.001,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.85,\n",
      " colsample_bytree=0.85,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string,figsize=(10,10),fontsize=14)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "lambdas = np.linspace(-5,13,200)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(2,X_train,y_train,lambdas,p=0.4,Graph=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "\n",
      "Ridge.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "--------------------\n",
      "train.drop([jupyter_string, jupyter_string, jupyter_string], axis = 1, inplace = True)\n",
      "train.head(1)\n",
      "=====\n",
      "train.drop(['Sex' madeupword0002, 'Embarked' <<unk>>, 'Name' <<unk>>, 'Ticket' <<unk>>], axis = 1, inplace = True)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string]\n",
      "data.head()\n",
      "=====\n",
      "import numpy\n",
      "\n",
      "def calc_entropy(column):\n",
      "    \"\"jupyter_string\"\"\n",
      "    \n",
      "    counts = numpy.bincount(column)\n",
      "    \n",
      "    \n",
      "    probabilities = counts / float(len(column))\n",
      "    \n",
      "    \n",
      "    entropy = 0\n",
      "    \n",
      "    \n",
      "    for prob in probabilities:\n",
      "        if prob > 0:\n",
      "            entropy += prob * math.log(prob, 2)\n",
      "    \n",
      "    return -entropy\n",
      "\n",
      "\n",
      "entropy = calc_entropy([1,1,0,0,1])\n",
      "print(jupyter_string, entropy)\n",
      "\n",
      "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
      "print(jupyter_string, information_gain)\n",
      "\n",
      "income_entropy = calc_entropy(income[\"high_income\"])\n",
      "\n",
      "median_age = income[\"age\"].median()\n",
      "\n",
      "left_split = income[income[\"age\"] <= median_age]\n",
      "right_split = income[income[\"age\"] > median_age]\n",
      "\n",
      "age_information_gain = income_entropy - ((float(left_split.shape[0]) / income.shape[0]) * calc_entropy(left_split[\"high_income\"]) + ((float(right_split.shape[0]) / income.shape[0]) * calc_entropy(right_split[\"high_income\"])))\n",
      "print(jupyter_string, age_information_gain)\n",
      "--------------------\n",
      "highest_gain = information_gains.index(max(information_gains))\n",
      "highest_gain\n",
      "=====\n",
      "def calc_information_gain(data, split_name, target_name):\n",
      "    \"\"jupyter_string\"\"\n",
      "    \n",
      "    original_entropy = calc_entropy(data[target_name])\n",
      "    \n",
      "    \n",
      "    column = data[split_name]\n",
      "    median = column.median()\n",
      "    \n",
      "    \n",
      "    left_split = data[column <= median]\n",
      "    right_split = data[column > median]\n",
      "    \n",
      "    \n",
      "    to_subtract = 0\n",
      "    for subset in [left_split, right_split]:\n",
      "        prob = float(subset.shape[0]) / data.shape[0]\n",
      "        to_subtract += prob * calc_entropy(subset[target_name])\n",
      "    \n",
      "    \n",
      "    return original_entropy - to_subtract\n",
      "\n",
      "\n",
      "age_information_gain = calc_information_gain(income, \"age\", \"high_income\")\n",
      "print(jupyter_string, age_information_gain)\n",
      "\n",
      "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
      "information_gains = []\n",
      "\n",
      "\n",
      "for col in columns:\n",
      "    information_gain = calc_information_gain(income, col, \"high_income\")\n",
      "    information_gains.append(information_gain)\n",
      "\n",
      "\n",
      "highest_gain_index = information_gains.index(max(information_gains))\n",
      "highest_gain = columns[highest_gain_index]\n",
      "\n",
      "print(jupyter_string, highest_gain_index)\n",
      "print(jupyter_string, highest_gain)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas\n",
      "\n",
      "\n",
      "income = pandas.read_csv(jupyter_string, index_col=False)\n",
      "income.head(5)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import scipy.stats as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "sample_mean = df.temperature.mean()\n",
      "sample_mean\n",
      "=====\n",
      "df['temperature' <<unk>>].mean()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "=====\n",
      "X = train.drop('Survived' <<unk>>, axis = 1) \n",
      "y = train['Survived' <<unk>>]\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import pprint\n",
      "\n",
      "filepath = jupyter_string\n",
      "df = pd.read_csv(filepath)\n",
      "pp = pprint.PrettyPrinter()\n",
      "\n",
      "print(jupyter_string)\n",
      "pp.pprint(df.head(1).T)\n",
      "\n",
      "--------------------\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], errors=jupyter_string)\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], errors=jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "categorical_vars = ['residence_area_type' <<unk>>,'sourcing_channel' <<unk>>]\n",
      "numerical_vars = list(set(list(df.columns)) - set(categorical_vars))\n",
      "\n",
      "sub_df = df[numerical_vars]\n",
      "sub_df['age_in_days' <<unk>>] = sub_df['age_in_days' <<unk>>] // 365\n",
      "--------------------\n",
      "sub_df[numerical_vars].hist(figsize=(20,20))\n",
      "plt.show()\n",
      "=====\n",
      "num_cols = 2 \n",
      "num_rows = len(numerical_vars) // 2 + 1\n",
      "\n",
      "fig = sub_df.hist(layout = (num_rows,num_cols), \n",
      "                    bins=30,\n",
      "                    figsize=(50,50),\n",
      "                   ylabelsize=50,\n",
      "                   xlabelsize=50)\n",
      "titles = [x.title.set_size(50) for x in fig.ravel()] \n",
      "\n",
      "--------------------\n",
      "corr = sub_df.corr()\n",
      "corr.style.background_gradient(cmap=plt.get_cmap(jupyter_string))\n",
      "=====\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def corr_matrix_generator(corr,drop_duplicates = True):\n",
      "    if drop_duplicates:    \n",
      "        mask = np.zeros_like(corr, dtype=np.bool)\n",
      "        mask[np.triu_indices_from(mask)] = True\n",
      "        \n",
      "    sns.set_style(style = jupyter_string)\n",
      "    f, ax = plt.subplots(figsize=(11, 9))\n",
      "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
      "    if drop_duplicates:\n",
      "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={jupyter_string: .5})\n",
      "    else:\n",
      "        sns.heatmap(corr, cmap=cmap, \n",
      "                square=True,\n",
      "                linewidth=.5, cbar_kws={jupyter_string: .5}, ax=ax)\n",
      "        \n",
      "        \n",
      "\n",
      "    \n",
      "corr = sub_df.corr()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(sub_df['renewal' <<unk>>].value_counts())\n",
      "print(jupyter_string)\n",
      "corr_matrix_generator(corr)\n",
      "\n",
      "--------------------\n",
      "iris = sns.load_dataset(jupyter_string)\n",
      "iris.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, data=data)\n",
      "--------------------\n",
      "sns.heatmap(subset.corr())\n",
      "plt.show()\n",
      "=====\n",
      "cols = ['TotalConsmp' <<unk>>,'TempOutSide' <<unk>>,'Press_mm_hg' <<unk>>,'H_OutSide' <<unk>>,'Windspeed' <<unk>>,'Visibility' <<unk>>]\n",
      "cm = np.corrcoef(subset.values.T)\n",
      "sns.set(font_scale=1.5)\n",
      "plt.figure(figsize=(10,10))\n",
      "hm = sns.heatmap(cm,\n",
      "                 cbar=True,\n",
      "                 annot=True,\n",
      "                 square=True,\n",
      "                 fmt=jupyter_string,\n",
      "                 annot_kws= {jupyter_string: 10},\n",
      "                 yticklabels = cols,\n",
      "                 xticklabels=cols\n",
      "                )\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.pairplot(data, hue=\"species\", vars=[\"sepal_length\", \"sepal_width\"])\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, hue='species' madeupword0002, data=data, fit_reg=False, scatter_kws={jupyter_string:5})\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax = dta.groupby(\"education\").size().plot(kind=jupyter_string, figsize=(8, 8))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ylabel = ax.yaxis.get_label()\n",
      "ylabel.set_fontsize(24)\n",
      "\n",
      "\n",
      "labels = ax.yaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "\n",
      "\n",
      "labels = ax.xaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels]\n",
      "[label.set_rotation(-45) for label in labels];\n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(8, 8))\n",
      "\n",
      "\n",
      "sns.countplot(x=\"education\", hue=\"y\", data=dta, palette=jupyter_string)\n",
      "\n",
      "\n",
      "ylabel = ax.yaxis.get_label()\n",
      "ylabel.set_fontsize(24)\n",
      "\n",
      "\n",
      "labels = ax.yaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "\n",
      "\n",
      "labels = ax.xaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "[label.set_rotation(-45) for label in labels];\n",
      "=====\n",
      "g = sns.factorplot(\"education_num\", \"hours_per_week\", hue=\"sex\", col=\"fifty_k\", data=dta)\n",
      "--------------------\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "pd.set_option(jupyter_string, 10)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "dta[dta.education_num <= 8].head()\n",
      "=====\n",
      "dta.groupby(\"work_class\").age.mean()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "dta = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "new_df_data = dta.iloc[:,:-1]\n",
      "new_df_labels = dta.iloc[:,-1]\n",
      "=====\n",
      "new_df_data = dta.iloc[:, 0:10]\n",
      "new_df_labels = dta.loc[:, 'fifty_k' <<unk>>]\n",
      "new_df_data.head()\n",
      "--------------------\n",
      "dta.head()\n",
      "=====\n",
      "dta.head()\n",
      "--------------------\n",
      "dta.info()\n",
      "=====\n",
      "dta.info()\n",
      "--------------------\n",
      "dta.describe()\n",
      "=====\n",
      "dta.describe()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string]\n",
      "df.head()\n",
      "=====\n",
      "new_df_transpose = new_df_data.transpose()\n",
      "\n",
      "data_into_dict = new_df_transpose.to_dict()\n",
      "census_data = [v for k, v in data_into_dict.items()]\n",
      "--------------------\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "vec = DictVectorizer()\n",
      "census_data_array = vec.fit_transform(census_data).toarray()\n",
      "=====\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "dv = DictVectorizer()\n",
      "transformed_data = dv.fit_transform(census_data).toarray()\n",
      "transformed_data\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(census_train, labels_train)\n",
      "=====\n",
      "knn = KNeighborsClassifier()\n",
      "knn.fit(census_train, labels_train)\n",
      "--------------------\n",
      "dta.groupby(\"work_class\").mean()\n",
      "=====\n",
      "dta.groupby(\"fifty_k\").education.describe()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(5,5)\n",
      "for i, a in enumerate(ax.flatten()):\n",
      "    a.hist(trainlabels)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(trainlabel)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(traindata)\n",
      "traindata = scaler.transform(traindata)\n",
      "testdata = scaler.transform(testdata)\n",
      "=====\n",
      "meanimage = traindata.mean(axis=0)\n",
      "--------------------\n",
      "traindata = traindata - meanimage\n",
      "testdata = testdata - meanimage\n",
      "=====\n",
      "plt.imshow(meanimage.reshape((28,28)))\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "meanimage = meanimage - meanimage.mean()\n",
      "plt.imshow(meanimage.reshape((28,28)))\n",
      "plt.show()\n",
      "=====\n",
      "traindata = np.subtract(traindata, meanimage)\n",
      "testdata = np.subtract(testdata, meanimage)\n",
      "--------------------\n",
      "ncomp = [30, 50, 100]\n",
      "reconstructed_data = []\n",
      "for i in ncomp:\n",
      "    reconstructed_data.append(reconstruct(X_train, X_train, i))\n",
      "reconstructed_data = np.array(reconstructed_data)\n",
      "=====\n",
      "train_reconst = np.add(traindata[:25,:], meanimage)\n",
      "plotdigits(train_reconst[20:], 1, 5)\n",
      "\n",
      "numcomp = [30, 50, 100]\n",
      "for num in numcomp:\n",
      "    \n",
      "    pca_num, recon_num = reconstruct(traindata, traindata[:25,:], num)\n",
      "    \n",
      "    recon_num = np.add(recon_num, meanimage)\n",
      "    \n",
      "    plotdigits(recon_num[20:], 1, 5)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "star_wars = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "\n",
      "star_wars = star_wars[star_wars['RespondentID' <<unk>>].notnull()]\n",
      "--------------------\n",
      "yes_no = {\n",
      "    \"Yes\": True,\n",
      "    \"No\": False\n",
      "}\n",
      "\n",
      "for col in [\n",
      "    \"Have you seen any of the 6 films in the Star Wars franchise?\",\n",
      "    \"Do you consider yourself to be a fan of the Star Wars film franchise?\"\n",
      "]:\n",
      "    star_wars[col] = star_wars[col].map(yes_no)\n",
      "\n",
      "star_wars.head()\n",
      "=====\n",
      "seen = 'Have you seen any of the 6 films in the Star Wars franchise?' <<unk>>\n",
      "fan = 'Do you consider yourself to be a fan of the Star Wars film franchise?' <<unk>>\n",
      "fan_st = 'Do you consider yourself to be a fan of the Star Trek franchise?' <<unk>>\n",
      "\n",
      "\n",
      "yes_no_map = {\n",
      "    jupyter_string: True,\n",
      "    jupyter_string: False\n",
      "}\n",
      "\n",
      "star_wars[seen] = star_wars[seen].map(yes_no_map)\n",
      "star_wars[fan] = star_wars[fan].map(yes_no_map)\n",
      "star_wars[fan_st] = star_wars[fan_st].map(yes_no_map)\n",
      "\n",
      "--------------------\n",
      "star_wars = star_wars.rename(columns={\n",
      "        \"Which of the following Star Wars films have you seen? Please select all that apply.\": jupyter_string,\n",
      "        \"Unnamed: 4\": jupyter_string,\n",
      "        \"Unnamed: 5\": jupyter_string,\n",
      "        \"Unnamed: 6\": jupyter_string,\n",
      "        \"Unnamed: 7\": jupyter_string,\n",
      "        \"Unnamed: 8\": jupyter_string\n",
      "        })\n",
      "star_wars.head()\n",
      "=====\n",
      "movie_name = {'Which of the following Star Wars films have you seen? Please select all that apply.' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 4' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 5' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 6' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 7' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 8' <<unk>>: jupyter_string}\n",
      "star_wars = star_wars.rename(columns={'Which of the following Star Wars films have you seen? Please select all that apply.' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 4' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 5' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 6' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 7' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 8' <<unk>>: jupyter_string})\n",
      "--------------------\n",
      "star_wars = star_wars.rename(columns={\n",
      "        \"Which of the following Star Wars films have you seen? Please select all that apply.\": jupyter_string,\n",
      "        \"Unnamed: 4\": jupyter_string,\n",
      "        \"Unnamed: 5\": jupyter_string,\n",
      "        \"Unnamed: 6\": jupyter_string,\n",
      "        \"Unnamed: 7\": jupyter_string,\n",
      "        \"Unnamed: 8\": jupyter_string\n",
      "        })\n",
      "=====\n",
      "star_wars[star_wars.columns[9:15]] = star_wars[star_wars.columns[9:15]].astype(float)\n",
      "--------------------\n",
      "df.corr()\n",
      "=====\n",
      "from pandas.plotting import scatter_matrix\n",
      "subset = df[['TotalConsmp' <<unk>>,'TempOutSide' <<unk>>,'Press_mm_hg' <<unk>>,'H_OutSide' <<unk>>,'Windspeed' <<unk>>,'Visibility' <<unk>>]]\n",
      "scatter_matrix(subset,figsize=(10,10)) \n",
      "plt.show()\n",
      "--------------------\n",
      "ranking_std = star_wars[star_wars.columns[3:9]].std()\n",
      "ranking_std\n",
      "=====\n",
      "seen_sum = star_wars[star_wars.columns[3:9]].sum()\n",
      "seen_sum\n",
      "--------------------\n",
      "plt.bar(movie_names, seen_sum)\n",
      "plt.xticks(rotation = 90)\n",
      "=====\n",
      "plt.bar(movie_names, seen_sum)\n",
      "plt.xticks(rotation = 90)\n",
      "--------------------\n",
      "star_wars_fan = star_wars[star_wars[fan_sw] == True]\n",
      "star_wars_not_fan = star_wars[star_wars[fan_sw] == False]\n",
      "\n",
      "\n",
      "ranking_star_wars_fan = star_wars_fan[star_wars_fan.columns[3:9]].mean()\n",
      "ranking_star_wars_not_fan = star_wars_not_fan[star_wars_not_fan.columns[3:9]].mean()\n",
      "seen_star_wars_fan = star_wars_fan[star_wars_fan.columns[9:15]].sum()\n",
      "seen_star_wars_not_fan = star_wars_not_fan[star_wars_not_fan.columns[9:15]].sum()\n",
      "=====\n",
      "st_fan = star_wars[star_wars[fan_st] == True]\n",
      "st_not_fan = star_wars[star_wars[fan_st] == False]\n",
      "\n",
      "\n",
      "ranking_stfans = st_fan[st_fan.columns[9:15]].mean()\n",
      "ranking_stNfans = st_not_fan[st_not_fan.columns[9:15]].mean()\n",
      "seen_stfans = st_fan[st_fan.columns[3:9]].sum()\n",
      "seen_stNfans = st_not_fan[st_not_fan.columns[3:9]].sum()\n",
      "\n",
      "plot_st = [ranking_stfans, ranking_stNfans, seen_stfans,seen_stNfans]\n",
      "titles_st = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "plot_bar(plot_st,titles_st)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats as stats\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.stats.multicomp as multi\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "from scipy.stats import f\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.rcParams[jupyter_string] = 14\n",
      "\n",
      "x = np.linspace(0.01, 6, num=250)\n",
      "groups = 4\n",
      "samples = 327\n",
      "df1 = groups - 1\n",
      "df2 = samples - groups\n",
      "y = f.pdf(x, df1, df2)\n",
      "plt.plot(x, y)\n",
      "--------------------\n",
      "baseball = pd.read_csv(jupyter_string)\n",
      "baseball.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df = df.head(327)\n",
      "--------------------\n",
      "plt.scatter(outfield, des_hit)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "x = np.linspace(0.01, 6, num=250)\n",
      "groups = 4\n",
      "samples = outfield.count() + infield.count() + des_hit.count() + catcher.count()\n",
      "df1 = groups - 1\n",
      "df2 = samples - groups\n",
      "y = f.pdf(x, df1, df2)\n",
      "x_right = np.linspace(F_statistic, 6)\n",
      "y_right = f.pdf(x_right, df1, df2)\n",
      "plt.fill_between(x_right, 0, y_right, facecolor=jupyter_string)\n",
      "plt.plot(x, y, jupyter_string, lw=1)\n",
      "--------------------\n",
      "url = jupyter_string\n",
      "source = requests.get(url).text\n",
      "soup = BeautifulSoup(source, jupyter_string)\n",
      "=====\n",
      "post_url = jupyter_string\n",
      "posts = requests.get(post_url)\n",
      "parser = BeautifulSoup(posts.content,jupyter_string)\n",
      "table = parser.find_all(jupyter_string)[0] \n",
      "df = pd.read_html(str(table),header=0)[0]\n",
      "df.head(12)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, na_values = [jupyter_string])\n",
      "df.head()\n",
      "--------------------\n",
      "df.ABV = df.ABV.str.replace(jupyter_string, jupyter_string)\n",
      "df.ABV = df.ABV.astype(float)\n",
      "=====\n",
      "df['ABV' <<unk>>].str.replace(jupyter_string, jupyter_string).head()\n",
      "--------------------\n",
      "df = df[df.Borough != jupyter_string]\n",
      "df.head(12)\n",
      "=====\n",
      "df = df[~df[jupyter_string].isin([jupyter_string])]\n",
      "df.reset_index(inplace=True,drop=True)\n",
      "df.head(12)\n",
      "--------------------\n",
      "brooklyn_brewery = Brooklyn_beer.Brewery.value_counts()\n",
      "brooklyn_brewery.head()\n",
      "=====\n",
      "Brook_beer['Brewery' <<unk>>].value_counts().head(1)\n",
      "--------------------\n",
      "df.IBUs.hist(bins=10)\n",
      "plt.show()\n",
      "=====\n",
      "df[\"IBUs\"].hist(bins=30)\n",
      "--------------------\n",
      "df[df[\"IBUs\"] > df[\"IBUs\"].quantile(0.75)]\n",
      "=====\n",
      "df[\"IBUs\"].describe(percentiles=None, include=None, exclude=None)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].median()\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(20).plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(20).plot(kind=jupyter_string)\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(20).plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().sort_values(ascending=False).head()\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(5)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(5)\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(5)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().plot(kind=jupyter_string, bins=20)\n",
      "=====\n",
      "choice_beer = df[(df['Style' <<unk>>] == jupyter_string) | (df['Style' <<unk>>] == jupyter_string) | (df['Style' <<unk>>] == jupyter_string)]\n",
      "--------------------\n",
      "choice_beer.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(20)\n",
      "=====\n",
      "choice_beer[\"IBUs\"].mean().astype(int)\n",
      "--------------------\n",
      "plt.hist(choice_beer[\"IBUs\"], bins=20)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "choice_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "--------------------\n",
      "choice_beer[choice_beer[\"Style\"] == jupyter_string][\"IBUs\"].mean()\n",
      "=====\n",
      "IPA_beer = df[df['Style' <<unk>>].str.contains(jupyter_string, na=False)]\n",
      "--------------------\n",
      "IPA_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "=====\n",
      "IPA_beer[\"IBUs\"].mean().astype(int)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
      "fig.set_size_inches(12,5)\n",
      "ax1.hist(IPA_beer[\"IBUs\"])\n",
      "ax1.set_title(jupyter_string)\n",
      "ax1.set_xlabel(jupyter_string)\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "ax2.hist(IPB_beer[\"IBUs\"])\n",
      "ax2.set_title(jupyter_string)\n",
      "ax2.set_xlabel(jupyter_string)\n",
      "ax2.set_ylabel(jupyter_string)\n",
      "=====\n",
      "ax = choice_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "--------------------\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string], prefix=jupyter_string)], axis=1)\n",
      "df.head()\n",
      "=====\n",
      "df = df.groupby([jupyter_string,jupyter_string])[jupyter_string].apply(jupyter_string.join).reset_index()\n",
      "df.head(12)\n",
      "--------------------\n",
      "ax = choice_beer[\"ABV\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"ABV\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "=====\n",
      "IPA_beer[jupyter_string].mean()\n",
      "--------------------\n",
      "ax = choice_beer[\"ABV\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"ABV\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "=====\n",
      "wheat_beer = df[df['Style' <<unk>>].str.contains(jupyter_string, na=False)]\n",
      "wheat_beer[jupyter_string].mean()\n",
      "--------------------\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "clf = LogisticRegression()\n",
      "parameters = {jupyter_string: [0.001, 0.1, 1, 10, 100]}\n",
      "fitmodel = GridSearchCV(clf, param_grid=parameters, cv=5)\n",
      "fitmodel.fit(Xlr, ylr)\n",
      "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_\n",
      "=====\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "parameters = {jupyter_string:Cs}\n",
      "clf_gs = LogisticRegression()\n",
      "clf_gs_cv = GridSearchCV(clf_gs, param_grid=parameters, cv=5)\n",
      "clf_gs_cv.fit(Xlr, ylr)\n",
      "print(clf_gs_cv.best_params_, clf_gs_cv.best_score_)\n",
      "--------------------\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "parameters = {jupyter_string:Cs}\n",
      "clf = LogisticRegression()\n",
      "clf_gs = GridSearchCV(clf, param_grid=parameters, cv=5)\n",
      "clf_gs.fit(Xlr, ylr)\n",
      "print(clf_gs.best_params_, clf_gs.best_score_)\n",
      "=====\n",
      "y_pred_gs = clf_gs_cv.predict(Xtestlr)\n",
      "accuracy_score(y_pred_gs, ytestlr)\n",
      "--------------------\n",
      "dflog = pd.read_csv(jupyter_string)\n",
      "dflog.head()\n",
      "=====\n",
      "dflog = pd.read_csv(jupyter_string)\n",
      "dflog.head()\n",
      "--------------------\n",
      "plt.scatter(dflog.Weight, dflog.Height, c=[cm_bright.colors[i] for i in dflog.Gender==jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "_ = plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog.Weight, dflog.Height)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.xlabel('Weight' <<unk>>)\n",
      "_ = plt.ylabel('Height' <<unk>>)\n",
      "--------------------\n",
      "h = lambda z: 1. / (1 + np.exp(-z))\n",
      "zs=np.arange(-5, 5, 0.1)\n",
      "plt.plot(zs, h(zs), alpha=0.5);\n",
      "=====\n",
      "h = lambda z: 1. / (1 + np.exp(-z))\n",
      "zs=np.arange(-5, 5, 0.1)\n",
      "plt.plot(zs, h(zs), alpha=0.5);\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data4=pd.read_csv(jupyter_string, low_memory=False)\n",
      "--------------------\n",
      "df = df[df.Borough != jupyter_string]\n",
      "=====\n",
      "df[jupyter_string] = df[jupyter_string].where(df[jupyter_string]==jupyter_string,df[jupyter_string])\n",
      "df.loc[df[jupyter_string] == jupyter_string]\n",
      "--------------------\n",
      "plt.figure()\n",
      "ax=plt.gca()\n",
      "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n",
      "=====\n",
      "plt.figure()\n",
      "ax=plt.gca()\n",
      "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n",
      "--------------------\n",
      "plt.figure()\n",
      "ax = plt.gca()\n",
      "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n",
      "=====\n",
      "plt.figure()\n",
      "ax = plt.gca()\n",
      "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "Xlr, Xtestlr, ylr, ytestlr = train_test_split(X, y)\n",
      "\n",
      "clf = LogisticRegression()\n",
      "clf.fit(Xlr, ylr)\n",
      "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n",
      "=====\n",
      "clf_best = LogisticRegression(C=0.1)\n",
      "clf_best.fit(Xlr, ylr)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "Xlr_train, Xlr_test, ylr_train, ylr_test = train_test_split(Xlr, ylr)\n",
      "clf = LogisticRegression()\n",
      "clf.fit(Xlr_train, ylr_train)\n",
      "print(accuracy_score(clf.predict(Xlr_test), ylr_test))\n",
      "=====\n",
      "y_pred_best = clf_best.predict(Xtestlr)\n",
      "accuracy_score(y_pred_best, ytestlr)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "import math\n",
      "\n",
      "import scipy\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "train.head()\n",
      "=====\n",
      "train_data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_data.head()\n",
      "=====\n",
      "train_data.head(5)\n",
      "--------------------\n",
      "df = pd.DataFrame()\n",
      "df[jupyter_string] = X_train[:,0]\n",
      "df[jupyter_string] = X_train[:,1]\n",
      "df[jupyter_string] = X_train[:,2]\n",
      "df[jupyter_string] = X_train[:,3]\n",
      "df[jupyter_string] = X_train[:,4]\n",
      "df[jupyter_string] = X_train[:,5]\n",
      "df[jupyter_string] = X_train[:,6]\n",
      "=====\n",
      "X_data_new = pd.DataFrame(X_data.iloc[:,:].apply(lambda row: features(row), axis = 1)) \n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "geodata = pd.read_csv(jupyter_string)\n",
      "geodata.head()\n",
      "=====\n",
      "geocode = pd.read_csv(jupyter_string)\n",
      "geocode.rename(columns={'Postal Code' madeupword0002: jupyter_string}, inplace=True)\n",
      "--------------------\n",
      "X = data.drop(\"label\", axis=1)\n",
      "y = data[\"label\"]\n",
      "=====\n",
      "X_data = train_data.iloc[:,1:]\n",
      "Y_data = train_data.iloc[:,0:1]\n",
      "--------------------\n",
      "X_train_mass = mass(X_train)\n",
      "X_test_mass = mass(X_test)\n",
      "=====\n",
      "X_data_new = pd.DataFrame(X_data.iloc[:,:].apply(lambda row: mass(row), axis = 1)) \n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "k_folds = 10\n",
      "for i in range(k_folds):\n",
      "    X_train, X_valid, Y_train, Y_valid = cross_validation.train_test_split(X_data, Y_data, test_size=0.3, random_state=i)\n",
      "    LDA_model.fit(X_train, Y_train.as_matrix().T.ravel())\n",
      "    print(LDA_model.score(X_valid, Y_valid))\n",
      "=====\n",
      "from sklearn.model_selection import cross_val_score\n",
      "Y_data_reshaped =  Y_data.as_matrix().reshape((Y_data.shape[0],))\n",
      "\n",
      "scores = cross_val_score(LDA_model, X_data, Y_data_reshaped, cv = 5, n_jobs = -1)\n",
      "print(scores)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "fig, axes = plt.subplots(3, 6, figsize=(12, 6),\n",
      "                         subplot_kw={jupyter_string: [], jupyter_string: []})\n",
      "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
      "axes = axes.ravel()\n",
      "im_len = len(X_train)\n",
      "print(im_len)\n",
      "for i in range(18):\n",
      "    \n",
      "    rand_index = random.randint(0,im_len)\n",
      "    image = X_train[rand_index]\n",
      "    axes[i].imshow(image)\n",
      "    axes[i].set_title((jupyter_string+str(y_train[rand_index])))\n",
      "--------------------\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "=====\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "def evaluate(X_data, y_data):\n",
      "    num_examples = len(X_data)\n",
      "    total_accuracy = 0\n",
      "    sess = tf.get_default_session()\n",
      "    for offset in range(0, num_examples, BATCH_SIZE):\n",
      "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
      "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
      "        total_accuracy += (accuracy * len(batch_x))\n",
      "    return total_accuracy / num_examples\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "new_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "D.to_frame()\n",
      "=====\n",
      "D.detected()\n",
      "--------------------\n",
      "geodata = pd.merge(geodata, geocode, on=jupyter_string)\n",
      "=====\n",
      "com_df = df.merge(geocode, how=jupyter_string, on=jupyter_string )\n",
      "com_df.head(12)\n",
      "--------------------\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "=====\n",
      "data_PMN_300 = pd.read_csv(jupyter_string)\n",
      "data_AMN_300 = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data_PMN_300.head()\n",
      "=====\n",
      "data_PMN_300.tail()\n",
      "--------------------\n",
      "data_AMN_300.tail()\n",
      "=====\n",
      "data_AMN_300.tail()\n",
      "--------------------\n",
      "p_MgO_PMN = np.array(p_MgO_PMN)\n",
      "p_MgO_AMN = np.array(p_MgO_AMN)\n",
      "p_Pt_PMN = np.array(p_Pt_PMN)\n",
      "p_Au_AMN = np.array(p_Au_AMN)\n",
      "=====\n",
      "f, axarr = plt.subplots(2, 2, \\\n",
      "                    figsize=(8,6))\n",
      "ax = [axarr[0,0], axarr[0,1], axarr[1,0], axarr[1,1]]\n",
      "ms = 8; mew = 0.7\n",
      "label = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "for i in range(4):\n",
      "    ax[i].axhline(y=0, c=jupyter_string, ls=jupyter_string)\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            unp.nominal_values(p_Au_AMN[i]) - unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_AMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Au_AMN[i]),\\\n",
      "            fmt=jupyter_string, mec=jupyter_string, mew=mew, label = jupyter_string, \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            unp.nominal_values(p_Pt_PMN[i]) - unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_PMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Pt_PMN[i]), \\\n",
      "            fmt=jupyter_string, mec=jupyter_string, mew=mew, label=jupyter_string, \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].set_xlabel(jupyter_string); ax[i].set_ylabel(jupyter_string)\n",
      "    l = ax[i].legend(loc=3, numpoints = 1, fontsize = 10)\n",
      "    l.get_frame().set_linewidth(0.5)\n",
      "    plt.tight_layout(pad=0.4)\n",
      "    ax[i].set_ylim(-5.,3.); ax[i].set_xlim(0.,140.)\n",
      "    ax[i].set_xticks(ax[i].get_xticks()[::2]); ax[i].set_yticks(ax[i].get_yticks()[::2])\n",
      "    ax[i].text(0.08, 0.83,label[i], horizontalalignment=jupyter_string,\\\n",
      "            verticalalignment=jupyter_string, transform = ax[i].transAxes,\\\n",
      "              fontsize = 24)\n",
      "\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string, \\\n",
      "                        pad_inches=0.1)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_reader = pd.read_csv(jupyter_string,\n",
      "                        chunksize=10)\n",
      "\n",
      "print(next(df_reader))\n",
      "print(next(df_reader))\n",
      "--------------------\n",
      "df_reader = pd.read_csv(jupyter_string,\n",
      "                        chunksize=10)\n",
      "\n",
      "df_reader = pd.DataFrame()\n",
      "\n",
      "for df in df_reader:\n",
      "    df_reader = df_reader.append(df)\n",
      "\n",
      "df_reader.head()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=10)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "print(df_urb_pop.head())\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string]==jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "           df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "--------------------\n",
      "urb_pop_df = pd.DataFrame(pops_list, columns=[jupyter_string, jupyter_string])\n",
      "urb_pop_df.head()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "            df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "\n",
      "\n",
      "\n",
      "df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                        for tup in pops_list]\n",
      "\n",
      "\n",
      "df_pop_ceb.plot(kind=jupyter_string, \n",
      "                x=jupyter_string, \n",
      "                y=jupyter_string,\n",
      "                title=jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "            df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "\n",
      "\n",
      "\n",
      "df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                        for tup in pops_list]\n",
      "\n",
      "\n",
      "df_pop_ceb.plot(kind=jupyter_string, \n",
      "                x=jupyter_string, \n",
      "                y=jupyter_string,\n",
      "                title=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "data = pd.DataFrame()\n",
      "\n",
      "\n",
      "for df_urb_pop in urb_pop_reader:\n",
      "    df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "    \n",
      "    pops = zip(df_pop_ceb[jupyter_string],\n",
      "               df_pop_ceb[jupyter_string])\n",
      "    \n",
      "    pops_list = list(pops)\n",
      "    \n",
      "    \n",
      "    \n",
      "    df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                            for tup in pops_list]\n",
      "    \n",
      "    \n",
      "    data = data.append(df_pop_ceb)\n",
      "    \n",
      "data.plot(kind=jupyter_string,\n",
      "          x=jupyter_string,\n",
      "          y=jupyter_string,\n",
      "          title=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "engine = create_engine(jupyter_string)\n",
      "conn = engine.connect()\n",
      "=====\n",
      "engine = create_engine(jupyter_string)\n",
      "conn = engine.connect()\n",
      "--------------------\n",
      "measurements = pd.read_csv(jupyter_string)\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "measurements = pd.read_csv(jupyter_string)\n",
      "measurements[\"date\"] = pd.to_datetime(measurements[\"date\"], format=jupyter_string)\n",
      "measurements.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns; sns.set();\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "stations.head()\n",
      "=====\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "stations.head()\n",
      "--------------------\n",
      "plt.plot(patSatTrain.patSat,patSatTrain.patSat,jupyter_string)\n",
      "plt.plot(patSatTrain.patSat,mod1Result.predict(patSatTrain),jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.cla\n",
      "sg=sns.jointplot(x=jupyter_string, y=jupyter_string,marker=jupyter_string,data=predObsDF)\n",
      "x0, x1 = sg.ax_joint.get_xlim()\n",
      "y0, y1 = sg.ax_joint.get_ylim()\n",
      "lims = [max(x0, y0), min(x1, y1)]\n",
      "sg.ax_joint.plot(lims, lims, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)  \n",
      "\n",
      "--------------------\n",
      "pd.DataFrame(mod1Result.summary())\n",
      "=====\n",
      "from statsmodels.stats.outliers_influence import OLSInfluence\n",
      "influenceResults=OLSInfluence(mod1Result)\n",
      "influenceResults.summary_frame().head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "warnings.resetwarnings()  \n",
      "--------------------\n",
      "influenceResults.plot_influence()\n",
      "influenceResults.plot_leverage_resid2()\n",
      "plt.show()\n",
      "=====\n",
      "from statsmodels.graphics.regressionplots import influence_plot\n",
      "fig, ax = plt.subplots(figsize=(10,8))\n",
      "influence_plot(mod1Result,ax=ax,plot_alpha=0.01,alpha=0.001,fontsize=10)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "patSatDF=pd.read_csv(jupyter_string)  \n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(10,8))\n",
      "influence_plot(mod1Result,ax=ax,plot_alpha=0.01,alpha=0.001,fontsize=10)\n",
      "=====\n",
      "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
      "fig, ax = plt.subplots(figsize=(12,10))\n",
      "plot_leverage_resid2(mod1Result, ax = ax)\n",
      "--------------------\n",
      "patSatDF = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "patSatDF2=patSatDF[['patSat' <<unk>>,'q2' <<unk>>,'q6' <<unk>>,'ptCat' <<unk>>]]\n",
      "--------------------\n",
      "patSatDF2.to_html(jupyter_string)\n",
      "=====\n",
      "patSatProfile=profile.ProfileReport(patSatDF2)\n",
      "patSatProfile.to_file(jupyter_string)  \n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "seattleData = pd.read_csv(jupyter_string)\n",
      "seattleData.shape\n",
      "ind = pd.DatetimeIndex(seattleData['Date Reported' <<unk>>])\n",
      "seattleData[jupyter_string] = ind.dayofweek\n",
      "seattleData[jupyter_string] = ind.strftime(jupyter_string)\n",
      "seattleData[jupyter_string] = (ind.dayofweek>4)\n",
      "seattleData[jupyter_string] = ind.date.astype(jupyter_string)\n",
      "seattleData[jupyter_string] = ind.hour\n",
      "\n",
      "\n",
      "--------------------\n",
      "patSatDF2 = patSatDF2.sample(frac=1).reset_index(drop=True)\n",
      "=====\n",
      "np.random.seed(3)  \n",
      "\n",
      "patSatDF2=patSatDF2.assign(train=pd.Series(np.random.random(len(patSatDF2))<=0.80))\n",
      "patSatTrain=patSatDF2[patSatDF2.train==True]\n",
      "patSatTest=patSatDF2[patSatDF2.train!=True]\n",
      "print(jupyter_string,len(patSatTrain))\n",
      "print(jupyter_string,len(patSatTest))\n",
      "--------------------\n",
      "X.head()\n",
      "=====\n",
      "X.head()\n",
      "--------------------\n",
      "sns.regplot(y,mod1.predict(X))\n",
      "=====\n",
      "predObsDF=pd.DataFrame({jupyter_string:patSatTrain.patSat,jupyter_string:mod1Result.predict(patSatTrain)})\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, index_col=jupyter_string, parse_dates = True)\n",
      "df\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "names = df[jupyter_string]\n",
      "names\n",
      "--------------------\n",
      "names = pd.read_csv(jupyter_string)\n",
      "names.head()\n",
      "=====\n",
      "first_three = df.values[0:1,:] \n",
      "first_three\n",
      "--------------------\n",
      "first_three = first_three.flatten()\n",
      "first_three\n",
      "=====\n",
      "text = df.values[:,3:4]\n",
      "text.flatten()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.gray() \n",
      "plt.matshow(digits.images[0]) \n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r)\n",
      "plt.show()\n",
      "--------------------\n",
      "X = np.reshape(digits.data, (len(digits.data), 1))\n",
      "=====\n",
      "data = digits.images.reshape((digits.images.shape[0], -1))\n",
      "--------------------\n",
      "mean = np.mean(data, axis=0)\n",
      "std = np.std(data, axis=0)\n",
      "=====\n",
      "from sklearn import preprocessing\n",
      "preprocessing.scale(digits.data)\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string,data=seattleData,order=range(25),palette=jupyter_string)\n",
      "plt.xlabel(jupyter_string, fontsize=14)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "\n",
      "--------------------\n",
      "plt.scatter(iris_X_train[:,0], iris_X_train[:,1], c=iris_y_train)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "X = np.array([\n",
      "    [ .5],\n",
      "    [1]\n",
      "])\n",
      "\n",
      "y = [0.5, 1]\n",
      "\n",
      "test = np.c_[ 0, 2, 2].T\n",
      "\n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(X, y)\n",
      "\n",
      "plt.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = 0.1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    plt.plot(test, regr.predict(test))\n",
      "    plt.scatter(this_X, y, s=3)\n",
      "plt.show()\n",
      "--------------------\n",
      "math = pd.read_csv(jupyter_string)\n",
      "math.head()\n",
      "=====\n",
      "math = pd.read_csv(jupyter_string)\n",
      "math.head(10)\n",
      "--------------------\n",
      "por = pd.read_csv(jupyter_string)\n",
      "por.head(10)\n",
      "=====\n",
      "por = pd.read_csv(jupyter_string)\n",
      "por.head(10)\n",
      "--------------------\n",
      "por.head()\n",
      "=====\n",
      "por.head()\n",
      "--------------------\n",
      "demo = pd.read_csv(jupyter_string)\n",
      "demo.head()\n",
      "=====\n",
      "math.groupby(jupyter_string)[jupyter_string].hist()\n",
      "--------------------\n",
      "math.groupby(jupyter_string)[jupyter_string].hist()\n",
      "=====\n",
      "por.groupby(jupyter_string)[jupyter_string].hist()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "math.groupby(math.school)[[jupyter_string, jupyter_string]].count()\n",
      "--------------------\n",
      "math.groupby(jupyter_string)[jupyter_string].count()\n",
      "=====\n",
      "math_gp = math[math.school == jupyter_string]\n",
      "math_gp.groupby(math_gp.school)[[jupyter_string]].count()\n",
      "--------------------\n",
      "math_gp.groupby(math_gp.school)[[jupyter_string, jupyter_string]].count()\n",
      "=====\n",
      "math_ms = math[math.school == jupyter_string]\n",
      "math_ms.groupby(math_ms.school)[[jupyter_string]].count()\n",
      "--------------------\n",
      "sns.countplot(jupyter_string,data=seattleData,order=range(25),palette=jupyter_string)\n",
      "plt.xlabel(jupyter_string, fontsize=14)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "=====\n",
      "crimeCategorygroup = seattleData.groupby([jupyter_string,jupyter_string])\n",
      "crimeCategorygroup = crimeCategorygroup.size().reset_index()\n",
      "crimeCategorygroup.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "g = sns.FacetGrid(crimeCategorygroup, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "--------------------\n",
      "por_ms = por_ms.dropna()\n",
      "por_ms.head()\n",
      "=====\n",
      "math.info()\n",
      "math.isnull().sum()\n",
      "--------------------\n",
      "por.info()\n",
      "=====\n",
      "por.info()\n",
      "por.isnull().sum()\n",
      "--------------------\n",
      "math.head()\n",
      "=====\n",
      "sns.pairplot(math_gp)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import os\n",
      "os.getcwd()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df_test.head()\n",
      "=====\n",
      "df_test.head(10)\n",
      "--------------------\n",
      "g = sns.FacetGrid(seattleData, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "=====\n",
      "crimeCategorygroup = seattleData.groupby([jupyter_string,jupyter_string,\"Summarized Offense Description\"])\n",
      "\n",
      "crimeCategorygroup = crimeCategorygroup.size().reset_index()\n",
      "\n",
      "crimeCategorygroup.columns = [jupyter_string,jupyter_string,'Summarized Offense Description' <<unk>>,jupyter_string]\n",
      "\n",
      "\n",
      "\n",
      "select_number = pd.DataFrame({jupyter_string: {jupyter_string: 5, jupyter_string: 5, \n",
      "                                                   jupyter_string: 5,jupyter_string: 5,\n",
      "                                                    jupyter_string : 2}})\n",
      "\n",
      "CCG =  crimeCategorygroup.groupby([jupyter_string,\"Summarized Offense Description\"]).sum().reset_index()\n",
      "CCG = CCG[CCG[jupyter_string].isin([jupyter_string, jupyter_string])]\n",
      "CCG = CCG.groupby([jupyter_string]).apply(lambda dfg: (dfg.nlargest(4, jupyter_string))).reset_index(drop=True)\n",
      "\n",
      "crimeCategorygroup= crimeCategorygroup[crimeCategorygroup[\"Summarized Offense Description\"].isin(CCG[\"Summarized Offense Description\"])] \n",
      "\n",
      "\n",
      "g = sns.FacetGrid(crimeCategorygroup, row=jupyter_string,row_order =[jupyter_string,jupyter_string],\n",
      "                      hue=\"Summarized Offense Description\", size=5, aspect=2,sharey=False,sharex=False) \n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "\n",
      "g.set(xlabel=jupyter_string,\n",
      "      xticks = np.arange(0,25,1),\n",
      "      ylabel=jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.subplot(1,2,1)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "df['ApplicantIncome' <<unk>>].hist(bins=50)\n",
      "--------------------\n",
      "df_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_test[['Loan_ID' <<unk>>, 'Loan_Status' <<unk>>]].to_csv(jupyter_string, sep=jupyter_string, index=False)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string].hist(bins=20)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.yticks(rotation=jupyter_string)\n",
      "plt.scatter(crimes[jupyter_string], crimes[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.yticks(rotation=jupyter_string)\n",
      "plt.scatter(crimes[jupyter_string], crimes[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(style=jupyter_string, color_codes=True)\n",
      "districtGroup = seattleData.groupby('District/Sector' <<unk>>)\n",
      "districtGroup = districtGroup.size().sort_values( ascending=False).head(10)\n",
      "subDataDistrict = seattleData[seattleData['District/Sector' <<unk>>].isin( districtGroup.index)]\n",
      "\n",
      "subDataDistrictGroup = subDataDistrict.groupby([jupyter_string,\"District/Sector\"])\n",
      "subDataDistrictGroup = subDataDistrictGroup.size().reset_index()\n",
      "subDataDistrictGroup.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "g = sns.FacetGrid(subDataDistrictGroup, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string].hist(bins=20)\n",
      "--------------------\n",
      "df.isnull().sum()\n",
      "=====\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "--------------------\n",
      "df.fillna(df.mean(), inplace = True)\n",
      "=====\n",
      "df[jupyter_string].fillna(np.mean(df[jupyter_string]), inplace=True)\n",
      "--------------------\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "=====\n",
      "df['Loan_Amount_Term' <<unk>>].fillna(360.0, inplace=True)\n",
      "--------------------\n",
      "gender_marriege = gender_marriege.div(gender_marriege.sum(1).astype(float), axis=0)\n",
      "=====\n",
      "gender_marriege.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "--------------------\n",
      "days = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "sns.countplot(x=jupyter_string,data=seattleData,palette=jupyter_string ,order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "var_mod = ['Married' <<unk>>, 'Dependents' <<unk>>, 'Education' <<unk>>, 'Self_Employed' <<unk>>,'Property_Area' <<unk>>]\n",
      "le = LabelEncoder()\n",
      "for i in var_mod:\n",
      "    df[i] = le.fit_transform(df[i].astype(str))\n",
      "    \n",
      "df['Credit_History' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "=====\n",
      "gender_marriege = pd.crosstab(df['Gender' <<unk>>], df['Married' <<unk>>])\n",
      "--------------------\n",
      "gender_marriege.div(gender_marriege.sum(1).astype(float), axis=0).plot(kind=jupyter_string, stacked=True)\n",
      "=====\n",
      "gender_marriege.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "--------------------\n",
      "data4.head()\n",
      "=====\n",
      "data4.head()\n",
      "--------------------\n",
      "sns.countplot(x=jupyter_string,data=seattleData,palette=jupyter_string ,order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "=====\n",
      "sns.countplot(x=jupyter_string,data=seattleData,hue=jupyter_string ,\n",
      "              order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string),\n",
      "              hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "airports.head()\n",
      "--------------------\n",
      "airports.info()\n",
      "=====\n",
      "airlines.head()\n",
      "--------------------\n",
      "airports.info()\n",
      "=====\n",
      "routes.head()\n",
      "--------------------\n",
      "routes.head()\n",
      "=====\n",
      "routes.head()\n",
      "--------------------\n",
      "data[jupyter_string] = data.apply(lambda row: get_distance(row.latitude, row.longitude, row.pcode_lat, row.pcode_lon), axis=1)\n",
      "=====\n",
      "routes[jupyter_string] = get_distance(routes[jupyter_string], routes[jupyter_string],\n",
      "                                  routes[jupyter_string], routes[jupyter_string])\n",
      "routes.distance.describe()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "plt.show() \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np \n",
      "import scipy as sp \n",
      "import matplotlib as mpl \n",
      "import matplotlib.cm as cm \n",
      "import matplotlib.pyplot as plt \n",
      "import pandas as pd \n",
      "\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "import seaborn.apionly as sns \n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "mtcars = pd.read_csv(jupyter_string)\n",
      "mtcars.head()\n",
      "=====\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "days = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "offenseGroupdf = seattleData.groupby('Summarized Offense Description' <<unk>>)\n",
      "offenseGroup = offenseGroupdf.size().sort_values( ascending=False).head(10)\n",
      "subDataOffense = seattleData[seattleData['Summarized Offense Description' <<unk>>].isin( offenseGroup.index)]\n",
      "sns.countplot(x=\"Summarized Offense Description\",hue=jupyter_string, hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string),\n",
      "              data=subDataOffense);\n",
      "plt.xticks(rotation=70)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "\n",
      "--------------------\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "=====\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "dfcars.rename(columns={\"Unnamed: 0\":\"name\"}, inplace=True)\n",
      "dfcars.head()\n",
      "=====\n",
      "dfcars = pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "dfcars.info()\n",
      "=====\n",
      "dfcars.info()\n",
      "--------------------\n",
      "dfcars.name = dfcars.name.astype(jupyter_string)\n",
      "dfcars.info()\n",
      "=====\n",
      "different_values = [jupyter_string, 1, 2, 3]\n",
      "different_series = pd.Series(different_values)\n",
      "different_series\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "similar_values = [2, 3, 4]\n",
      "similar_series = pd.Series(similar_values)\n",
      "similar_series\n",
      "--------------------\n",
      "dfcars.to_csv(jupyter_string)\n",
      "=====\n",
      "dfcars.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "dfcars.describe()\n",
      "--------------------\n",
      "a = np.array([1,2,3,4,5])\n",
      "b = np.array([1,2,3,4,5])\n",
      "c = np.array([1,2,3,4,5])\n",
      "=====\n",
      "my_array = np.array([1,2,3], dtype=jupyter_string)\n",
      "my_array\n",
      "--------------------\n",
      "my_array.dtype\n",
      "=====\n",
      "my_array = np.array([1,2,3,4,5], dtype=jupyter_string)\n",
      "my_array\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "color_palette = sns.color_palette()\n",
      "color_palette\n",
      "--------------------\n",
      "sns.palplot(color_palette)\n",
      "=====\n",
      "plt.show()\n",
      "sns.palplot(color_palette)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, index_col=0)\n",
      "df.head()\n",
      "=====\n",
      "dfcars.mpg.hist()\n",
      "plt.xlabel(\"mpg\");\n",
      "--------------------\n",
      "dfcars.mpg.hist(bins=20, color=jupyter_string)\n",
      "plt.xlabel(\"mpg\");\n",
      "=====\n",
      "plt.hist(dfcars.mpg, bins=15, alpha=0.5);\n",
      "plt.xlabel(\"mpg\");\n",
      "plt.title(jupyter_string);\n",
      "--------------------\n",
      "dfcars.mpg.values\n",
      "=====\n",
      "plt.hist(dfcars.mpg.values, bins=15, alpha=0.5);\n",
      "plt.xlim(5, 40)\n",
      "plt.ylim(0, 10)\n",
      "plt.xlabel(\"mpg\");\n",
      "plt.title(jupyter_string);\n",
      "--------------------\n",
      "with sns.plotting_context(jupyter_string): \n",
      "    plt.scatter(dfcars.wt, dfcars.mpg)\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg)\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string);\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string);\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string) \n",
      "--------------------\n",
      "dfcars = dfcars[dfcars.mpg < 20]\n",
      "=====\n",
      "np.sum(dfcars.mpg < 20)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "print(df.columns)\n",
      "\n",
      "--------------------\n",
      "np.mean(dfcars.mpg < 20)\n",
      "=====\n",
      "(dfcars.mpg < 20).mean()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, parse_dates=['DATE' <<unk>>])\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "ppd = pd.read_csv(jupyter_string)\n",
      "ppd_disc = pd.read_csv(jupyter_string)\n",
      "\n",
      "--------------------\n",
      "ppd.head()\n",
      "=====\n",
      "ppd.head()\n",
      "--------------------\n",
      "df = pd.merge(left=ppd, right=ppd_disc, how=jupyter_string, left_on=jupyter_string, right_on=jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "ppd = ppd.merge(ppd_disc, how=jupyter_string, on=\"cap_number\")\n",
      "--------------------\n",
      "ppd.head()\n",
      "=====\n",
      "ppd.head()\n",
      "--------------------\n",
      "ppd.describe()\n",
      "=====\n",
      "ppd[\"investigative_findings\"].value_counts()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.columns.to_series().groupby(df.dtypes).groups\n",
      "\n",
      "df['chas' <<unk>>] = pd.to_numeric(df['chas' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "df['dva_chasa' <<unk>>] = pd.to_numeric(df['dva_chasa' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "df['noch' madeupword0002] = pd.to_numeric(df['noch' madeupword0002], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['ves' <<unk>>] = pd.to_numeric(df['ves' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['rost' <<unk>>] = pd.to_numeric(df['rost' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['vozrast' <<unk>>] = pd.to_numeric(df['vozrast' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['razmer' <<unk>>] = pd.to_numeric(df['razmer' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "sns.countplot(x=\"investigative_findings\", data=ppd)\n",
      "=====\n",
      "ppd[\"disciplinary_findings\"].value_counts()\n",
      "--------------------\n",
      "ppd[jupyter_string].value_counts()\n",
      "=====\n",
      "ppd[\"allegations_investigated\"].value_counts()\n",
      "--------------------\n",
      "ppd[\"airport_code\"].value_counts()\n",
      "=====\n",
      "ppd[\"dist_occurrence\"].value_counts()\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "sns.countplot(x=\"dist_occurrence\", data=ppd, palette=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "guiltyfindings = ppd[ppd[\"disciplinary_findings\"] == jupyter_string]\n",
      "print(guiltyfindings)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "tcfindings = ppd[ppd[\"disciplinary_findings\"] == jupyter_string]\n",
      "print(tcfindings.head())\n",
      "--------------------\n",
      "tcfindings = tcfindings.reset_index(drop=True)\n",
      "print(tcfindings.head())\n",
      "=====\n",
      "tcfindings[\"cap_number\"].value_counts()\n",
      "--------------------\n",
      "tcfindings = tcfindings[tcfindings[\"cap_number\"] > 1]\n",
      "print(tcfindings.head())\n",
      "tcfindings[\"cap_number\"].value_counts()\n",
      "=====\n",
      "tcfindings[\"general_cap_classification\"].value_counts()\n",
      "--------------------\n",
      "tcfindings[jupyter_string] = tcfindings[\"general_cap_classification\"].apply(lambda x: 1 if x == jupyter_string else 0)\n",
      "tcfindings.head()\n",
      "=====\n",
      "tcfindings[\"allegations_investigated\"].value_counts()\n",
      "--------------------\n",
      "tcfindings[tcfindings[\"general_cap_classification\"] == jupyter_string][\"allegations_investigated\"].value_counts()\n",
      "=====\n",
      "tcfindings[\"po_sex\"].value_counts()\n",
      "--------------------\n",
      "complaints_by_officer = complaints.groupby(\"officer\").size().reset_index(name=jupyter_string)\n",
      "complaints_by_officer.head()\n",
      "=====\n",
      "ppd[jupyter_string] = ppd[[\"po_initials\",\"po_race\",\"po_sex\",\"dist_occurrence\"]].apply(lambda x: jupyter_string.join(x.astype(str)), axis=1)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "print(df.columns)\n",
      "df.columns.to_series().groupby(df.dtypes).groups\n",
      "=====\n",
      "back = df['vpopu' <<unk>>].copy()\n",
      "df = df.assign(backdoor=back)\n",
      "\n",
      "df['vpopu' <<unk>>][df['vpopu' <<unk>>] > 1] = 1\n",
      "\n",
      "\n",
      "    \n",
      "df[jupyter_string][0 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string][1 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df.chas.value_counts(normalize=False, sort=True,\n",
      "                         ascending=False, bins=None, dropna=True)\n",
      "\n",
      "df.vpopu\n",
      "print(df.head())\n",
      "--------------------\n",
      "ppd[jupyter_string] = ppd[[\"po_initials\",\"po_race\",\"po_sex\"]].apply(lambda x: jupyter_string.join(x.astype(str)), axis=1)\n",
      "=====\n",
      "ppd[jupyter_string].value_counts().head(25)\n",
      "--------------------\n",
      "ppd[jupyter_string].value_counts().tail(25)\n",
      "=====\n",
      "ppd[~ppd[jupyter_string].isin([jupyter_string])].value_counts().head(25)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head(3)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "sns.distplot(df.temperature, bins=25)\n",
      "--------------------\n",
      "df.temperature.describe()\n",
      "=====\n",
      "print (jupyter_string).format(df.temperature.size)\n",
      "--------------------\n",
      "temp_mean = df.temperature.mean()\n",
      "temp_mean\n",
      "=====\n",
      "sample_mean =df.temperature.mean()\n",
      "sample_std =df.temperature.std()\n",
      "noise =(sample_std/ np.sqrt(130))\n",
      "z_score =(sample_mean-98.6)/noise\n",
      "print (jupyter_string)% z_score\n",
      "--------------------\n",
      "male_temp = df.temperature[df.gender == jupyter_string]\n",
      "female_temp = df.temperature[df.gender == jupyter_string]\n",
      "=====\n",
      "df_male = df[df.gender==jupyter_string]\n",
      "df_female = df[df.gender==jupyter_string]\n",
      "print (jupyter_string) % df_male.temperature.mean()\n",
      "print (jupyter_string) % df_female.temperature.mean()\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "binwidth = 0.025\n",
      "\n",
      "print (data.steering_angle[0:100])\n",
      "\n",
      "mysteer=data.steering_angle\n",
      "mysteer1 =mysteer[1:]\n",
      "\n",
      "\n",
      "steer_floats = [float(x) for x in mysteer1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams\n",
      "=====\n",
      "def brightness_shift(img, bright_value=None):\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
      "    \n",
      "    if bright_value:\n",
      "        img[:,:,2] += bright_value\n",
      "    else:\n",
      "        random_bright = .25 + np.random.uniform()\n",
      "        img[:,:,2] = img[:,:,2] * random_bright\n",
      "    \n",
      "    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
      "    return img\n",
      "\n",
      "\n",
      "img_in = cv2.imread(jupyter_string)\n",
      "\n",
      "img = brightness_shift(img_in, 200)\n",
      "\n",
      "pts1 = np.float32([[0,60],[0,120],[300,60],[300,120]])\n",
      "\n",
      "pts2 = np.float32([[0,120],[0,180],[300,120],[300,180]])\n",
      "\n",
      "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
      "dst = cv2.warpPerspective(img,M,(320,160))\n",
      "\n",
      "\n",
      "pts1 = np.float32([[0,60],[0,120],[300,60],[300,120]])\n",
      "\n",
      "pts2 = np.float32([[0,0],[0,60],[300,0],[300,60]])\n",
      "M2 = cv2.getPerspectiveTransform(pts1,pts2)\n",
      "dst2 = cv2.warpPerspective(img,M2,(320,160))\n",
      "\n",
      "\n",
      " \n",
      "plt.subplots(figsize=(24, 96))\n",
      "\n",
      "plt.subplot(141),plt.imshow(img_in),plt.title(jupyter_string)\n",
      "plt.subplot(142),plt.imshow(img),plt.title(jupyter_string)\n",
      "plt.subplot(143),plt.imshow(dst),plt.title(jupyter_string)\n",
      "plt.subplot(144),plt.imshow(dst2),plt.title(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm_notebook\n",
      "import time\n",
      "import shutil\n",
      "import os\n",
      "import random\n",
      "import cv2\n",
      "import math\n",
      "import json\n",
      "import seaborn as sns \n",
      "import tensorflow as tf\n",
      "\n",
      "import keras\n",
      "from keras.preprocessing.image import *\n",
      "from keras.models import Sequential, Model\n",
      "from keras.layers import Convolution2D, Flatten, MaxPooling2D, Lambda, ELU\n",
      "from keras.layers.core import Dense, Dropout, Activation\n",
      "from keras.optimizers import Adam\n",
      "from keras.callbacks import Callback\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.regularizers import l2\n",
      "\n",
      "from IPython.display import display \n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "seed = 42\n",
      "np.random.seed(seed)\n",
      "tf.set_random_seed(seed)\n",
      "tf.reset_default_graph()\n",
      "\n",
      "columns = ['center' madeupword0002, 'left' <<unk>>, 'right' <<unk>>, jupyter_string, 'throttle' <<unk>>, 'brake' <<unk>>, 'speed' <<unk>>]\n",
      "data = pd.read_csv(jupyter_string, names=columns)\n",
      "\n",
      "print(jupyter_string, columns, jupyter_string)\n",
      "print(jupyter_string, data.shape, jupyter_string)\n",
      "print(data.describe(), jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import seaborn as sns \n",
      "sns.set(color_codes=True)\n",
      "\n",
      "\n",
      "plt.show() \n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.to_csv(jupyter_string)\n",
      "=====\n",
      "df2 = pd.DataFrame({jupyter_string:range(10), jupyter_string: np.random.randint(10, 100, size=10)})\n",
      "--------------------\n",
      "df.head() \n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.tail()\n",
      "=====\n",
      "df.tail()\n",
      "--------------------\n",
      "df.max()\n",
      "=====\n",
      "df['Temp' <<unk>>].min(), df[jupyter_string].max()\n",
      "--------------------\n",
      "df[jupyter_string].nunique()\n",
      "=====\n",
      "df['Date' <<unk>>].unique()\n",
      "\n",
      "--------------------\n",
      "corrmat = df.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "sns.heatmap(corrmat,linewidths=.1, annot=True, annot_kws={jupyter_string: 7})\n",
      "=====\n",
      "data = (df.chas.dropna())\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 12)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "ax.set_xticks(np.arange(0,31000,1000))\n",
      "\n",
      "ax.xaxis.labelpad = 20\n",
      "ax.yaxis.labelpad = 20\n",
      "\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "neworder  = ['Time' <<unk>>, 'Date' <<unk>>, jupyter_string, 'Temp' <<unk>>, 'Names' madeupword0002, jupyter_string]\n",
      "df = df[neworder]\n",
      "df\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df.set_index('Names' madeupword0002)\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, index_col=jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "pd.read_csv(jupyter_string, index_col=['Names' madeupword0002])\n",
      "\n",
      "--------------------\n",
      "df[jupyter_string].head()\n",
      "=====\n",
      "df[jupyter_string].head() \n",
      "--------------------\n",
      "df.drop([jupyter_string, jupyter_string], axis=1, inplace=True)\n",
      "=====\n",
      "df.drop(['Time' <<unk>>, 'Date' <<unk>>], axis=1, inplace=True)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "pd.to_datetime(df[jupyter_string], format=jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string] = pd.to_datetime(df[jupyter_string])\n",
      "=====\n",
      "df[jupyter_string] = pd.to_datetime(df[jupyter_string])\n",
      "--------------------\n",
      "df.set_index(jupyter_string, inplace=True)\n",
      "df.head()\n",
      "=====\n",
      "df.set_index(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "df[jupyter_string].resample(jupyter_string).mean()\n",
      "=====\n",
      "df.resample(jupyter_string)['Temp' <<unk>>].max()\n",
      "--------------------\n",
      "data = (df.price.dropna())\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 12)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "ax.set_xticks(np.arange(0,31000,1000))\n",
      "\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data = (df.chas.dropna()*0.018)\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 14)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.set_xticks(np.arange(0,550,50))\n",
      "ax.set_yticks(np.arange(0,550,50))\n",
      "ax.xaxis.labelpad = 20\n",
      "ax.yaxis.labelpad = 20\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, color = jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "=====\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "--------------------\n",
      "plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "pandas_plot = df.plot(jupyter_string, jupyter_string, color=jupyter_string)\n",
      "plt.title(jupyter_string);\n",
      "\n",
      "--------------------\n",
      "sns_plot = sns.regplot(x=jupyter_string, y=jupyter_string, data=df)\n",
      "plt.title(jupyter_string);\n",
      "=====\n",
      "seaborn_plot = sns.FacetGrid(df,  size=3, aspect= 2) \n",
      "seaborn_plot.map(plt.scatter, jupyter_string, jupyter_string)\n",
      "seaborn_plot.map(plt.plot, jupyter_string, jupyter_string)\n",
      "sns.plt.title(jupyter_string);\n",
      "--------------------\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "=====\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.close() \n",
      "--------------------\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "fig = pandas_plot.get_figure()\n",
      "fig.savefig (jupyter_string)\n",
      "--------------------\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 1], alpha=opacity, label=jupyter_string)\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 2], alpha=opacity, label=jupyter_string)\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 3], alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "from scipy.stats import ttest_ind\n",
      "fare_from_q = titanic_data[titanic_data.Embarked == jupyter_string]\n",
      "fare_from_c = titanic_data[titanic_data.Embarked == jupyter_string]\n",
      "t_stat, p_value = ttest_ind(fare_from_q.Fare, fare_from_c.Fare)\n",
      "\n",
      "print(jupyter_string % (t_stat, p_value))\n",
      "--------------------\n",
      "t_stat, p_value = ttest_ind(fare_from_q.Fare, fare_from_c.Fare)\n",
      "\n",
      "print(jupyter_string % (t_stat, p_value))\n",
      "=====\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(fare_from_q.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.hist(fare_from_c.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head()\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head(5)\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data.info()\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data.info()\n",
      "--------------------\n",
      "men = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "men_prob = (len(men) / len(titanic_data))\n",
      "print(jupyter_string + str(men_prob) + jupyter_string)\n",
      "=====\n",
      "male_passenger = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "prob_male = (len(male_passenger) / len(titanic_data))\n",
      "print(jupyter_string + str(prob_male) + jupyter_string)\n",
      "--------------------\n",
      "herbourg_passenger = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "prob_herbourg = (len(herbourg_passenger) / len(titanic_data))\n",
      "print(jupyter_string + str(prob_herbourg) + jupyter_string)\n",
      "=====\n",
      "c_port = survivors[survivors.Embarked == jupyter_string]\n",
      "prob_c = (len(c_port) / len(survivors))\n",
      "print(jupyter_string + str(prob_c) + jupyter_string)\n",
      "--------------------\n",
      "plt.hist(survivors.Age, bins = 25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "all_ages = []\n",
      "age_mean = np.mean(titanic_data.Age)\n",
      "\n",
      "for i, k in enumerate(titanic_data.Age): \n",
      "    if round(k, 3) != round(age_mean, 3):\n",
      "        all_ages.append(k)\n",
      "\n",
      "H, edges = np.histogram(all_ages, bins=25)\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "ax.bar(edges[:-1], H / float(sum(H)), width=edges[1] - edges[0])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.minorticks_on()\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.distplot(survivors_male.Age)\n",
      "sns.distplot(survivors_female.Age)\n",
      "=====\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(survivors_male.Age, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.hist(survivors_female.Age, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.io import show, output_notebook\n",
      "\n",
      "output_notebook()\n",
      "\n",
      "p = figure(plot_width=400, plot_height=400)\n",
      "\n",
      "\n",
      "p.circle([1, 2, 3, 4, 5], [6, 7, 2, 4, 5], size=20, color=jupyter_string, alpha=0.5)\n",
      "\n",
      "\n",
      "show(p)\n",
      "=====\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.io import show, output_notebook\n",
      "\n",
      "output_notebook()\n",
      "\n",
      "p = figure(plot_width=400, plot_height=400)\n",
      "\n",
      "\n",
      "p.square([1, 2, 3, 4, 5], [6, 7, 2, 4, 5], size=20, color=jupyter_string, alpha=0.5)\n",
      "\n",
      "\n",
      "show(p)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "leitor = pd.read_csv(jupyter_string)\n",
      "leitor.head()\n",
      "\n",
      "leitor.plot(kind = jupyter_string, x = [\"Year\"], y = [\"Biology\"], title = jupyter_string, color = jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(women_degrees['Year' <<unk>>], \n",
      "         women_degrees['Biology' <<unk>>], c=jupyter_string, label=jupyter_string)\n",
      "plt.plot(women_degrees['Year' <<unk>>], \n",
      "         100-women_degrees['Biology' <<unk>>], c=jupyter_string, label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "data4.info()\n",
      "=====\n",
      "y=data4['sale_price' madeupword0089]\n",
      "X=data4['gross_sq_feet' <<unk>>]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "from selenium.webdriver.support.ui import Select\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "driver=webdriver.Chrome()\n",
      "driver.get(jupyter_string)\n",
      "--------------------\n",
      "tow_trucks = pd.read_csv(jupyter_string)\n",
      "tow_trucks.head()\n",
      "=====\n",
      "df_add = pd.read_csv(jupyter_string)\n",
      "df_add\n",
      "--------------------\n",
      "df_add.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "pd.set_option(jupyter_string, -1)\n",
      "def get_url(row):\n",
      "    return jupyter_string.format(row[jupyter_string])\n",
      "df_add[jupyter_string] = df_add.apply(get_url, axis=1)\n",
      "df_add\n",
      "--------------------\n",
      "search_button = driver.find_element_by_id(jupyter_string)\n",
      "search_button.click()\n",
      "=====\n",
      "business_name = driver.find_elements_by_tag_name(jupyter_string)[5].text\n",
      "business_name.split(jupyter_string)\n",
      "\n",
      "\n",
      "owner = driver.find_elements_by_tag_name(jupyter_string)[7].text\n",
      "owner\n",
      "\n",
      "\n",
      "phone = driver.find_elements_by_tag_name(jupyter_string)[9].text\n",
      "phone\n",
      "\n",
      "\n",
      "status = driver.find_elements_by_tag_name(jupyter_string)[12].text\n",
      "status\n",
      "\n",
      "\n",
      "address_info = driver.find_elements_by_tag_name(jupyter_string)[14].text\n",
      "address = address_info.split(jupyter_string)[5:7]\n",
      "address\n",
      "--------------------\n",
      "df_add.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_add = df_add.apply(get_towing_info, axis=1).join(df)\n",
      "df_add\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df\n",
      "--------------------\n",
      "pd.set_option(jupyter_string, -1)\n",
      "=====\n",
      "pd.set_option(jupyter_string, -1)\n",
      "def get_url(row):\n",
      "    return jupyter_string.format(row[jupyter_string])\n",
      "df[jupyter_string] = df.apply(get_url, axis=1)\n",
      "df.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "def get_towing_info(row):\n",
      "    \n",
      "    \n",
      "    url = row[jupyter_string]\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        business_name = driver.find_elements_by_tag_name(jupyter_string)[5].text\n",
      "        business = business_name.split(jupyter_string)[1]\n",
      "        print(business)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        owner = driver.find_elements_by_tag_name(jupyter_string)[7].text\n",
      "        print(owner)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        phone = driver.find_elements_by_tag_name(jupyter_string)[9].text\n",
      "        print(phone)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        status = driver.find_elements_by_tag_name(jupyter_string)[12].text\n",
      "        print(status)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        address_info = driver.find_elements_by_tag_name(jupyter_string)[14].text\n",
      "        address = address_info.split(jupyter_string)[5:7]\n",
      "        print(address)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "df.apply(get_towing_info, axis=1)\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: x.split(jupyter_string)[0])\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: x.split(jupyter_string)[1])\n",
      "df.head()\n",
      "=====\n",
      "new_df = df.apply(get_towing_info, axis=1).join(df)\n",
      "new_df\n",
      "--------------------\n",
      "new_df.to_csv(jupyter_string)\n",
      "=====\n",
      "new_df.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "X = data.drop([jupyter_string], axis = 1)\n",
      "y = data[jupyter_string]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
      "=====\n",
      "Y1 = pd.DataFrame(data[jupyter_string]) \n",
      "Y2 = pd.DataFrame(data[jupyter_string]) \n",
      "Y3 = pd.DataFrame.join(Y1, Y2) \n",
      "\n",
      "\n",
      "X1 = data[col_names[2:]] \n",
      "X2 = data[col_names[5:]] \n",
      "X3 = data[col_names[2:5]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rs = 42\n",
      "\n",
      "ts1 = 0.3\n",
      "\n",
      "X_1_train, X_1_test, Y_1_train, Y_1_test = train_test_split(X1, Y1, test_size = ts1, random_state = rs)\n",
      "X_2_train, X_2_test, Y_2_train, Y_2_test = train_test_split(X2, Y2, test_size = ts1, random_state = rs)\n",
      "X_3_train, X_3_test, Y_3_train, Y_3_test = train_test_split(X3, Y3, test_size = ts1, random_state = rs)\n",
      "\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "lr = LinearRegression()\n",
      "lr.fit(X_1_train, Y_1_train)\n",
      "Y_1_pred = lr.predict(X_1_test)\n",
      "\n",
      "lr.fit(X_2_train, Y_2_train)\n",
      "Y_2_pred = lr.predict(X_2_test)\n",
      "\n",
      "lr.fit(X_3_train, Y_3_train)\n",
      "Y_3_pred = lr.predict(X_3_test)\n",
      "=====\n",
      "reg = linear_model.LinearRegression()\n",
      "reg1 = reg.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred1 = reg1.predict(X_1_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg1.coef_.T, index = X1.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % reg1.score(X_1_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,1], jupyter_string)\n",
      "--------------------\n",
      "reg3 = linear_model.LinearRegression()\n",
      "reg3.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred3 = reg3.predict(X_3_test)\n",
      "\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % reg3.score(X_3_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,0],jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,1], jupyter_string)\n",
      "=====\n",
      "reg3 = linear_model.LinearRegression()\n",
      "reg3.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred3 = reg3.predict(X_3_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg3.coef_.T, index = X3.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % reg3.score(X_3_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,0],jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,1], jupyter_string)\n",
      "\n",
      "--------------------\n",
      "reg4 = linear_model.LinearRegression()\n",
      "reg4.fit(X1_train, Y1_train)\n",
      "\n",
      "\n",
      "y_pred4 = reg4.predict(X1_test)\n",
      "\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y1_test, y_pred4))\n",
      "\n",
      "print(jupyter_string % r2_score(Y1_test, y_pred4))\n",
      "\n",
      "print(jupyter_string % reg4.score(X1_test, Y1_test))\n",
      "\n",
      "\n",
      "plotSummary(Y1_test[jupyter_string], y_pred4[:,0],jupyter_string)\n",
      "plotSummary(Y1_test[jupyter_string], y_pred4[:,1], jupyter_string)\n",
      "=====\n",
      "model1 = sm.OLS(Y_1_train, X_1_train)\n",
      "results1 = model1.fit()\n",
      "\n",
      "\n",
      "predictions1 = results1.predict(X_1_test) \n",
      "\n",
      "print(results1.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions1,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model2 = sm.OLS(Y_2_train, X_2_train)\n",
      "results2 = model2.fit()\n",
      "\n",
      "\n",
      "predictions2 = results2.predict(X_2_test) \n",
      "\n",
      "print(results2.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions2,jupyter_string)\n",
      "=====\n",
      "model1a = sm.OLS(Y_2_train, X_1_train)\n",
      "results1a = model1a.fit()\n",
      "\n",
      "\n",
      "predictions1a = results1a.predict(X_1_test) \n",
      "\n",
      "print(results1a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions1a,jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string, results1a.resid.abs().sum())\n",
      "print(jupyter_string , results1a.ssr)\n",
      "print(jupyter_string, results1a.rsquared)\n",
      "\n",
      "print(jupyter_string, np.sqrt(mean_squared_error(Y_2_test, predictions1a)))\n",
      "print(jupyter_string, np.sqrt(mean_squared_error(Y_2_train, results1a.predict(X_1_train))))\n",
      "--------------------\n",
      "X2 = sm.add_constant(X2)\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "model2.summary()\n",
      "=====\n",
      "model2 = sm.OLS(Y_1_train, X_2_train)\n",
      "results2 = model2.fit()\n",
      "\n",
      "\n",
      "predictions2 = results2.predict(X_2_test) \n",
      "\n",
      "print(results2.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions2,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model3 = sm.OLS(Y_2_train, X_2_train)\n",
      "results3 = model3.fit()\n",
      "\n",
      "\n",
      "predictions3 = results3.predict(X_2_test) \n",
      "\n",
      "print(results3.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions3,jupyter_string)\n",
      "=====\n",
      "model2a = sm.OLS(Y_2_train, X_2_train)\n",
      "results2a = model2a.fit()\n",
      "\n",
      "\n",
      "predictions2a = results2a.predict(X_2_test) \n",
      "\n",
      "print(results2a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions2a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model3a = sm.OLS(Y_3_train, X_3_train)\n",
      "results3a = model3a.fit()\n",
      "\n",
      "\n",
      "predictions3a = results3a.predict(X_3_test) \n",
      "\n",
      "print(results3a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], predictions3a,jupyter_string)\n",
      "=====\n",
      "model3 = sm.OLS(Y_1_train, X_3_train)\n",
      "results3 = model3.fit()\n",
      "\n",
      "\n",
      "predictions3 = results3.predict(X_3_test) \n",
      "\n",
      "print(results3.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions3,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model4 = sm.OLS(Y_2_train, X_3_train)\n",
      "results4 = model4.fit()\n",
      "\n",
      "\n",
      "predictions4 = results4.predict(X_3_test) \n",
      "\n",
      "print(results4.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4,jupyter_string)\n",
      "=====\n",
      "model3a = sm.OLS(Y_2_train, X_3_train)\n",
      "results3a = model3a.fit()\n",
      "\n",
      "\n",
      "predictions3a = results3a.predict(X_3_test) \n",
      "\n",
      "print(results3a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions3a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "y=data4[jupyter_string]\n",
      "X=data4[jupyter_string]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "=====\n",
      "data4[jupyter_string]=np.log(data4['mean' madeupword0184]).round(decimals=3)\n",
      "y=data4[jupyter_string]\n",
      "X=data4[[jupyter_string,jupyter_string]]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "--------------------\n",
      "model3b = sm.GLM(Y_1_train, X_1_train)\n",
      "results3b = model3b.fit()\n",
      "\n",
      "\n",
      "predictions3b = results3b.predict(X_1_test) \n",
      "\n",
      "print(results3b.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions3b,jupyter_string)\n",
      "=====\n",
      "model4 = sm.GLM(Y_1_train,X_1_train)\n",
      "results4 = model4.fit()\n",
      "\n",
      "\n",
      "predictions4 = results4.predict(X_1_test)\n",
      "\n",
      "print(results4.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions4,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model5 = sm.GLM(Y_2_train,X_2_train)\n",
      "results5 = model5.fit()\n",
      "\n",
      "\n",
      "predictions5 = results5.predict(X_2_test)\n",
      "\n",
      "print(results5.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5,jupyter_string)\n",
      "=====\n",
      "model4a = sm.GLM(Y_2_train,X_1_train)\n",
      "results4a = model4a.fit()\n",
      "\n",
      "\n",
      "predictions4a = results4a.predict(X_1_test)\n",
      "\n",
      "print(results4a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model4b = sm.GLM(Y_2_train,X_2_train)\n",
      "results4b = model4b.fit()\n",
      "\n",
      "\n",
      "predictions4b = results4b.predict(X_2_test)\n",
      "\n",
      "print(results4b.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4b,jupyter_string)\n",
      "=====\n",
      "model5 = sm.GLM(Y_1_train,X_2_train)\n",
      "results5 = model5.fit()\n",
      "\n",
      "\n",
      "predictions5 = results5.predict(X_2_test)\n",
      "\n",
      "print(results5.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions5,jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "model6 = sm.GLM(Y_2_train,X_2_train)\n",
      "results6 = model6.fit()\n",
      "\n",
      "\n",
      "predictions6 = results6.predict(X_2_test)\n",
      "\n",
      "print(results6.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions6,jupyter_string)\n",
      "=====\n",
      "model5a = sm.GLM(Y_2_train,X_2_train)\n",
      "results5a = model5a.fit()\n",
      "\n",
      "\n",
      "predictions5a = results5a.predict(X_2_test)\n",
      "\n",
      "print(results5a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5a,jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "model5b = sm.GLM(Y_2_train,X_2_train)\n",
      "results5b = model5b.fit()\n",
      "\n",
      "\n",
      "predictions5b = results5b.predict(X_2_test)\n",
      "\n",
      "print(results5b.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5b,jupyter_string)\n",
      "=====\n",
      "model6a = sm.GLM(Y_1_train,X_3_train)\n",
      "results6a = model6a.fit()\n",
      "\n",
      "\n",
      "predictions6a = results6a.predict(X_3_test)\n",
      "\n",
      "print(results6a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions6a,jupyter_string)\n",
      "--------------------\n",
      "model7a = sm.GLM(Y_2_train,X_2_train)\n",
      "results7a = model7a.fit()\n",
      "\n",
      "\n",
      "predictions7a = results7a.predict(X_2_test)\n",
      "\n",
      "print(results7a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions7a,jupyter_string)\n",
      "=====\n",
      "model7a = sm.GLM(Y_2_train,X_3_train)\n",
      "results7a = model7a.fit()\n",
      "\n",
      "\n",
      "predictions7a = results7a.predict(X_3_test)\n",
      "\n",
      "print(results7a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions7a,jupyter_string)\n",
      "--------------------\n",
      "regr_1 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_2 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "regr_1.fit(X_1_train, Y_3_train)\n",
      "regr_2.fit(X_1_train, Y_3_train)\n",
      "regr_3.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data1 = pd.DataFrame(regr_1.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data1[jupyter_string] = pd.Series(regr_2.feature_importances_.T)\n",
      "data1[jupyter_string] = p\n",
      "=====\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "--------------------\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "=====\n",
      "regr_1a = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_2a = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3a = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "new_index = data1[data1[jupyter_string]>0.01].drop(jupyter_string, axis = 0).index\n",
      "X_1a_train = X_1_train[new_index]\n",
      "X_1a_test = X_1_test[new_index]\n",
      "X1a = X1[new_index]\n",
      "\n",
      "\n",
      "regr_1a.fit(X_1a_train, Y_3_train)\n",
      "regr_2a.fit(X_1a_train, Y_3_train)\n",
      "regr_3a.fit(X_1a_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data1a = pd.DataFrame(regr_1a.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data1a[jupyter_string] = pd.Series(regr_2a.feature_importances_.T)\n",
      "data1a[jupyter_string] = pd.Series(regr_3a.feature_importances_.T)\n",
      "data1a.index = X_1a_train.columns\n",
      "test1a = np.array([regr_1a.max_depth, regr_2a.max_depth, regr_3a.max_depth]).reshape(1,3)\n",
      "model_infoa = pd.DataFrame(test1a, columns = data1a.columns, index = [jupyter_string])\n",
      "data1a = data1a.append(model_infoa)\n",
      "print(data1a)\n",
      "\n",
      "\n",
      "\n",
      "y_1a = regr_1a.predict(X_1a_test)\n",
      "y_2a = regr_2a.predict(X_1a_test)\n",
      "y_3a = regr_3a.predict(X_1a_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_1a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_1a[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_2a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_2a[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_3a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_3a[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_1a.score(X_1a_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_2a.score(X_1a_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_3a.score(X_1a_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X1a, Y3, cv=10))\n",
      "--------------------\n",
      "regr_1b = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_2b = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3b = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "new_index = data2[data2[jupyter_string]>0.01].drop(jupyter_string, axis = 0).index\n",
      "X_1b_train = X_1_train[new_index]\n",
      "X_1b_test = X_1_test[new_index]\n",
      "X2 = X2[new_index]\n",
      "\n",
      "\n",
      "regr_1b.fit(X_1b_train, Y_3_train)\n",
      "regr_2b.fit(X_1b_train, Y_3_train)\n",
      "=====\n",
      "regr_4 = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_5 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_6 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "\n",
      "regr_4.fit(X_2_train, Y_3_train)\n",
      "regr_5.fit(X_2_train, Y_3_train)\n",
      "regr_6.fit(X_2_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data2 = pd.DataFrame(regr_4.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data2[jupyter_string] = pd.Series(regr_5.feature_importances_.T)\n",
      "data2[jupyter_string] = pd.Series(regr_6.feature_importances_.T)\n",
      "data2.index = X_2_train.columns\n",
      "test2 = np.array([regr_4.max_depth, regr_5.max_depth, regr_6.max_depth]).reshape(1,3)\n",
      "model_info2 = pd.DataFrame(test2, columns = data2.columns, index = [jupyter_string])\n",
      "data2 = data2.append(model_info2)\n",
      "print(data2)\n",
      "\n",
      "\n",
      "\n",
      "y_4 = regr_4.predict(X_2_test)\n",
      "y_5 = regr_5.predict(X_2_test)\n",
      "y_6 = regr_6.predict(X_2_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_4[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_4[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_5[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_5[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_6[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_6[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_4.score(X_2_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_5.score(X_2_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_6.score(X_2_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X2, Y3, cv=10))\n",
      "--------------------\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1a_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1a_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "=====\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_2_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_2_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import LSTM\n",
      "from keras.layers import Dropout\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import Convolution1D\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "=====\n",
      "scaler = StandardScaler()  \n",
      "scaler.fit(X_1_train)\n",
      "X_1_traina = scaler.transform(X_1_train)  \n",
      "X_1_testa = scaler.transform(X_1_test)\n",
      "\n",
      "clf = MLPRegressor(solver=jupyter_string, alpha=1e-5,  random_state=1)\n",
      "clf = clf.fit(X_1_traina, Y_3_train)\n",
      "\n",
      "pred_nn_1 = clf.predict(X_1_testa)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_1[:,1], jupyter_string)\n",
      "clf.loss_\n",
      "clf.hidden_layer_sizes\n",
      "\n",
      "print(jupyter_string, r2_score(Y_3_test, pred_nn_1))\n",
      "print(jupyter_string, r2_score(Y_3_test[jupyter_string], pred_nn_1[:,0]))\n",
      "print(jupyter_string, r2_score(Y_3_test[jupyter_string], pred_nn_1[:,1]))\n",
      "--------------------\n",
      "scaler = StandardScaler()  \n",
      "scaler.fit(X_2_train)\n",
      "X_2_traina = scaler.transform(X_2_train)  \n",
      "X_2_testa = scaler.transform(X_2_test)\n",
      "\n",
      "clf = MLPRegressor(solver=jupyter_string, alpha=1e-5,  random_state=1)\n",
      "clf = clf.fit(X_2_traina, Y_2_train)\n",
      "\n",
      "pred_nn_2 = clf.predict(X_2_testa)\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], pred_nn_2[:,0], jupyter_string)\n",
      "plotSummary(Y_2_test[jupyter_string], pred_nn_2[:,1], jupyter_string)\n",
      "=====\n",
      "layer1 = 20\n",
      "layer2 = 20\n",
      "layer3 = 10\n",
      "clf2 = MLPRegressor(hidden_layer_sizes=(layer1,layer2, layer3), max_iter=10, alpha=1e-4, solver=jupyter_string, verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
      "\n",
      "\n",
      "clf2 = clf2.fit(X_1_traina, Y_3_train)\n",
      "print(jupyter_string % clf2.score(X_1_traina, Y_3_train))\n",
      "print(jupyter_string % clf2.score(X_1_testa, Y_3_test))\n",
      "\n",
      "print(jupyter_string, clf2.loss_)\n",
      "print(jupyter_string, clf2.n_iter_)\n",
      "print(jupyter_string, clf2.n_layers_)\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "summary_nn1 = pd.DataFrame(clf2.coefs_[0], index = X_1_train.columns, columns = [(jupyter_string+ str(i)) for i in range(1,layer1+1)])\n",
      "summary_nn1a = pd.DataFrame(clf2.coefs_[1], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[1].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[1].shape[1]+1)])\n",
      "summary_nn1b = pd.DataFrame(clf2.coefs_[2], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[2].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[2].shape[1]+1)])\n",
      "summary_nn1c = pd.DataFrame(clf2.coefs_[3], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[3].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[3].shape[1]+1)])\n",
      "\n",
      "pred_nn_2 = clf2.predict(X_1_testa)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_2[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_2[:,1], jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(summary_nn1)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1a)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1b)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1c)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "countries = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "countries.head()\n",
      "=====\n",
      "countries.head(3)\n",
      "--------------------\n",
      "countries.tail(3)\n",
      "=====\n",
      "countries.tail(3)\n",
      "--------------------\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "ridge = Ridge(alpha=0.1)\n",
      "ridge.fit(X_train, y_train)\n",
      "y_pred = ridge.predict(X_test)\n",
      "print(jupyter_string, ridge.score(X_test, y_test))\n",
      "print(jupyter_string, mean_squared_error(y_test, y_pred))\n",
      "print(jupyter_string, r2_score(y_test, y_pred))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=1) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=1)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "--------------------\n",
      "countries.info()\n",
      "=====\n",
      "countries.describe()\n",
      "--------------------\n",
      "countries.tail(3)\n",
      "=====\n",
      "countries.tail(3)\n",
      "--------------------\n",
      "countries.info()\n",
      "=====\n",
      "countries['continent' madeupword0002].value_counts()\n",
      "--------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries.describe()\n",
      "--------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries['fertility' <<unk>>].cumsum()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "\n",
      "train_df = pd.read_csv(jupyter_string)\n",
      "test_df = pd.read_csv(jupyter_string)\n",
      "submission = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_df.landmark_id.value_counts().head(100)\n",
      "=====\n",
      "rank_number = 100 \n",
      "sampling_rate = 0.02 \n",
      "random_state = 17 \n",
      "\n",
      "landmarks=train_df.groupby(by='landmark_id' <<unk>>).count().loc[:,'id' <<unk>>]\n",
      "l = landmarks.sort_values(ascending=False)\n",
      "\n",
      "\n",
      "lmks = pd.concat([l, l/l.sum(), l.cumsum()/l.sum()], axis=1, ignore_index=True)\n",
      "lmks.columns=[jupyter_string, jupyter_string, jupyter_string]\n",
      "ranked = lmks[0:rank_number]\n",
      "\n",
      "train_ordered = train_df[train_df.landmark_id.isin(ranked.index)]\n",
      "sample_gby = train_ordered.groupby(by='landmark_id' <<unk>>).apply(lambda x: x.sample(frac=sampling_rate, random_state=random_state))\n",
      "sample_idx = sample_gby.index.levels[1]\n",
      "train_sample = train_df.iloc[sample_idx, :]\n",
      "\n",
      "\n",
      "train_sample.to_csv(jupyter_string, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
      "train_sample_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=1) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=1)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS\n",
      "=====\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "train_df = pd.read_csv(jupyter_string)\n",
      "train_df.head()\n",
      "=====\n",
      "train_sample.head()\n",
      "--------------------\n",
      "plt.figure(figsize = (14, 6))\n",
      "g = sns.countplot(x=\"landmark_id\", data=train_sample)\n",
      "g.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "=====\n",
      "train_sample.nunique()\n",
      "--------------------\n",
      "plt.figure(figsize = (12, 5))\n",
      "fig = plt.bar(range(100), landmark_dist)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "img = cv2.imread(jupyter_string)\n",
      "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
      "plt.imshow(cv_rgb)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
      "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
      "=====\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "\n",
      "X = train_sample_df['id' <<unk>>]\n",
      "y = train_sample_df['landmark_id' <<unk>>]\n",
      "\n",
      "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=10)\n",
      "\n",
      "for train_id, test_id in sss.split(X, y):\n",
      "    X_train, X_tmp = X.iloc[train_id], X.iloc[test_id]\n",
      "    y_train, y_tmp = y.iloc[train_id], y.iloc[test_id]\n",
      "\n",
      "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=10)\n",
      "\n",
      "for train_id, test_id in sss1.split(X_tmp, y_tmp):\n",
      "    X_valid, X_test = X_tmp.iloc[train_id], X_tmp.iloc[test_id]\n",
      "    y_valid, y_test = y_tmp.iloc[train_id], y_tmp.iloc[test_id]\n",
      "--------------------\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_test)\n",
      "h.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_test)\n",
      "h.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import random\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "prob_id = np.array([(y_test == id).sum() / len(y_test) for id in y_test]) \n",
      "seed = [3, 10, 27, 31, 48, 55, 67, 95, 105, 117]\n",
      "\n",
      "expected_val_df = pd.DataFrame(columns=[jupyter_string, jupyter_string])\n",
      "\n",
      "for i in seed:\n",
      "    \n",
      "    random.seed(i)\n",
      "    randsample = random.sample(range(len(y_test)), 20)\n",
      "    \n",
      "    prob_id_montecarlo = prob_id[randsample]  \n",
      "    expected_val_df.loc[len(expected_val_df)] = expected_acc(prob_id_montecarlo)\n",
      "--------------------\n",
      "plt.imshow(train_tensors[0])\n",
      "=====\n",
      "plt.imshow(train_tensors[2])\n",
      "--------------------\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_r_optimal)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var\n",
      "=====\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_l_optimal=Regularization_fit_lambda(2,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "model = Sequential()\n",
      "model.add(Conv2D(32, (3, 3), activation=jupyter_string, input_shape=(32, 32, 3)))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(64, (3, 3), activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(128, (3, 3), activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten())\n",
      "=====\n",
      "input_shape = img_shape + (3,)\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(filters=16, kernel_size=4, padding=jupyter_string, activation=jupyter_string, input_shape=input_shape))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=32, kernel_size=3, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=64, kernel_size=3, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=128, kernel_size=2, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=256, kernel_size=2, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Flatten())\n",
      "model.add(Dense(1024, activation=jupyter_string))\n",
      "model.add(Dropout(0.3))\n",
      "model.add(Dense(512, activation=jupyter_string))\n",
      "model.add(Dropout(0.3))\n",
      "model.add(Dense(100, activation=jupyter_string))\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(optimizer=jupyter_string, loss=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "history = model.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=len(train_generator),\n",
      "    epochs=20,\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=len(validation_generator),\n",
      "    verbose=1\n",
      ")\n",
      "=====\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "epochs = 10\n",
      "\n",
      "checkpointer = ModelCheckpoint(filepath=jupyter_string, \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "hist = model.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "--------------------\n",
      "plt.plot(hist.history[jupyter_string])\n",
      "plt.plot(hist.history[jupyter_string])\n",
      "plt.legend([jupyter_string, jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "\n",
      "train_datagen = ImageDataGenerator(\n",
      "    rotation_range=40,\n",
      "    width_shift_range=0.2,\n",
      "    height_shift_range=0.2,\n",
      "    shear_range=0.2,\n",
      "    zoom_range=0.2,\n",
      "    horizontal_flip=True,\n",
      "    fill_mode=jupyter_string\n",
      ")\n",
      "\n",
      "train_generator = train_datagen.flow(train_tensors, train_target, batch_size=32)\n",
      "=====\n",
      "train_datagen = ImageDataGenerator(\n",
      "    rotation_range=45, \n",
      "    width_shift_range=0.2, \n",
      "    height_shift_range=0.2, \n",
      "    zoom_range=0.3)\n",
      "\n",
      "valid_datagen = ImageDataGenerator()\n",
      "\n",
      "train_generator = train_datagen.flow(train_tensors, train_target, batch_size= 128)\n",
      "valid_generator = valid_datagen.flow(valid_tensors, valid_target, batch_size= 128)\n",
      "--------------------\n",
      "plt.plot(hist_aug.history[jupyter_string])\n",
      "plt.plot(hist_aug.history[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.legend([jupyter_string, jupyter_string], loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "landmark_prob = [np.amax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "landmark_prob = [np.amax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "=====\n",
      "sort_id = np.argsort(landmark_prob)[::-1]\n",
      "landmark_pred_sorted = [landmark_pred[sid] for sid in sort_id]\n",
      "test_target_sorted = [test_target[sid] for sid in sort_id]\n",
      "--------------------\n",
      "modelBN.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "adam = optimizers.Adam(lr=0.02)\n",
      "modelBN.compile(optimizer=jupyter_string, loss=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_l_optimal=Regularization_fit_lambda(3,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_l_optimal))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_l_optimal)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "--------------------\n",
      "historyBN = modelBN.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=nb_train_samples // batch_size,\n",
      "    epochs=nb_epochs,\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=nb_validation_samples // batch_size\n",
      ")\n",
      "=====\n",
      "checkpointer = ModelCheckpoint(filepath=jupyter_string, \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "epochs_batch = 15\n",
      "\n",
      "hist_BN = modelBN.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs_batch, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "--------------------\n",
      "plt.plot(hist_BN.history[jupyter_string])\n",
      "plt.plot(hist_BN.history[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.legend([jupyter_string, jupyter_string], loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "modelBN.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(modelBN.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "plt.plot(hist_BaseCNN[jupyter_string], hist_BaseCNN[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "hist_BaseCNN_df = pd.DataFrame(hist_BaseCNN)\n",
      "hist_BaseCNN_df.columns = [jupyter_string, jupyter_string, jupyter_string,jupyter_string]\n",
      "hist_BaseCNN_df\n",
      "--------------------\n",
      "plt.plot(hist_BaseCNN_df[jupyter_string], hist_BaseCNN_df[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax1 = hist_BaseCNN_df.plot(marker=jupyter_string)\n",
      "ax1.set_xlabel(jupyter_string)\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "--------------------\n",
      "titanic = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "titanic = pd.read_csv(jupyter_string)      \n",
      "char_cabin = titanic[\"Cabin\"].astype(str)    \n",
      "new_Cabin = np.array([cabin[0] for cabin in char_cabin]) \n",
      "titanic[\"Cabin\"] = pd.Categorical(new_Cabin)  \n",
      "--------------------\n",
      "pd.crosstab(titanic[\"Survived\"], titanic[\"Pclass\"])\n",
      "=====\n",
      "my_tab = pd.crosstab(index=titanic[\"Survived\"],     \n",
      "                              columns=jupyter_string)      \n",
      "\n",
      "my_tab\n",
      "--------------------\n",
      "my_tab = pd.crosstab(index=titanic[\"Survived\"],     \n",
      "                              columns=jupyter_string)      \n",
      "\n",
      "my_tab\n",
      "=====\n",
      "pd.crosstab(index=titanic[\"Pclass\"],        \n",
      "                      columns=jupyter_string)      \n",
      "--------------------\n",
      "pd.crosstab(index=titanic[\"Sex\"],        \n",
      "                      columns=jupyter_string)      \n",
      "=====\n",
      "pd.crosstab(index=titanic[\"Sex\"],        \n",
      "                      columns=jupyter_string)      \n",
      "--------------------\n",
      "pd.crosstab(index=titanic[\"Embarked\"],        \n",
      "                      columns=jupyter_string)      \n",
      "=====\n",
      "cabin_tab = pd.crosstab(index=titanic[\"Cabin\"],        \n",
      "                        columns=jupyter_string)               \n",
      "\n",
      "cabin_tab \n",
      "--------------------\n",
      "pd.crosstab(titanic_df.Pclass, titanic_df.Survived)\n",
      "=====\n",
      "survived_sex = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                           columns=titanic[\"Sex\"])\n",
      "\n",
      "survived_sex.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_sex\n",
      "--------------------\n",
      "plt.plot(lambdas,R_2_IS_Ridge,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_IS_Lasso,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_OS_Ridge,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_OS_Lasso,label=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "Number_variables=[]\n",
      "OLS_R_2_IS=[] \n",
      "Ridge_R_2_IS=[]\n",
      "Lasso_R_2_IS=[]\n",
      "OLS_R_2_OS=[] \n",
      "Ridge_R_2_OS=[]\n",
      "Lasso_R_2_OS=[]\n",
      "depend_variable=[jupyter_string,jupyter_string]\n",
      "t=0\n",
      "\n",
      "\n",
      "for j in ([list_comp[i] for i in range(var)]):\n",
      "\n",
      "    t+=1\n",
      "    Number_variables.append(t)\n",
      "\n",
      "    depend_variable.append(j)\n",
      "\n",
      "\n",
      "    X=data_train[depend_variable]\n",
      "    y=data_train[jupyter_string]\n",
      "    result=ols(y=y,x=X)\n",
      "    OLS_R_2_IS.append(result.r2)\n",
      "    \n",
      "    X=np.matrix(data_train[depend_variable])\n",
      "    y=np.array(data_train[jupyter_string])\n",
      "    Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal)\n",
      "    Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_l_optimal)\n",
      "    Ridge.fit(X,y)\n",
      "    Lasso.fit(X,y)\n",
      "    p_IS=Ridge.predict(X)\n",
      "    err_IS=p_IS-y\n",
      "    R_2_IS_Ridge=1-np.var(err_IS)/np.var(y)\n",
      "    p_IS=Lasso.predict(X)\n",
      "    err_IS=p_IS-y\n",
      "    R_2_IS_Lasso=1-np.var(err_IS)/np.var(y)\n",
      "    Ridge_R_2_IS.append(R_2_IS_Ridge)\n",
      "    Lasso_R_2_IS.append(R_2_IS_Lasso)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    temp=data_test[depend_variable]\n",
      "\n",
      "    a=np.array(temp)\n",
      "    b=np.array(result.beta)\n",
      "    c=np.sum(a*b[0:-1],axis=1)+b[-1]\n",
      "    \n",
      "    X_test=np.matrix(data_test[depend_variable])\n",
      "    y_test=np.array(data_test[jupyter_string])\n",
      "\n",
      "    error=data_test[jupyter_string]-c\n",
      "    R_2=1-error.var()/data_test[jupyter_string].var()\n",
      "    if R_2>0:\n",
      "        OLS_R_2_OS.append(R_2)\n",
      "    else:\n",
      "        OLS_R_2_OS.append(0)\n",
      "        \n",
      "    p_OS=Ridge.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "    p_OS=Lasso.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test) \n",
      "    Ridge_R_2_OS.append(R_2_OS_Ridge)\n",
      "    Lasso_R_2_OS.append(R_2_OS_Lasso)\n",
      "\n",
      "\n",
      "pylab.title(jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Ridge_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Ridge_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Lasso_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Lasso_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.xlabel(jupyter_string)\n",
      "pylab.ylabel(jupyter_string)\n",
      "pylab.draw()\n",
      "--------------------\n",
      "survived_sex.div(survived_sex.sum(1).astype(float), axis=0)\n",
      "=====\n",
      "survived_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                            columns=titanic[\"Pclass\"])\n",
      "\n",
      "survived_class.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "survived_class.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_class\n",
      "--------------------\n",
      "survived_sex = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                           columns=titanic[\"Sex\"], margins=True)\n",
      "\n",
      "survived_sex.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_sex\n",
      "=====\n",
      "survived_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                            columns=titanic[\"Pclass\"],\n",
      "                             margins=True)   \n",
      "\n",
      "survived_class.columns = [jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "survived_class.index= [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "survived_class\n",
      "--------------------\n",
      "survived_class.div(survived_class.ix[jupyter_string], axis=0)\n",
      "=====\n",
      "survived_class.div(survived_class[jupyter_string],\n",
      "                   axis=jupyter_string)\n",
      "--------------------\n",
      "survival = pd.crosstab(index=titanic_df.survived, columns=titanic_df.sex)\n",
      "survival\n",
      "=====\n",
      "surv_sex_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                             columns=[titanic[\"Pclass\"],\n",
      "                                      titanic[\"Sex\"]],\n",
      "                             margins=True)   \n",
      "\n",
      "surv_sex_class\n",
      "--------------------\n",
      "X_test, y_test = problem.get_test_data()\n",
      "=====\n",
      "train_is, test_is = list(problem.get_cv(X_train, y_train))[0]\n",
      "print(len(train_is), len(test_is))\n",
      "--------------------\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "\n",
      "=====\n",
      "years = np.arange(850, 2005)\n",
      "def plot_grid(ens, year, var):\n",
      "    ti = np.where(year == years)[0][0]\n",
      "    fig = plt.figure(figsize=(10, 5))\n",
      "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
      "    ax.coastlines()\n",
      "    min_val = train_X[var].min()\n",
      "    max_val = train_X[var].max()\n",
      "    cont = ax.contourf(train_X[jupyter_string] - 180, train_X[jupyter_string], \n",
      "                       train_X[var].sel(ens=ens, time=train_X[jupyter_string].values[ti]),\n",
      "                       np.linspace(min_val, max_val, 20))\n",
      "    ax.set_title(var + jupyter_string + jupyter_string.format(year, ens))\n",
      "    plt.colorbar(cont)\n",
      "interact(plot_grid, ens=[0, 1, 2, 3], year=SelectionSlider(options=years.tolist()), \n",
      "         var=data_vars)\n",
      "--------------------\n",
      "from IPython.core.interactiveshell import InteractiveShell\n",
      "InteractiveShell.ast_node_interactivity = jupyter_string\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(8, 5))\n",
      "participant_list = np.array([8, 13, 15, 15])\n",
      "categories = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "plt.bar(np.arange(4), participant_list / 21)\n",
      "plt.xticks(np.arange(4), categories, fontsize=12)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string, fontsize=14)\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string, dpi=200)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 5))\n",
      "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
      "ax.coastlines()\n",
      "min_val = -5\n",
      "max_val = 5\n",
      "cont = ax.contourf(train_X_anomalies[jupyter_string] - 180, train_X_anomalies[jupyter_string], \n",
      "               train_X_anomalies[jupyter_string], cmap=jupyter_string)\n",
      "ax.set_title(jupyter_string)\n",
      "plt.colorbar(cont)\n",
      "interact(plot_anomaly, ens=[0, 1, 2, 3], year=SelectionSlider(options=years.tolist()), \n",
      "         var=data_vars)\n",
      "=====\n",
      "rain_data = pd.read_csv(jupyter_string, index_col=\"Year\")\n",
      "rain_data.rolling(25).mean().plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "rain_data = pd.read_csv(jupyter_string, index_col=\"Year\")\n",
      "rain_data.rolling(25).mean().plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "rain_data.hist(bins=np.arange(0, 1600, 100), figsize=(10, 5))\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings\n",
      "warnings.simplefilter(action = jupyter_string, category = FutureWarning)\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn.metrics import mean_absolute_error as mae\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams, style\n",
      "style.use(jupyter_string)\n",
      "--------------------\n",
      "rain_data.plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "lags = np.arange(1, 20)\n",
      "autocorr = np.zeros((rain_data.columns.size, lags.size))\n",
      "plt.figure(figsize=(8, 5))\n",
      "for c, col in enumerate(rain_data.columns):\n",
      "    autocorr[c] = np.array([rain_data[col].autocorr(l) for l in range(1, 20)])\n",
      "    plt.plot(lags, np.abs(autocorr[c]), label=col)\n",
      "plt.xticks(lags)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.legend(loc=0)\n",
      "--------------------\n",
      "clf = Classifier()\n",
      "clf.fit(train_X.sel(lat=train_X[jupyter_string] > -30), train_y)\n",
      "=====\n",
      "fe = FeatureExtractor()\n",
      "fe.fit(train_X, train_y)\n",
      "X = fe.transform(train_X)\n",
      "cls = Classifier()\n",
      "cls.fit(X, train_y)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "X_transformed = pca.transform(X)\n",
      "\n",
      "plt.scatter(X_transformed[:,0], X_transformed[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "coefs = cls.clf.coef_[0]\n",
      "coef_rankings = np.argsort(np.abs(coefs))[::-1]\n",
      "fig, axes = plt.subplots(3, 3, figsize=(16, 9), \n",
      "                         subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=180)))\n",
      "axef = axes.ravel()\n",
      "for c, coef_rank in enumerate(coef_rankings[:9]):\n",
      "    c_var = data_vars[int(np.floor(coef_rank / fe.num_comps))]\n",
      "    c_comp = coef_rank % fe.num_comps\n",
      "    comp_vals = fe.pca[c_var].components_[c_comp]\n",
      "    axef[c].coastlines()\n",
      "    axef[c].contourf(train_X[jupyter_string] - 180, \n",
      "                     train_X[jupyter_string], \n",
      "                     fe.pca[c_var].components_[c_comp].reshape(train_X[c_var].shape[1:]),\n",
      "                     np.linspace(-0.04, 0.04, 11), cmap=jupyter_string)\n",
      "    axef[c].set_title(jupyter_string.format(c_var, c_comp, coefs[coef_rank]))\n",
      "--------------------\n",
      "ratings = pd.read_csv(jupyter_string, sep=jupyter_string, names=column_names)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string,sep = jupyter_string,names=column_names)\n",
      "--------------------\n",
      "df.groupby(jupyter_string)[jupyter_string].count().sort_values(ascending=False).head()\n",
      "=====\n",
      "sb.set_style(jupyter_string)\n",
      "--------------------\n",
      "plt.hist(ratings[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "=====\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "--------------------\n",
      "gms_all = pd.read_csv(jupyter_string)\n",
      "gms_10 = pd.read_csv(jupyter_string)\n",
      "gms_exc_14 = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "gms_all = pd.read_csv(jupyter_string)\n",
      "\n",
      "gms_10 = gms_all.loc[gms_all['key' <<unk>>]==jupyter_string]\n",
      "gms_14 = gms_all.loc[gms_all['key' <<unk>>]==jupyter_string]\n",
      "gms_exc_14 = gms_all[gms_all['key' <<unk>>]!=jupyter_string]\n",
      "display(gms_10.describe())\n",
      "display(gms_14.describe())\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "=====\n",
      "sb.jointplot(x=jupyter_string,y=jupyter_string,data=ratings,alpha=0.5)\n",
      "--------------------\n",
      "ratings.sort_values(jupyter_string,ascending=False).head(10)\n",
      "=====\n",
      "ratings.sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "starwars_user_ratings = ratings[ratings.title == jupyter_string]\n",
      "animation_user_ratings = ratings[ratings.title == jupyter_string]\n",
      "=====\n",
      "starwars_user_rating = filmmat[jupyter_string]\n",
      "toystory_user_rating = filmmat[jupyter_string]\n",
      "\n",
      "starwars_user_rating.head()\n",
      "--------------------\n",
      "similar_to_starwars = filmmat.corrwith(starwars_user_rating)\n",
      "similar_to_animation = filmmat.corrwith(animation_user_rating)\n",
      "=====\n",
      "similar_to_starwars = filmmat.corrwith(starwars_user_rating)\n",
      "similar_to_toystory = filmmat.corrwith(toystory_user_rating)\n",
      "--------------------\n",
      "corr_starwars = pd.DataFrame(similar_to_starwars,columns=[jupyter_string])\n",
      "corr_starwars.dropna(inplace=True)\n",
      "=====\n",
      "corr_starwars = pd.DataFrame(similar_to_starwars,columns=[jupyter_string])\n",
      "corr_starwars.dropna(inplace=True)\n",
      "corr_starwars.head()\n",
      "--------------------\n",
      "corr_starwars.sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "corr_starwars.sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "corr_starwars = corr_starwars.join(ratings[jupyter_string])\n",
      "corr_starwars.head()\n",
      "=====\n",
      "corr_starwars =  corr_starwars.join(ratings[jupyter_string])\n",
      "corr_starwars.head()\n",
      "--------------------\n",
      "corr_starwars[corr_starwars[jupyter_string]>100].sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "corr_starwars[corr_starwars[jupyter_string] > 100].sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "corr_toystory = pd.DataFrame(similar_to_toystory,columns=[jupyter_string])\n",
      "corr_toystory.dropna(inplace=True)\n",
      "=====\n",
      "corr_toystory = pd.DataFrame(similar_to_toystory,columns=[jupyter_string])\n",
      "corr_toystory.dropna(inplace=True)\n",
      "corr_toystory = corr_toystory.join(ratings[jupyter_string])\n",
      "corr_toystory[corr_toystory[jupyter_string] > 100].sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "gms_10.head()\n",
      "=====\n",
      "rcParams[jupyter_string] = 8,8\n",
      "gls = pd.concat([gms_all.rename(columns={'score1' <<unk>>:jupyter_string}),\n",
      "                gms_all.rename(columns={'score2' <<unk>>:jupyter_string,'team1' <<unk>>:'team2' <<unk>>,'team2' <<unk>>:'team1' <<unk>>})])\n",
      "gls_groups = gls.groupby('key' <<unk>>)[jupyter_string]\n",
      "gls_means = gls_groups.mean().rename(jupyter_string)\n",
      "gls_stds = gls_groups.std().rename(jupyter_string)\n",
      "std_mean_df = pd.concat([gls_means, gls_stds],axis=1)\n",
      "plt.xlim(0, 2.8)\n",
      "plt.ylim(0, 2.8)\n",
      "g=sns.regplot(x=jupyter_string,y=jupyter_string,data=std_mean_df).set_title(''jupyter_string'')\n",
      "sns.despine()\n",
      "plt.tick_params(bottom=False,left=False)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "g.axes.set_title(r''jupyter_string'',ha=jupyter_string,position=(0,1),size=13)\n",
      "plt.show()\n",
      "--------------------\n",
      "milk.head()\n",
      "=====\n",
      "milk.index = pd.to_datetime(milk.index, format=jupyter_string, errors=jupyter_string)\n",
      "milk.head()\n",
      "--------------------\n",
      "milk.plot()\n",
      "=====\n",
      "plt.figure()\n",
      "plt.plot(milk)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "=====\n",
      "train_set = milk.head(156)\n",
      "test_set = milk.tail(12)\n",
      "--------------------\n",
      "test_set.plot(x=jupyter_string,y=jupyter_string,kind=jupyter_string)\n",
      "=====\n",
      "test_set.plot()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dji_2011 = pd.read_csv(jupyter_string)\n",
      "print(dji_2011.dtypes)\n",
      "dji_2011.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "jnj_2011 = dji_2011.loc[lambda df: dji_2011['stock' <<unk>>] == jupyter_string]\n",
      "print(jnj_2011.dtypes)\n",
      "jnj_2011.head()\n",
      "--------------------\n",
      "jnj_2011_df = pd.DataFrame(jnj_2011_dict)\n",
      "jnj_2011_df.head()\n",
      "=====\n",
      "plt.figure(figsize=(12, 14))\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['open' <<unk>>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['high' madeupword0002], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['low' <<unk>>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['close' <<unk>>], label=jupyter_string)\n",
      "plt.ylim([57, 68])\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "plt.yticks(np.arange(57, 69, 1))\n",
      "plt.legend()\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011['volume' <<unk>>])\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.show()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sb\n",
      "from matplotlib import pyplot as plt\n",
      "from collections import Counter\n",
      "from __future__ import division\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.info()\n",
      "--------------------\n",
      "train.describe()\n",
      "=====\n",
      "train.describe()\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,3))\n",
      "--------------------\n",
      "plt.figure(figsize=(16,6))\n",
      "plt.subplot(1,2,1)\n",
      "sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\n",
      "plt.subplot(1,2,2)\n",
      "sns.barplot(x=\"Pclass\",y=\"Survived\",data=train)\n",
      "=====\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "--------------------\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train[jupyter_string] = pd.cut(train[\"Age\"],bins=range(0,90,10),precision=1)\n",
      "train.groupby([jupyter_string,\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "train = train.drop(jupyter_string,axis=1)\n",
      "--------------------\n",
      "train[\"Name\"] = train[\"Name\"].apply(lambda x: x.split(jupyter_string)[0])\n",
      "train[\"Ticket\"] = train[\"Ticket\"].apply(lambda x: x.split(jupyter_string)[1])\n",
      "=====\n",
      "ctrain = train.drop([\"Name\",\"Ticket\"],axis=1).replace({\"Sex\": {jupyter_string: 1, jupyter_string:0}})\n",
      "ctrain[\"Cabin\"]=ctrain[\"Cabin\"].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
      "def embarked_letters_to_integers(letter):\n",
      "    if letter == jupyter_string:\n",
      "        return 0\n",
      "    elif letter == jupyter_string:\n",
      "        return 1\n",
      "    elif letter == jupyter_string:\n",
      "        return 2\n",
      "ctrain[\"Embarked\"] = ctrain[\"Embarked\"].apply(embarked_letters_to_integers)\n",
      "--------------------\n",
      "from scipy.stats import poisson\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "=====\n",
      "pd.set_option(jupyter_string, None)\n",
      "def pred_team_gls(model, team1, team2, gls_df, have_teams=None):\n",
      "    if have_teams is None: \n",
      "        have_teams = set(gls_df['team1' <<unk>>])|set(gls_df['team2' <<unk>>])\n",
      "    if team1 in have_teams: \n",
      "        if team2 in have_teams:\n",
      "            pred = model.predict(pd.DataFrame(data={'team1' <<unk>>: team1, 'team2' <<unk>>: team2},index=[0])).values[0]\n",
      "        else:\n",
      "            team1_gls = pd.concat([gls_df.loc[gls_df['team1' <<unk>>]==team1,'score1' <<unk>>],\n",
      "                                gls_df.loc[gls_df['team2' <<unk>>]==team1,'score2' <<unk>>]])\n",
      "            pred = team1_gls.mean()\n",
      "    else:\n",
      "        if team2 in have_teams:\n",
      "            team2_gls = pd.concat([gls_df.loc[gls_df['team1' <<unk>>]==team2,'score1' <<unk>>],\n",
      "                                gls_df.loc[gls_df['team2' <<unk>>]==team2,'score2' <<unk>>]])\n",
      "            pred = team2_gls.mean()        \n",
      "        else:\n",
      "            pred = gls_df[jupyter_string].mean()\n",
      "    return np.round(pred,8)\n",
      "\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(poisson_model, r['team1' <<unk>>],r['team2' <<unk>>], gls_10), axis=1)\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(poisson_model, r['team2' <<unk>>],r['team1' <<unk>>], gls_10), axis=1)\n",
      "pred_14_df = gms_14[['score1' <<unk>>, 'score2' <<unk>>, 'team1' <<unk>>, 'team2' <<unk>>, jupyter_string, jupyter_string]]\n",
      "pred_14_df.head()\n",
      "--------------------\n",
      "ctrain = ctrain.drop([\"Name\",\"Ticket\"],axis=1).replace({\"Sex\": {jupyter_string: 1, jupyter_string:0}})\n",
      "def embarked_letters_to_integers(letter):\n",
      "    if letter == jupyter_string:\n",
      "        return 0\n",
      "    elif letter == jupyter_string:\n",
      "        return 1\n",
      "    elif letter == jupyter_string:\n",
      "        return 2\n",
      "    elif letter == jupyter_string:\n",
      "        return 3\n",
      "    elif letter == jupyter_string:\n",
      "        return 4\n",
      "ctrain[\"Embarked\"] = ctrain[\"Embarked\"].apply(embarked_letters_to_integers)\n",
      "=====\n",
      "train[jupyter_string] = train[\"Sex\"].map({jupyter_string:0,jupyter_string:1}).astype(int)\n",
      "train[jupyter_string] = train[\"Embarked\"].map({jupyter_string:0,jupyter_string:1,jupyter_string:2}).fillna(0).astype(int)\n",
      "train[jupyter_string] = train[\"Cabin\"].map(lambda x: 0 if pd.isnull(x) else 1)\n",
      "--------------------\n",
      "train[jupyter_string] = train.groupby([\"Sex\", \"Pclass\"])[\"Age\"].transform(jupyter_string)\n",
      "train[jupyter_string] = train.groupby([\"Pclass\"])[\"Age\"].transform(jupyter_string)\n",
      "=====\n",
      "mean_age_per_sex_class = train.groupby([\"Sex\",\"Pclass\"]).mean()[\"Age\"]\n",
      "mean_age_per_sex_class.unstack(\"Pclass\").plot.bar()\n",
      "--------------------\n",
      "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\n",
      "test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())\n",
      "=====\n",
      "train[jupyter_string] = train[\"Age\"]\n",
      "train[jupyter_string] = pd.isnull(train[\"Age\"]).astype(int)\n",
      "for sex in [jupyter_string,jupyter_string]:\n",
      "    for pclass in [1,2,3]:\n",
      "        train.loc[train.Age.isnull() & (train.Sex==sex) & (train.Pclass==pclass),jupyter_string] = mean_age_per_sex_class[sex][pclass]\n",
      "--------------------\n",
      "train.info()\n",
      "=====\n",
      "train.describe().round(2)\n",
      "--------------------\n",
      "test[jupyter_string] = test[\"SibSp\"] + test[\"Parch\"]\n",
      "test[[jupyter_string,\"SibSp\",\"Parch\"]].head(10)\n",
      "=====\n",
      "train[jupyter_string] = train[jupyter_string] * train[\"Pclass\"]\n",
      "train[[jupyter_string,jupyter_string,\"Pclass\"]].head(10)\n",
      "--------------------\n",
      "train = train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
      "test = test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
      "=====\n",
      "text_info_columns = train.dtypes[train.dtypes.map(lambda x: x==jupyter_string)].index.tolist()\n",
      "train = train.drop(text_info_columns,axis=1)\n",
      "text_info_columns\n",
      "--------------------\n",
      "train.info()\n",
      "=====\n",
      "train.info()\n",
      "--------------------\n",
      "train.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "=====\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,6));\n",
      "--------------------\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,6));\n",
      "=====\n",
      "train.Fare.hist(range=(0,100));plt.title(jupyter_string)\n",
      "--------------------\n",
      "model = MultiClassLogisticRegressorPurePython(lr=0.001, n_iter=100)\n",
      "model.fit(X_train, y_train)\n",
      "y_pred = model.predict(X_test)\n",
      "print(jupyter_string, accuracy_score(y_test, y_pred))\n",
      "=====\n",
      "model_3 = MultiClassLogisticRegressorPurePython(n_iter=1000, lr=0.1)\n",
      "model_3.fit(X_train, y_train)\n",
      "plt.plot(model_3.cost_)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from __future__ import print_function \n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "names = [jupyter_string]\n",
      "feature_names = map(str, range(784))\n",
      "names.extend(feature_names)\n",
      "\n",
      "df_mnist_train = pd.read_csv(jupyter_string, header=None, nrows=1000, names=names)\n",
      "df_mnist_test = pd.read_csv(jupyter_string, header=None, nrows=100, names=names)\n",
      "\n",
      "\n",
      "max_pixel =  255\n",
      "df_mnist_train.iloc[:, 1:] /= max_pixel\n",
      "df_mnist_test.iloc[:, 1:] /= max_pixel\n",
      "\n",
      "print(df_mnist_train.shape, df_mnist_test.shape)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "X_filled_mice = fancyimpute.MICE().complete(np.asarray(carmpg.ix[:, carmpg.columns != 'Auto' <<unk>>]))\n",
      "--------------------\n",
      "carmpg_filled = pd.DataFrame(X_filled_mice)\n",
      "carmpg_filled.columns = carmpg.columns\n",
      "carmpg_filled.head()\n",
      "=====\n",
      "X_filled_mice = pd.DataFrame(X_filled_mice)\n",
      "X_filled_mice.columns = ['MPG' <<unk>>,'CYLINDERS' madeupword0002,'SIZE' <<unk>>,'HP' <<unk>>,'WEIGHT' <<unk>>,'ACCEL' <<unk>>,'ENG_TYPE' <<unk>>]\n",
      "X_filled_mice.insert(0, 'Auto' <<unk>>, carmpg['Auto' <<unk>>])\n",
      "\n",
      "X_filled_mice\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "carmpg = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "carmpg.info()\n",
      "=====\n",
      "carmpg.info()\n",
      "--------------------\n",
      "df = df.dropna()\n",
      "df.head()\n",
      "=====\n",
      "carmpg_listwise = carmpg.dropna(axis=0)\n",
      "--------------------\n",
      "carmpg_listwise.info()\n",
      "=====\n",
      "carmpg_listwise.info()\n",
      "--------------------\n",
      "dataset = pd.read_csv(jupyter_string)\n",
      "X = dataset.iloc[:, :-1].values\n",
      "y = dataset.iloc[:, 4].values\n",
      "=====\n",
      "training_set = pd.read_csv(jupyter_string)\n",
      "training_set = training_set.iloc[:, 1:2].values\n",
      "training_set\n",
      "\n",
      "--------------------\n",
      "X_train = training_set[0:training_set.shape[0], 1:]\n",
      "y_train = training_set[0:training_set.shape[0], 0]\n",
      "=====\n",
      "X_train = training_set[0:1257]\n",
      "y_train = training_set[1:1258]\n",
      "--------------------\n",
      "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
      "=====\n",
      "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
      "X_train.shape\n",
      "--------------------\n",
      "regressor = Sequential()\n",
      "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
      "regressor.add(LSTM(units = 50, return_sequences = True))\n",
      "regressor.add(LSTM(units = 50))\n",
      "regressor.add(Dense(units = 1))\n",
      "=====\n",
      "regressor = Sequential()\n",
      "\n",
      "\n",
      "regressor.add(LSTM(units = 4, activation = jupyter_string, input_shape = (None, 1)))\n",
      "\n",
      "\n",
      "regressor.add(Dense(units = 1))\n",
      "\n",
      "\n",
      "regressor.compile(optimizer = jupyter_string, loss = jupyter_string)\n",
      "--------------------\n",
      "dataset_test = pd.read_csv(jupyter_string)\n",
      "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
      "=====\n",
      "test_set = pd.read_csv(jupyter_string)\n",
      "real_stock_price = test_set.iloc[:, 1:2].values\n",
      "real_stock_price\n",
      "--------------------\n",
      "predicted_stock_price = regressor.predict(X_test)\n",
      "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
      "predicted_stock_price\n",
      "=====\n",
      "inputs = real_stock_price\n",
      "inputs = sc.transform(inputs)\n",
      "inputs = np.reshape(inputs, (20, 1, 1))\n",
      "predicted_stock_price = regressor.predict(inputs)\n",
      "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
      "--------------------\n",
      "plt.plot(real_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.plot(predicted_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(real_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.plot(predicted_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "from pandas import *\n",
      "pd.options.mode.chained_assignment = None  \n",
      "\n",
      "population_f = pd.read_csv(jupyter_string, index_col=0)\n",
      "population_m = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "tests_f_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_f_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_m_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_m_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "diagnoses_f_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_f_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_m_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_m_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "--------------------\n",
      "neg_binomial = NegativeBinomialNB()\n",
      "neg_binomial.fit(X_train, y_train)\n",
      "evaluate_predictions(neg_binomial)\n",
      "=====\n",
      "regr = smf.glm(formula=jupyter_string, data=gls_exc_14, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team1' <<unk>>],r['team2' <<unk>>], gls_exc_14), axis=1)\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team2' <<unk>>],r['team1' <<unk>>], gls_exc_14), axis=1)\n",
      "evaluate_predictions(gms_14)\n",
      "--------------------\n",
      "tests_f_max = tests_f_max.dropna()\n",
      "tests_f_min = tests_f_min.dropna()\n",
      "tests_m_max = tests_m_max.dropna()\n",
      "tests_m_min = tests_m_min.dropna()\n",
      "=====\n",
      "import matplotlib\n",
      "matplotlib.use(jupyter_string)\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid.anchored_artists import AnchoredText\n",
      "from matplotlib.patches import Polygon\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "tax = pd.read_csv(jupyter_string,encoding=jupyter_string,usecols=[0,1,2,3,4,5,6])\n",
      "--------------------\n",
      "tax.head()\n",
      "=====\n",
      "tax.head()\n",
      "--------------------\n",
      "tax.info()\n",
      "=====\n",
      "tax.info()\n",
      "--------------------\n",
      "tax.head()\n",
      "=====\n",
      "tax.plot(kind=jupyter_string,x='Total Income' <<unk>>,y='Total' <<unk>>,c=jupyter_string,figsize=(15,10),colormap=jupyter_string,s=100,alpha=0.7)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from datetime import datetime as dt \n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "plt.show()\n",
      "\n",
      "sphist = pd.read_csv(jupyter_string)\n",
      "sphist.shape\n",
      "--------------------\n",
      "sphist.head()\n",
      "=====\n",
      "sphist.head(4)\n",
      "--------------------\n",
      "sphist.tail(4)\n",
      "=====\n",
      "sphist.Date = pd.to_datetime(sphist.Date)\n",
      "sphist = sphist.sort_values(by = 'Date' <<unk>>, ascending=False)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "rf = RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "=====\n",
      "fitted_logreg = LogisticRegressionCV().fit(predictions_tune, y_tune)\n",
      "print(jupyter_string.format(fitted_logreg.coef_.shape[1]))\n",
      "\n",
      "\n",
      "y_hat = fitted_logreg.predict(predictions_test)\n",
      "\n",
      "print(jupyter_string, accuracy_score(y_test, y_hat))\n",
      "\n",
      "\n",
      "--------------------\n",
      "logreg = LogisticRegressionCV().fit(predictions_tune, y_tune)\n",
      "print(jupyter_string.format(logreg.coef_.shape[1]))\n",
      "\n",
      "\n",
      "y_hat = fitted_logreg.predict(predictions_test)\n",
      "\n",
      "print(jupyter_string, accuracy_score(y_test, y_hat))\n",
      "=====\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "for cur_depth in range(1,8):\n",
      "    model = DecisionTreeClassifier(max_depth = cur_depth)\n",
      "    scores = cross_val_score(model, predictions_tune, y_tune, cv=5)\n",
      "    print(jupyter_string.format(np.mean(scores), np.std(scores)))\n",
      "\n",
      "DecisionTreeClassifier(max_depth=4).fit(predictions_tune, y_tune).score(predictions_test, y_test)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import average_precision_score\n",
      "\n",
      "=====\n",
      "plt.show()\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_train = pd.read_csv(jupyter_string, index_col=0)\n",
      "df_test = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "data = pandas.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "data\n",
      "=====\n",
      "data = pandas.read_csv(jupyter_string)\n",
      "data.head()\n",
      "targets = pandas.DataFrame(data.TARGET.value_counts())\n",
      "targets[jupyter_string] = 100*targets['TARGET' madeupword0322]/data.shape[0]\n",
      "targets\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "x = np.arange(0, 2*np.pi, 0.1)\n",
      "y = np.sin(x) + 0.1*np.random.normal(size=x.shape[0])\n",
      "--------------------\n",
      "plt.plot(x, estgb.predict(x.reshape(-1,1)), label=jupyter_string)\n",
      "plt.plot(x, y, label=jupyter_string)\n",
      "plt.legend();\n",
      "=====\n",
      "display_iters = [0, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 400, 500]\n",
      "\n",
      "\n",
      "\n",
      "import time\n",
      "from IPython import display\n",
      "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
      "ax[0].plot(x, y, jupyter_string);\n",
      "ax[0].set_color_cycle([plt.cm.viridis(i) for i in np.linspace(0, 1, len(display_iters))])\n",
      "sleep_time = 2\n",
      "\n",
      "\n",
      "overall_predictions = list(estgb.staged_predict(x.reshape(-1,1)))\n",
      "overall_predictions = [np.mean(y)*np.ones_like(x)] + overall_predictions\n",
      "\n",
      "\n",
      "for i in display_iters:\n",
      "    \n",
      "    \n",
      "    cur_overall_prediction = overall_predictions[i]\n",
      "    ax[0].plot(x, cur_overall_prediction, alpha=0.7, label=str(i), lw=2)\n",
      "    ax[0].legend()\n",
      "    \n",
      "    \n",
      "    resid = y - cur_overall_prediction\n",
      "    ax[1].cla()\n",
      "    ax[1].scatter(x,resid, label=jupyter_string)\n",
      "    ax[1].axhline(0)\n",
      "    \n",
      "    \n",
      "    if i <=5:\n",
      "        cur_est = estgb.estimators_[i,0]\n",
      "        cur_prediction = cur_est.predict(x.reshape(-1,1))\n",
      "        ax[1].plot(x, cur_prediction, color=jupyter_string, label=jupyter_string)\n",
      "    else:\n",
      "        \n",
      "        sleep_time = sleep_time/2\n",
      "    ax[1].legend()\n",
      "    \n",
      "    \n",
      "    display.display(fig)\n",
      "    display.clear_output(wait=True)\n",
      "    time.sleep(sleep_time)\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import make_moons\n",
      "\n",
      "X, y = make_moons(n_samples=100, noise=0.25, random_state=1)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators=10, random_state=1)\n",
      "forest.fit(X_train, y_train)\n",
      "forest.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(100).fit(x_train,y_train).score(x_test,y_test)\n",
      "--------------------\n",
      "model.fit(x_train, y_train_cat, epochs=5, batch_size=32, validation_split = .2)\n",
      "=====\n",
      "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
      "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegressionCV\n",
      "\n",
      "LR_score = LogisticRegressionCV().fit(x_train, y_train).score(x_test,y_test)\n",
      "\n",
      "scores = []\n",
      "for cur_model in models:\n",
      "    scores.append(cur_model.score(x_test,y_test))\n",
      "    \n",
      "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
      "ax.hist(scores,20, label=jupyter_string);\n",
      "\n",
      "\n",
      "ax.axvline(LR_score, color=jupyter_string,label=jupyter_string)\n",
      "ax.set_xlabel(jupyter_string, fontsize=24) \n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.legend(loc=jupyter_string, fontsize=24)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "predictions_tune = pd.read_csv(jupyter_string, index_col=0)\n",
      "predictions_test = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "predictions_tune.head()\n",
      "--------------------\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(10, activation=jupyter_string))\n",
      "=====\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "scaler = MinMaxScaler().fit(x_train_flat)\n",
      "x_train_scaled = scaler.transform(x_train_flat)\n",
      "x_test_scaled = scaler.transform(x_test_flat)\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "from keras.datasets import mnist\n",
      "from keras.utils import to_categorical\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "\n",
      "\n",
      "y_train_cat = to_categorical(y_train, num_classes=10)\n",
      "y_test_cat  = to_categorical(y_test, num_classes=10)\n",
      "\n",
      "\n",
      "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
      "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
      "\n",
      "\n",
      "scaler = MinMaxScaler().fit(x_train_flat)\n",
      "x_train_scaled = scaler.transform(x_train_flat)\n",
      "x_test_scaled = scaler.transform(x_test_flat)\n",
      "\n",
      "--------------------\n",
      "removeConstantColumns(data)\n",
      "=====\n",
      "removeDuplicates(data)\n",
      "removeConstantColumns(data)\n",
      "data.describe()\n",
      "--------------------\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
      "model.add(keras.layers.Dense(300, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(100, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(10, activation=jupyter_string))\n",
      "=====\n",
      "model = Sequential([\n",
      "    Dense(500, input_shape=(784,), activation=jupyter_string),\n",
      "    Dense(100, activation=jupyter_string),\n",
      "    Dense(50, activation=jupyter_string),\n",
      "    Dense(10, activation=jupyter_string)\n",
      "])\n",
      "\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
      "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
      "\n",
      "print(x_train.shape)\n",
      "print(x_test.shape)\n",
      "=====\n",
      "plt.imshow(x_train[10,:,:]);\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib\n",
      "plt.show()\n",
      "matplotlib.style.use(jupyter_string)\n",
      "drivingLog = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string],header=None)\n",
      "\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "--------------------\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "=====\n",
      "fig, axes = plt.subplots(nrows=4, ncols=1,figsize=(20,10))\n",
      "drivingLog[jupyter_string].plot(ax=axes[0],color=jupyter_string); axes[0].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[1],color=jupyter_string); axes[1].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[2],color=jupyter_string); axes[2].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[3],color=jupyter_string); axes[3].set_title(jupyter_string);\n",
      "--------------------\n",
      "drivingLog = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string],header=None)\n",
      "\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import csv\n",
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "\n",
      "\n",
      "class AngleNum(dict):\n",
      "\tdef __missing__(self, key):\n",
      "\t\treturn 0\n",
      "\n",
      "angle_num = AngleNum()\n",
      "\n",
      "\n",
      "with open(jupyter_string) as csvfile:\n",
      "    spamreader = csv.reader(csvfile)\n",
      "    for row in spamreader:\n",
      "    \tangle = round(float(row[3]),3)*100\n",
      "    \tangle_num[angle] = angle_num[angle]+1\n",
      "\n",
      "        \n",
      "labels=[]\n",
      "label_num=[]\n",
      "\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,1000,100)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "--------------------\n",
      "labels=[]\n",
      "label_num=[]\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "    if label!=0:\n",
      "        sum+=num\n",
      "    \n",
      "print(sum)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,100,10)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "=====\n",
      "class AngleNum1(dict):\n",
      "\tdef __missing__(self, key):\n",
      "\t\treturn 0\n",
      "\n",
      "angle_num_m = AngleNum1()\n",
      "\n",
      "with open(jupyter_string) as csvfile:\n",
      "    spamreader = csv.reader(csvfile)\n",
      "    for row in spamreader:\n",
      "    \tangle_m = round(float(row[3]),3)*100\n",
      "    \tangle_num_m[angle_m] = angle_num_m[angle_m]+1\n",
      "\n",
      "labels_m=[]\n",
      "labels_num_m=[]\n",
      "print(jupyter_string,angle_num_m[0])\n",
      "angle_num_m[0]=250\n",
      "for (label, num) in angle_num_m.items():\n",
      "    labels_m.append(label)\n",
      "    labels_num_m.append(num)\n",
      "\n",
      "\n",
      "    \n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,300,10)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels_m,labels_num_m)\n",
      "plt.show()\n",
      "--------------------\n",
      "x_train_br = np.array(x_train_br)\n",
      "y_train = np.array(y_train)\n",
      "=====\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "factors = [10,2]\n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)  \n",
      "    val = np.array(x_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    img.set_title(y_train[i])    \n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "    \n",
      "--------------------\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)  \n",
      "    val = np.array(x_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    img.set_title(y_train[i])    \n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "    \n",
      "=====\n",
      "def add_random_shadow(image):\n",
      "    top_y = 320*np.random.uniform()\n",
      "    top_x = 0\n",
      "    bot_x = 160\n",
      "    bot_y = 320*np.random.uniform()\n",
      "    image_hls = cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
      "    shadow_mask = 0*image_hls[:,:,1]\n",
      "    X_m = np.mgrid[0:image.shape[0],0:image.shape[1]][0]\n",
      "    Y_m = np.mgrid[0:image.shape[0],0:image.shape[1]][1]\n",
      "    shadow_mask[((X_m-top_x)*(bot_y-top_y) -(bot_x - top_x)*(Y_m-top_y) >=0)]=1\n",
      "    \n",
      "    if np.random.randint(2)==1:\n",
      "        random_bright = .5\n",
      "        cond1 = shadow_mask==1\n",
      "        cond0 = shadow_mask==0\n",
      "        if np.random.randint(2)==1:\n",
      "            image_hls[:,:,1][cond1] = image_hls[:,:,1][cond1]*random_bright\n",
      "        else:\n",
      "            image_hls[:,:,1][cond0] = image_hls[:,:,1][cond0]*random_bright    \n",
      "    image = cv2.cvtColor(image_hls,cv2.COLOR_HLS2RGB)\n",
      "    return image\n",
      "\n",
      "for i in range(len(x_train)):\n",
      "    x_train[i] = add_random_shadow(np.array(x_train[i]))\n",
      "\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "factors = [10,2]\n",
      "    \n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)\n",
      "    val = np.array(x_train[i])\n",
      "    img.set_title(y_train[i])\n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.utils import shuffle\n",
      "x_train,y_train = shuffle(x_train,y_train)\n",
      "x_train = x_train.reshape(x_train.shape[0],64,64,1)\n",
      "x_test = x_test.reshape(x_test.shape[0],64,64,1)\n",
      "x_valid = x_valid.reshape(x_valid.shape[0],64,64,1)\n",
      "=====\n",
      "for i in range(len(x_train)):\n",
      "    x_train[i] = cv2.resize(np.array(x_train[i]),(200,66),interpolation=cv2.INTER_AREA) \n",
      "\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,10))\n",
      "factors = [4,4]\n",
      "    \n",
      "for ind in range(16):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)\n",
      "    val = np.array(x_train[i])\n",
      "    img.set_title(y_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
      "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=42)\n",
      "=====\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "x_train, y_train = shuffle(x_train, y_train)\n",
      "x_train = np.array(x_train)\n",
      "y_train = np.array(y_train)\n",
      "\n",
      "print(x_train.shape)\n",
      "\n",
      "\n",
      "\n",
      "train_features, test_features, train_labels, test_labels = train_test_split(\n",
      "    x_train,\n",
      "    y_train,\n",
      "    test_size=0.1,\n",
      "    random_state=40)\n",
      "\n",
      "\n",
      "\n",
      "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
      "    train_features,\n",
      "    train_labels,\n",
      "    test_size=0.2,\n",
      "    random_state=11)\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Activation, Dropout, Flatten\n",
      "from keras.layers import Convolution2D, MaxPooling2D\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Convolution2D(32, 3, 3, border_mode=jupyter_string, input_shape=(32, 32, 3)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Convolution2D(64, 3, 3, border_mode=jupyter_string))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten())\n",
      "=====\n",
      "from keras.models import Sequential\n",
      "from keras.layers.core import Dense, Activation, Flatten,Dropout,Lambda\n",
      "from keras.layers.convolutional import Convolution2D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Lambda(lambda x: x/255.-0.5, input_shape=(33,100,3),))\n",
      "model.add(Convolution2D(24, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(36, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(48, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(64, 3, 3))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(64, 3, 3))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(1164))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "\n",
      "model.add(Dense(100))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(50))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(10))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "\n",
      "model.add(Dense(1,name=jupyter_string))\n",
      "model.summary()\n",
      "\n",
      "--------------------\n",
      "x_train = np.array(x_train)\n",
      "y_train = np.array(y_train)\n",
      "print(x_train.shape)\n",
      "print(y_train.shape)\n",
      "=====\n",
      "y_num = AngleNum()\n",
      "\n",
      "\n",
      "for angle in y_train:\n",
      "    y_num[angle*100] = y_num[angle*100]+1\n",
      "    \n",
      "        \n",
      "labels_m=[]\n",
      "labels_num_m=[]\n",
      "for (label, num) in y_num.items():\n",
      "    labels_m.append(label)\n",
      "    labels_num_m.append(num)\n",
      "\n",
      "\n",
      "x_label = np.arange(-100,100,10)\n",
      "y_label = np.arange(0,20,2)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels_m,labels_num_m)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df=pd.read_csv(jupyter_string)\n",
      "df.describe()\n",
      "--------------------\n",
      "df_clean[jupyter_string]=np.log(df_clean[jupyter_string])\n",
      "df_clean[jupyter_string]=np.log(df_clean[jupyter_string])\n",
      "=====\n",
      "plist=list(jupyter_string)\n",
      "logdf=np.log(df_clean[plist])\n",
      "logdf['day' <<unk>>]=df_clean['day' <<unk>>].copy()\n",
      "logdf['timestr' <<unk>>]=df_clean['timestr' <<unk>>].copy()\n",
      "logdf=logdf[df_clean.columns.tolist()]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(logdf[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(logdf[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "logdf.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "--------------------\n",
      "logdf_day.head()\n",
      "=====\n",
      "logdf_day.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "=====\n",
      "train_data = data.as_matrix()\n",
      "Targets = train_data[:,-1]\n",
      "Targets = Targets.reshape((-1,1))\n",
      "train_data = np.delete(train_data, -1, 1)\n",
      "print (train_data.shape)\n",
      "\n",
      "--------------------\n",
      "normdiff.plot()\n",
      "daydiff.plot()\n",
      "enddiff.plot()\n",
      "=====\n",
      "normdiff[normdiff['c' <<unk>>]>-0.6].hist(bins=40);plt.show() \n",
      "daydiff.hist(bins=40);plt.show()\n",
      "enddiff.hist(bins=30);plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "=====\n",
      "logdflt327=logdf[logdf['day' <<unk>>]<327]\n",
      "logdfgt327=logdf[logdf['day' <<unk>>]>327]\n",
      "logdf327temp=logdf[logdf['day' <<unk>>]==327].copy() \n",
      "new_index=pd.Index(logdf[logdf['day' <<unk>>]==330]['timestr' <<unk>>]) \n",
      "logdf327=logdf327temp.set_index('timestr' <<unk>>).reindex(new_index).reset_index() \n",
      "\n",
      "logdf327['day' <<unk>>]=327\n",
      "\n",
      "logdf_clean=pd.concat([logdflt327,logdf327,logdfgt327],ignore_index=True) \n",
      "logdf_clean=logdf_clean[['day' <<unk>>,'timestr' <<unk>>]+plist] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "logdf_clean.head()\n",
      "=====\n",
      "t_d = 391 \n",
      "t_m = t_d*21 \n",
      "R_m = logdf_clean[plist].shift(-t_m) - logdf_clean[plist] \n",
      "\n",
      "R_m[['day' <<unk>>,'timestr' <<unk>>]]=logdf_clean[['day' <<unk>>,'timestr' <<unk>>]].copy();R_m=R_m[['day' <<unk>>,'timestr' <<unk>>]+plist]\n",
      "\n",
      "sigma_m = logdf_clean.rolling(t_d,min_periods=t_d//2).std(ddof=1)\n",
      "sigma_m[['day' <<unk>>,'timestr' <<unk>>]]=R_m[['day' <<unk>>,'timestr' <<unk>>]].copy()\n",
      "sigma_m=sigma_m[['day' <<unk>>,'timestr' <<unk>>]+plist]\n",
      "sigma_m.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "plt.ylim([0,0.1])\n",
      "--------------------\n",
      "fig=plt.figure(figsize=(12,8))\n",
      "plt.title(jupyter_string)\n",
      "ax1=fig.add_subplot(121)\n",
      "fig = sm.graphics.tsa.plot_acf(sigma_m_coarse[jupyter_string],lags=30,ax=ax1)\n",
      "ax2=fig.add_subplot(122)\n",
      "fig = sm.graphics.tsa.plot_pacf(sigma_m_coarse[jupyter_string],lags=30,ax=ax2)\n",
      "=====\n",
      "pq = pd.DataFrame(index=[jupyter_string,jupyter_string],columns=plist)\n",
      "pq.set_value(jupyter_string,'a' madeupword0002,1);pq.set_value(jupyter_string,'a' madeupword0002,0)\n",
      "pq.set_value(jupyter_string,'b' <<unk>>,3);pq.set_value(jupyter_string,'b' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'c' <<unk>>,0);pq.set_value(jupyter_string,'c' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'d' <<unk>>,0);pq.set_value(jupyter_string,'d' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'e' <<unk>>,0);pq.set_value(jupyter_string,'e' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'f' <<unk>>,1);pq.set_value(jupyter_string,'f' <<unk>>,1)\n",
      "--------------------\n",
      "from statsmodels.tsa.arima_model import ARMA\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "=====\n",
      "a_model = sm.tsa.ARMA(sigma_m_coarse['a' madeupword0002].values,(1,0)).fit(disp=False)\n",
      "b_model = sm.tsa.ARMA(sigma_m_coarse['b' <<unk>>].values,(3,0)).fit(disp=False)\n",
      "f_model = sm.tsa.ARMA(sigma_m_coarse['f' <<unk>>].values,(1,1)).fit(disp=False)\n",
      "\n",
      "--------------------\n",
      "a_pred = a_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "b_pred = b_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "f_pred = f_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "=====\n",
      "beg=len(sigma_m_coarse['a' madeupword0002].values)\n",
      "predict_a = a_model.predict(start=beg,end=beg+21)\n",
      "predict_b = b_model.predict(start=beg,end=beg+21)\n",
      "predict_f = f_model.predict(start=beg,end=beg+21)\n",
      "print(predict_a)\n",
      "print(predict_b)\n",
      "print(predict_f)\n",
      "--------------------\n",
      "test_data = data.as_matrix()\n",
      "Targets = test_data[:,-1]\n",
      "Targets = Targets.reshape((-1,1))\n",
      "test_data = np.delete(test_data, -1, 1)\n",
      "print (test_data.shape)\n",
      "=====\n",
      "lda = ql.LDA()\n",
      "lda.train(train_data, Targets)\n",
      "pclass, probabilities, discriminants = lda.use(train_data)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "--------------------\n",
      "b_model.summary()\n",
      "=====\n",
      "b_model.summary()\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string, data=df)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "sns.boxplot(data=sigma_m_coarse[['b' <<unk>>,'c' <<unk>>,'d' <<unk>>]])\n",
      "plt.ylim([0,0.05])\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "plt.show()\n",
      "\n",
      "bls_all = pd.read_csv(jupyter_string)\n",
      "bls_all.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "=====\n",
      "bls_n = pd.read_excel(jupyter_string)\n",
      "\n",
      "bls_n.head(15)\n",
      "--------------------\n",
      "bls_n.info()\n",
      "=====\n",
      "bls_n = bls_n[12:]\n",
      "bls_n.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "                jupyter_string, jupyter_string]\n",
      "\n",
      "\n",
      "bls_n[jupyter_string] = (bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string]\\\n",
      "                                    + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string]\\\n",
      "                                    + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string])\\\n",
      "                                    / 12.0\n",
      "\n",
      "bls_n.head()\n",
      "--------------------\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "=====\n",
      "bls_n.Year_Average.plot()\n",
      "--------------------\n",
      "CT_n.Year_Average.plot()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "ct_by_mo = ct.transpose()\n",
      "ct_by_mo\n",
      "--------------------\n",
      "qda = ql.QDA()\n",
      "qda.train(train_data, Targets)\n",
      "pclass, probabilities, discriminants = qda.use(train_data)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "=====\n",
      "qda = ql.QDA()\n",
      "qda.train(train_data, Targets)\n",
      "pc, prob, d = qda.use(train_data)\n",
      "showResults(pc, Targets, jupyter_string)\n",
      "--------------------\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "=====\n",
      "ct_by_mo = ct_by_mo[1:]\n",
      "ct_by_mo = ct_by_mo[0:-5]\n",
      "\n",
      "\n",
      "\n",
      "ct_by_mo = ct_by_mo[191:]\n",
      "\n",
      "ct_by_mo.plot()\n",
      "--------------------\n",
      "ct_by_mo = ct_by_mo[1:]\n",
      "ct_by_mo = ct_by_mo[0:-5]\n",
      "\n",
      "\n",
      "\n",
      "ct_by_mo = ct_by_mo[191:]\n",
      "\n",
      "ct_by_mo.plot()\n",
      "=====\n",
      "ct_by_mo.tail(6)\n",
      "--------------------\n",
      "ct_by_mo.count()\n",
      "=====\n",
      "ct_3_mo = ct_by_mo.tail(3)\n",
      "feb_to_apr_loss = (ct_3_mo.iloc[0] - ct_3_mo.iloc[2]) * 1000\n",
      "print(jupyter_string + str(feb_to_apr_loss))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "plt.show()\n",
      "df=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "def divide_int_float_strings(df):\n",
      "    list_remove=[]\n",
      "    new_list=[]\n",
      "    for items in df:\n",
      "        if isinstance(df[items][0],str):\n",
      "            list_remove.append(items)\n",
      "        else:\n",
      "            new_list.append(items)\n",
      "    return new_list,list_remove\n",
      "new_list=divide_int_float_strings(df)[0]\n",
      "--------------------\n",
      "df=df[new_list]\n",
      "df.head()\n",
      "=====\n",
      "df[new_list].corr()\n",
      "--------------------\n",
      "df[jupyter_string]=np.log(df[jupyter_string])\n",
      "df[jupyter_string]=np.sqrt(df[jupyter_string])\n",
      "df.head()\n",
      "=====\n",
      "def better_relations(df,new_list):\n",
      "    better_relation_dict={}\n",
      "    better_relation_name_d={}\n",
      "    for items in new_list:\n",
      "        log=df[jupyter_string].corr(np.log10(df[items]))\n",
      "        square=df[jupyter_string].corr(np.square(df[items]))\n",
      "        sqrt=df[jupyter_string].corr(np.sqrt(df[items]))\n",
      "        normal=df[jupyter_string].corr(df[items])\n",
      "        method=[log,square,sqrt,normal]\n",
      "        max1=np.nanmax(method)\n",
      "        better_relation_dict[items]=max1\n",
      "        if log==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if square==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if sqrt==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if normal==max1:\n",
      "            better_relation_name_d[items]=jupyter_string       \n",
      "    return better_relation_dict,better_relation_name_d\n",
      "transformer=better_relations(df)[1]\n",
      "--------------------\n",
      "df[jupyter_string]=np.log10(df[jupyter_string])\n",
      "=====\n",
      "df_copy=df.copy()\n",
      "df_copy[jupyter_string]=np.log10(df_copy[jupyter_string])\n",
      "improve_score(better_relations(df_copy)[0],better_relations(df)[0])\n",
      "--------------------\n",
      "transformed_data=transform_data(df,better_relations(df)[1])\n",
      "transformed_data[new_list].corr()\n",
      "=====\n",
      "df_corr=pd.DataFrame(better_relations(df)[0])\n",
      "df_corr=df_corr.sort_values(jupyter_string,ascending=False)\n",
      "df_corr=df_corr.drop(df_corr.index[0])\n",
      "df_corr\n",
      "--------------------\n",
      "model = ExtraTreesClassifier(random_state=361)\n",
      "grid_obj = GridSearchCV(model, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_obj = grid_obj.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "trainNN, evaluateNN = ml.trainValidateTestKFoldsClassification(trainNN, evaluateNN, train_data, Targets, [[[5], 10]], nFolds=10, shuffle=True, verbose=False)\n",
      "printResults(jupyter_string, resultsNN)\n",
      "=====\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(random_state=1729)\n",
      "data = data.drop([\"TARGET\"], axis =1)\n",
      "selector = clf.fit(data, Targets)\n",
      "\n",
      "\n",
      "feat_imp = pandas.Series(clf.feature_importances_, index = data.columns.values).sort_values(ascending=False)\n",
      "feat_imp[:40].plot(kind=jupyter_string, title=jupyter_string, figsize=(12, 8))\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "fs = SelectFromModel(selector, prefit=True)\n",
      "\n",
      "newTrain = fs.transform(data)\n",
      "--------------------\n",
      "X_train_dict = OrderedDict([(jupyter_string, pcaed_X_train),\n",
      "                            (jupyter_string, subspaced_X_train),\n",
      "                            (jupyter_string, fs_and_pca_X_train),\n",
      "                            (jupyter_string, X_train)])\n",
      "X_test_dict = OrderedDict([(jupyter_string, pcaed_X_test),\n",
      "                           (jupyter_string, subspaced_X_test),\n",
      "                           (jupyter_string, fs_and_pca_X_test),\n",
      "                           (jupyter_string, X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "=====\n",
      "y_train_et = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_et = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_test_rf)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.plot([0, 1], [0, 1], jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "_ = joblib.dump(clf, jupyter_string)\n",
      "--------------------\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import Pandas\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "clf = GradientBoostingClassifier(random_state=42)\n",
      "clf.fit(X_train, y_train)\n",
      "=====\n",
      "X_total = sample[:, :-1]\n",
      "X_total.shape, y.shape\n",
      "--------------------\n",
      "clf.fit(X_total, y)\n",
      "=====\n",
      "clf.fit(X_total, y)\n",
      "y_hat = clf.predict_proba(X_total)[:, 1]\n",
      "roc_auc_score(y, y_hat)\n",
      "--------------------\n",
      "np.sort(y_test_rf)[-int(np.sum(y_test)):][::-1]\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist(y_hat, 100)\n",
      "--------------------\n",
      "df[pd.isnull(df[jupyter_string])]\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "unlabeled_indices = np.where(y_hat == 1)[0]\n",
      "unlabeled_indices_test = np.where(y_test == 1)[0]\n",
      "=====\n",
      "n_examples = 3\n",
      "indices = y_hat[border:].argsort()[-n_examples:][::-1].tolist()\n",
      "indices = [x + border for x in indices]\n",
      "indices\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist(y_hat[indices], 100)\n",
      "=====\n",
      "found_candidates = random_smiles.iloc[indices, 0].values.tolist()\n",
      "found_candidates\n",
      "--------------------\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(random_state=1729)\n",
      "data = data.drop([\"TARGET\"], axis =1)\n",
      "selector = clf.fit(data, Targets)\n",
      "\n",
      "\n",
      "feat_imp = pandas.Series(clf.feature_importances_, index = data.columns.values).sort_values(ascending=False)\n",
      "feat_imp[:40].plot(kind=jupyter_string, title=jupyter_string, figsize=(12, 8))\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "fs = SelectFromModel(selector, prefit=True)\n",
      "\n",
      "newTest = fs.transform(test)\n",
      "print (newTest.shape)\n",
      "=====\n",
      "lda = ql.LDA()\n",
      "lda.train(newTrain, Targets)\n",
      "pclass, probabilities, discriminants = lda.use(newTrain)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].apply(smiles_to_numpy)\n",
      "df.head()\n",
      "=====\n",
      "path_to_zinc = jupyter_string\n",
      "if os.path.isfile(path_to_zinc):\n",
      "    zinc_smiles = pd.read_csv(path_to_zinc)\n",
      "else:\n",
      "    zinc_smiles = pd.read_csv(jupyter_string +\n",
      "                              jupyter_string +\n",
      "                              jupyter_string)\n",
      "zinc_smiles.head()\n",
      "--------------------\n",
      "zinc_smiles.drop(zinc_smiles.index[random_indices], inplace=True)\n",
      "=====\n",
      "random_smiles = zinc_smiles.iloc[random_indices, :]\n",
      "random_smiles.head()\n",
      "--------------------\n",
      "df = pd.DataFrame(sample, columns=[jupyter_string, jupyter_string])\n",
      "df.head()\n",
      "=====\n",
      "border = positives.shape[0]\n",
      "--------------------\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "np.mean(positives[:, 1152])\n",
      "--------------------\n",
      "np.mean(negatives[:, 1152])\n",
      "=====\n",
      "np.mean(negatives[:, 1152])\n",
      "--------------------\n",
      "plt.scatter(positives[:, 1152], negatives[:, 1152])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist([scores[key] for key in scores.keys()], 50)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "=====\n",
      "weights = np.array([abs(scores[key] - 0.5) for key in scores.keys()])\n",
      "weighted_positives = positives[:, :-1] * weights\n",
      "weighted_negatives = negatives[:, :-1] * weights\n",
      "weighted_sample = sample[:, :-1] * weights\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "ax_one.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 1], c=jupyter_string, s=50)\n",
      "_ = ax_one.scatter(pcaed_positives[:, 0], pcaed_positives[:, 1], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 0], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 1], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 1], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "sns.kdeplot(pcaed_positives[:, 0], pcaed_positives[:, 2],\n",
      "            cmap=jupyter_string, shade=True, shade_lowest=False, ax=ax_one)\n",
      "ax_two = fig.add_subplot(122, sharex=ax_one, sharey=ax_one)\n",
      "ax_two.set_title(jupyter_string)\n",
      "ax_two.set_xlabel(jupyter_string)\n",
      "ax_two.set_ylabel(jupyter_string)\n",
      "_ = sns.kdeplot(pcaed_negatives[:, 0], pcaed_negatives[:, 2],\n",
      "            cmap=jupyter_string, shade=True, shade_lowest=False, ax=ax_two)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import math\n",
      "import numpy as np\n",
      "from pylab import *\n",
      "import seaborn as sns\n",
      "import scipy.stats\n",
      "import random\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "_ = ax_one.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 1], c=jupyter_string, s=50)\n",
      "_ = ax_one.scatter(pcaed_positives[:, 0], pcaed_positives[:, 1], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 1], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 1], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "X = sample[:, :-1]\n",
      "y = sample[:, -1]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=361)\n",
      "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "X_train = pca.fit_transform(X_train)\n",
      "X_test = pca.transform(X_test)\n",
      "X_train.shape, X_test.shape\n",
      "=====\n",
      "scores = {}\n",
      "for i in range(X_train.shape[1]):\n",
      "    scores[i] = roc_auc_score(y_train, X_train[:, i])\n",
      "weights = np.array([abs(scores[key] - 0.5) for key in scores.keys()])\n",
      "weighted_X_train = X_train * weights\n",
      "weighted_X_test = X_test * weights\n",
      "--------------------\n",
      "plt.plot(evr)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.plot(evr.cumsum())\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.plot(evr.cumsum())\n",
      "=====\n",
      "pca = PCA(n_components=20, random_state=361)\n",
      "pcaed_X_train = pca.fit_transform(weighted_X_train)\n",
      "pcaed_X_test = pca.transform(weighted_X_test)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import f_classif\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=20)\n",
      "selector.fit(X_train, y_train)\n",
      "\n",
      "X_train_selected = selector.transform(X_train)\n",
      "X_test_selected = selector.transform(X_test)\n",
      "=====\n",
      "feature_selector = SelectKBest(chi2, k=80)\n",
      "subspaced_X_train = feature_selector.fit_transform(X_train, y_train)\n",
      "subspaced_X_test = feature_selector.transform(X_test)\n",
      "--------------------\n",
      "subspaced_X_train = pd.DataFrame(subspaced_X_train)\n",
      "subspaced_X_test = pd.DataFrame(subspaced_X_test)\n",
      "=====\n",
      "fs_and_pca_X_train = np.hstack((subspaced_X_train, pcaed_X_train))\n",
      "fs_and_pca_X_test = np.hstack((subspaced_X_test, pcaed_X_test))\n",
      "fs_and_pca_X_train.shape, fs_and_pca_X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "X_train = fs_and_pca_X_train[:, fs_and_pca_X_train.shape[1] - 1]\n",
      "X_test = fs_and_pca_X_test[:, fs_and_pca_X_test.shape[1] - 1]\n",
      "=====\n",
      "all_and_pca_X_train = np.hstack((X_train, pcaed_X_train))\n",
      "all_and_pca_X_test = np.hstack((X_test, pcaed_X_test))\n",
      "all_and_pca_X_train.shape, all_and_pca_X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "X_train = pd.DataFrame(X_train_dict)\n",
      "X_test = pd.DataFrame(X_test_dict)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "temp = df['temperature' <<unk>>]\n",
      "print(np.mean(temp))\n",
      "print(np.std(temp))\n",
      "sns.set()\n",
      "sns.distplot(temp)\n",
      "sns.plt.show()\n",
      "--------------------\n",
      "gs = GridSearchCV(estimator= X_train_dict[jupyter_string],\n",
      "                  param_grid= grid_params,\n",
      "                  scoring= jupyter_string,\n",
      "                  cv= kf,\n",
      "                  n_jobs= -1)\n",
      "gs = gs.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "X_train_dict = OrderedDict([(jupyter_string, pcaed_X_train),\n",
      "                            (jupyter_string, subspaced_X_train),\n",
      "                            (jupyter_string, fs_and_pca_X_train),\n",
      "                            (jupyter_string, X_train)])\n",
      "X_test_dict = OrderedDict([(jupyter_string, pcaed_X_test),\n",
      "                           (jupyter_string, subspaced_X_test),\n",
      "                           (jupyter_string, fs_and_pca_X_test),\n",
      "                           (jupyter_string, X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "=====\n",
      "y_train_gb = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_gb = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "classes = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "classes = classes.values[:, 1]\n",
      "print(classes)\n",
      "print(classes.shape)\n",
      "--------------------\n",
      "predicates = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "predicates = predicates.values[:, 1]\n",
      "print(predicates)\n",
      "print(predicates.shape)\n",
      "=====\n",
      "predicates = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "predicates = predicates.values[:,1]\n",
      "print(predicates.shape)\n",
      "print(predicates)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "classes = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "classes = classes.values[:, 1]\n",
      "print(classes)\n",
      "print(classes.shape)\n",
      "=====\n",
      "dataset = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "dataset = dataset.values\n",
      "print(dataset.shape)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(dataset)\n",
      "X = pca.transform(dataset)\n",
      "print(X.shape)\n",
      "=====\n",
      "projected = np.matmul(direction, dataset.T)\n",
      "projected.shape\n",
      "--------------------\n",
      "plt.scatter(projected[:,0], projected[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "from pylab import rcParams\n",
      "from IPython.display import set_matplotlib_formats\n",
      "set_matplotlib_formats(jupyter_string)\n",
      "rcParams[jupyter_string] = 13,13\n",
      "\n",
      "plt.scatter(projected[0,:], projected[1, :])\n",
      "\n",
      "for i in range(classes.shape[0]):\n",
      "    x = projected[0, i]\n",
      "    y = projected[1, i]\n",
      "    plt.annotate(classes[i], xy=(x,y), xytext=(x+1,y+1))\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "plt.hist(x, bins=20, alpha=0.75)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "l = list(df[jupyter_string])\n",
      "n, bins, patches = plt.hist(l, 20, normed=1, facecolor=jupyter_string, alpha=0.75)\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "l = list(df[jupyter_string])\n",
      "n, bins, patches = plt.hist(l, 20, normed=1, facecolor=jupyter_string, alpha=0.75)\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "=====\n",
      "param = norm.fit(l)\n",
      "x = linspace(0,1,100) \n",
      "pdf_fitted = norm.pdf(x, loc=param[0], scale=param[1])\n",
      "\n",
      "title(jupyter_string)\n",
      "text(0.1,1.7, rjupyter_string)\n",
      "grid()\n",
      "xlabel(jupyter_string)\n",
      "ylabel(jupyter_string)\n",
      "plot(x, pdf_fitted, jupyter_string)\n",
      "hist(l, normed=1,alpha=0.5, bins=20)\n",
      "savefig(jupyter_string, bbox_inches=jupyter_string)\n",
      "show()\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "df.loc[df.gender == jupyter_string, 'gender' madeupword0002] = 0\n",
      "df.loc[df.gender == jupyter_string, 'gender' madeupword0002] = 1\n",
      "\n",
      "chi2, p, dof, ex = scipy.stats.chi2_contingency(df.head())\n",
      "print(jupyter_string, chi2)\n",
      "print(jupyter_string, p)\n",
      "print(jupyter_string,dof)\n",
      "\n",
      "--------------------\n",
      "m.components_[0].__str__().split(jupyter_string)[1].split(jupyter_string)[0]\n",
      "=====\n",
      "x = linspace(0,1,100) \n",
      "\n",
      "\n",
      "f1 = norm.pdf(x, 0.769444751466, 0.1)\n",
      "f2 = norm.pdf(x, 0.564284885974, 0.1)\n",
      "f3 = norm.pdf(x, 0.483220658218, 0.146212605803)\n",
      "f4 = norm.pdf(x, 0.769446855025, 0.1)\n",
      "f5 = norm.pdf(x, 0.397788035948, 0.136193314069)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig, ax1 = plt.subplots()\n",
      "title(jupyter_string)\n",
      "grid()\n",
      "xlabel(jupyter_string)\n",
      "ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "n, bins, patches = ax1.hist(l, normed=False,alpha=0.5, bins=20)\n",
      "\n",
      "ax2 = ax1.twinx()\n",
      "ax2.plot(x, f1, jupyter_string,\n",
      "     x, f2, jupyter_string,\n",
      "     x, f3, jupyter_string,\n",
      "     x, f4, jupyter_string,\n",
      "     x, f5, jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "ax2.get_yaxis().set_ticks([])\n",
      "savefig(jupyter_string, bbox_inches=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.style.use(jupyter_string)\n",
      "matplotlib.rcParams[jupyter_string] = [16.0, 10.0]\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data.boxplot(['radius_mean' <<unk>>, 'texture_mean' <<unk>>, 'perimeter_mean' <<unk>>, 'texture_worst' <<unk>>, 'perimeter_worst' <<unk>>, 'area_se' <<unk>>, 'radius_worst' <<unk>>, 'texture_worst' <<unk>>])\n",
      "plt.show()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data.hist()\n",
      "plt.show()\n",
      "--------------------\n",
      "data.corr(method=jupyter_string, min_periods=1)\n",
      "=====\n",
      "import seaborn as sns \n",
      "corr = data.corr()\n",
      "plt.subplots(figsize=(16,16))\n",
      "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, \n",
      "            cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df08 = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "df12 = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "df08 = df08[df08[jupyter_string] != 1]\n",
      "=====\n",
      "filtered_cols = [jupyter_string, jupyter_string,\n",
      "                 jupyter_string, jupyter_string,\n",
      "                 jupyter_string, jupyter_string]\n",
      "is_invalid_filter = (df08[filtered_cols] > 1).any(axis=1)\n",
      "n_rows_to_drop = is_invalid_filter[is_invalid_filter].shape[0]\n",
      "print(jupyter_string.format(n_rows_to_drop))\n",
      "df08 = df08[~is_invalid_filter]\n",
      "df08.shape[0]\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08[jupyter_string], ax=ax[0])\n",
      "sns.distplot(df08[jupyter_string], ax=ax[1])\n",
      "--------------------\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08[jupyter_string], ax=ax[0])\n",
      "sns.distplot(df08[jupyter_string], ax=ax[1])\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2016' <<unk>>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "--------------------\n",
      "reshaped = reshaped.rename(columns={jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string})\n",
      "reshaped.head()\n",
      "=====\n",
      "data = reshaped.pivot_table(index=['fips_code' <<unk>>, 'county' <<unk>>, jupyter_string], columns=[jupyter_string])\n",
      "data.head()\n",
      "--------------------\n",
      "data = data.reset_index()\n",
      "data.head()\n",
      "=====\n",
      "flattened = data.copy()\n",
      "flattened.columns = flattened.columns.get_level_values(1)\n",
      "flattened = pd.DataFrame(flattened.to_records())\n",
      "--------------------\n",
      "temp_mean = np.mean(df.temperature)\n",
      "temp_mean\n",
      "=====\n",
      "m = df.loc[df.gender == 1, 'temperature' <<unk>>]\n",
      "f = df.loc[df.gender == 0, 'temperature' <<unk>>]\n",
      "--------------------\n",
      "flattened.head()\n",
      "=====\n",
      "alt.data_transformers.enable(jupyter_string, max_rows=1000000)\n",
      "alt.Chart(flattened).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X(jupyter_string,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string,\n",
      "    color=jupyter_string)\n",
      "--------------------\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "=====\n",
      "def choose_winner(row):\n",
      "    ''jupyter_string''\n",
      "    if row[jupyter_string] > row[jupyter_string] and row[jupyter_string] > row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    elif row[jupyter_string] >= row[jupyter_string] and row[jupyter_string] >= row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    elif row[jupyter_string] >= row[jupyter_string] and row[jupyter_string] >= row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    else:\n",
      "        return jupyter_string\n",
      "flattened[jupyter_string] = flattened.apply(choose_winner, axis=1)\n",
      "flattened[jupyter_string] = flattened[jupyter_string] - flattened[jupyter_string]\n",
      "flattened.head()\n",
      "--------------------\n",
      "flattened.to_csv(jupyter_string)\n",
      "=====\n",
      "flattened.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "flattened.head()\n",
      "=====\n",
      "pd.read_csv(jupyter_string).head()\n",
      "--------------------\n",
      "plot_decision_regions(X, y, classifier=ppn)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plot_decision_regions(X, y, classifier=ppn)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.scatter(df[0],df[1])\n",
      "plt.show()\n",
      "=====\n",
      "y = df.iloc[0:100, 4].values\n",
      "y = np.where(y == jupyter_string, -1, 1)\n",
      "X = df.iloc[0:100, [0, 2]].values\n",
      "\n",
      "plt.scatter(X[:50, 0], X[:50, 1], color=jupyter_string, marker=jupyter_string, label=jupyter_string)\n",
      "plt.scatter(X[50:100, 0], X[50:100, 1], color=jupyter_string, marker=jupyter_string, label=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "\n",
      "df.head(df.size)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "\n",
      "np.random.seed(42)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams[jupyter_string] = 14\n",
      "plt.rcParams[jupyter_string] = 12\n",
      "plt.rcParams[jupyter_string] = 12\n",
      "\n",
      "train_data=pd.read_csv(jupyter_string)\n",
      "test_data=pd.read_csv(jupyter_string)\n",
      "train_data.head()\n",
      "--------------------\n",
      "test_data.head()\n",
      "=====\n",
      "train_data.info()\n",
      "--------------------\n",
      "temp_m = np.mean(m)\n",
      "temp_f = np.mean(f)\n",
      "temp_m, temp_f\n",
      "=====\n",
      "diff_mean = m.mean() - f.mean()\n",
      "m_var = np.var(m)\n",
      "f_var = np.var(f)\n",
      "var = math.sqrt((m_var/len(m)) + (f_var/len(f)))\n",
      "\n",
      "lim = 1.96 * var\n",
      "up = diff_mean + lim\n",
      "low = diff_mean - lim\n",
      "\n",
      "test = 1.65 * var\n",
      "print(jupyter_string,abs(diff_mean))\n",
      "print(jupyter_string,test)\n",
      "print(jupyter_string, up,jupyter_string,low,jupyter_string)\n",
      "--------------------\n",
      "train_data.describe()\n",
      "=====\n",
      "test_data.info()\n",
      "--------------------\n",
      "test_data.describe()\n",
      "=====\n",
      "train_data.describe()\n",
      "--------------------\n",
      "strat_test_set[\"Pclass\"].value_counts()\n",
      "=====\n",
      "def Pclass_proportions(data):\n",
      "    return data[\"Pclass\"].value_counts() / len(data)\n",
      "\n",
      "train_set, dev_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
      "\n",
      "compare_props = pd.DataFrame({\n",
      "    jupyter_string: Pclass_proportions(train_data),\n",
      "    jupyter_string: Pclass_proportions(strat_test_set),\n",
      "    jupyter_string: Pclass_proportions(dev_set),\n",
      "}).sort_index()\n",
      "compare_props[jupyter_string] = 100 * compare_props[jupyter_string] / compare_props[jupyter_string] - 100\n",
      "compare_props[jupyter_string] = 100 * compare_props[jupyter_string] / compare_props[jupyter_string] - 100\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train_set=strat_train_set.copy()\n",
      "--------------------\n",
      "corr_matrix = train_set.corr()\n",
      "corr_matrix\n",
      "=====\n",
      "corr_mat=train_set.corr()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import urllib\n",
      "urllib.urlretrieve(jupyter_string, jupyter_string)\n",
      "--------------------\n",
      "train_data.head()\n",
      "=====\n",
      "train_data=train_data.drop({jupyter_string,jupyter_string,jupyter_string,jupyter_string},axis=1)\n",
      "train_data\n",
      "--------------------\n",
      "train_data[[jupyter_string,jupyter_string]].groupby([jupyter_string],as_index=False).mean().sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "g = sns.FacetGrid(train_set, col='Sex' <<unk>>,row='Survived' <<unk>>)\n",
      "g.map(plt.hist, 'Age' <<unk>>, bins=20)\n",
      "g.add_legend\n",
      "--------------------\n",
      "test_set=strat_test_set.copy()\n",
      "test_set_labels=test_set[\"Survived\"].copy()\n",
      "test_set=test_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "test_set\n",
      "=====\n",
      "dev_set=strat_test_set.copy()\n",
      "dev_set.info()\n",
      "--------------------\n",
      "test_set=strat_test_set.copy()\n",
      "test_set.info()\n",
      "=====\n",
      "dev_set_labels=dev_set[\"Survived\"].copy()\n",
      "dev_set=dev_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "test_set=test_data.copy()\n",
      "test_set=test_set.drop([\"PassengerId\"],axis=1)\n",
      "--------------------\n",
      "dev_set.info()\n",
      "=====\n",
      "train_set_cat=train_set[[\"Name\",\"Embarked\",\"Fare\"]].copy()\n",
      "train_set_num=train_set.drop([\"Name\",\"Embarked\",\"Fare\"],axis=1)\n",
      "cat_attribs=list(train_set_cat)\n",
      "num_attribs=list(train_set_num)\n",
      "num_attribs\n",
      "--------------------\n",
      "def get_title(name):\n",
      "    title_search = re.search(jupyter_string, name)\n",
      "    if title_search:\n",
      "        return title_search.group(1)\n",
      "    return jupyter_string\n",
      "\n",
      "train_set[jupyter_string]=train_set[\"Name\"].apply(get_title)\n",
      "test_set[jupyter_string]=test_set[\"Name\"].apply(get_title)\n",
      "=====\n",
      "train_set_cat[jupyter_string]=pd.Series(train_set_cat['Name' <<unk>>]).str.extract(jupyter_string, expand=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_set_cat.head()\n",
      "=====\n",
      "pd.crosstab(train_set_cat[jupyter_string], train_set_num['Sex' <<unk>>])\n",
      "--------------------\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "full_pipeline=FeatureUnion([(jupyter_string,num_pipeline),(jupyter_string,cat_pipeline)])\n",
      "full_pipeline.fit_transform(train_set)\n",
      "=====\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "full_pipeline=FeatureUnion(transformer_list=[(jupyter_string,num_pipeline),(jupyter_string,cat_pipeline)])\n",
      "\n",
      "X_train=full_pipeline.fit_transform(train_set)\n",
      "X_train.shape\n",
      "--------------------\n",
      "forest_clf.fit(X_train,y_train)\n",
      "=====\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = {\n",
      "    jupyter_string: range(100, 400, 50),\n",
      "    jupyter_string: range(2, 7, 1),\n",
      "    jupyter_string:[jupyter_string,jupyter_string]\n",
      "}\n",
      "cross_validation = StratifiedKFold(n_splits=5)\n",
      "grid_clf = GridSearchCV(estimator = forest_clf, param_grid = param_grid, scoring=jupyter_string, cv=cross_validation)\n",
      "grid_clf.fit(X_train, y_train)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "--------------------\n",
      "forest_clf=RandomForestClassifier(max_features=jupyter_string)\n",
      "forest_clf.fit(X_train, y_train)\n",
      "forest_clf.score(X_dev, y_dev)\n",
      "=====\n",
      "X_dev=full_pipeline.transform(dev_set)\n",
      "y_dev=dev_set_labels\n",
      "--------------------\n",
      "forest_clf.fit(X_train, y_train)\n",
      "forest_clf.score(X_dev, y_dev)\n",
      "=====\n",
      "y_pred=grid_clf.best_estimator_.predict(X_dev)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = df.fillna(0)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "train_data=train_data.drop(['PassengerId' madeupword0002],axis=1)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.33, random_state=42)\n",
      "=====\n",
      "grid_clf.fit(X_full, y_full)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "--------------------\n",
      "df_climate = pd.read_csv(jupyter_string)\n",
      "df_climate.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df_climate = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "columns = s.split(jupyter_string)\n",
      "columns\n",
      "=====\n",
      "column_labels_list = s.split(jupyter_string)\n",
      "column_labels_list\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.drop(columns_to_drop, axis=1, inplace=True)\n",
      "=====\n",
      "new_df = df.drop(columns_to_drop, axis=jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "new_df.head()\n",
      "=====\n",
      "new_df.head()\n",
      "--------------------\n",
      "new_df[jupyter_string] = pd.to_datetime(new_df[jupyter_string])\n",
      "new_df[jupyter_string] = pd.to_datetime(new_df[jupyter_string])\n",
      "new_df.head()\n",
      "=====\n",
      "new_df[jupyter_string] = new_df[jupyter_string].astype(str)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = new_df[jupyter_string].apply(lambda x:jupyter_string.format(x))\n",
      "\n",
      "\n",
      "date_string = new_df[jupyter_string] + new_df[jupyter_string]\n",
      "\n",
      "\n",
      "date_times = pd.to_datetime(date_string, format=jupyter_string)\n",
      "\n",
      "\n",
      "new_df = new_df.set_index(date_times)\n",
      "\n",
      "\n",
      "print(new_df.head())\n",
      "--------------------\n",
      "new_df[jupyter_string] = new_df[jupyter_string].astype(str)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = new_df[jupyter_string].apply(lambda x:jupyter_string.format(x))\n",
      "\n",
      "\n",
      "date_string = new_df[jupyter_string] + new_df[jupyter_string]\n",
      "\n",
      "\n",
      "date_times = pd.to_datetime(date_string, format=jupyter_string)\n",
      "\n",
      "\n",
      "new_df = new_df.set_index(date_times)\n",
      "\n",
      "\n",
      "print(new_df.head())\n",
      "=====\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "print(new_df.loc[jupyter_string:jupyter_string, [jupyter_string, jupyter_string, jupyter_string]])\n",
      "--------------------\n",
      "new_df = new_df[new_df[jupyter_string] >= 1984]\n",
      "new_df = new_df[new_df[jupyter_string] <= 2010]\n",
      "new_df = new_df.reset_index(drop=True)\n",
      "=====\n",
      "df_climate.head()\n",
      "--------------------\n",
      "df_climate.head()\n",
      "=====\n",
      "df_climate.head()\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "new_df.describe()\n",
      "\n",
      "--------------------\n",
      "new_df.info()\n",
      "=====\n",
      "df_climate.describe()\n",
      "--------------------\n",
      "daily_temp_2010 = daily_mean_2010[jupyter_string].values\n",
      "daily_temp_2010[:10]\n",
      "=====\n",
      "daily_climate = df_climate.resample(jupyter_string).mean()\n",
      "daily_climate.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.corr()\n",
      "--------------------\n",
      "daily_temp_2011 = daily_climate[jupyter_string].values\n",
      "daily_temp_2011[:10]\n",
      "=====\n",
      "daily_temp_climate = daily_climate.reset_index()['Temperature' <<unk>>]\n",
      "daily_temp_climate[:10]\n",
      "--------------------\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011.head()\n",
      "=====\n",
      "sunny = new_df.loc[new_df[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "overcast = new_df.loc[new_df[jupyter_string].str.contains(jupyter_string)]\n",
      "--------------------\n",
      "temp_sunny = sunny.groupby(jupyter_string)[jupyter_string].mean()\n",
      "temp_overcast = overcast.groupby(jupyter_string)[jupyter_string].mean()\n",
      "=====\n",
      "sunny_daily_max = sunny.resample(jupyter_string).max()\n",
      "overcast_daily_max = overcast.resample(jupyter_string).max()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "weekly_mean = new_df[[jupyter_string, jupyter_string]].resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "print(weekly_mean.corr())\n",
      "\n",
      "\n",
      "weekly_mean.plot(subplots=True)\n",
      "plt.show()\n",
      "--------------------\n",
      "weekly_mean.plot(kind=jupyter_string, subplots=True)\n",
      "plt.show()\n",
      "=====\n",
      "sunny = new_df[jupyter_string] == jupyter_string\n",
      "\n",
      "\n",
      "sunny_hours = sunny.resample(jupyter_string).sum()\n",
      "\n",
      "\n",
      "total_hours = sunny.resample(jupyter_string).count()\n",
      "\n",
      "\n",
      "sunny_fraction = sunny_hours / total_hours\n",
      "\n",
      "\n",
      "sunny_fraction.plot(kind=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "months = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "monthly_max = new_df[[jupyter_string, jupyter_string]].resample(jupyter_string).max()\n",
      "\n",
      "\n",
      "monthly_max.plot(kind=jupyter_string, bins=8, alpha=0.5, subplots=True)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "august_2011 = monthly_max[monthly_max.index.month == 8]\n",
      "august_2010 = monthly_max[monthly_max.index.month == 9]\n",
      "august_2011 = august_2011.resample(jupyter_string).max()\n",
      "august_2010 = august_2010.resample(jupyter_string).max()\n",
      "=====\n",
      "august_max = df_climate.loc[jupyter_string,'Temperature' <<unk>>].max()\n",
      "print(august_max)\n",
      "--------------------\n",
      "august_2011.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "august_2011_high = august_2011.loc[august_2011 > august_max]\n",
      "\n",
      "\n",
      "august_2011_high.plot(kind=jupyter_string, normed=True, cumulative=True, bins=25, legend=jupyter_string, grid=True)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "dt.score(X_test, y_test)\n",
      "=====\n",
      "ens = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_split=30)\n",
      "ens.fit(X_train,y_train)\n",
      "sk_rf_pred = ens.predict(X_test)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "model = smf.ols(formula=formula, data=df)\n",
      "res=model.fit()\n",
      "\n",
      "--------------------\n",
      "iris = load_iris()\n",
      "X = iris.data[:, :2]\n",
      "y = iris.target\n",
      "=====\n",
      "Xy = pd.read_csv(jupyter_string,header=None)\n",
      "Xy[60] = Xy[60].map({'R' <<unk>>:0,jupyter_string:1})\n",
      "X = np.array(Xy.iloc[:,:-1])\n",
      "y = np.array(Xy.iloc[:,-1])\n",
      "Xy = np.array(Xy)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "forest = random_forest(num_trees=10)\n",
      "forest.fit(X_train, y_train)\n",
      "forest.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "Xy = pd.read_csv(jupyter_string,header=None)\n",
      "Xy[60] = Xy[60].map({'R' <<unk>>:0,jupyter_string:1})\n",
      "X = np.array(Xy.iloc[:,:-1])\n",
      "y = np.array(Xy.iloc[:,-1])\n",
      "Xy = np.array(Xy)\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=35)\n",
      "--------------------\n",
      "df = pandas.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dataframe = pandas.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "dataframe\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "dataframe.plot()\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "=====\n",
      "dataframe.plot(subplots=True)\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "=====\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string, stacked=True)\n",
      "=====\n",
      "dataframe.plot(kind=jupyter_string, stacked=True)\n",
      "--------------------\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "=====\n",
      "from scipy.stats import randint\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "param_dist = {jupyter_string: randint(1, 9),\n",
      "              jupyter_string: randint(1, 9)}\n",
      "n_iter_search = 20\n",
      "random_search = RandomizedSearchCV(rfc, param_distributions=param_dist,\n",
      "                                   n_iter=n_iter_search)\n",
      "random_search.fit(X_train_std, Y_train)\n",
      "random_search.best_score_\n",
      "random_search.best_params_\n",
      "--------------------\n",
      "res.summary()\n",
      "=====\n",
      "res.summary()\n",
      "--------------------\n",
      "rfc = RandomForestClassifier(n_estimators=100, max_features=4)\n",
      "rfc.fit(X_train_std, Y_train)\n",
      "rfc.score(X_train_std, Y_train)\n",
      "=====\n",
      "rfc_opt = RandomForestClassifier(random_state=3, max_features=4)\n",
      "rfc_opt.fit(X_train_std, Y_train)\n",
      "Y_pred = rfc_opt.predict(X_test_std)\n",
      "rfc_opt_score_train = rfc_opt.score(X_train_std, Y_train)\n",
      "print(jupyter_string,rfc_opt_score_train)\n",
      "rfc_opt_score_test = rfc_opt.score(X_test_std, Y_test)\n",
      "print(jupyter_string,rfc_opt_score_test)\n",
      "print(jupyter_string % metrics.f1_score(Y_test, Y_pred, average=jupyter_string))\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df.salary.value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "df.hist(column='average_montly_hours' <<unk>>)\n",
      "--------------------\n",
      "df_left = df[df.left == 1]\n",
      "df_no_left = df[df.left == 0]\n",
      "=====\n",
      "dfleft, dfstay = [x for _, x in df.groupby(df[\"left\"]==0)]\n",
      "dfleft.describe()\n",
      "--------------------\n",
      "dfstay.describe()\n",
      "=====\n",
      "dfstay.describe()\n",
      "--------------------\n",
      "sns.regplot(x=res.fittedvalues, y=res.resid)\n",
      "=====\n",
      "plt.scatter(df['medv' <<unk>>], res.fittedvalues)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_test = scaler.transform(X_test)\n",
      "=====\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "stdsc = StandardScaler()\n",
      "X_train_std = stdsc.fit_transform(X_train)\n",
      "X_test_std = stdsc.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_std, y_train)\n",
      "print(jupyter_string.format(lr.score(X_train_std, y_train)))\n",
      "print(jupyter_string.format(lr.score(X_test_std, y_test)))\n",
      "=====\n",
      "from sklearn import linear_model, metrics, linear_model\n",
      "logit = linear_model.LogisticRegression()\n",
      "logit.fit(X_train_std, Y_train)\n",
      "Y_pred = logit.predict(X_test_std)\n",
      "logit_score_train = logit.score(X_train_std, Y_train)\n",
      "print(jupyter_string,logit_score_train)\n",
      "logit_score_test = logit.score(X_test_std, Y_test)\n",
      "print(jupyter_string,logit_score_test)\n",
      "print(jupyter_string % metrics.f1_score(Y_test, Y_pred, average=jupyter_string))\n",
      "--------------------\n",
      "all_user_predicted_ratings = np.dot(np.dot(u, sigma), vt)\n",
      "all_user_predicted_ratings.shape\n",
      "=====\n",
      "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n",
      "all_user_predicted_ratings\n",
      "--------------------\n",
      "predictions = all_user_predicted_ratings[all_user_predicted_ratings.nonzero()]\n",
      "=====\n",
      "cf_preds_df = pd.DataFrame(all_user_predicted_ratings, columns = users_items_pivot_matrix_df.columns, index=users_ids).transpose()\n",
      "cf_preds_df.head(10)\n",
      "--------------------\n",
      "articles = pd.read_csv(jupyter_string)\n",
      "articles.head()\n",
      "=====\n",
      "articles_df = pd.read_csv(jupyter_string)\n",
      "articles_df = articles_df[articles_df['eventType' <<unk>>] == jupyter_string]\n",
      "articles_df.head(5)\n",
      "--------------------\n",
      "users_interactions_df = pd.read_csv(jupyter_string)\n",
      "users_interactions_df.head(5)\n",
      "=====\n",
      "interactions_df = pd.read_csv(jupyter_string)\n",
      "interactions_df.head(10)\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "=====\n",
      "print(jupyter_string)\n",
      "cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\n",
      "print(jupyter_string % cf_global_metrics)\n",
      "cf_detailed_results_df.head(10)\n",
      "--------------------\n",
      "weights_df = pd.read_csv(jupyter_string)\n",
      "weights_df.head(10)\n",
      "=====\n",
      "event_type_strength = {\n",
      "   jupyter_string: 1.0,\n",
      "   jupyter_string: 2.0, \n",
      "   jupyter_string: 2.5, \n",
      "   jupyter_string: 3.0,\n",
      "   jupyter_string: 4.0,  \n",
      "}\n",
      "\n",
      "interactions_df[jupyter_string] = interactions_df['eventType' <<unk>>].apply(lambda x: event_type_strength[x])\n",
      "--------------------\n",
      "interactions_df = interactions_df[interactions_df[jupyter_string] > 5]\n",
      "interactions_df.head()\n",
      "=====\n",
      "users_interactions_count_df = interactions_df.groupby(['personId' <<unk>>, 'contentId' <<unk>>]).size().groupby('personId' <<unk>>).size()\n",
      "print(jupyter_string % len(users_interactions_count_df))\n",
      "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId' <<unk>>]]\n",
      "print(jupyter_string % len(users_with_enough_interactions_df))\n",
      "--------------------\n",
      "users_with_enough_interactions_df.head()\n",
      "=====\n",
      "print(jupyter_string % len(interactions_df))\n",
      "interactions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n",
      "               how = jupyter_string,\n",
      "               left_on = 'personId' <<unk>>,\n",
      "               right_on = 'personId' <<unk>>)\n",
      "print(jupyter_string % len(interactions_from_selected_users_df))\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "=====\n",
      "Number_variables=range(len(data_train.columns[:-1]))\n",
      "OLS_R_2_OS_F=[]\n",
      "OLS_R_2_IS_F=[]\n",
      "\n",
      "R2_IS_Lasso=[]\n",
      "R2_OS_Lasso=[]\n",
      "\n",
      "R2_IS_Ridge=[]\n",
      "R2_OS_Ridge=[]\n",
      "\n",
      "for i in Number_variables:\n",
      "    \n",
      "    lm = smf.ols(formula = jupyter_string+ jupyter_string.join(data_train.columns[:i+1]), data = data_train).fit()\n",
      "    R2 = modelEval(lm)\n",
      "    OLS_R_2_IS_F.append(lm.rsquared)\n",
      "    OLS_R_2_OS_F.append(R2 if R2 > 0 else 0)\n",
      "    \n",
      "for i in Number_variables:\n",
      "    \n",
      "    Lasso=linear_model.Lasso(fit_intercept=True,alpha=30, )\n",
      "    Lasso.fit(X_train,y_train)\n",
      "    \n",
      "    p_IS=Lasso.predict(X_train)\n",
      "    err_IS=p_IS-y_train.T\n",
      "    R2_IS_Lasso.append(1-np.var(err_IS.T)/np.var(y_train))\n",
      "    \n",
      "    p_OS=Lasso.predict(X_test)\n",
      "    err_OS=p_OS-y_test.T\n",
      "    R2_OS_Lasso.append(1-np.var(err_OS.T)/np.var(y_test))\n",
      "    \n",
      "    \n",
      "for i in Number_variables:\n",
      "    \n",
      "    Ridge=linear_model.Ridge(fit_intercept=True,alpha=3000)\n",
      "    Ridge.fit(X_train,y_train)\n",
      "    \n",
      "    p_OS=Ridge.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R2_OS_Ridge.append(1-np.var(err_OS)/np.var(y_test))\n",
      "    \n",
      "    p_IS=Ridge.predict(X_train)\n",
      "    err_IS=p_IS-y_train\n",
      "    R2_IS_Ridge.append(1-np.var(err_IS)/np.var(y_train))\n",
      "    \n",
      "figure(figsize=(10,10))\n",
      "plt.title(jupyter_string)\n",
      "plt.plot(Number_variables,OLS_R_2_OS_F,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,OLS_R_2_IS_F,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_OS_Lasso,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_IS_Lasso,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_OS_Ridge,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_IS_Ridge,jupyter_string,label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "interactions_from_selected_users_df[jupyter_string] = np.log(interactions_from_selected_users_df[jupyter_string] + 1)\n",
      "interactions_from_selected_users_df.head()\n",
      "=====\n",
      "def smooth_user_preference(x):\n",
      "    return math.log(1+x, 2)\n",
      "    \n",
      "interactions_full_df = interactions_from_selected_users_df \\\n",
      "                    .groupby(['personId' <<unk>>, 'contentId' <<unk>>])[jupyter_string].sum() \\\n",
      "                    .apply(smooth_user_preference).reset_index()\n",
      "print(jupyter_string % len(interactions_full_df))\n",
      "interactions_full_df.head(10)\n",
      "--------------------\n",
      "popularity_model = gl.popularity_recommender.create(interactions_full_indexed_df, user_id=jupyter_string, item_id=jupyter_string, target=jupyter_string)\n",
      "=====\n",
      "item_popularity_df = interactions_full_df.groupby('contentId' <<unk>>)[jupyter_string].sum().sort_values(ascending=False).reset_index()\n",
      "item_popularity_df.head(10)\n",
      "--------------------\n",
      "popularity_recommender = PopularityRecommender(item_popularity_df, items_df=interactions_full_df)\n",
      "popularity_recommender.get_model_name()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\n",
      "print(jupyter_string % pop_global_metrics)\n",
      "pop_detailed_results_df.head(10)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId' <<unk>>, \n",
      "                                                          columns='contentId' <<unk>>, \n",
      "                                                          values=jupyter_string).fillna(0)\n",
      "\n",
      "users_items_pivot_matrix_df.head(10)\n",
      "--------------------\n",
      "users_items_pivot_matrix = users_items_pivot_matrix_df.as_matrix()\n",
      "=====\n",
      "users_items_pivot_matrix = users_items_pivot_matrix_df.as_matrix()\n",
      "users_items_pivot_matrix[:10]\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pickle\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import gaussian_kde\n",
      "from scipy.spatial import distance\n",
      "from scipy.cluster import hierarchy\n",
      "\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas.tools.plotting as pdplt\n",
      "import seaborn as sns\n",
      "\n",
      "plt.show()\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "clr_plt = sns.color_palette()\n",
      "pd.options.display.float_format = jupyter_string.format\n",
      "--------------------\n",
      "ensemble = pd.read_csv(jupyter_string)\n",
      "ensemble.head()\n",
      "=====\n",
      "na_ensemble = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_ensemble.head()\n",
      "--------------------\n",
      "na_ensemble[jupyter_string] = (na_ensemble[jupyter_string] - na_ensemble[jupyter_string].mean()) / na_ensemble[jupyter_string].std()\n",
      "na_ensemble.head()\n",
      "=====\n",
      "p_ranges = pd.read_csv(jupyter_string, index_col='name' <<unk>>)\n",
      "\n",
      "p_ranges\n",
      "--------------------\n",
      "na_cluster = na_params[jupyter_string]\n",
      "na_cluster\n",
      "=====\n",
      "na_clusters = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_clusters.head()\n",
      "--------------------\n",
      "na_cluster_centers = na_clusters.mean(axis=1)\n",
      "na_cluster_centers.head()\n",
      "=====\n",
      "na_centroids = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_centroids.head()\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "na_labels = {\n",
      "    'P_0' <<unk>>: jupyter_string,\n",
      "    'K_r12' <<unk>>: jupyter_string,\n",
      "    'K_d' <<unk>>: jupyter_string,\n",
      "    'K_dd' <<unk>>: jupyter_string,\n",
      "    'K_g' <<unk>>: jupyter_string,\n",
      "    'h_init' madeupword0002: jupyter_string,\n",
      "    'misfit' <<unk>>: jupyter_string,\n",
      "    'D_mean' <<unk>>: jupyter_string,\n",
      "    'RMSD' <<unk>>: 'RMSD' <<unk>>,\n",
      "    'cluster' <<unk>>: 'cluster' <<unk>>\n",
      "}\n",
      "\n",
      "\n",
      "pd.options.display.float_format = jupyter_string.format\n",
      "\n",
      "\n",
      "pd.options.mode.chained_assignment = None\n",
      "\n",
      "\n",
      "cat_clr = [mpl.colors.rgb2hex(rgb)\n",
      "           for rgb in sns.color_palette(n_colors=20)]\n",
      "quant_cmap = sns.cubehelix_palette(start=.5, rot=-.75,\n",
      "                                   as_cmap=True, reverse=True)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(quant_cmap)\n",
      "=====\n",
      "def scatter_misfit_stats(dataset):\n",
      "    \"\"jupyter_string\"\"\n",
      "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "    col_pairs = (('RMSD' <<unk>>, 'D_mean' <<unk>>),\n",
      "                 ('RMSD' <<unk>>, 'misfit' <<unk>>),\n",
      "                 ('D_mean' <<unk>>, 'misfit' <<unk>>))\n",
      "\n",
      "    for (x, y), ax in zip(col_pairs, axes):\n",
      "        dataset.plot(kind=jupyter_string, x=x, y=y,\n",
      "                     ax=ax, alpha=0.2, c=jupyter_string)\n",
      "        plt.setp(ax, xlabel=na_labels[x],\n",
      "                 ylabel=na_labels[y])\n",
      "\n",
      "    fig.tight_layout()\n",
      "    return fig, axes\n",
      "\n",
      "fig, axes = scatter_misfit_stats(na_ensemble)\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "\n",
      "for (x, y), ax in zip(col_pairs, axes):\n",
      "    dataset.plot(kind=jupyter_string, x=x, y=y,\n",
      "                 ax=ax, alpha=0.2, c=jupyter_string)\n",
      "    plt.setp(ax, xlabel=na_labels[x],\n",
      "             ylabel=na_labels[y])\n",
      "\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "for col, ax in zip(['D_mean' <<unk>>, 'RMSD' <<unk>>, 'misfit' <<unk>>], axes.flatten()):\n",
      "    trace = na_ensemble.get(col)\n",
      "    na_ensemble.plot(kind=jupyter_string, x='NA_iter' <<unk>>, y=col, ax=ax,\n",
      "                     s=50, alpha=0.15, color=jupyter_string)\n",
      "    plt.setp(ax, xlim=[-1, na_params[jupyter_string]])\n",
      "    plt.setp(ax, ylabel=na_labels[col], xlabel=jupyter_string)\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "from sklearn.metrics import adjusted_rand_score\n",
      "from sklearn.metrics import adjusted_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_\n",
      "=====\n",
      "misfit_max = -5.4\n",
      "\n",
      "drop_cols = ['misfit' <<unk>>, 'D_mean' <<unk>>, 'RMSD' <<unk>>]\n",
      "\n",
      "\n",
      "\n",
      "na_misfit_sel = na_ensemble.misfit < misfit_max\n",
      "na_subset = na_ensemble[na_misfit_sel]\n",
      "na_subset.drop('NA_iter' <<unk>>, axis=1, inplace=True)\n",
      "na_subset_log = log_df(na_subset)\n",
      "na_subset_norm = norm_df(na_subset_log)\n",
      "na_subset_norm_p = na_subset_norm.drop(drop_cols, axis=1)\n",
      "\n",
      "\n",
      "dist_matrix = distance.cdist(na_subset_norm_p, na_subset_norm_p)\n",
      "\n",
      "\n",
      "link_matrix = hierarchy.linkage(distance.squareform(dist_matrix),\n",
      "                                method=jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(10, 11))\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "dendrogram(\n",
      "    link_matrix,\n",
      "    truncate_mode=jupyter_string, \n",
      ")\n",
      "plt.show()\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
      "\n",
      "\n",
      "dist_tresh = 0.6  \n",
      "\n",
      "cat_clr = [jupyter_string] * 5\n",
      "hierarchy.set_link_color_palette(cat_clr)\n",
      "\n",
      "dendrogram = hierarchy.dendrogram(link_matrix,\n",
      "                                  color_threshold=dist_tresh,\n",
      "                                  labels=na_subset_norm.index,\n",
      "                                  orientation=jupyter_string, ax=axes[1])\n",
      "\n",
      "plt.setp(axes[1], xlabel=jupyter_string,\n",
      "         ylabel=jupyter_string, xticklabels=[])\n",
      "\n",
      "\n",
      "na_clusters_log = log_df(na_clusters)\n",
      "na_clusters_lognorm = norm_df(na_clusters_log)\n",
      "na_centroids_lognorm = na_clusters_lognorm.groupby('cluster' <<unk>>).mean()\n",
      "\n",
      "cluster_markers = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "cat_clr = [jupyter_string] * 5\n",
      " \n",
      "pdplt.parallel_coordinates(\n",
      "    na_clusters_lognorm.sort('cluster' <<unk>>), 'cluster' <<unk>>,\n",
      "    alpha=0.4, color=cat_clr, ax=axes[0],\n",
      "    axvlines=False, xticks=range(9)\n",
      ")\n",
      "axes[0].legend_.remove()\n",
      "axes[0].grid(False)\n",
      "\n",
      "lines = []\n",
      "for c, ctr in enumerate(na_centroids_lognorm.values):\n",
      "    l, = axes[0].plot(ctr, linewidth=1.5,\n",
      "                 label=jupyter_string.format(c + 1),\n",
      "                 marker=cluster_markers[c], markersize=13,\n",
      "                 markerfacecolor=jupyter_string, color=jupyter_string)\n",
      "    lines.append(l)\n",
      "\n",
      "leg = axes[0].legend(handles=lines, loc=jupyter_string, ncol=3,\n",
      "                     frameon=True, labelspacing=1.3)\n",
      "leg.get_frame().set_color(jupyter_string)\n",
      "\n",
      "for x in range(9):\n",
      "    axes[0].axvline(x=x, color=jupyter_string, linestyle=jupyter_string)\n",
      "\n",
      "xlbls = [jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "         jupyter_string, jupyter_string, jupyter_string, jupyter_string, 'RMSD' <<unk>>,]\n",
      "plt.setp(axes[0], ylabel=jupyter_string,\n",
      "         xticklabels=xlbls, ylim=[-1, 1])\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_city_data = pd.read_csv(jupyter_string)\n",
      "df_global_data = pd.read_csv(jupyter_string)\n",
      "df_city_list = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.corr()\n",
      "=====\n",
      "c_pune.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.ticker as ticker\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "\n",
      "plt.show()  \n",
      "--------------------\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib as mpl\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "mu = 3.35294117647\n",
      "variance = 0.485071250073\n",
      "sigma = math.sqrt(variance)\n",
      "x = np.linspace(mu - 5*sigma, mu + 5*sigma, 100)\n",
      "plt.plot(x,mlab.normpdf(x, mu, sigma))\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_sgd = pd.read_csv(jupyter_string, header=0, index_col='DATE' <<unk>>)\n",
      "df_jpy = pd.read_csv(jupyter_string, header=0, index_col='DATE' <<unk>>)\n",
      "--------------------\n",
      "df_sgd = df_sgd.fillna(method=jupyter_string)\n",
      "df_jpy = df_jpy.fillna(method=jupyter_string)\n",
      "=====\n",
      "df_sgd = df_sgd[jupyter_string:]\n",
      "df_sgd.DEXSIUS = pd.to_numeric(df_sgd.DEXSIUS, errors=jupyter_string)\n",
      "df_sgd = df_sgd.fillna(method=jupyter_string)\n",
      "--------------------\n",
      "df_sgd.info()\n",
      "=====\n",
      "df_jpy = df_jpy[jupyter_string:]\n",
      "df_jpy.DEXJPUS = pd.to_numeric(df_jpy.DEXJPUS, errors=jupyter_string)\n",
      "df_jpy = df_jpy.fillna(method=jupyter_string)\n",
      "--------------------\n",
      "df_sgd = df_sgd.dropna()\n",
      "df_jpy = df_jpy.dropna()\n",
      "=====\n",
      "df = df_sgd.join(df_jpy, how=jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string].plot(title=jupyter_string)\n",
      "=====\n",
      "df_pc.plot()\n",
      "--------------------\n",
      "df_sgd.plot()\n",
      "=====\n",
      "df_pc.describe()\n",
      "--------------------\n",
      "from scipy import stats\n",
      "stats.probplot(df_pc, dist=jupyter_string, plot=pylab)\n",
      "pylab.show()\n",
      "=====\n",
      "fig=plt.figure(figsize=(19, 8), dpi= 80, facecolor=jupyter_string, edgecolor=jupyter_string)\n",
      "plt.subplot(1, 3, 1)\n",
      "plt.plot(df_pc['DEXJPUS' madeupword0002])\n",
      "plt.subplot(1, 3, 2)\n",
      "plt.plot(df_pc['DEXSIUS' <<unk>>])\n",
      "plt.subplot(1, 3, 3)\n",
      "plt.plot(df_pc[jupyter_string])\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "y_pred = regressor.predict(X_test)\n",
      "\n",
      "print(jupyter_string, regressor.score(X_test, y_test))\n",
      "print(jupyter_string, r2_score(y_test, y_pred))\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data_train=pd.read_csv(jupyter_string)\n",
      "data_test=pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "data_train.drop('Unnamed: 0' <<unk>>,axis=1, inplace=True)\n",
      "data_test.drop('Unnamed: 0' <<unk>>,axis=1, inplace=True)\n",
      "--------------------\n",
      "df_pc.corr()\n",
      "=====\n",
      "print(jupyter_string.format(df_pc['DEXJPUS' madeupword0002].corr(df_pc['DEXSIUS' <<unk>>])))\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.scatter(x=df_pc['DEXJPUS' madeupword0002], y=df_pc['DEXSIUS' <<unk>>])\n",
      "--------------------\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "=====\n",
      "df_pc.index = pd.to_datetime(df_pc.index)\n",
      "df_pc_weekly = df_pc.dropna().resample(rule=jupyter_string).last()\n",
      "df_pc_weekly.head()\n",
      "--------------------\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot\n",
      "=====\n",
      "df_pc_weekly['DEXSIUS' <<unk>>].autocorr()\n",
      "--------------------\n",
      "df_pc_monthly[jupyter_string].autocorr()\n",
      "=====\n",
      "df_pc_monthly['DEXSIUS' <<unk>>].autocorr()\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.legend()\n",
      "\n",
      "=====\n",
      "df_std_by_months = df_pc.dropna().groupby(index_months).std()\n",
      "df_std_by_months\n",
      "--------------------\n",
      "df_std_by_months.plot()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "--------------------\n",
      "df_std_by_months = df_pc.dropna().groupby(index_months).std()\n",
      "_ = df_std_by_months.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "=====\n",
      "df_std_by_months_years = df_pc.dropna().groupby([df_pc.dropna().index.year, df_pc.dropna().index.month]).std()\n",
      "\n",
      "_ = df_std_by_months_years.loc[2000].plot(title=jupyter_string)\n",
      "--------------------\n",
      "_ = df_std_by_months_years.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months_years.loc[2001].plot(title=jupyter_string)\n",
      "--------------------\n",
      "_ = df_std_by_months_years.loc[2000].plot(title=jupyter_string)\n",
      "_ = df_std_by_months_years.loc[2001].plot(title=jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months_years.loc[2002].plot(title=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "sample_mean = np.mean(df.temperature)\n",
      "sample_std = np.std(df.temperature)\n",
      "sample_size = len(df.temperature)\n",
      "sample_mean, sample_std, sample_size\n",
      "=====\n",
      "conf_int = np.percentile(df.temperature,[2.5,97.5])\n",
      "conf_int\n",
      "--------------------\n",
      "data_test.head()\n",
      "=====\n",
      "data_test.head()\n",
      "--------------------\n",
      "temp_mean = np.mean(df.temperature)\n",
      "temp_std = np.std(df.temperature)\n",
      "temp_std\n",
      "=====\n",
      "mean = df['temperature' <<unk>>].mean()\n",
      "std = df['temperature' <<unk>>].std()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "training_data = pd.read_csv(jupyter_string)\n",
      "training_data['Well Name' <<unk>>] = training_data['Well Name' <<unk>>].astype(jupyter_string)\n",
      "training_data['Formation' <<unk>>] = training_data['Formation' <<unk>>].astype(jupyter_string)\n",
      "\n",
      "blind = pd.read_csv(jupyter_string)\n",
      "blind['Well Name' <<unk>>] = blind['Well Name' <<unk>>].astype(jupyter_string)\n",
      "blind['Formation' <<unk>>] = blind['Formation' <<unk>>].astype(jupyter_string)\n",
      "--------------------\n",
      "y_pred = V_classifier.fit(X_train, y_train).predict(X_test)\n",
      "=====\n",
      "y_blind = V_classifier.fit(X, y).predict(X_blind) \n",
      "blind['Facies' <<unk>>] = y_blind\n",
      "blind.to_csv(jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "world_firearms = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "world_firearms.head()\n",
      "=====\n",
      "world_firearms.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "=====\n",
      "X_train = data_train.iloc[:,:40]\n",
      "\n",
      "y_train = data_train.iloc[:,40:]\n",
      "\n",
      "X_test = data_train.iloc[:,:40]\n",
      "\n",
      "y_test = data_train.iloc[:,40:]\n",
      "--------------------\n",
      "world_Firearms = world_Firearms.dropna()\n",
      "world_Firearms.head()\n",
      "=====\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "\n",
      "\n",
      "world_firearms = world_firearms.dropna(axis=0, how=jupyter_string)\n",
      "--------------------\n",
      "world_firearms.head()\n",
      "=====\n",
      "world_firearms.to_csv(jupyter_string)\n",
      "--------------------\n",
      "deaths = pd.read_csv(jupyter_string)\n",
      "deaths.head()\n",
      "=====\n",
      "law_score = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "law_score.head()\n",
      "=====\n",
      "law_score.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "population = pd.read_csv(jupyter_string)\n",
      "gun_ownership = pd.read_csv(jupyter_string)\n",
      "\n",
      "gun_ownership = population.merge(gun_ownership, left_on=jupyter_string, right_on='Location' madeupword0002, how=jupyter_string)\n",
      "--------------------\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "=====\n",
      "gun_ownership.head()\n",
      "--------------------\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "gun_ownership.drop('Location' madeupword0002, axis=1, inplace=True)\n",
      "\n",
      "gun_ownership['Gun Ownership by April 2017' <<unk>>] = gun_ownership['Gun Ownership by April 2017' <<unk>>].astype(int)\n",
      "\n",
      "gun_ownership[jupyter_string] = gun_ownership['Gun Ownership by April 2017' <<unk>>] / gun_ownership[\n",
      "    jupyter_string] * 1000\n",
      "\n",
      "gun_ownership[jupyter_string]=pd.qcut(gun_ownership[jupyter_string], 3, labels=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "gun_ownership.to_csv(jupyter_string)\n",
      "--------------------\n",
      "data[jupyter_string] = data[jupyter_string].shift(-1)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-2)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-3)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-4)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-5)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-6)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-7)\n",
      "=====\n",
      "pd.options.mode.chained_assignment = None\n",
      "\n",
      "for i in range(1, len(data)):\n",
      "    if data[jupyter_string].iloc[i] == 1:\n",
      "        data[jupyter_string].iloc[i] = data['Adj Close' <<unk>>].iloc[i] - data['Adj Close' <<unk>>].iloc[i-1]\n",
      "    elif data[jupyter_string].iloc[i] == -1:\n",
      "        data[jupyter_string].iloc[i] = - (data['Adj Close' <<unk>>].iloc[i] - data['Adj Close' <<unk>>].iloc[i-1])\n",
      "\n",
      "total_gains = data[jupyter_string].sum()\n",
      "print(total_gains)\n",
      "        \n",
      "--------------------\n",
      "Lasso=linear_model.Lasso(fit_intercept=False, alpha=30) \n",
      "\n",
      "\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "=====\n",
      "Lasso=linear_model.Lasso(fit_intercept=False,alpha=30)\n",
      "\n",
      "\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train.T\n",
      "\n",
      "R_2_IS_Lasso=1-np.var(err_IS.T)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test.T\n",
      "R_2_OS_Lasso=1-np.var(err_OS.T)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "\n",
      "--------------------\n",
      "dates = pd.date_range(start=jupyter_string, end=jupyter_string, freq=jupyter_string)\n",
      "df = get_data(symbol, dates)\n",
      "df.head()\n",
      "=====\n",
      "dates = pd.date_range(jupyter_string, jupyter_string)\n",
      "symbol = jupyter_string\n",
      "data = get_data(symbol, dates)\n",
      "data.head()\n",
      "    \n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "df_feature = pd.read_csv(jupyter_string, index_col = 0)\n",
      "df_genre = pd.read_csv(jupyter_string, index_col = 0)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_train = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df_train.head()\n",
      "=====\n",
      "df_train.head()\n",
      "--------------------\n",
      "sns.distplot(df_train[jupyter_string]);\n",
      "=====\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "df_train.plot.box()\n",
      "=====\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(8, 6))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "--------------------\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(8, 6))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "=====\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(16, 8))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "plt.xticks(rotation=90);\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import utils\n",
      "\n",
      "df = utils.read_csv(jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "corrmat = df_train.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat, vmax=.8, square=True);\n",
      "=====\n",
      "corrmat = df_train.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat, vmax=.8, square=True);\n",
      "--------------------\n",
      "df_train.isnull().sum()\n",
      "=====\n",
      "missing_features = df_train.isnull().sum()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "names_allbp =[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "data_allbp = pd.read_csv(jupyter_string, names=names_allbp, index_col=False, na_values=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "--------------------\n",
      "data_allbp.head()\n",
      "=====\n",
      "data_allbp.drop(jupyter_string, axis = 1, inplace = True)\n",
      "data_allbp.head()\n",
      "\n",
      "--------------------\n",
      "data_allbp.info()\n",
      "=====\n",
      "data_allbp[jupyter_string] = data_allbp[jupyter_string].apply( lambda x: x.split(jupyter_string)[0])\n",
      "data_allbp[jupyter_string].value_counts()\n",
      "\n",
      "\n",
      "--------------------\n",
      "data_allhyper = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "names_allhyper =[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "data_allhyper = pd.read_csv(jupyter_string, names=names_allhyper, index_col=False, na_values=jupyter_string)\n",
      "data_allhyper.head()\n",
      "--------------------\n",
      "data_allhyper = pd.read_csv(jupyter_string, index_col=False, na_values=jupyter_string)\n",
      "data_allhyper.head()\n",
      "=====\n",
      "names_new_thyroid = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "data_new_thyroid = pd.read_csv(jupyter_string, names = names_new_thyroid, index_col = False)\n",
      "\n",
      "data_new_thyroid.head()\n",
      "--------------------\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X = df[[\"flarea\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "fig.tight_layout()\n",
      "\n",
      "\n",
      "def h(x, beta1):\n",
      "    return x * beta1\n",
      "\n",
      "\n",
      "\n",
      "beta1 = 8\n",
      "\n",
      "\n",
      "ax0.scatter(X, y, color = jupyter_string)\n",
      "\n",
      "\n",
      "xvals = np.linspace(-500, 500, 3)\n",
      "ax0.plot(xvals, h(xvals, beta1), color = jupyter_string)\n",
      "\n",
      "\n",
      "ax1.scatter(beta1, J(X, y, beta1), color = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X = df[[\"flarea\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "\n",
      "=====\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "fig.tight_layout()\n",
      "\n",
      "\n",
      "beta1s = np.linspace(-10, 10, 21)\n",
      "\n",
      "\n",
      "ax0.scatter(X, y, color = jupyter_string)\n",
      "\n",
      "\n",
      "xvals = np.linspace(-500, 500, 3)\n",
      "for beta1 in beta1s:\n",
      "    ax0.plot(xvals, h(xvals, beta1), color = jupyter_string)\n",
      "\n",
      "\n",
      "ax1.scatter(beta1s, [J(X, y, beta1) for beta1 in beta1s], color = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df[jupyter_string].mean()\n",
      "=====\n",
      "stacked = df.stack().reset_index()\n",
      "stacked.columns = [jupyter_string, jupyter_string, jupyter_string]\n",
      "stacked.head()\n",
      "--------------------\n",
      "fig = plt.figure() \n",
      "ax = Axes3D(fig)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_zlabel(jupyter_string)\n",
      "xvals = np.linspace(-100, 200, 301)\n",
      "yvals = np.linspace(-100, 200, 301)\n",
      "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
      "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
      "zvals = zs.reshape(xxvals.shape)\n",
      "ax.plot_surface(xxvals, yyvals, zvals)\n",
      "plt.show()\n",
      "=====\n",
      "fig = plt.figure(figsize = (6, 6)) \n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "xvals = np.linspace(-100, 200, 301)\n",
      "yvals = np.linspace(-100, 200, 301)\n",
      "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
      "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
      "zvals = zs.reshape(xxvals.shape)\n",
      "C = plt.contour(xxvals, yyvals, zvals, 15, colors = jupyter_string, linewidth = 0.5)\n",
      "plt.clabel(C, inline=1, fontsize=10)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X_without_dummy = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "X = add_dummy_feature(X_without_dummy)\n",
      "\n",
      "\n",
      "beta = npla.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X_without_dummy = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "X = add_dummy_feature(X_without_dummy)\n",
      "\n",
      "\n",
      "beta = nplt.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "=====\n",
      "beta = npla.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, index_col='id' <<unk>>)\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = res\n",
      "importances_bal = imps\n",
      "=====\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "res, imps = test_classification(df_routeset)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "=====\n",
      "res, imps = test_classification(df_routesets)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "df = df.drop(jupyter_string, axis=1)\n",
      "df.head()\n",
      "=====\n",
      "df.drop('trust_value' <<unk>>, axis=1, inplace=True)\n",
      "df.shape  \n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "REsample1 = pd.read_csv(jupyter_string)\n",
      "REsample1.head()\n",
      "--------------------\n",
      "base = pd.pivot_table(stacked, index=jupyter_string, columns=jupyter_string, values=jupyter_string, aggfunc=jupyter_string)\n",
      "base.head()\n",
      "=====\n",
      "mean_power = df.mean()\n",
      "mean_power.head()\n",
      "--------------------\n",
      "results_bal.to_csv(jupyter_string)\n",
      "importances_bal.to_csv(jupyter_string)\n",
      "=====\n",
      "results_unb[jupyter_string] = False\n",
      "results_bal[jupyter_string] = True\n",
      "results = results_unb.append(results_bal, ignore_index=True)\n",
      "--------------------\n",
      "plt.plot(results[jupyter_string], results[jupyter_string], jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "results.Accuracy.plot(kind=jupyter_string)\n",
      "=====\n",
      "pal = sns.light_palette(jupyter_string, n_colors=3, reverse=True)\n",
      "g = sns.factorplot(data=results, x=jupyter_string, y=jupyter_string, hue=jupyter_string, col=jupyter_string,\n",
      "                   kind=jupyter_string, palette=pal, aspect=1.2, errwidth=1, capsize=0.04)\n",
      "g.set(ylim=(88, 98))\n",
      "--------------------\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_routes = res\n",
      "importances_routes = imps\n",
      "=====\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "res, imps = test_classification(df_routeset)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "=====\n",
      "res, imps = test_classification(df_routesets)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "buildings = pd.read_csv(jupyter_string)\n",
      "buildings.head()\n",
      "=====\n",
      "df_buildings.label.value_counts()\n",
      "--------------------\n",
      "df_routes = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_routes.label.value_counts()\n",
      "--------------------\n",
      "df_routes.head()\n",
      "=====\n",
      "df_routesets.label.value_counts()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
      "\n",
      "reg = LinearRegression()\n",
      "reg.fit(X_train, Y_train)\n",
      "Y_pred = reg.predict(X_test)\n",
      "\n",
      "print(jupyter_string, reg.score(X_test, Y_test))\n",
      "print(jupyter_string, mean_squared_error(Y_test, Y_pred))\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Ypred = X.dot(coefs)\n",
      "    erro = Y - Ypred\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "mean_power.describe()\n",
      "=====\n",
      "mean_power = mean_power.reset_index()\n",
      "mean_power\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None, delim_whitespace=True)\n",
      "\n",
      "dfTrain = df[ :30]\n",
      "dfTest = df[30 : ]\n",
      "X = dfTrain.values[ : , : -1]\n",
      "n = np.ones(len(X))\n",
      "X = np.c_[n, X]\n",
      "Y = dfTrain.values[ : , -1]\n",
      "\n",
      "XTest = dfTest.values[ : , : -1]\n",
      "n = np.ones(len(XTest))\n",
      "XTest = np.c_[n, XTest]\n",
      "YTest = dfTest.values[ : , -1]\n",
      "\n",
      "--------------------\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Ypred = X.dot(coefs)\n",
      "    erro = Y - Ypred\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Yhat = XTest.dot(coefs)\n",
      "    erro = YTest - Yhat\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "coefs_OLS = coefs_OLS(X_train, Y_train, 1)\n",
      "print(jupyter_string, coefs_OLS)\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5])\n",
      "coef = coefs_OLS(X, Y , lambdas)\n",
      "print(coef)\n",
      "--------------------\n",
      "goldenpercent.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "--------------------\n",
      "df = pd.read_csv(goldencsvpath)\n",
      "df.head()\n",
      "=====\n",
      "usertweetinfo = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "usertweetinfo2 = usertweetinfo[['User' <<unk>>,'Golden trust' <<unk>>,'Majority trust' <<unk>>]]\n",
      "usertweetinfo2[17:19]\n",
      "--------------------\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "=====\n",
      "goldenpercent1.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "majority_report = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "majority_report.head()\n",
      "=====\n",
      "majoritypercent = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "\n",
      "majoritypercent[:5]\n",
      "--------------------\n",
      "goldenpercent1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "--------------------\n",
      "mean_power = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "housedf = pd.read_csv(jupyter_string)\n",
      "housedf.head()\n",
      "--------------------\n",
      "majoritypercent[jupyter_string] = 0\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 1, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 3, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 5, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 7, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 9, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 10, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 11, jupyter_string] = jupyter_string\n",
      "majoritypercent[:10]\n",
      "=====\n",
      "majoritypercent[jupyter_string]=0\n",
      "majoritypercent.FullAgreement = ((majoritypercent.Tristeza == 1) | (majoritypercent.CÃ³lera == 1) | (majoritypercent.NoEmocion == 1) | (majoritypercent.Asco == 1) | (majoritypercent.Miedo == 1) | (majoritypercent.Sorpresa == 1) | (majoritypercent.Felicidad == 1)).astype(int)\n",
      "majoritypercent[:5]\n",
      "--------------------\n",
      "majority1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=majority1)\n",
      "--------------------\n",
      "majority1[jupyter_string].value_counts()\n",
      "=====\n",
      "majority1.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "majority1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=majority1)\n",
      "--------------------\n",
      "golden = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "goldenpercent = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "goldenpercent[jupyter_string] = 0\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 1, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 3, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 5, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 7, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 9, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 10, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 11, jupyter_string] = jupyter_string\n",
      "goldenpercent[:10]\n",
      "=====\n",
      "goldenpercent[jupyter_string]=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "goldenpercent = goldenpercent[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "goldenpercent[:5]\n",
      "=====\n",
      "goldenpercent[jupyter_string]=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "--------------------\n",
      "df1 = pd.DataFrame({\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "})\n",
      "df1\n",
      "=====\n",
      "pd.merge(mean_power, housedf).head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "rad_recording01162018 = pd.read_csv(jupyter_string, header=None )\n",
      "rad_recording01162018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,rad_recording01162018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "rad_recording01162018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "rad_recording01162018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,rad_recording01162018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Baseline02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "Baseline02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Baseline02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Baseline02042018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "Baseline02042018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Baseline02042018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StreamingLTE02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "StreamingLTE02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,StreamingLTE02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02062018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02062018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02062018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02062018v3 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02062018v3.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02062018v3.shape, jupyter_string)\n",
      "\n",
      "--------------------\n",
      "baseline02042018 = Baseline02042018.head(5)\n",
      "baseline02042018.head()\n",
      "=====\n",
      "Baseline02042018v2.head()\n",
      "--------------------\n",
      "baseline02042018v2.info()\n",
      "=====\n",
      "from matplotlib import pyplot\n",
      "import seaborn as sns; sns.set(color_codes=True)\n",
      "--------------------\n",
      "pyplot.plot(Baseline02042018v2[jupyter_string])\n",
      "pyplot.show()\n",
      "=====\n",
      "plt.show()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.tsplot(ax=ax, data=Baseline02042018v2[jupyter_string], ci=[68, 95], color=jupyter_string)\n",
      "--------------------\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], color=jupyter_string)\n",
      "=====\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "--------------------\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Baseline02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "b3_sample = rad_recording01162018v2[jupyter_string]\n",
      "print(stats.describe(b3_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "print(stats.ttest_ind(b1_sample, b3_sample, equal_var = False) )\n",
      "print(stats.ttest_ind(b2_sample, b3_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b3_sample, ax=ax, hist=False, rug=True)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "b3_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b3_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "print(stats.ttest_ind(b1_sample, b3_sample, equal_var = False) )\n",
      "print(stats.ttest_ind(b2_sample, b3_sample, equal_var = False) )\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = StreamingLTE02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = StreamingLTE02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Detector02062018v2[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "=====\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "    jupyter_string, header=None,\n",
      "    names=[\n",
      "        jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string\n",
      "    ])\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "pd.merge(mean_power, housedf, left_on=jupyter_string, right_on=jupyter_string)\n",
      "=====\n",
      "pd.merge(mean_power, housedf, left_on=jupyter_string, right_on='triplex_meter' madeupword0002).head()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "=====\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train_std = sc.fit_transform(X_train)\n",
      "X_test_std = sc.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "pca = PCA(0.95)\n",
      "X_train_pca = pca.fit_transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_pca, y_train)\n",
      "print(jupyter_string, lr.score(X_train_pca, y_train))\n",
      "print(jupyter_string, lr.score(X_test_pca, y_test))\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "pca = PCA(n_components=num_components)\n",
      "\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "\n",
      "lr_pca = LogisticRegression().fit(X_train_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_pca.predict(X_train_pca)),\n",
      "    accuracy_score(y_test, lr_pca.predict(X_test_pca))))    \n",
      "\n",
      "\n",
      "lr = LogisticRegression().fit(X_train_std, y_train)\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr.predict(X_train_std)),\n",
      "    accuracy_score(y_test, lr.predict(X_test_std))))    \n",
      "\n",
      "--------------------\n",
      "X_train_std = np.copy(X_train)\n",
      "X_train_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
      "X_train_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
      "=====\n",
      "my_pca = MyPCA(n_components=num_components)\n",
      "\n",
      "X_train_my_pca = my_pca.fit_transform(X_train_std)\n",
      "X_test_my_pca = my_pca.transform(X_test_std)\n",
      "\n",
      "lr_my_pca = LogisticRegression().fit(X_train_my_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_my_pca.predict(X_train_my_pca)),\n",
      "    accuracy_score(y_test, lr_my_pca.predict(X_test_my_pca))))    \n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X_train)\n",
      "X_train_pca = pca.transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "\n",
      "print(jupyter_string, X_train_pca.shape)\n",
      "print(jupyter_string, X_test_pca.shape)\n",
      "=====\n",
      "my_svd_pca = MySVDPCA(n_components=num_components)\n",
      "\n",
      "X_train_my_svd_pca = my_pca.fit_transform(X_train_std)\n",
      "X_test_my_svd_pca = my_pca.transform(X_test_std)\n",
      "\n",
      "lr_my_svd_pca = LogisticRegression().fit(X_train_my_svd_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_my_svd_pca.predict(X_train_my_svd_pca)),\n",
      "    accuracy_score(y_test, lr_my_svd_pca.predict(X_test_my_svd_pca))))    \n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "\n",
      "lr_pca = LogisticRegression().fit(X_train_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(accuracy_score(y_train, lr_pca.predict(X_train_pca)), accuracy_score(y_test, lr_pca.predict(X_test_pca))))\n",
      "=====\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "faces_dataset = fetch_olivetti_faces(shuffle=True, random_state=0)\n",
      "faces = faces_dataset.images.reshape(len(faces_dataset.images), -1)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
      "init_notebook_mode(connected=True)\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "plt.show()\n",
      "\n",
      "co2_ice = pd.read_csv(jupyter_string)\n",
      "\n",
      "s = co2_ice.plot(x='ice Age' <<unk>>, y='CO2 ppmv' <<unk>>)\n",
      "--------------------\n",
      "fig = sns_plot.get_figure()\n",
      "fig.savefig(jupyter_string)\n",
      "=====\n",
      "import pdvega\n",
      "\n",
      "co2_mlo.vgplot.line(x='decimal' <<unk>>, y='ppm' <<unk>>, alpha=0.5)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_train_val = pd.read_csv(jupyter_string)\n",
      "n,m = df_train_val.shape\n",
      "n_val = int(n*0.8)\n",
      "df_val = df_train_val.iloc[n_val:,:]\n",
      "df_train = df_train_val.iloc[:n_val-1:]\n",
      "print(df_train.shape,df_val.shape,df_test.shape)\n",
      "FOG_weight = (len(df_train)/np.sum(df_train.iloc[:,0]))\n",
      "--------------------\n",
      "sample_weight = df_train.iloc[:,0] * FOG_weight +1\n",
      "\n",
      "svm = SVC(C=1, kernel=jupyter_string)\n",
      "svm.fit(df_train.iloc[:,1:77],df_train.iloc[:,0])\n",
      "y_val_pred_svm = svm.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_svm, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_svm)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_svm = svm.predict(df_train_val.iloc[:,1:77])\n",
      "=====\n",
      "from sklearn import svm\n",
      "\n",
      "svml = svm.LinearSVC(class_weight=jupyter_string)\n",
      "svml.fit(df_train.iloc[:,1:77], df_train.iloc[:,0])\n",
      "\n",
      "y_val_pred_svm = svml.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_svm, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_svm)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_svm = svml.predict(df_train_val.iloc[:,1:77])\n",
      "--------------------\n",
      "from sklearn import tree\n",
      "\n",
      "dt = tree.DecisionTreeClassifier(class_weight=jupyter_string)\n",
      "dt.fit(df_train.iloc[:,1:77], df_train.iloc[:,0])\n",
      "\n",
      "y_val_pred_dt = dt.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dt, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dt)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_dt = dt.predict(df_train_val.iloc[:,1:77])\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "min_s_l_options = np.arange(0.1,0.5,0.025)\n",
      "max_depth_options = np.arange(1,11)\n",
      "max_features_option = np.arange(10,76,6)\n",
      "parameters = {jupyter_string: min_s_l_options, jupyter_string:max_depth_options} \n",
      "dtc = DecisionTreeClassifier(criterion = jupyter_string, class_weight = jupyter_string)\n",
      "dtc_cv = GridSearchCV(dtc, parameters,scoring=geo_cost)\n",
      "dtc_cv.fit(df_train.iloc[:,1:77],df_train.iloc[:,0])\n",
      "\n",
      "--------------------\n",
      "y_val_pred_dtc = dtc_cv.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dtc, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dtc)\n",
      "print(jupyter_string.format(cost))\n",
      "=====\n",
      "scores = dtc_cv.cv_results_[jupyter_string]\n",
      "scores = scores.reshape(len(min_s_l_options), len(max_depth_options))\n",
      "plt.imshow(scores)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "y_val_pred_dtc = dtc_cv.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dtc, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dtc)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_dtc_cv = dtc_cv.predict(df_train_val.iloc[:,1:77])\n",
      "--------------------\n",
      "y_tv_log_en_cv = log_en.predict(df_train_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_train_val.iloc[:,0], y_tv_log_en_cv, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_train_val.iloc[:,0], y_tv_log_en_cv)\n",
      "print(jupyter_string.format(cost))\n",
      "=====\n",
      "y_pred_log = log.predict(df_test.iloc[:,1:77])\n",
      "y_pred_svm = svml.predict(df_test.iloc[:,1:77])\n",
      "y_pred_dtc = dtc_cv.predict(df_test.iloc[:,1:77])\n",
      "\n",
      "X_test = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis]))\n",
      "y_tv_log_en = log_en.predict(X_test)\n",
      "\n",
      "\n",
      "print(classification_report(df_test.iloc[:,0], y_tv_log_en, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_test.iloc[:,0], y_tv_log_en)\n",
      "print(jupyter_string.format(cost))\n",
      "--------------------\n",
      "y_pred_log = log.predict(df_train.iloc[:,1:77])\n",
      "y_pred_svm = svml.predict(df_train.iloc[:,1:77])\n",
      "y_pred_dtc = dtc_cv.predict(df_train.iloc[:,1:77])\n",
      "\n",
      "X_train = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis]))\n",
      "=====\n",
      "list_of_tv_pred_train = (y_tv_pred_log[:,np.newaxis],y_tv_pred_svm[:,np.newaxis],y_tv_pred_dtc_cv[:,np.newaxis],y_tv_log_en_train[:,np.newaxis])\n",
      "list_of_tv_pred_test =(y_pred_log[:,np.newaxis] , y_pred_svm[:,np.newaxis], y_pred_dtc[:,np.newaxis], y_tv_log_en[:,np.newaxis] )\n",
      "\n",
      "cost_list_tv = list()\n",
      "cost_list_test = list()\n",
      "for i, tv_pred in enumerate(list_of_tv_pred_train):\n",
      "    cost_list_tv.append( geomean_cost(df_train_val.iloc[:,0], tv_pred))\n",
      "    cost_list_test.append( geomean_cost(df_test.iloc[:,0], list_of_tv_pred_test[i]))\n",
      "    \n",
      "    \n",
      "width = 0.8       \n",
      "\n",
      "ind_train = np.array([1,4,7,10])    \n",
      "ind_test = np.array([2,5,8,11])    \n",
      "\n",
      "p1 = plt.bar(ind_train,np.array(cost_list_tv), width, color=jupyter_string)\n",
      "p2 = plt.bar(ind_test,np.array(cost_list_test), width, color=jupyter_string)\n",
      "\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xticks(ind_train+0.5, (jupyter_string, jupyter_string, jupyter_string, jupyter_string))\n",
      "plt.yticks(np.arange(.65, .85, 0.05))\n",
      "plt.ylim(0.65,.85)\n",
      "plt.legend((p1[0], p2[0]), (jupyter_string, jupyter_string), loc = jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClass\n",
      "=====\n",
      "plt.scatter(np.arange(svml.coef_.shape[1]),np.abs(svml.coef_[0,:]))\n",
      "plt.plot(np.array([0,80]),np.array([1,1]))\n",
      "plt.show()\n",
      "\n",
      "extra_features = np.abs(svml.coef_) > 1\n",
      "print(df_train_val.iloc[:,1:77].columns.values[extra_features[0]])\n",
      "\n",
      "\n",
      "log_enp = LogisticRegression(solver=jupyter_string, max_iter= 200)\n",
      "X_train_plus = np.hstack((\n",
      "        np.hstack((y_tv_pred_log[:,np.newaxis],y_tv_pred_svm[:,np.newaxis],y_tv_pred_dtc_cv[:,np.newaxis])),\n",
      "        df_train_val.iloc[:,extra_features[0]]))\n",
      "\n",
      "sample_weight = df_train_val.iloc[:,0] * FOG_weight +1\n",
      "log_enp.fit(X_train_plus, df_train_val.iloc[:,0], sample_weight)\n",
      "y_tv_log_enp_train = log_enp.predict(X_train_plus)\n",
      "\n",
      "\n",
      "print(classification_report(df_train_val.iloc[:,0], y_tv_log_enp_train, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_train_val.iloc[:,0], y_tv_log_enp_train)\n",
      "print(jupyter_string.format(cost))\n",
      "cost_list_tv.append(cost)\n",
      "\n",
      "X_test = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis],df_test.iloc[:,extra_features[0]]))\n",
      "y_tv_log_enp = log_enp.predict(X_test)\n",
      "cost_list_test.append( geomean_cost(df_test.iloc[:,0], y_tv_log_enp[:,np.newaxis]))\n",
      "--------------------\n",
      "plt.scatter(np.arange(log_enp.coef_.shape[1]),np.abs(log_enp.coef_[0,:]))\n",
      "plt.plot(np.array([0,80]),np.array([1,1]))\n",
      "plt.show()\n",
      "=====\n",
      "ind_train = np.array([1,4,7,10,13])    \n",
      "ind_test = np.array([2,5,8,11,14])    \n",
      "\n",
      "print((cost_list_tv,cost_list_test))\n",
      "p1 = plt.bar(ind_train,np.array(cost_list_tv), width, color=jupyter_string)\n",
      "p2 = plt.bar(ind_test,np.array(cost_list_test), width, color=jupyter_string)\n",
      "\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xticks(ind_train+0.5, (jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string))\n",
      "plt.yticks(np.arange(.65, .85, 0.05))\n",
      "plt.ylim(0.65,.85)\n",
      "plt.legend((p1[0], p2[0]), (jupyter_string, jupyter_string), loc = jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "pd.merge(mean_power, housedf, on=jupyter_string, how=jupyter_string).head()\n",
      "=====\n",
      "merged = pd.merge(mean_power, housedf, on=jupyter_string)\n",
      "merged.head()\n",
      "--------------------\n",
      "X_test_counts = vect.transform(X_test)\n",
      "X_test_tfidf = tfidf.transform(X_test_counts)\n",
      "y_pred = clf.predict(X_test_tfidf)\n",
      "=====\n",
      "X_test_counts = vect.transform(X_test)\n",
      "X_test_tfidf = tfidf.transform(X_test_counts)\n",
      "\n",
      "\n",
      "y_pred = clf.predict(X_test_tfidf)\n",
      "--------------------\n",
      "pd.crosstab(y_test, y_pred, rownames=[jupyter_string], colnames=[jupyter_string])\n",
      "=====\n",
      "labels = np.unique(y_pred)\n",
      "confusion_mat = confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
      "accuracy = accuracy = (y_pred == y_test).mean()\n",
      "\n",
      "print(jupyter_string, labels)\n",
      "print(jupyter_string, confusion_mat)\n",
      "print(jupyter_string, accuracy)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import os\n",
      "import sys\n",
      "import numpy \n",
      "import pandas \n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import LogNorm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.formula.api import ols\n",
      "    \n",
      "\n",
      "pandas.set_option(jupyter_string, True)\n",
      "pandas.set_option(jupyter_string, 20)\n",
      "pandas.set_option(jupyter_string, 50)\n",
      "\n",
      "from decimal import getcontext, Decimal\n",
      "\n",
      "getcontext().prec = 2\n",
      "--------------------\n",
      "df = pandas.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dm = pandas.read_csv(jupyter_string)\n",
      "dm = dm[(dm['advantagetypeshooter' <<unk>>]==jupyter_string)] \n",
      "dm = dm[dm['zone' <<unk>>]==jupyter_string]\n",
      "--------------------\n",
      "dm.head()\n",
      "=====\n",
      "dm.head(2)\n",
      "--------------------\n",
      "dm.tail(2)\n",
      "=====\n",
      "dm.describe()\n",
      "--------------------\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "=====\n",
      "dm[jupyter_string] = dm['eventtype' <<unk>>]==jupyter_string\n",
      "dm[jupyter_string] = 99 - dm['XNorm' <<unk>>]\n",
      "dm.groupby(['eventtype' <<unk>>])[[jupyter_string, 'YNorm' <<unk>>]].describe()\n",
      "--------------------\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "=====\n",
      "dw = dm[dm['shotType' <<unk>>]==jupyter_string]\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.heatmap(dm.corr(), annot=True)\n",
      "=====\n",
      "plt.hist2d(dw[jupyter_string], dw['YNorm' <<unk>>],bins=30)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.api as sm\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "m1 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "\n",
      "\n",
      "m1.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "\n",
      "--------------------\n",
      "m2 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string]).fit())\n",
      "\n",
      "\n",
      "m2.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "=====\n",
      "dw[jupyter_string] = m1.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string,x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "dw = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "dw.summary()\n",
      "=====\n",
      "m2 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "m2.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "\n",
      "\n",
      "--------------------\n",
      "dw[jupyter_string] = m2.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string,x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "dw[jupyter_string] = m1.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "m3 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[[jupyter_string, jupyter_string]])).fit()\n",
      "m3.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "--------------------\n",
      "grouped = grp.mean()\n",
      "type(grouped)\n",
      "=====\n",
      "grp.agg([jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "dw[jupyter_string] = m3.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "dw[jupyter_string] = m3.predict()\n",
      "--------------------\n",
      "dw[jupyter_string] = dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string]\n",
      "=====\n",
      "dw.describe()\n",
      "--------------------\n",
      "dw.head()\n",
      "=====\n",
      "dw.groupby(['season' <<unk>>])[[jupyter_string, jupyter_string]].sum()\n",
      "--------------------\n",
      "dw.groupby([jupyter_string, jupyter_string])[[jupyter_string, jupyter_string]].sum()\n",
      "=====\n",
      "dw.groupby(['teamcode' <<unk>>])[[jupyter_string, jupyter_string]].sum()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data_uncleaned=pd.read_csv(jupyter_string)\n",
      "\n",
      "data=pd.read_csv(rjupyter_string, encoding=jupyter_string)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "Intro_Course_Required = []\n",
      "for row in data.values[:,9]:\n",
      "    Intro_Course_Required.append(row)\n",
      "Intro_Course_Required_Y = Intro_Course_Required.count(jupyter_string)\n",
      "Intro_Course_Required_N = Intro_Course_Required.count(jupyter_string)\n",
      "Prop_Intro_Course_Required_Y = Intro_Course_Required_Y/(Intro_Course_Required_Y+Intro_Course_Required_N)\n",
      "Prop_Intro_Course_Required_N = Intro_Course_Required_N/(Intro_Course_Required_Y+Intro_Course_Required_N)\n",
      "\n",
      "Intro_Course_Elective = []\n",
      "for row in data.values[:,10]:\n",
      "    Intro_Course_Elective.append(row)\n",
      "Intro_Course_Elective_Y = Intro_Course_Elective.count(jupyter_string)\n",
      "Intro_Course_Elective_N = Intro_Course_Elective.count(jupyter_string)\n",
      "Prop_Intro_Course_Elective_Y = Intro_Course_Elective_Y/(Intro_Course_Elective_Y+Intro_Course_Elective_N)\n",
      "Prop_Intro_Course_Elective_N = Intro_Course_Elective_N/(Intro_Course_Elective_Y+Intro_Course_Elective_N)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.pie([Intro_Course_Required_Y,Intro_Course_Required_N],labels=[jupyter_string,jupyter_string],explode=(0,0.1),autopct=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.pie([Intro_Course_Elective_Y,Intro_Course_Elective_N],labels=[jupyter_string,jupyter_string],explode=(0,0.1),autopct=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Com_Dir_dist_ax1 = [Prop_Intro_Course_Required_Y, Prop_Intro_Course_Required_N]\n",
      "Com_Dir_dist_ax2 = [Prop_Intro_Course_Elective_Y, Prop_Intro_Course_Elective_N]\n",
      "ax2.bar(ticks2,Com_Dir_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Com_Dir_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "grp.mean()\n",
      "=====\n",
      "merged.pivot(index=jupyter_string, columns='type' <<unk>>, values=jupyter_string).head()\n",
      "--------------------\n",
      "Q2e_det_3a_dist_ax1 = [Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_N]\n",
      "Q2e_det_3a_dist_ax2 = [Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6a_det_3a_dist_ax1 = [Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_N]\n",
      "Q6a_det_3a_dist_ax2 = [Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6c_det_3a_dist_ax1 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "Q6c_det_3a_dist_ax2 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6c_det_3a_dist_ax1 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "Q6c_det_3a_dist_ax2 = [Prop_Comp_Exm_Ques_N_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "REsample12=REsample1.loc[100:]\n",
      "\n",
      "RE12w0=sum(REsample12.sale_price*REsample12.gross_sq_feet)/sum(REsample12.gross_sq_feet**2)\n",
      "print(jupyter_string.format(RE12w0))\n",
      "--------------------\n",
      "Q11a_det_3a_dist_ax1 = [Prop_Mtmtca_Lcns_Y_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11a_det_3a_dist_ax2 = [Prop_Mtmtca_Lcns_N_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q11b_det_3a_dist_ax1 = [Prop_Mtmtca_Lcns_Y_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11b_det_3a_dist_ax2 = [Prop_Mtmtca_Lcns_N_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "houses = merged.pivot(index=jupyter_string, columns='type' <<unk>>, values=jupyter_string)\n",
      "houses.to_csv(jupyter_string)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks4 = [1,2,3,4]\n",
      "xticklabelsagreestr=[jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "ax1.bar(ticks4,Fact_Rel_Time_Y_and_Taught_Intro_Y,width=.25, align=jupyter_string)\n",
      "ax2.bar(ticks4,Fact_Rel_Time_Y_and_Taught_Intro_N,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks4)\n",
      "ax1.set_xticklabels(xticklabelsagreestr, rotation=90)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14a_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Y_and_Taught_Intro_N]\n",
      "Q14a_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15a_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_N]\n",
      "Q15a_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15b_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_N]\n",
      "Q15b_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15d_det_3a_dist_ax1 = [Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_N]\n",
      "Q15d_det_3a_dist_ax2 = [Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15f_det_3a_dist_ax1 = [Prop_Fact_Train_Intg_Comp_Y_and_Taught_Intro_Y, Prop_Fact_Train_Intg_Comp_Y_and_Taught_Intro_N]\n",
      "Q15f_det_3a_dist_ax2 = [Prop_Fact_Train_Intg_Comp_N_and_Taught_Intro_Y, Prop_Fact_Train_Intg_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15f_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q15h_det_3a_dist_ax1 = [Prop_Rel_Time_Dev_Y_and_Taught_Intro_Y, Prop_Rel_Time_Dev_Y_and_Taught_Intro_N]\n",
      "Q15h_det_3a_dist_ax2 = [Prop_Rel_Time_Dev_N_and_Taught_Intro_Y, Prop_Rel_Time_Dev_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15h_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15h_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax2\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15i_det_3a_dist_ax1 = [Prop_Rel_Time_Dev_Y_and_Taught_Intro_Y, Prop_Rel_Time_Dev_Y_and_Taught_Intro_N]\n",
      "Q15i_det_3a_dist_ax2 = [Prop_Rel_Time_Dev_N_and_Taught_Intro_Y, Prop_Rel_Time_Dev_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15i_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15i_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "total_count = flags.shape[0]\n",
      "red_count = flags[flags[jupyter_string] == 1].shape[0]\n",
      "\n",
      "one_red = (float(red_count) / total_count) \n",
      "print(jupyter_string, one_red)\n",
      "\n",
      "two_red = one_red * (float(red_count - 1) / (total_count - 1))\n",
      "print(jupyter_string, two_red)\n",
      "\n",
      "three_red = two_red * (float(red_count - 2) / (total_count - 2))\n",
      "print(jupyter_string, three_red)\n",
      "--------------------\n",
      "Acc_Reas_Experience_YN = []\n",
      "\n",
      "for row in data.values[:,86]:\n",
      "    Acc_Reas_Experience_YN.append(row)\n",
      "Acc_Reas_Experience_Y_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Experience_Y_and_Taught_Intro_N=0\n",
      "Acc_Reas_Experience_N_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Experience_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Acc_Reas_Experience_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Experience_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_\n",
      "=====\n",
      "Reas_Exp_Opp_YN = []\n",
      "\n",
      "for row in data.values[:,86]:\n",
      "    Reas_Exp_Opp_YN.append(row)\n",
      "Reas_Exp_Opp_Y_and_Taught_Intro_Y=0\n",
      "Reas_Exp_Opp_Y_and_Taught_Intro_N=0\n",
      "Reas_Exp_Opp_N_and_Taught_Intro_Y=0\n",
      "Reas_Exp_Opp_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Reas_Exp_Opp_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_N_and_Taught_Intro_N+=1\n",
      "Prop_Reas_Exp_Opp_Y_and_Taught_Intro_Y = Reas_Exp_Opp_Y_and_Taught_Intro_Y/(Reas_Exp_Opp_Y_and_Taught_Intro_Y+Reas_Exp_Opp_Y_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_Y_and_Taught_Intro_N = Reas_Exp_Opp_Y_and_Taught_Intro_N/(Reas_Exp_Opp_Y_and_Taught_Intro_Y+Reas_Exp_Opp_Y_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_N_and_Taught_Intro_Y = Reas_Exp_Opp_N_and_Taught_Intro_Y/(Reas_Exp_Opp_N_and_Taught_Intro_Y+Reas_Exp_Opp_N_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_N_and_Taught_Intro_N = Reas_Exp_Opp_N_and_Taught_Intro_N/(Reas_Exp_Opp_N_and_Taught_Intro_Y+Reas_Exp_Opp_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16e_det_3a_dist_ax1 = [Prop_Reas_Exp_Opp_Y_and_Taught_Intro_Y, Prop_Reas_Exp_Opp_Y_and_Taught_Intro_N]\n",
      "Q16e_det_3a_dist_ax2 = [Prop_Reas_Exp_Opp_N_and_Taught_Intro_Y, Prop_Reas_Exp_Opp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Ind_Stud_Support_YN = []\n",
      "\n",
      "for row in data.values[:,88]:\n",
      "    Ind_Stud_Support_YN.append(row)\n",
      "Ind_Stud_Support_Y_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Support_Y_and_Taught_Intro_N=0\n",
      "Ind_Stud_Support_N_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Support_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Ind_Stud_Support_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Support_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Support_Y_and_\n",
      "=====\n",
      "Other_YN = []\n",
      "\n",
      "for row in data.values[:,88]:\n",
      "    Other_YN.append(row)\n",
      "Other_Y_and_Taught_Intro_Y=0\n",
      "Other_Y_and_Taught_Intro_N=0\n",
      "Other_N_and_Taught_Intro_Y=0\n",
      "Other_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Other_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_N+=1\n",
      "Prop_Other_Y_and_Taught_Intro_Y = Other_Y_and_Taught_Intro_Y/(Other_Y_and_Taught_Intro_Y+Other_Y_and_Taught_Intro_N)  \n",
      "Prop_Other_Y_and_Taught_Intro_N = Other_Y_and_Taught_Intro_N/(Other_Y_and_Taught_Intro_Y+Other_Y_and_Taught_Intro_N)  \n",
      "Prop_Other_N_and_Taught_Intro_Y = Other_N_and_Taught_Intro_Y/(Other_N_and_Taught_Intro_Y+Other_N_and_Taught_Intro_N)  \n",
      "Prop_Other_N_and_Taught_Intro_N = Other_N_and_Taught_Intro_N/(Other_N_and_Taught_Intro_Y+Other_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16g_det_3a_dist_ax1 = [Prop_Other_Y_and_Taught_Intro_Y, Prop_Other_Y_and_Taught_Intro_N]\n",
      "Q16g_det_3a_dist_ax2 = [Prop_Other_N_and_Taught_Intro_Y, Prop_Other_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16g_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Other_Y_and_Taught_Intro_Y=0\n",
      "Other_Y_and_Taught_Intro_N=0\n",
      "Other_N_and_Taught_Intro_Y=0\n",
      "Other_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Other_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Int\n",
      "=====\n",
      "None_Above_YN = []\n",
      "\n",
      "for row in data.values[:,89]:\n",
      "    None_Above_YN.append(row)\n",
      "None_Above_Y_and_Taught_Intro_Y=0\n",
      "None_Above_Y_and_Taught_Intro_N=0\n",
      "None_Above_N_and_Taught_Intro_Y=0\n",
      "None_Above_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(None_Above_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_N_and_Taught_Intro_N+=1\n",
      "Prop_None_Above_Y_and_Taught_Intro_Y = None_Above_Y_and_Taught_Intro_Y/(None_Above_Y_and_Taught_Intro_Y+None_Above_Y_and_Taught_Intro_N)  \n",
      "Prop_None_Above_Y_and_Taught_Intro_N = None_Above_Y_and_Taught_Intro_N/(None_Above_Y_and_Taught_Intro_Y+None_Above_Y_and_Taught_Intro_N)  \n",
      "Prop_None_Above_N_and_Taught_Intro_Y = None_Above_N_and_Taught_Intro_Y/(None_Above_N_and_Taught_Intro_Y+None_Above_N_and_Taught_Intro_N)  \n",
      "Prop_None_Above_N_and_Taught_Intro_N = None_Above_N_and_Taught_Intro_N/(None_Above_N_and_Taught_Intro_Y+None_Above_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16j_det_3a_dist_ax1 = [Prop_None_Above_Y_and_Taught_Intro_Y, Prop_None_Above_Y_and_Taught_Intro_N]\n",
      "Q16j_det_3a_dist_ax2 = [Prop_None_Above_N_and_Taught_Intro_Y, Prop_None_Above_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16j_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16j_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(Taught_in_Intro_YN, bins=20)\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(None_Above_YN, bins=20)\n",
      "plt.show()\n",
      "=====\n",
      "Teach_Mot_Reas_Int_YN = []\n",
      "\n",
      "for row in data.values[:,91]:\n",
      "    Teach_Mot_Reas_Int_YN.append(row)\n",
      "Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y=0\n",
      "Teach_Mot_Reas_Int_Y_and_Taught_Intro_N=0\n",
      "Teach_Mot_Reas_Int_N_and_Taught_Intro_Y=0\n",
      "Teach_Mot_Reas_Int_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Teach_Mot_Reas_Int_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_N_and_Taught_Intro_N+=1\n",
      "Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y = Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y/(Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+Teach_Mot_Reas_Int_Y_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_N = Teach_Mot_Reas_Int_Y_and_Taught_Intro_N/(Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+Teach_Mot_Reas_Int_Y_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_Y = Teach_Mot_Reas_Int_N_and_Taught_Intro_Y/(Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+Teach_Mot_Reas_Int_N_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_N = Teach_Mot_Reas_Int_N_and_Taught_Intro_N/(Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+Teach_Mot_Reas_Int_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q17a_det_3a_dist_ax1 = [Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y, Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_N]\n",
      "Q17a_det_3a_dist_ax2 = [Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_Y, Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q17a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q17a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "animals = pd.read_csv(jupyter_string, index_col='AnimalID' <<unk>>)\n",
      "animals.head(3)\n",
      "--------------------\n",
      "X = animals[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "y = animals[jupyter_string]\n",
      "=====\n",
      "columns = ['AnimalType' <<unk>>, 'Sex' <<unk>>, 'IsNamed' <<unk>>, 'IsMixed' <<unk>>, 'IsIntact' <<unk>>, 'AgeCategory' <<unk>>, 'ColorCategory' <<unk>>, 'OutcomeType' <<unk>>]\n",
      "animals = pd.DataFrame(animals, columns=columns)\n",
      "animals.head()\n",
      "--------------------\n",
      "dogs = dogs.sample(frac=1).reset_index(drop=True)\n",
      "dogs.head()\n",
      "=====\n",
      "animals = pd.get_dummies(animals, columns=['Sex' <<unk>>, 'AgeCategory' <<unk>>, 'ColorCategory' <<unk>>])\n",
      "animals.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "vote = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "vote.head(5)\n",
      "--------------------\n",
      "dogs.head()\n",
      "=====\n",
      "X_dogs = dogs.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "=====\n",
      "clf_RF_dogs = RandomForestClassifier()\n",
      "clf_RF_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_RF_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_RF_dogs.score(X_test_dogs, y_test_dogs))\n",
      "--------------------\n",
      "clf_RF_dogs = RandomForestClassifier()\n",
      "clf_RF_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_RF_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_RF_dogs.score(X_test_dogs, y_test_dogs))\n",
      "=====\n",
      "clf_RF_cats = RandomForestClassifier()\n",
      "clf_RF_cats.fit(X_train_cats, y_train_cats)\n",
      "print(clf_RF_cats.score(X_train_cats, y_train_cats))\n",
      "print(clf_RF_cats.score(X_test_cats, y_test_cats))\n",
      "--------------------\n",
      "print(pd.Series(clf_RF_dogs.feature_importances_,\n",
      "                index=X_dogs.columns).sort_values(ascending=False))\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
      "pd.Series(clf_RF_dogs.feature_importances_,\n",
      "                index=X_dogs.columns).sort_values(ascending=False).plot(ax=ax1, title=jupyter_string)\n",
      "pd.Series(clf_RF_cats.feature_importances_,\n",
      "                index=X_cats.columns).sort_values(ascending=False).plot(ax=ax2, title=jupyter_string)\n",
      "--------------------\n",
      "dogs_fi.head()\n",
      "=====\n",
      "X_dogs = dogs_fi.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs_fi.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "=====\n",
      "X = dogs_train.drop('OutcomeType' <<unk>>, axis=1)\n",
      "y = dogs_train['OutcomeType' <<unk>>]\n",
      "\n",
      "clf_LR_dogs = LogisticRegression().fit(X, y)\n",
      "--------------------\n",
      "y_pred_dogs = clf_LR_dogs.predict(dogs_test)\n",
      "=====\n",
      "dogs_train[jupyter_string] = clf_LR_dogs.predict(X)\n",
      "dogs_train.head()\n",
      "--------------------\n",
      "dogs_train[jupyter_string] = clf_LR_dogs.predict_proba(dogs_train[feature_cols])[:,1]\n",
      "dogs_train.head()\n",
      "=====\n",
      "dogs_train['OutcomeType' <<unk>>].unique()\n",
      "--------------------\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = vote.loc[vote.Party == jupyter_string, jupyter_string] - vote.loc[vote.Party == jupyter_string, jupyter_string]\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = vote.loc[vote.Party == jupyter_string, jupyter_string] - vote.loc[vote.Party == jupyter_string, jupyter_string]\n",
      "=====\n",
      "vote[jupyter_string] = vote.Gen1 / (vote.Gen1 + vote.Gen2)\n",
      "vote[jupyter_string] = vote.Pri1 / (vote.Pri1 + vote.Pri2)\n",
      "vote[jupyter_string] = vote.GenShare - vote.PriShare\n",
      "vote[jupyter_string] = vote.DIME1 - vote.DIME2\n",
      "vote[jupyter_string] = vote.PriOpp / vote.PriTotal * 100\n",
      "vote[jupyter_string] = vote.OppShare * vote.DIMEChange\n",
      "vote.head(5)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(dogs_train[dogs_train.columns[:-1]], dogs_train[dogs_train.columns[-1]])\n",
      "dogs_train[jupyter_string] = knn.predict(dogs_train[dogs_train.columns[:-1]])\n",
      "=====\n",
      "X_dogs = dogs.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "knn_dogs = KNeighborsClassifier(n_neighbors=5)\n",
      "knn_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "knn_dogs.score(X_test_dogs, y_test_dogs)\n",
      "=====\n",
      "X_cats = cats.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_cats = cats.OutcomeType\n",
      "\n",
      "X_train_cats, X_test_cats, y_train_cats, y_test_cats = train_test_split(X_cats, y_cats, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "clf_KNN_dogs = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_KNN_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_KNN_dogs.score(X_test_dogs, y_test_dogs))\n",
      "=====\n",
      "clf_KNN_cats = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_cats.fit(X_train_cats, y_train_cats)\n",
      "print(clf_KNN_cats.score(X_train_cats, y_train_cats))\n",
      "print(clf_KNN_cats.score(X_test_cats, y_test_cats))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "np.mean([2,4,6])\n",
      "--------------------\n",
      "os.getcwd()\n",
      "=====\n",
      "import os\n",
      "\n",
      "\n",
      "os.getcwd()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "mousedata = pd.read_csv(jupyter_string)\n",
      "\n",
      "mousedata\n",
      "--------------------\n",
      "mousedata.head()\n",
      "=====\n",
      "mousedata.head()\n",
      "--------------------\n",
      "mousedata.describe()\n",
      "=====\n",
      "mousedata.describe()\n",
      "--------------------\n",
      "data_m.mean()\n",
      "=====\n",
      "np.mean(data_m)\n",
      "--------------------\n",
      "vote[jupyter_string] = -1*vote.DIMEChange\n",
      "vote.head(5)\n",
      "=====\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] *=  -1\n",
      "vote[[jupyter_string,jupyter_string,jupyter_string,jupyter_string]].head(5)\n",
      "--------------------\n",
      "group_mouse.mean()\n",
      "=====\n",
      "group_mouse.get_group(jupyter_string)\n",
      "--------------------\n",
      "group_mouse.apply(np.mean)\n",
      "=====\n",
      "mousedata.groupby(['Sex' madeupword0002]).apply(np.mean)\n",
      "--------------------\n",
      "maze_files = os.listdir(jupyter_string)\n",
      "maze_files\n",
      "=====\n",
      "import os\n",
      "\n",
      "path = jupyter_string\n",
      "file_list = os.listdir(path)\n",
      "file_list\n",
      "\n",
      "--------------------\n",
      "mouse_list = pd.concat(mouse_list)\n",
      "\n",
      "mouse_list\n",
      "=====\n",
      "big_mouse_frame = pd.concat(mouse_list)\n",
      "\n",
      "big_mouse_frame\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "point_stats = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "point_stats.head()\n",
      "=====\n",
      "point_stats.head()\n",
      "--------------------\n",
      "point_stats.info()\n",
      "=====\n",
      "point_stats.tail()\n",
      "--------------------\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = jupyter_string\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = jupyter_string\n",
      "vote.head(5)\n",
      "=====\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "--------------------\n",
      "point_stats.Name.unique()\n",
      "=====\n",
      "point_stats.Name.unique()\n",
      "--------------------\n",
      "point_stats[point_stats[\"ID_Object\"]==0].head()\n",
      "=====\n",
      "point_stats[point_stats[\"Name\"]==jupyter_string].head(20)\n",
      "\n",
      "point_stats[point_stats[\"ID_StatisticsType\"]==237].head(20)\n",
      "--------------------\n",
      "point_stats_matrix = point_stats.pivot(index=\"Name\", columns=\"ID_StatisticsType\", values=\"Value\")\n",
      "point_stats_matrix.head()\n",
      "=====\n",
      "point_stats_matrix = point_stats.pivot(index='ID_Object' <<unk>>, columns='Name' <<unk>>, values='Value' <<unk>>)\n",
      "point_stats_matrix.head()\n",
      "--------------------\n",
      "point_stats_matrix = point_stats_matrix.drop(-1)\n",
      "point_stats_matrix.head()\n",
      "=====\n",
      "point_stats_matrix = point_stats_matrix.drop(-1)\n",
      "point_stats_matrix.head(20)\n",
      "--------------------\n",
      "colnames = [jupyter_string, jupyter_string, jupyter_string]\n",
      "point_stats.loc[:, colnames]\n",
      "=====\n",
      "colnames = [jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "diameter = point_stats_matrix.loc[:,colnames]\n",
      "\n",
      "diameter.head(10)\n",
      "--------------------\n",
      "point_stats_matrix.filter(regex=jupyter_string, axis=1)\n",
      "=====\n",
      "diameter=point_stats_matrix.filter(regex=jupyter_string, axis=1)\n",
      "diameter.head()\n",
      "--------------------\n",
      "sns.boxplot(point_stats_matrix.Area)\n",
      "=====\n",
      "sns.boxplot(y=jupyter_string, data=point_stats_matrix)\n",
      "--------------------\n",
      "plt.scatter(filtered_points.Longitude, filtered_points.Latitude)\n",
      "plt.show()\n",
      "=====\n",
      "sns.lmplot(x=jupyter_string, y=jupyter_string, fit_reg=False, data=filtered_points)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "=====\n",
      "import statsmodels.api as sm\n",
      "model = sm.OLS(vote.loc[vote.Opp == jupyter_string].VoteChange, sm.add_constant(vote.loc[vote.Opp == jupyter_string].DIMEChange)).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "point_stats = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "points_value = pd.read_table(jupyter_string, header=None)\n",
      "points_value.head()\n",
      "--------------------\n",
      "points_value.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "points_value.head()\n",
      "=====\n",
      "new_col_names = [\"ID_Time\",\"ID_Object\",\"ID_StatisticsType\",\"Value\"]\n",
      "points_value.columns = new_col_names\n",
      "points_value.head()\n",
      "--------------------\n",
      "points_statistics_type = pd.read_csv(jupyter_string)\n",
      "points_statistics_type.head()\n",
      "=====\n",
      "points_type = pd.read_table(jupyter_string, header=None)\n",
      "\n",
      "new_col_names = [\"ID_StatisticsType\", \"ID_Category\", \"ID_FactorList\", \"Name\", \"Unit\"]\n",
      "points_type.columns = new_col_names\n",
      "\n",
      "points_type.head()\n",
      "--------------------\n",
      "pd.merge(left=points_value, right=points_type, left_on=\"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
      "=====\n",
      "merged_table = pd.merge(left=points_value, right=points_type, \n",
      "         left_on = \"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
      "\n",
      "merged_table.head()\n",
      "--------------------\n",
      "merged_table.to_csv(jupyter_string)\n",
      "=====\n",
      "point_stats_matrix.to_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "food_info = pd.read_csv(jupyter_string)\n",
      "print(type(food_info))\n",
      "--------------------\n",
      "food_info.head()\n",
      "=====\n",
      "food_info.head()\n",
      "--------------------\n",
      "food_info[[\"Energ_Kcal\", \"Protein\"]].head()\n",
      "=====\n",
      "col_names = food_info.columns.tolist()\n",
      "\n",
      "gram_columns = []\n",
      "for el in col_names:\n",
      "    if el.endswith(jupyter_string):\n",
      "        gram_columns.append(el)\n",
      "print(gram_columns,jupyter_string)\n",
      "\n",
      "gram_df = food_info[gram_columns]\n",
      "print(gram_df)\n",
      "--------------------\n",
      "food_info[jupyter_string] = food_info[\"Carbohydrt_(g)\"] + food_info[\"Water_(g)\"]\n",
      "food_info\n",
      "=====\n",
      "food_info[jupyter_string] = water_energy\n",
      "food_info.head()\n",
      "--------------------\n",
      "s22=sigma**-2; s12=RE11s**-2\n",
      "RE12w=(s22*sum(REsample1.sale_price*REsample1.gross_sq_feet)+s12*RE11w)/(s22*sum(REsample1.gross_sq_feet**2)+s12)\n",
      "RE12s=(s22*sum(REsample1.gross_sq_feet**2)+s12)**(-0.5)\n",
      "print(jupyter_string.format(RE12w,RE12s))\n",
      "=====\n",
      "lm = smf.ols(formula=jupyter_string, data = REsample1).fit()\n",
      "print(lm.summary())\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "model.summary()\n",
      "=====\n",
      "model = sm.OLS(vote.loc[vote.Opp == jupyter_string].VoteChange, sm.add_constant(vote.loc[vote.Opp == jupyter_string].DIMEChange)).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "food_info = food_info.drop(jupyter_string, axis=1)\n",
      "food_info.head()\n",
      "=====\n",
      "del food_info[jupyter_string]\n",
      "food_info.head()\n",
      "--------------------\n",
      "food_info.sort_values(jupyter_string, ascending=False).head()\n",
      "=====\n",
      "food_info.sort_values(\"Energ_Kcal\", inplace=True, ascending=False)\n",
      "food_info.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data_df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "--------------------\n",
      "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
      "=====\n",
      "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "=====\n",
      "count_vect = CountVectorizer()\n",
      "X_train_counts = count_vect.fit_transform(X_train)\n",
      "X_test_counts = count_vect.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "nb = MultinomialNB()\n",
      "nb.fit(X_train_counts, y_train)\n",
      "=====\n",
      "gnb = GaussianNB()\n",
      "y_pred = gnb.fit(X_train_counts.toarray(), y_train).predict(X_test_counts.toarray())\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(y_test, y_pred))\n",
      "=====\n",
      "rocscores = roc_curve(y_test, y_pred)\n",
      "plt.plot(rocscores[0], rocscores[1])\n",
      "plt.plot(rocscores[0],rocscores[0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "from collections import Counter\n",
      "\n",
      "\n",
      "pd.set_option(jupyter_string,10)\n",
      "pd.set_option(jupyter_string,20)\n",
      "--------------------\n",
      "rebates = pd.read_csv(jupyter_string)\n",
      "rebates.head()\n",
      "=====\n",
      "rebates = pd.read_csv(jupyter_string)\n",
      "rebates.columns = [c.replace(jupyter_string, jupyter_string) for c in rebates.columns] \n",
      "rebates\n",
      "--------------------\n",
      "rebates[\"County\"] = rebates[\"County\"].str.strip()\n",
      "=====\n",
      "rebates[\"County\"] = rebates[\"County\"].str.strip()\n",
      "Counter(rebates[\"County\"])\n",
      "--------------------\n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.strip()\n",
      "Counter(rebates[\"Make\"])\n",
      "=====\n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.title() \n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.strip() \n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.replace(jupyter_string,jupyter_string) \n",
      "Counter(rebates[\"Make\"]).most_common()\n",
      "--------------------\n",
      "rebates[\"Model\"] =rebates[\"Model\"].str.strip()\n",
      "=====\n",
      "rebates[\"Model\"] = rebates[\"Model\"].str.strip() \n",
      "Counter(rebates[\"Model\"]).most_common()\n",
      "--------------------\n",
      "rebates[\"Vehicle Type\"].value_counts()\n",
      "=====\n",
      "Counter(rebates[jupyter_string]).most_common()\n",
      "--------------------\n",
      "rebates[\"City\"].value_counts()\n",
      "=====\n",
      "rebates[\"City\"] = rebates[\"City\"].str.strip() \n",
      "Counter(rebates[\"City\"]).most_common()\n",
      "--------------------\n",
      "rebates[jupyter_string] = pd.to_datetime(rebates[jupyter_string])\n",
      "=====\n",
      "rebates[jupyter_string] = pd.to_datetime(rebates[jupyter_string])\n",
      "rebates[jupyter_string] < jupyter_string\n",
      "--------------------\n",
      "rebates.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "rebates.to_csv(jupyter_string,index=False)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pandas import DataFrame,Series\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "MEDIUM_SIZE = 14\n",
      "BIGGER_SIZE = 16\n",
      "\n",
      "plt.rc(jupyter_string, size=MEDIUM_SIZE)          \n",
      "plt.rc(jupyter_string, titlesize=BIGGER_SIZE)     \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, fontsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, titlesize=BIGGER_SIZE)  \n",
      "\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string,data.shape)\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "vote = vote[vote.columns[10:]]\n",
      "=====\n",
      "vote2 = vote[vote.columns[10:]]\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.head(5)\n",
      "--------------------\n",
      "data.info()\n",
      "=====\n",
      "data.info()\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "data.describe()\n",
      "--------------------\n",
      "sns.heatmap(titanic_df.corr(), annot=True)\n",
      "=====\n",
      "f, ax =plt.subplots(figsize=(15,10))\n",
      "sns.heatmap(data.corr(),\n",
      "            annot = True,\n",
      "            ax = ax)\n",
      "sns.plt.show()\n",
      "--------------------\n",
      "f, ax =plt.subplots(figsize=(15,10))\n",
      "sns.countplot(x=jupyter_string, hue=jupyter_string, data=data)\n",
      "sns.plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(16,3))\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "data.Sex.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "data.Pclass.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "           \n",
      "plt.subplot(1,3,3)\n",
      "data.Embarked.value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(16,3))\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "data.Sex.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "data.Pclass.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "           \n",
      "plt.subplot(1,3,3)\n",
      "data.Embarked.value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "plt.close()\n",
      "=====\n",
      "suvival_by_sex = data.groupby(['Sex' madeupword0002,'Survived' <<unk>>]).size().unstack()\n",
      "suvival_by_sex.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "\n",
      "survival_by_pclass = data.groupby(['Pclass' <<unk>>,'Survived' <<unk>>]).size().unstack()\n",
      "survival_by_pclass.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "\n",
      "survival_by_embarked = data.groupby(['Embarked' <<unk>>,'Survived' <<unk>>]).size().unstack()\n",
      "survival_by_embarked.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "data.loc[1980:1989][[jupyter_string, jupyter_string, jupyter_string]]\n",
      "=====\n",
      "results = data.loc[1980:1989][[jupyter_string, jupyter_string, jupyter_string]]\n",
      "results.to_csv(jupyter_string)\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "plt.imshow(X_train[0])\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import csv\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "traffic_db = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "traffic_db.head()\n",
      "=====\n",
      "fig = plt.figure(figsize=(50, 50))\n",
      "fig.suptitle(jupyter_string, fontsize=40)\n",
      "for id in traffic_db['ClassId' madeupword0002]:\n",
      "    fig.add_subplot(11, 4, id+1)\n",
      "    plt.title(str(id)+jupyter_string+traffic_db['SignName' <<unk>>][id], fontsize=30)\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(X_train_clean[random.choice(class_indices[id])])\n",
      "--------------------\n",
      "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)\n",
      "X_valid = X_valid.reshape(X_valid.shape[0], 32, 32, 1)\n",
      "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)\n",
      "=====\n",
      "X_train, y_train = shuffle(X_train, y_train)\n",
      "\n",
      "class_indices_new = defaultdict(list)\n",
      "for c in range(n_classes):\n",
      "    for i in range(np.shape(X_train)[0]):\n",
      "        if y_train[i] == c:\n",
      "            class_indices_new[c].append(i)\n",
      "\n",
      "class_id = random.choice(range(43))\n",
      "\n",
      "images = [X_train[random.choice(class_indices_new[class_id])].squeeze()]\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(3, 3))\n",
      "fig.suptitle(jupyter_string, fontsize=10)\n",
      "image_index=0\n",
      "for image in images:\n",
      "    image_index+=1\n",
      "    fig.add_subplot(2, 2, image_index)\n",
      "    \n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(image, cmap=jupyter_string)\n",
      "--------------------\n",
      "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
      "y = tf.placeholder(tf.int32, (None))\n",
      "one_hot_y = tf.one_hot(y, n_classes)\n",
      "\n",
      "rate = 0.001\n",
      "\n",
      "logits = LeNet(x)\n",
      "\n",
      "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
      "\n",
      "loss_operation = tf.reduce_mean(cross_entropy)\n",
      "\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
      "\n",
      "training_operation = optimizer.minimize(loss_operation)\n",
      "=====\n",
      "x = tf.placeholder(tf.float32, (None, image_shape[0], image_shape[1], 1))\n",
      "y = tf.placeholder(tf.int32, (None))\n",
      "one_hot_y = tf.one_hot(y, n_classes)\n",
      "keep_prob = tf.placeholder(tf.float32)\n",
      "\n",
      "\n",
      "EPOCHS = 10\n",
      "BATCH_SIZE = 32\n",
      "\n",
      "global_step = tf.Variable(0, trainable=False)\n",
      "starter_learning_rate = 0.0005\n",
      "rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
      "\n",
      "logits = LeNet(x)\n",
      "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
      "loss_operation = tf.reduce_mean(cross_entropy)\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
      "training_operation = optimizer.minimize(loss_operation)\n",
      "\n",
      "\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "def evaluate(X_data, y_data):\n",
      "    num_examples = len(X_data)\n",
      "    total_accuracy = 0\n",
      "    sess = tf.get_default_session()\n",
      "    for offset in range(0, num_examples, BATCH_SIZE):\n",
      "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
      "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
      "        total_accuracy += (accuracy * len(batch_x))\n",
      "    return total_accuracy / num_examples\n",
      "\n",
      "\n",
      "with tf.Session() as sess:\n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    num_examples = len(X_train)\n",
      "    \n",
      "    print(jupyter_string)\n",
      "    print()\n",
      "    for i in range(EPOCHS):\n",
      "        X_train, y_train = shuffle(X_train, y_train)\n",
      "        for offset in range(0, num_examples, BATCH_SIZE):\n",
      "            end = offset + BATCH_SIZE\n",
      "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
      "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
      "            \n",
      "        validation_accuracy = evaluate(X_valid, y_valid)\n",
      "        print(jupyter_string.format(i+1))\n",
      "        print(jupyter_string.format(validation_accuracy))\n",
      "        print()\n",
      "        \n",
      "    saver.save(sess, jupyter_string)\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "import glob\n",
      "import matplotlib.image as mpimg\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "new_images = glob.glob(jupyter_string)\n",
      "new_images = np.array(new_images)\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "new_images = new_images.astype(jupyter_string)\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "=====\n",
      "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
      "import numpy as np\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "images = plt.imread(jupyter_string)\n",
      "images = np.expand_dims(images, axis=0)\n",
      "images = np.insert(images, 1, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 2, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 3, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 4, plt.imread(jupyter_string), axis=0)\n",
      "\n",
      "labels = [2, 14, 11, 25, 14]\n",
      "\n",
      "images, labels = shuffle(images, labels)\n",
      "\n",
      "fig = plt.figure(figsize=(8, 2))\n",
      "fig.suptitle(jupyter_string, fontsize=10)\n",
      "image_index=0\n",
      "for image in images:\n",
      "    fig.add_subplot(1, 5, image_index+1)\n",
      "    plt.title(jupyter_string+str(labels[image_index])+jupyter_string+traffic_db['SignName' <<unk>>][labels[image_index]], fontsize=5)\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(image.squeeze())\n",
      "    image_index+=1\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.scatter(sales[jupyter_string], sales[jupyter_string])\n",
      "plt.plot(sales[jupyter_string], simple_model_predictions)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(sales.sqft_living, sales.price, color=jupyter_string, s = 2)\n",
      "plt.plot(sales.sqft_living, simple_model_predictions, color=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel('sqft_living' <<unk>>)\n",
      "plt.ylabel('price' <<unk>>)\n",
      "plt.ticklabel_format(style = jupyter_string)\n",
      "--------------------\n",
      "house = pd.read_csv(jupyter_string)\n",
      "house.head()\n",
      "=====\n",
      "sales = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "sales.head()\n",
      "=====\n",
      "sales.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "N = 100\n",
      "for _ in xrange(5):\n",
      "    s = np.zeros(N)\n",
      "    s[1:] = np.random.binomial(1, .5, size=(N-1,))*2-1\n",
      "    s = pd.Series(s)\n",
      "    s = s.cumsum()\n",
      "    plt.ylim([-50, 50])\n",
      "    s.plot()\n",
      "plt.show()\n",
      "--------------------\n",
      "predictions = simple_model.predict(test_data[simple_features])\n",
      "=====\n",
      "house1 = sales[sales.id== 5309101200]\n",
      "house2 = sales[sales.id == 1925069082]\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "=====\n",
      "sales[simple_features+['price' <<unk>>]].head()\n",
      "--------------------\n",
      "house1_pred = regressor.predict(house1[feature_cols])\n",
      "house2_pred = regressor.predict(house2[feature_cols])\n",
      "=====\n",
      "house1.head()\n",
      "--------------------\n",
      "house2.head()\n",
      "=====\n",
      "house2.head()\n",
      "--------------------\n",
      "house3 = pd.DataFrame(bill_gates)\n",
      "house3.head()\n",
      "=====\n",
      "house3 = pd.DataFrame(data=bill_gates)\n",
      "--------------------\n",
      "house3.head()\n",
      "=====\n",
      "house3.head()\n",
      "--------------------\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(train_data[[jupyter_string]], train_data[jupyter_string])\n",
      "=====\n",
      "simple_model = linear_model.LinearRegression()\n",
      "X = train_data[simple_features]\n",
      "y = train_data.price.to_frame()\n",
      "--------------------\n",
      "simple_model.fit(X,y)\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(action=jupyter_string, module=jupyter_string, message=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "app_store = pd.read_csv(jupyter_string,delimiter=jupyter_string)\n",
      "\n",
      "number_of_apps = len(app_store.id)\n",
      "print(jupyter_string + str(number_of_apps))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "name = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "sex = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "age = [20, 21, 18, 22, 19, 20, 20, 19, 20]\n",
      "rank = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "ID = range(9)\n",
      "aid = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "GPA = [3.8, 3.5, 3.0, 3.9, 2.8, 2.9, 3.8, 3.4, 3.7]\n",
      "mathID = [0, 1, 5, 6, 3]\n",
      "mathGd = [4.0, 3.0, 3.5, 3.0, 4.0]\n",
      "major = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "studentInfo = pd.DataFrame({jupyter_string: ID, jupyter_string: name, jupyter_string: sex, jupyter_string: age, jupyter_string: rank})\n",
      "otherInfo = pd.DataFrame({jupyter_string: ID, jupyter_string: GPA, jupyter_string: aid})\n",
      "mathInfo = pd.DataFrame({jupyter_string: mathID, jupyter_string: mathGd, jupyter_string: major})\n",
      "--------------------\n",
      "app_store.head()\n",
      "=====\n",
      "app_store.info()\n",
      "--------------------\n",
      "count = np.unique(app_store_data[jupyter_string],return_counts=True)\n",
      "unique = app_store_data[jupyter_string].nunique()\n",
      "print(count)\n",
      "count = count/count.sum()\n",
      "\n",
      "print(jupyter_string.format(count[0]))\n",
      "\n",
      "pie_chart(count, index, jupyter_string)\n",
      "=====\n",
      "revenue = []\n",
      "\n",
      "for index in range(7197):\n",
      "\n",
      "    revenue.append(app_store_data.loc[[index]]['price' <<unk>>][index]*app_store_data.loc[[index]]['rating_count_tot' <<unk>>][index])\n",
      "    \n",
      "revenue_series = pd.Series(revenue)\n",
      "total_revenue = revenue_series.sum()\n",
      "app_store_data[jupyter_string] = revenue_series.values\n",
      "\n",
      "print(total_revenue)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.bar(range(len(total_revenue)), total_revenue, align=jupyter_string)\n",
      "plt.xticks(range(len(total_revenue)), total_revenue.index, rotation=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "total_revenue = revenue_series.sum()\n",
      "print(total_revenue)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.bar(range(len(total_revenue)), total_revenue)\n",
      "plt.xticks(range(len(total_revenue)), total_revenue.index, rotation=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "index, count = np.unique(app_store_data[\"prime_genre\"],return_counts=True)\n",
      "stats = app_store_data.groupby(\"prime_genre\")[jupyter_string].sum()\n",
      "print(stats)\n",
      "type(stats)\n",
      "devices, count = np.unique(app_store_data[\"prime_genre\"],return_counts=True)\n",
      "describe = stats.describe()\n",
      "describe[jupyter_string]=devices\n",
      "\n",
      "\n",
      "bar_chart(stats,index,(20,8),jupyter_string, jupyter_string,jupyter_string ,60)\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "''jupyter_string''\n",
      "app_store_data['user_rating' <<unk>>] = app_store_data['user_rating' <<unk>>].replace([3.5, 3, 2.5, 2, 1.5, 1, 0.5], 0)\n",
      "app_store_data['user_rating' <<unk>>] = app_store_data['user_rating' <<unk>>].replace([4, 4.5, 5], 1)\n",
      "app_store_data.head()\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "\n",
      "C = np.logspace(0, 4, 20)\n",
      "gamma = [jupyter_string, jupyter_string]\n",
      "hyperparameters = dict(C=C, gamma=gamma)\n",
      "\n",
      "\n",
      "clf = GridSearchCV(svm_model, hyperparameters, cv=5, verbose=0)\n",
      "\n",
      "best_model = clf.fit(X, y)\n",
      "\n",
      "\n",
      "selected_C=best_model.best_estimator_.get_params()[jupyter_string]\n",
      "selected_gamma=best_model.best_estimator_.get_params()[jupyter_string]\n",
      "=====\n",
      "labels = new_df2['user_rating' <<unk>>]\n",
      "\n",
      "--------------------\n",
      "studentInfo.head()\n",
      "=====\n",
      "studentInfo.info()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
      "=====\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler().fit(new_df2.drop('user_rating' <<unk>>, axis=1))\n",
      "\n",
      "new_df = scaler.transform(new_df2.drop('user_rating' <<unk>>, axis=1))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "pd.set_option(jupyter_string, 140)\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import strip_accents\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "\n",
      "=====\n",
      "from gensim.parsing.preprocessing import preprocess_string\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(preprocess_string)\n",
      "--------------------\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "\n",
      "=====\n",
      "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \\\n",
      "    remove_stopwords\n",
      "\n",
      "def drop_short(tweet):\n",
      "    \n",
      "    return jupyter_string.join(x for x in tweet.split() if len(x) >= 3)\n",
      "\n",
      "def to_lowercase(tweet):\n",
      "    return tweet  \n",
      "\n",
      "def drop_usernames(tweet):\n",
      "    return tweet  \n",
      "    \n",
      "my_filters = [ to_lowercase, drop_usernames, strip_multiple_whitespaces, strip_punctuation, strip_numeric,\n",
      "               remove_stopwords, drop_short ]\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(lambda x: preprocess_string(x, my_filters))\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_context(jupyter_string, font_scale=1.5)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_context(jupyter_string, font_scale=1.5)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "import seaborn as sns\n",
      "\n",
      "plt.show()\n",
      "\n",
      "sns.countplot(df['class' <<unk>>])\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "otherInfo.info()\n",
      "=====\n",
      "otherInfo.info()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.isnull().sum().sort_values(ascending=False)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.distplot(df['GDP_perCap' <<unk>>], bins=15)\n",
      "--------------------\n",
      "sns.heatmap(df.corr(), annot=True)\n",
      "=====\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "\n",
      "sns.heatmap(df.corr(), annot=True, cmap=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoost\n",
      "=====\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "--------------------\n",
      "crime = pd.read_csv(jupyter_string)\n",
      "crime.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, header=1, index_col=0)\n",
      "data.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "model = LinearRegression()\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train)\n",
      "=====\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train, y_train)\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "weights.sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "weights.set_index(keys=jupyter_string)\n",
      "--------------------\n",
      "weights.sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15,10))\n",
      "\n",
      "\n",
      "ax1 = fig.add_subplot(211)\n",
      "\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "ax1.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "weights.set_index(keys=jupyter_string)[jupyter_string].plot(kind=jupyter_string, ax=ax1)\n",
      "\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(212)\n",
      "ax2.set_ylabel(jupyter_string)\n",
      "ax2.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "weights.set_index(keys=jupyter_string)[jupyter_string].plot(kind=jupyter_string, ax=ax2)\n",
      "\n",
      "plt.tight_layout()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_train_pca = pca.fit_transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "explained_variance = pca.explained_variance_ratio_\n",
      "explained_variance\n",
      "=====\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "=====\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "pca.explained_variance_ratio_\n",
      "=====\n",
      "pcaDF = pd.DataFrame(data=pca.components_, index=X.columns)\n",
      "\n",
      "\n",
      "pcaDF.head()\n",
      "--------------------\n",
      "plt.plot(range(1, len(exp_var)+1), exp_var, lw=2)\n",
      "plt.scatter(range(1, len(exp_var)+1), exp_var, s=120)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "=====\n",
      "pcaDF[0].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "pcaDF[1].plot(kind=jupyter_string, figsize=(15,6))\n",
      "=====\n",
      "pcaDF[1].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "pcaDF[2].plot(kind=jupyter_string, figsize=(15,6))\n",
      "=====\n",
      "pcaDF[2].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(pcaDF)\n",
      "pcaDF = pca.transform(pcaDF)\n",
      "=====\n",
      "pcaX_train = pca.transform(X_train)\n",
      "\n",
      "\n",
      "pcaX_test = pca.transform(X_test)\n",
      "\n",
      "\n",
      "pcaX_train = pd.DataFrame(pcaX_train)\n",
      "\n",
      "\n",
      "pcaX_test = pd.DataFrame(pcaX_test)\n",
      "--------------------\n",
      "sns.heatmap(pcaX_train.corr(), annot=True)\n",
      "plt.show()\n",
      "=====\n",
      "sns.heatmap(X_train.corr())\n",
      "--------------------\n",
      "sns.heatmap(X_test.corr())\n",
      "=====\n",
      "sns.heatmap(pcaX_train.corr())\n",
      "--------------------\n",
      "model1.fit(pcaX_train, y_train)\n",
      "model2.fit(pcaX_train, y_train)\n",
      "model3.fit(pcaX_train, y_train)\n",
      "model4.fit(pcaX_train, y_train)\n",
      "model5.fit(pcaX_train, y_train)\n",
      "model6.fit(pcaX_train, y_train)\n",
      "model7.fit(pcaX_train, y_train)\n",
      "model8.fit(pcaX_train, y_train)\n",
      "=====\n",
      "model5.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "model7.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "\n",
      "scoreRF = model5.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseRF = np.sqrt(mean_squared_error(y_test, model5.predict(pcaX_test.iloc[:,:10])))\n",
      "\n",
      "scoreGB = model7.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseGB = np.sqrt(mean_squared_error(y_test, model7.predict(pcaX_test.iloc[:,:10])))\n",
      "               \n",
      "print(jupyter_string.format(scoreRF,rmseRF))\n",
      "print(jupyter_string.format(scoreGB,rmseGB))\n",
      "--------------------\n",
      "from tpot import TPOTClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "=====\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "--------------------\n",
      "from tpot import TPOTRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.33, random_state=42)\n",
      "\n",
      "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)\n",
      "tpot.fit(X_train, y_train)\n",
      "tpot.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=46)\n",
      "\n",
      "\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "from tpot import TPOTRegressor\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "from tpot import TPOTClassifier\n",
      "\n",
      "tpot = TPOTClassifier()\n",
      "tpot.fit(X_train, y_train)\n",
      "tpot.score(X_test, y_test)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from sklearn.svm import LinearSVR\n",
      "from tpot.builtins import StackingEstimator\n",
      "\n",
      "exported_pipeline = make_pipeline(\n",
      "    VarianceThreshold(threshold=0.3),\n",
      "    StackingEstimator(estimator=RidgeCV()),\n",
      "    StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, max_features=0.75, min_samples_leaf=20, min_samples_split=5, n_estimators=100)),\n",
      "    StackingEstimator(estimator=LinearSVR(C=0.1, dual=True, epsilon=0.1, loss=jupyter_string, tol=0.001)),\n",
      "    StackingEstimator(estimator=KNeighborsRegressor(n_neighbors=22, p=1, weights=jupyter_string)),\n",
      "    StackingEstimator(estimator=KNeighborsRegressor(n_neighbors=4, p=1, weights=jupyter_string)),\n",
      "    SelectFromModel(estimator=ExtraTreesRegressor(max_features=1.0, n_estimators=100), threshold=0.2),\n",
      "    ExtraTreesRegressor(bootstrap=False, max_features=1.0, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n",
      ")\n",
      "\n",
      "\n",
      "exported_pipeline.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "y_pred = exported_pipeline.predict(X_test)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for h, t in zip(hypothesis,reference):\n",
    "    if syntax(h):\n",
    "        print(h)\n",
    "        print('='*5)\n",
    "        print(t)\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ff36ec1-05e7-4e66-b34d-4d0d8e5726fa",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-11T00:41:03.665841Z",
     "iopub.status.busy": "2021-08-11T00:41:03.665609Z",
     "iopub.status.idle": "2021-08-11T00:41:04.522386Z",
     "shell.execute_reply": "2021-08-11T00:41:04.521727Z",
     "shell.execute_reply.started": "2021-08-11T00:41:03.665817Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data3=pd.read_csv(jupyter_string) \n",
      "y=np.asarray(data3.iloc[:,-1]) \n",
      "X=np.asarray(data3.iloc[:,0:-1]) \n",
      "--------------------\n",
      "plt.plot(data.Year, data.Crime_Rate)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(data.index, data[jupyter_string])\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "print(rmse)\n",
      "=====\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "np.sqrt(mean_squared_error(y_test, y_pred))\n",
      "--------------------\n",
      "predict_df.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "submission_filename = jupyter_string\n",
      "predict_df.to_csv(submission_filename, index=False)\n",
      "--------------------\n",
      "train_data = pd.read_csv(jupyter_string)\n",
      "train_labels = pd.read_csv(jupyter_string)\n",
      "test_data = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "X_train = pd.read_csv(jupyter_string)\n",
      "X_train.week_start_date = pd.to_datetime(X_train.week_start_date)\n",
      "print(jupyter_string)\n",
      "\n",
      "y_train = pd.read_csv(jupyter_string, \n",
      "                      usecols=['total_cases' <<unk>>])\n",
      "print(jupyter_string)\n",
      "\n",
      "X_test = pd.read_csv(jupyter_string)\n",
      "X_test.week_start_date = pd.to_datetime(X_test.week_start_date)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "X_train.head()\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "y_train.head()\n",
      "=====\n",
      "y_train.head()\n",
      "--------------------\n",
      "submission_df = pd.read_csv(jupyter_string)\n",
      "submission_df.head()\n",
      "=====\n",
      "df1 = pd.read_csv(jupyter_string, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df2 = pd.read_csv(submission_filename, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df = pd.merge(df1, df2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df[(df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string])]\n",
      "--------------------\n",
      "X_train = pd.concat([X_train, y_train], axis=1)\n",
      "=====\n",
      "Xy_train = pd.concat([y_train, X_train], axis=1) \n",
      "print(jupyter_string)\n",
      "Xy_train.head()\n",
      "--------------------\n",
      "df1 = pd.read_csv(jupyter_string, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df2 = pd.read_csv(submission_filename, \n",
      "                  usecols=[0, 1, 2], header=0, names=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df = pd.merge(df1, df2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "df[(df[jupyter_string] != df[jupyter_string]) | \n",
      "   (df[jupyter_string] != df[jupyter_string])]\n",
      "=====\n",
      "f1 = pd.read_csv(jupyter_string)\n",
      "f2 = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "Xy_train_sj = Xy_train[Xy_train['city' <unk>] == jupyter_string]\n",
      "Xy_train_iq = Xy_train[Xy_train['city' <unk>] == jupyter_string]\n",
      "=====\n",
      "Xy_sj = Xy_train.loc[Xy_train.city == jupyter_string, :]\n",
      "Xy_iq = Xy_train.loc[Xy_train.city == jupyter_string, :]\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "df = pd.merge(f1, f2, how=jupyter_string, \n",
      "              left_on=[jupyter_string, jupyter_string, jupyter_string], \n",
      "              right_on=[jupyter_string, jupyter_string, jupyter_string])\n",
      "=====\n",
      "f2['total_cases' <<unk>>] = ((f1['total_cases' <<unk>>] + f2['total_cases' <<unk>>])/2).astype(int)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "f2.loc[f2['city' <<unk>>] == jupyter_string] = f1.loc[f1['city' <<unk>>] == jupyter_string]\n",
      "\n",
      "f2.tail()\n",
      "f2.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "plt.plot(data[jupyter_string], data[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(data[jupyter_string], data[jupyter_string])\n",
      "--------------------\n",
      "sj_corr = Xy_sj.corr()\n",
      "sj_corr.total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "--------------------\n",
      "Xy_iq.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_iq.corr().total_cases.sort_values(ascending=False)\n",
      "--------------------\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.total_cases.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_sj.corr().total_cases.sort_values(ascending=False)\n",
      "=====\n",
      "Xy_sj.groupby(['weekofyear' <<unk>>]).total_cases.sum().plot(kind=jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_iq.total_cases.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "=====\n",
      "Xy_sj.reanalysis_dew_point_temp_k.plot(kind=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "Xy_sj.isnull().sum()\n",
      "=====\n",
      "X_train.info()\n",
      "--------------------\n",
      "X_train = X_train.reset_index(drop=True)\n",
      "=====\n",
      "X_train = X_train.reset_index()\n",
      "X_test = X_test.reset_index()\n",
      "--------------------\n",
      "X_train_sj = X_train[X_train['city' <unk>] == jupyter_string]\n",
      "X_train_iq = X_train[X_train['city' <unk>] == jupyter_string]\n",
      "=====\n",
      "X_train_sj = X_train.loc[X_train.city == jupyter_string, :].copy()\n",
      "X_train_iq = X_train.loc[X_train.city == jupyter_string, :].copy()\n",
      "\n",
      "y_train_sj = y_train.loc[X_train.city == jupyter_string, :].copy()\n",
      "y_train_iq = y_train.loc[X_train.city == jupyter_string, :].copy()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "X_test_sj = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "X_test_iq = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "\n",
      "y_test_sj = y_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "y_test_iq = y_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "X_test_sj = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "X_test_iq = X_test.loc[X_test.city == jupyter_string, :].copy()\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "X_train_sj = normalize(X_train_sj)\n",
      "X_train_iq = normalize(X_train_iq)\n",
      "X_test_sj = normalize(X_test_sj)\n",
      "X_test_iq = normalize(X_test_iq)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "features_to_normalize = features + new_features\n",
      "\n",
      "X_train_sj[features_to_normalize] = X_train_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_train_iq[features_to_normalize] = X_train_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_sj[features_to_normalize] = X_test_sj[features_to_normalize].apply(normalize, axis=0)\n",
      "X_test_iq[features_to_normalize] = X_test_iq[features_to_normalize].apply(normalize, axis=0)\n",
      "--------------------\n",
      "fluTrain = pd.read_csv(jupyter_string)\n",
      "fluTrain.head()\n",
      "=====\n",
      "fluTrain=pd.read_csv(jupyter_string,parse_dates=True)\n",
      "--------------------\n",
      "X_train_sj = X_train_sj.reset_index()\n",
      "X_train_iq = X_train_iq.reset_index()\n",
      "X_test_sj = X_test_sj.reset_index()\n",
      "X_test_iq = X_test_iq.reset_index()\n",
      "=====\n",
      "X_train = pd.concat([X_train_sj, X_train_iq], axis=0)\n",
      "X_train.set_index(jupyter_string, inplace=True)\n",
      "X_train.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "=====\n",
      "X_train_iq.head()\n",
      "--------------------\n",
      "test_df = pd.read_csv(jupyter_string)\n",
      "test_df.head()\n",
      "=====\n",
      "predict_sj = X_test_sj[keys].copy()\n",
      "predict_iq = X_test_iq[keys].copy()\n",
      "--------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "y_sj_pred_final = np.array([sum(x)/2.0 for x in zip(y_sj_pred, y_sj_pred_2) ])\n",
      "--------------------\n",
      "y_iq_pred = reg_iq.predict(X_test_iq)\n",
      "y_iq_pred_2 = reg_iq_2.predict(X_test_iq)\n",
      "=====\n",
      "predict_sj['total_cases' <<unk>>] = y_sj_pred.round().astype(int)\n",
      "predict_sj.head()\n",
      "--------------------\n",
      "predict_sj.to_csv(jupyter_string, index=False)\n",
      "predict_iq.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "predict_df = pd.concat([predict_sj, predict_iq], axis=0)\n",
      "--------------------\n",
      "df_nike = pd.read_csv(jupyter_string, index_col='Date' <unk>)\n",
      "df_sp500 = pd.read_csv(jupyter_string, index_col='Date' <unk>)\n",
      "=====\n",
      "nike = pd.read_csv(jupyter_string, parse_dates = True, index_col = 'Date' <<unk>>)\n",
      "\n",
      "\n",
      "spy = pd.read_csv(jupyter_string, parse_dates = True, index_col = 'Date' <<unk>>)\n",
      "--------------------\n",
      "nike.head()\n",
      "=====\n",
      "nike.head()\n",
      "--------------------\n",
      "spy.head()\n",
      "=====\n",
      "spy.head()\n",
      "--------------------\n",
      "spy = spy[['Adj Close' <unk>, 'Volume' <unk>]]\n",
      "spy.head()\n",
      "=====\n",
      "df = pd.concat([nike['Close' <<unk>>], spy['Close' <<unk>>]], axis=1)\n",
      "\n",
      "\n",
      "df.columns = [jupyter_string, jupyter_string]\n",
      "\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "fluTrain.head()\n",
      "=====\n",
      "fluTrain.head()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].pct_change()\n",
      "\n",
      "\n",
      "df.head()\n",
      "=====\n",
      "returns = df.pct_change()\n",
      "\n",
      "\n",
      "returns.head()\n",
      "--------------------\n",
      "returns = returns.dropna()\n",
      "\n",
      "\n",
      "returns.head()\n",
      "=====\n",
      "returns = returns.dropna(axis = 0)\n",
      "\n",
      "\n",
      "returns.head()\n",
      "--------------------\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "model = smf.ols(formula = jupyter_string, data = returns)\n",
      "results = model.fit()\n",
      "print(results.summary())\n",
      "=====\n",
      "y = returns[jupyter_string]\n",
      "x = returns[jupyter_string]\n",
      "x1 = sm.add_constant(x)\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(x1[:5]) \n",
      "--------------------\n",
      "model = sm.OLS(y, x1)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "linreg = sm.OLS(y, x1)\n",
      "\n",
      "\n",
      "results = linreg.fit()\n",
      "\n",
      "\n",
      "results.summary()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "sns.set_palette(jupyter_string)\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "mental = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "mental.head()\n",
      "=====\n",
      "print(type(mental))\n",
      "mental.head()\n",
      "--------------------\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "mental[jupyter_string] = mental[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "mental['What is your gender?' <<unk>>] = mental['What is your gender?' <<unk>>].replace([\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "    jupyter_string, jupyter_string, jupyter_string], jupyter_string)\n",
      "--------------------\n",
      "mental['What is your gender?' <unk>].value_counts()\n",
      "=====\n",
      "employee_health = mental.groupby(['What is your gender?' <<unk>>,'Have you had a mental health disorder in the past?' <<unk>>])\n",
      "\n",
      "''jupyter_string''    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "employee_health.describe()\n",
      "=====\n",
      "female_health = employee_health.apply(len)\n",
      "\n",
      "\n",
      "print(female_health)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + jupyter_string + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "fluTrain.ILI.hist()\n",
      "=====\n",
      "fluTrain.ILI.hist()\n",
      "--------------------\n",
      "self_employed = mental.groupby(['What is your gender?' <unk>,'Have you ever tried to meet up with hometown friends on Thanksgiving night?' <unk>])\n",
      "\n",
      "''jupyter_string''    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "self_employed = self_employed.apply(len)\n",
      "\n",
      "\n",
      "print(self_employed)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + jupyter_string + jupyter_string + jupyter_string + jupyter_string)\n",
      "=====\n",
      "work_locations = mental.groupby(['Are you self-employed?' <<unk>>,'Do you work remotely?' <<unk>>])\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "employee_work_locations = work_locations.size()\n",
      "\n",
      "\n",
      "print(employee_work_locations)\n",
      "=====\n",
      "work_space = work_locations.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(work_space)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + str(126 +146) + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "work_locations[jupyter_string].value_counts()\n",
      "=====\n",
      "disorders = {}\n",
      "\n",
      "disorderCounts = dict(mental['If yes, what condition(s) have you been diagnosed with?' <<unk>>].value_counts())\n",
      "for i in disorderCounts:\n",
      "    \n",
      "    disorderList = i.split(jupyter_string)\n",
      "    for j in disorderList:\n",
      "        j = j.split(jupyter_string)[0]\n",
      "        disorders[j] = disorders.get(j, 0) + disorderCounts[i]\n",
      "\n",
      "tmp = pd.DataFrame()\n",
      "for i in disorders:\n",
      "    tmp = tmp.append([i] * disorders[i])\n",
      "    \n",
      "\n",
      "tmp.rename(columns={ tmp.columns[0]: jupyter_string }, inplace= True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tmp.Disorder.value_counts()\n",
      "\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "disorderCounts = {}\n",
      "familyCounts = {}\n",
      "\n",
      "disorderCounts = dict(mental['If yes, what condition(s) have you been diagnosed with?' <unk>].value_counts())\n",
      "for i in disorderCounts:\n",
      "    \n",
      "    disorderList = i.split(jupyter_string)\n",
      "    for j in disorderList:\n",
      "        j = j.split(jupyter_string)[0]\n",
      "        disorderCounts[j] = disorderCounts.get(j, 0) + disorderCounts[i]\n",
      "\n",
      "tmp = pd.DataFrame()\n",
      "for i in disorderCounts:\n",
      "    tmp = tmp.append([i] * disorderCounts[i])\n",
      "    \n",
      "\n",
      "tmp.rename(columns={ tmp.columns[0]: jupyter_string }, inplace= True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tmp.Family.value_counts()\n",
      "\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "family_health = mental.groupby(['Have you had a mental health disorder in the past?' <<unk>>, \\\n",
      "                                'Do you have a family history of mental illness?' <<unk>>])\n",
      "\n",
      "''jupyter_string''    \n",
      "\n",
      "--------------------\n",
      "family_health.describe()\n",
      "=====\n",
      "health_history = family_health.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(health_history)\n",
      "print()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + jupyter_string + jupyter_string + jupyter_string)\n",
      "--------------------\n",
      "mental.groupby(['Have you ever tried to meet up with hometown friends on Thanksgiving night?' <unk>, 'Have you ever attended a \"Friendsgiving?\"' <unk>])['Have you ever attended a \"Friendsgiving?\"' <unk>].count()\n",
      "=====\n",
      "mental_by_state = mental.groupby(['What country do you live in?' <<unk>>, \\\n",
      "                               'Have you been diagnosed with a mental health condition by a medical professional?' <<unk>>])\n",
      "\n",
      "''jupyter_string''    \n",
      "\n",
      "--------------------\n",
      "mental_by_state = mental_by_state.mean()\n",
      "mental_by_state.head()\n",
      "=====\n",
      "mental_state = mental_by_state.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(type(mental_state))\n",
      "print(mental_state.nlargest(10))\n",
      "--------------------\n",
      "employee_by_state = mental.groupby(['What country do you live in?' <unk>, \\\n",
      "                               'Have you ever tried to meet up with hometown friends on Thanksgiving night?' <unk>])\n",
      "\n",
      "''jupyter_string''    \n",
      "\n",
      "employee_by_state = employee_by_state.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(type(employee_by_state))\n",
      "print(employee_by_state.nlargest(10))\n",
      "=====\n",
      "comp_compare = mental.groupby(['How many employees does your company or organization have?' <<unk>>, \n",
      "                               'Do you currently have a mental health disorder?' <<unk>>])\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "comp_compare = comp_compare.apply(len)\n",
      "\n",
      "''jupyter_string''    \n",
      "print(type(comp_compare))\n",
      "print(comp_compare)\n",
      "=====\n",
      "work_size = comp_compare.apply(len)\n",
      "\n",
      "\n",
      "print(work_size)\n",
      "print()\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string + str(24 + 109) + jupyter_string + jupyter_string)\n",
      "print()\n",
      "print(jupyter_string + str(19 + 99 + 127 + 73) + jupyter_string + jupyter_string)\n",
      "print()\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "data = pd.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "=====\n",
      "loans_one_hot_enc = pd.get_dummies(loans)\n",
      "--------------------\n",
      "plt.scatter(np.log(df.ILI), np.log(df.Queries))\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import math\n",
      "fluTrain[jupyter_string]= fluTrain.ILI.apply(lambda x:math.log(x))\n",
      "plt.scatter(fluTrain.Queries, fluTrain.ILI_aslog)\n",
      "plt.xlabel(\"Queries\")\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "import json\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn, sklearn.tree\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "loans = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "loans = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_df = pd.DataFrame(train_idx_lst, columns=[jupyter_string])\n",
      "validation_df = pd.DataFrame(validation_idx_lst, columns=[jupyter_string])\n",
      "=====\n",
      "train_data = loans_one_hot_enc.ix[train_idx_lst]\n",
      "validation_data = loans_one_hot_enc.ix[validation_idx_lst]\n",
      "--------------------\n",
      "loans.head()\n",
      "=====\n",
      "loans.head()\n",
      "--------------------\n",
      "loans['grade' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(10,6))\n",
      "loans['grade' <<unk>>].value_counts().plot(kind=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.title(jupyter_string, fontsize=18)\n",
      "plt.xlabel(jupyter_string, fontsize=18)\n",
      "plt.ylabel(jupyter_string, fontsize=18)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,6))\n",
      "loans['home_ownership' <unk>].value_counts().plot(kind=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.title(jupyter_string, fontsize=18)\n",
      "plt.xlabel(jupyter_string, fontsize=18)\n",
      "plt.ylabel(jupyter_string, fontsize=18)\n",
      "=====\n",
      "plt.figure(figsize=(10,6))\n",
      "loans['home_ownership' <<unk>>].value_counts().plot(kind=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.title(jupyter_string, fontsize=18)\n",
      "plt.xlabel(jupyter_string, fontsize=18)\n",
      "plt.ylabel(jupyter_string, fontsize=18)\n",
      "--------------------\n",
      "predictions = small_model.predict(validation_data.ix[:, validation_data.columns != jupyter_string])\n",
      "predictions\n",
      "=====\n",
      "validation_safe_loans = validation_data[validation_data[target] == 1]\n",
      "validation_risky_loans = validation_data[validation_data[target] == -1]\n",
      "\n",
      "sample_validation_data_risky = validation_risky_loans[0:2]\n",
      "sample_validation_data_safe = validation_safe_loans[0:2]\n",
      "\n",
      "sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)\n",
      "sample_validation_data\n",
      "--------------------\n",
      "loans[jupyter_string] = loans['bad_loans' <unk>].apply(lambda x: +1 if x == 0 else -1)\n",
      "loans = loans.drop('bad_loans' <unk>, axis=1)\n",
      "=====\n",
      "loans[jupyter_string] = loans['bad_loans' <<unk>>].apply(lambda x : +1 if x==0 else -1)\n",
      "loans = loans.drop('bad_loans' <<unk>>, 1)\n",
      "--------------------\n",
      "loans['safe_loans' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(10,6))\n",
      "loans[jupyter_string].value_counts().plot(kind=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.tick_params(axis=jupyter_string, labelsize=18)\n",
      "plt.title(jupyter_string, fontsize=18)\n",
      "plt.xlabel(jupyter_string, fontsize=18)\n",
      "plt.ylabel(jupyter_string, fontsize=18)\n",
      "--------------------\n",
      "sample_validation_data.head()\n",
      "=====\n",
      "samp_vald_data_prob = decision_tree_model.predict_proba(sample_validation_data.ix[:, sample_validation_data.columns != jupyter_string])[:,1]\n",
      "--------------------\n",
      "plt.scatter(fluTrain.Queries, np.log(fluTrain.ILI_aslog))\n",
      "plt.xlabel(\"Queries\")\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(fluTrain.Queries, np.log(fluTrain.ILI))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "loans_data = pd.read_csv(jupyter_string)\n",
      "loans_data.info()\n",
      "=====\n",
      "features = ['grade' <<unk>>,                     \n",
      "            'sub_grade' <<unk>>,                 \n",
      "            'short_emp' <<unk>>,                 \n",
      "            'emp_length_num' <<unk>>,            \n",
      "            'home_ownership' <<unk>>,            \n",
      "            'dti' <<unk>>,                       \n",
      "            'purpose' <<unk>>,                   \n",
      "            'term' <<unk>>,                      \n",
      "            'last_delinq_none' <<unk>>,          \n",
      "            'last_major_derog_none' <<unk>>,     \n",
      "            'revol_util' <<unk>>,                \n",
      "            'total_rec_late_fee' <<unk>>,        \n",
      "           ]\n",
      "\n",
      "target = jupyter_string                   \n",
      "\n",
      "\n",
      "loans = loans[features + [target]]\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "b_train = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string.format(b_train.shape))\n",
      "\n",
      "\n",
      "--------------------\n",
      "b_train.head()\n",
      "=====\n",
      "b_test = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string.format(b_test.shape))\n",
      "\n",
      "\n",
      "--------------------\n",
      "X_train = b_train.drop(['target' <unk>], axis=1)\n",
      "y_train = b_train['target' <unk>]\n",
      "X_test = b_test.drop(['target' <unk>], axis=1)\n",
      "y_test = b_test['target' <unk>]\n",
      "=====\n",
      "all_features = ['total_bids' <<unk>>,'total_auctions' madeupword0002,'bids_per_auction' <<unk>>,\n",
      "                'mean_time_diff' <<unk>>, 'mean_response' <<unk>>, 'min_response' <<unk>>,\n",
      "                'ip_entropy' <<unk>>, 'url_entropy' <<unk>>]\n",
      "\n",
      "features = ['total_bids' <<unk>>, 'total_auctions' madeupword0002, 'bids_per_auction' <<unk>>, 'mean_time_diff' <<unk>>, 'ip_entropy' <<unk>>, 'url_entropy' <<unk>>]\n",
      "target = ['outcome' <<unk>>]\n",
      "\n",
      "X = np.array(b_train[features])\n",
      "y = np.array(b_train[target]).ravel()\n",
      "print(jupyter_string.format(X.shape, y.shape))\n",
      "\n",
      "X_submission = np.array(b_test[features])\n",
      "print(jupyter_string.format(X_submission.shape))\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "print(jupyter_string.format(X_train.shape, y_train.shape))\n",
      "print(jupyter_string.format(X_test.shape, y_test.shape))\n",
      "=====\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "y_test_transformed = np.hstack((1 - y_test.reshape(y_test.size,1),\n",
      "                                y_test.reshape(y_test.size,1)))\n",
      "--------------------\n",
      "xgb_grid_search = GridSearchCV(XGB_Classifier, xgb_param_grid, cv=CV_SSS, scoring=jupyter_string)\n",
      "xgb_grid_search.fit(X_train, y_train)\n",
      "=====\n",
      "random_grid_XGB_CV = RandomizedSearchCV(XGB_Classifier, xgb_param_grid, scoring = jupyter_string, cv = CV_SSS,\n",
      "                                        n_iter = 40)\n",
      "random_grid_XGB_CV.fit(X,y)\n",
      "print(random_grid_XGB_CV.best_score_)\n",
      "print(random_grid_XGB_CV.best_params_)\n",
      "--------------------\n",
      "from sklearn.metrics import roc_curve, auc\n",
      "\n",
      "XGB_Classifier = xgboost.XGBClassifier(max_depth=xgb_param[jupyter_string],\n",
      "                                       learning_rate=xgb_param[jupyter_string],\n",
      "                                       n_estimators=xgb_param[jupyter_string],\n",
      "\n",
      "\n",
      "                                       reg_alpha=xgb_param[jupyter_string],\n",
      "                                       reg_lambda=xgb_param[jupyter_string])\n",
      "\n",
      "XGB_Classifier.fit(X_train, y_train)\n",
      "\n",
      "y_pred_prob = XGB_Classifier.predict_proba(X_test)[:,1]\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
      "\n",
      "plt.plot([0, 1], [0, 1], jupyter_string)\n",
      "plt.plot(fpr, tpr\n",
      "=====\n",
      "XGB_Classifier = xgboost.XGBClassifier(max_depth=xgb_param[jupyter_string],\n",
      "                                       learning_rate=xgb_param[jupyter_string],\n",
      "                                       n_estimators=xgb_param[jupyter_string],\n",
      "                                       reg_alpha=xgb_param[jupyter_string],\n",
      "                                       reg_lambda=xgb_param[jupyter_string])\n",
      "XGB_Classifier.fit(X,y)\n",
      "\n",
      "print(XGB_Classifier.feature_importances_)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "y_score = XGB_Classifier.predict_proba(X_test)\n",
      "auc_score = roc_auc_score(y_test_transformed, y_score, average=jupyter_string)\n",
      "print(jupyter_string.format(auc_score))\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test_transformed, y_score)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.grid(True)\n",
      "=====\n",
      "fpr = dict()\n",
      "tpr = dict()\n",
      "roc_auc = dict()\n",
      "for i in range(2):\n",
      "    fpr[i], tpr[i], _ = roc_curve(y_test_transformed[:, i], y_score[:, i])\n",
      "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "lw = 2\n",
      "\n",
      "plt.plot(fpr[1], tpr[1], color=jupyter_string,\n",
      "         lw=lw, label=jupyter_string % roc_auc[1])\n",
      "plt.plot([0, 1], [0, 1], color=jupyter_string, lw=lw, linestyle=jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "submission = pd.DataFrame({\n",
      "        \"PassengerId\": test_df.PassengerId,\n",
      "        \"Survived\": XGB_Classifier.predict_proba(test_df)[:,1]\n",
      "    })\n",
      "submission.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "XGB_Classifier.fit(X, y)\n",
      "submission_prediction = XGB_Classifier.predict_proba(X_submission)[:,1]\n",
      "print(submission_prediction)\n",
      "--------------------\n",
      "submission_df = pd.DataFrame({'id' <unk>:test_df.id, 'target' <unk>:submission_prediction})\n",
      "submission_df.head()\n",
      "=====\n",
      "b_test[jupyter_string] = submission_prediction\n",
      "b_test.head()\n",
      "--------------------\n",
      "model1.fit().summary()\n",
      "=====\n",
      "fitted=model1.fit()\n",
      "\n",
      "fitted.fittedvalues\n",
      "--------------------\n",
      "b_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "b_test[['bidder_id' <<unk>>,jupyter_string]].to_csv(jupyter_string, sep=jupyter_string, header=True, index=False)\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test, bst.predict_proba(test_data)[:,1])\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "test_data = xgb.DMatrix(X_test)\n",
      "y_score = bst.predict(test_data)\n",
      "y_score = np.hstack((1 - y_score.reshape(y_score.size,1),\n",
      "                     y_score.reshape(y_score.size,1)))\n",
      "\n",
      "fpr = {}\n",
      "tpr = {}\n",
      "roc_auc = {}\n",
      "for i in range(2):\n",
      "    fpr[i], tpr[i], _ = roc_curve(y_test_transformed[:, i], y_score[:, i])\n",
      "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
      "\n",
      "plt.figure()\n",
      "lw = 2\n",
      "plt.plot(fpr[1], tpr[1], color=jupyter_string,\n",
      "         lw=lw, label=jupyter_string % roc_auc[1])\n",
      "plt.plot([0, 1], [0, 1], color=jupyter_string, lw=lw, linestyle=jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "submission = pd.read_csv(jupyter_string)\n",
      "submission['target' <unk>] = y_score\n",
      "submission.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "b_test[jupyter_string] = submission_prediction \n",
      "b_test.head()\n",
      "--------------------\n",
      "b_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "b_test[['bidder_id' <<unk>>,jupyter_string]].to_csv(jupyter_string, sep=jupyter_string, header=True, index=False)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.bar(np.arange(10),tmdb['revenue' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "revenue_dict = {}\n",
      "\n",
      "\n",
      "movies_and_revenue = data[[\"original_title\", \"revenue_adj\"]]\n",
      "movies_and_budget = data[['original_title' <<unk>>,'budget_adj' <<unk>>]]\n",
      "movies_and_popularity = data[['original_title' <<unk>>,'popularity' <<unk>>]]\n",
      "movies_and_votes= data[['original_title' <<unk>>,'vote_average' <<unk>>]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.3)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_revenue.sort_values(by = \"revenue_adj\", ascending=False).head(10).original_title, \n",
      "    movies_and_revenue.sort_values(by = \"revenue_adj\", ascending=False).head(10).revenue_adj)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "ax = sns.barplot(\n",
      "    movies_and_budget.sort_values(by = \"budget_adj\", ascending=False).head(10).original_title, \n",
      "    movies_and_budget.sort_values(by = \"budget_adj\", ascending=False).head(10).budget_adj)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_budget.sort_values(by=\"budget_adj\", ascending=False).head(10).original_title, \n",
      "    movies_and_budget.sort_values(by=\"budget_adj\", ascending=False).head(10).budget_adj)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = sns.barplot(\n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).original_title, \n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).popularity)\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).original_title, \n",
      "    movies_and_popularity.sort_values(by=\"popularity\", ascending=False).head(10).popularity)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel='popularity' <<unk>>, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "data.head(10)\n",
      "=====\n",
      "data.head(7)\n",
      "--------------------\n",
      "ax = sns.barplot(\n",
      "    movies_and_vote.sort_values(by=\"vote_average\", ascending=False).head(10).original_title, \n",
      "    movies_and_vote.sort_values(by=\"vote_average\", ascending=False).head(10).vote_average)\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel='vote_average' <unk>, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "\n",
      "\n",
      "ax = sns.barplot(\n",
      "    movies_and_votes.sort_values(by=\"vote_average\", ascending=False).head(10).original_title, \n",
      "    movies_and_votes.sort_values(by=\"vote_average\", ascending=False).head(10).vote_average)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.scatter(fitted.fittedvalues, fitted.resid)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fluTrain.corr()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head(7)\n",
      "=====\n",
      "data = data[data[\"cast\"].isnull() == False]\n",
      "data = data[data[\"genres\"].isnull() == False]\n",
      "\n",
      "data = data[data.budget_adj != 0]\n",
      "data = data[data.revenue_adj != 0]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.boxplot(df['vote_average' <unk>])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.boxplot(df['vote_count' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"vote_average\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.distplot(temp_df.vote_average)\n",
      "\n",
      "ax = sns.boxplot(x = temp_df.vote_average)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "data.describe()\n",
      "--------------------\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"rating\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.distplot(temp_df.rating)\n",
      "\n",
      "ax = sns.boxplot(x = temp_df.rating)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"release_year\", \"vote_average\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.violinplot(x = temp_df.vote_average, y = temp_df.release_year, orient =jupyter_string)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df.cast.value_counts().head(10)\n",
      "=====\n",
      "actor_dict = {}\n",
      "\n",
      "actors = data[\"cast\"]\n",
      "actors = actors.str.split(jupyter_string)\n",
      "actors = np.array(actors)\n",
      "for actorList in actors:\n",
      "    \n",
      "    for actor in actorList:\n",
      "        actor = actor.lstrip() \n",
      "        if actor not in actor_dict:\n",
      "            actor_dict[actor] = 1\n",
      "        else:\n",
      "            actor_dict[actor] += 1\n",
      "                \n",
      "\n",
      "\n",
      "sorted_actor_dict = sorted(actor_dict.items(), key = operator.itemgetter(1), reverse = True)\n",
      "\n",
      "\n",
      "\n",
      "x_axis = list()\n",
      "y_axis = list()\n",
      "\n",
      "for item in sorted_actor_dict[0:20]:\n",
      "    x_axis.append(item[0])\n",
      "    y_axis.append(item[1])\n",
      "\n",
      "\n",
      "sns.set(rc={jupyter_string:(12,10)}, font_scale=1.4)\n",
      "ax = sns.barplot(x_axis, y_axis, palette=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3)\n",
      "\n",
      "temp_df = data[[\"adjusted_revenue\", \"adjusted_budget\", \"popularity\", \"vote_average\"]]\n",
      "\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "ax = sns.regplot(x = \"adjusted_revenue\", y = \"adjusted_budget\", data = temp_df)\n",
      "\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "aux_df = data[['revenue_adj' <<unk>>, 'budget_adj' <<unk>>, 'popularity' <<unk>>, 'vote_average' <<unk>>]]\n",
      "\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3, style=jupyter_string)\n",
      "\n",
      "\n",
      "f1 = sns.jointplot(x = \"budget_adj\", y = \"revenue_adj\", kind = jupyter_string, data = aux_df)\n",
      "f1.fig.suptitle(jupyter_string)\n",
      "\n",
      "f2 = sns.jointplot(x = \"budget_adj\", y = \"popularity\", kind = jupyter_string, data = aux_df)\n",
      "f2.fig.suptitle(jupyter_string)\n",
      "f3 = sns.jointplot(x = \"budget_adj\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f3.fig.suptitle(jupyter_string)\n",
      "\n",
      "f4 = sns.jointplot(x = \"revenue_adj\", y = \"popularity\", kind = jupyter_string, data = aux_df)\n",
      "f4.fig.suptitle(jupyter_string)\n",
      "f5 = sns.jointplot(x = \"revenue_adj\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f5.fig.suptitle(jupyter_string)\n",
      "\n",
      "f6 = sns.jointplot(x = \"popularity\", y = \"vote_average\", kind = jupyter_string, data = aux_df)\n",
      "f6.fig.suptitle(jupyter_string)\n",
      "--------------------\n",
      "df_genres = pd.DataFrame(df.genres.str.split(jupyter_string).tolist(), columns=[jupyter_string, jupyter_string])\n",
      "df_genres = df_genres.set_index(jupyter_string)\n",
      "df_genres.head()\n",
      "=====\n",
      "year_set = set()\n",
      "genre_set = set()\n",
      "genres_and_year = data[[\"genres\", \"release_year\"]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "production_year = genres_and_year[\"release_year\"]\n",
      "production_year = production_year.drop_duplicates()\n",
      "for year in production_year:\n",
      "    if year not in year_set:\n",
      "        year_set.add(year)\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split(jupyter_string)\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_set:\n",
      "                genre_set.add(genre)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gerne_count_per_year_df = pd.DataFrame(index = year_set, columns=genre_set)\n",
      "gerne_count_per_year_df[:] = 0  \n",
      "\n",
      "for year in year_set:\n",
      "    genre_dict = {}\n",
      "    genres_in_year = genres_and_year[genres_and_year.release_year == year]\n",
      "    genres_in_year = genres_in_year[\"genres\"].values\n",
      "    for elem in genres_in_year:\n",
      "        genres_row = elem.split(jupyter_string)\n",
      "        for genre in genres_row:\n",
      "            if genre not in genre_dict:\n",
      "                genre_dict[genre] = 1\n",
      "            else:\n",
      "                genre_dict[genre] = genre_dict[genre] + 1\n",
      "                    \n",
      "    aux_df = pd.DataFrame(genre_dict, index = [year])\n",
      "    gerne_count_per_year_df.loc[year, aux_df.columns] = gerne_count_per_year_df.loc[year, aux_df.columns] + aux_df.loc[year]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "most_popular_genre_by_year = pd.DataFrame([gerne_count_per_year_df.idxmax(axis = 1).values,\n",
      "                                          gerne_count_per_year_df.apply( max, axis=1 ).values],\n",
      "                                          columns = gerne_count_per_year_df.index,\n",
      "                                         index = [jupyter_string, jupyter_string])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "sns.set(rc={jupyter_string:(15,15)}, font_scale=1.3, style=jupyter_string)\n",
      "\n",
      "\n",
      "f1 = sns.pairplot(data[['revenue_adj' <unk>, 'budget_adj' <unk>, 'popularity' <unk>, 'vote_average' <unk>]])\n",
      "f1.fig.suptitle(jupyter_string)\n",
      "=====\n",
      "f1 = sns.pairplot(aux_df, kind=jupyter_string, diag_kind=jupyter_string, diag_kws=dict(shade=True))\n",
      "f1.fig.suptitle(jupyter_string)\n",
      "f1.fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.bar(most_popular_genre_by_year.index, most_popular_genre_by_year[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.bar(most_popular_genre_by_year.index, most_popular_genre_by_year[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(rc={jupyter_string:(12,12)}, font_scale=1.3)\n",
      "sns.set_palette(jupyter_string, 20, .65)\n",
      "\n",
      "\n",
      "ax = gerne_count_per_year_df.plot.bar(stacked=True);\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "ax = gerne_count_per_year_df.plot.area(stacked=True);\n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "gerne_count_per_genre_df = pd.DataFrame()\n",
      "gerne_count_per_genre_df[jupyter_string] = gerne_count_per_year_df.sum(axis=1)\n",
      "gerne_count_per_genre_df[jupyter_string] = gerne_count_per_genre_df.sum(axis=0)\n",
      "gerne_count_per_genre_df[jupyter_string] = gerne_count_per_genre_df.sum(axis=1)\n",
      "gerne_count_per_genre_df[jupyter_string] = gerne_count_per_genre_df.sum(axis=0)\n",
      "gerne_count_per_genre_df[jupyter_string] = gerne_count_per_genre_df.sum(axis=1)\n",
      "gerne_count_per_genre_df = gerne_count_per_genre_df[\n",
      "=====\n",
      "temp = gerne_count_per_year_df.apply(sum)\n",
      "temp = temp.sort_values(ascending= False)\n",
      "\n",
      "sns.set(rc={jupyter_string:(12,9)}, font_scale=1.4)\n",
      "ax = sns.barplot(temp.index, temp, palette=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "for item in ax.get_xticklabels():\n",
      "    item.set_rotation(85)\n",
      "    \n",
      "ax.set(xlabel=jupyter_string, ylabel=jupyter_string, title = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_test.head()\n",
      "=====\n",
      "test=pd.read_csv(jupyter_string,parse_dates=True)\n",
      "test.head()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test[jupyter_string]=np.exp(fitted.predict(test))\n",
      "--------------------\n",
      "wine_type = wine_data['wine_type' <unk>]\n",
      "wine_data = wine_data.drop('wine_type' <unk>, axis=1)\n",
      "=====\n",
      "wine_data_mag = wine_data.loc[:, ['magnesium' madeupword0002, 'color' <<unk>>]]\n",
      "wine_data_abv = wine_data.loc[:, 'abv' <<unk>>]\n",
      "wine_data_mag.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "query = input(jupyter_string)\n",
      "greet = check_for_greeting(query)\n",
      "while greet != jupyter_string:\n",
      "    new_query = greet + jupyter_string\n",
      "    query = input(new_query)\n",
      "    greet = check_for_greeting(query)\n",
      "question = jupyter_string.join(l for l in query if l not in string.punctuation)\n",
      "city = get_location(question)\n",
      "day_of_week = get_day_of_week(question)\n",
      "weather, time, city = get_weather(city,day_of_week)\n",
      "if city == None:\n",
      "    print(jupyter_string)\n",
      "else:   \n",
      "    output = get_output(query)\n",
      "    \n",
      "    weather_data = output\n",
      "weather_data\n",
      "--------------------\n",
      "surveys = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "surveys_df_subset = surveys_df[['species_id' <unk>]]\n",
      "=====\n",
      "surveys_species = surveys_df['species_id' <<unk>>]\n",
      "--------------------\n",
      "surveys_df.isnull()\n",
      "=====\n",
      "pd.isnull(surveys_df)\n",
      "--------------------\n",
      "surveys_df['weight' <unk>].isnull()\n",
      "=====\n",
      "empty_weights = surveys_df[pd.isnull(surveys_df['weight' <<unk>>])]['weight' <<unk>>]\n",
      "print(empty_weights)\n",
      "--------------------\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "surveys_df.head()\n",
      "=====\n",
      "true_copy_surveys_df = surveys_df.copy()\n",
      "\n",
      "\n",
      "ref_surveys_df = surveys_df\n",
      "--------------------\n",
      "surveys_df = surveys_df[pd.notnull(surveys_df['sex' <unk>])]\n",
      "print(surveys_df.shape)\n",
      "surveys_df.head()\n",
      "=====\n",
      "nosex_df = surveys_df[~surveys_df['sex' madeupword0002].isin([jupyter_string, jupyter_string])]\n",
      "--------------------\n",
      "nosex_df.head()\n",
      "=====\n",
      "sex_df = surveys_df[surveys_df['sex' madeupword0002].isin([jupyter_string, jupyter_string]) \\\n",
      "                    & surveys_df['weight' <<unk>>]]\n",
      "--------------------\n",
      "nosex_df.head()\n",
      "=====\n",
      "grouped = sex_df.groupby(['plot_id' <<unk>>, 'sex' madeupword0002])['weight' <<unk>>]\n",
      "grouped.mean().unstack().plot(kind=jupyter_string, stacked=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "print(len(data3))\n",
      "data3.head()\n",
      "--------------------\n",
      "np.sqrt(mean_squared_error(test.ILI,test.predict_test))\n",
      "=====\n",
      "SSE=((test.predict_test - test.ILI)**2).sum()\n",
      "nrow,ncol=test.shape\n",
      "RMSE=math.sqrt(SSE/nrow)\n",
      "RMSE\n",
      "--------------------\n",
      "surveys_df.head()\n",
      "=====\n",
      "ref_surveys_df.head()\n",
      "--------------------\n",
      "surveys_df.head()\n",
      "=====\n",
      "surveys_df.head()\n",
      "--------------------\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "surveys_df.head()\n",
      "=====\n",
      "surveys_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "lens.groupby(jupyter_string)[jupyter_string].count().sort_values(ascending=False)[:25]\n",
      "=====\n",
      "most_rated=lens.groupby(jupyter_string).size().sort_values(ascending=False)[:25]\n",
      "most_rated\n",
      "--------------------\n",
      "lens.groupby('title' <unk>)['rating' <unk>].mean().sort_values(ascending=False)[:10]\n",
      "=====\n",
      "movie_stats=lens.groupby(jupyter_string).agg({jupyter_string:[np.size,np.mean]})\n",
      "movie_stats.head()\n",
      "--------------------\n",
      "movie_stats.sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "movie_stats.sort_values([(jupyter_string,jupyter_string)],ascending=False).head()\n",
      "--------------------\n",
      "atleast_100=movie_stats[jupyter_string][jupyter_string]>=100\n",
      "movie_stats[atleast_100].sort_values([(jupyter_string,jupyter_string)],ascending=False)[:15]\n",
      "=====\n",
      "movie_stats.sort_values([(jupyter_string,jupyter_string)],ascending=False).head()\n",
      "atleast_100=movie_stats[jupyter_string][jupyter_string]>=100\n",
      "movie_stats[atleast_100].sort_values([(jupyter_string,jupyter_string)],ascending=False)[:15]\n",
      "--------------------\n",
      "most_rated_movies=movie_stats[jupyter_string][jupyter_string]\n",
      "most_rated_movies.head()\n",
      "=====\n",
      "most_50=lens.groupby(jupyter_string).size().sort_values(ascending=False)[:50]\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "users.age.plot.hist(bins=30)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "--------------------\n",
      "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "age_labels = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "users[jupyter_string] = pd.cut(users.age, bins=age_bins, labels=age_labels)\n",
      "=====\n",
      "labels = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "lens[jupyter_string] = pd.cut(lens.age, range(0, 81, 10), right=False, labels=labels)\n",
      "lens[[jupyter_string, jupyter_string]].drop_duplicates()[:10]\n",
      "--------------------\n",
      "train=data.iloc[0:train.shape[0],:]\n",
      "test=data.iloc[train.shape[0]:,:]\n",
      "=====\n",
      "fluTrain[jupyter_string]=fluTrain.ILI.shift(2)\n",
      "fluTrain.ILI_lag2[0:10]\n",
      "--------------------\n",
      "lens[[jupyter_string, jupyter_string]].drop_duplicates()\n",
      "=====\n",
      "lens.groupby(jupyter_string).agg({jupyter_string: [np.size, np.mean]})\n",
      "\n",
      "--------------------\n",
      "lens.groupby(jupyter_string).agg({jupyter_string: [np.size, np.mean]})\n",
      "=====\n",
      "lens.set_index(jupyter_string, inplace=True)\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_pca = pca.fit_transform(X)\n",
      "plt.scatter(X_pca[:,0], X_pca[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.arange(1, 7)\n",
      "plt.plot(x, np.cumsum(pca.explained_variance_ratio_), jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.show()\n",
      "--------------------\n",
      "pca = PCA(n_components=3)\n",
      "pca.fit(X)\n",
      "X_pca = pca.transform(X)\n",
      "=====\n",
      "first_pc = pca.components_[0]\n",
      "second_pc = pca.components_[1]\n",
      "\n",
      "transformed_data = pca.transform(data)\n",
      "plt.close()\n",
      "for ii in transformed_data:\n",
      "    plt.scatter(first_pc[0]*ii[0], first_pc[1]*ii[0], color=jupyter_string)\n",
      "    plt.scatter(second_pc[0]*ii[1], second_pc[1]*ii[1], color=jupyter_string)\n",
      "    plt.scatter(ii[0], ii[1], color=jupyter_string)\n",
      "    \n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(6,9))\n",
      "\n",
      "labels = [ujupyter_string, ujupyter_string, ujupyter_string]\n",
      "\n",
      "sizes = [2698,464,8]\n",
      "colors = [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "explode = (0.05,0,0)\n",
      "\n",
      "patches,l_text,p_text = plt.pie(sizes,explode=explode,labels=labels,colors=colors,\n",
      "                                labeldistance = 1.1,autopct = jupyter_string,shadow = False,\n",
      "                                startangle = 90,pctdistance = 0.6)\n",
      "\n",
      "for t in l_text:\n",
      "    t.set_size=(30)\n",
      "for t in p_text:\n",
      "    t.set_size=(20)\n",
      "\n",
      "plt.axis(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "dcounty.plot(kind=jupyter_string,stacked=True, color=[jupyter_string,jupyter_string])\n",
      "plt.show() \n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "data = pd.read_csv(jupyter_string)\n",
      "print(data.head())\n",
      "--------------------\n",
      "df.plot(kind=jupyter_string,stacked=True, color=[jupyter_string,jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.factorplot('SocioecStatus' <<unk>>,data=data,palette=jupyter_string, kind=jupyter_string,hue=jupyter_string, size=8)\n",
      "plt.show\n",
      "--------------------\n",
      "sns.factorplot('MaritalStatus' <unk>,data=data,palette=jupyter_string, kind=jupyter_string,hue=jupyter_string, size=8)\n",
      "plt.show\n",
      "=====\n",
      "plt.figure(figsize=(9,6))\n",
      "sns.stripplot(x='MaritalStatus' <<unk>>,y='Age' <<unk>>,hue='SocioecStatus' <<unk>>, jitter=True, data=data, palette=jupyter_string, size=5)\n",
      "plt.show()\n",
      "--------------------\n",
      "print(jupyter_string.format(len(data[data['Gender' <unk>] == jupyter_string])))\n",
      "print(jupyter_string.format(len(data[data['Gender' <unk>] == jupyter_string])))\n",
      "=====\n",
      "import numpy as np \n",
      "\n",
      "print(data['AccusedRef' <<unk>>].shape[0], jupyter_string)\n",
      "\n",
      "datamale=data.loc[(data['Sex' <<unk>>] ==jupyter_string)] \n",
      "datafemale=data.loc[(data['Sex' <<unk>>] ==jupyter_string)] \n",
      "datasexunknow=data.shape[0]-datamale.shape[0]-datafemale.shape[0]\n",
      "\n",
      "print(datamale.shape[0], jupyter_string)\n",
      "print(datafemale.shape[0], jupyter_string)\n",
      "print(datasexunknow, jupyter_string)\n",
      "--------------------\n",
      "data['Age' <unk>].min(), data['Age' <unk>].max(), data['Age' <unk>].mean()\n",
      "=====\n",
      "dataage=data.loc[(data['Age' <<unk>>] >0)] \n",
      "print(dataage.shape[0], jupyter_string)\n",
      "\n",
      "print(jupyter_string,np.min(dataage['Age' <<unk>>])) \n",
      "print(jupyter_string,np.max(dataage['Age' <<unk>>])) \n",
      "print(jupyter_string, int(np.mean(dataage['Age' <<unk>>])) )\n",
      "--------------------\n",
      "fluTrain[jupyter_string]=np.log(fluTrain.ILI_lag2)\n",
      "fluTrain[jupyter_string]=np.log(fluTrain.IL1)\n",
      "fluTrain.head()\n",
      "=====\n",
      "plt.scatter(np.log(fluTrain.ILI_lag2),np.log(fluTrain.ILI))\n",
      "plt.xlabel=\"ILI\"\n",
      "plt.ylabel=jupyter_string\n",
      "plt.show()\n",
      "--------------------\n",
      "data.loc[(data['Age' <unk>] < 16) & (data['Sex' madeupword0002] == jupyter_string), 'Sex' madeupword0002].value_counts()\n",
      "=====\n",
      "def male_famle_child(witches):\n",
      "    age,sex = witches\n",
      "     \n",
      "    if age < 16:\n",
      "        return jupyter_string\n",
      "    else:\n",
      "        return sex\n",
      "\n",
      "data[jupyter_string] = data[[\"Age\",\"Sex\"]].apply(male_famle_child,axis=1)\n",
      "count=data[jupyter_string].value_counts()\n",
      "print(count)\n",
      "\n",
      "datachild=data.loc[(data[jupyter_string] ==jupyter_string)]\n",
      "print(datachild.loc[:'FirstName' <<unk>> ,'Notes' <<unk>>])\n",
      "--------------------\n",
      "data[jupyter_string].value_counts().plot(kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data1=dataage.loc[~dataage.Age_estcareer]\n",
      "print(data1.shape[0], jupyter_string)\n",
      "data2=dataage.loc[~dataage.Age_estchild]\n",
      "print(data2.shape[0], jupyter_string)\n",
      "--------------------\n",
      "data1.groupby('County' <unk>)['County' <unk>].count()\n",
      "=====\n",
      "dcounty=pd.crosstab(data.Res_county,data.Sex, margins=True)\n",
      "dcounty.sort_values(by=jupyter_string, inplace=True)\n",
      "print(dcounty)\n",
      "--------------------\n",
      "dmarital=pd.crosstab(data.Married,data.Sex, margins=True)\n",
      "dmarital.sort_values(by=jupyter_string, inplace=True)\n",
      "print(dmarital)\n",
      "=====\n",
      "dmaritalstatus=pd.crosstab(data.MaritalStatus,data.Sex, margins=True)\n",
      "dmaritalstatus.sort_values(by=jupyter_string, inplace=True)\n",
      "print(dmaritalstatus)\n",
      "--------------------\n",
      "firstname=pd.crosstab(data.FirstName,data.Sex, margins=True)\n",
      "firstname.sort_values(by=jupyter_string, inplace=True)\n",
      "print(firstname)\n",
      "=====\n",
      "count=data[\"FirstName\"].value_counts()\n",
      "print(count)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "sns.distplot(df['age' <unk>])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.show() \n",
      "\n",
      "import seaborn as sns \n",
      "import matplotlib.pyplot as plt \n",
      "\n",
      "sns.distplot(dataage['Age' <<unk>>])\n",
      "plt.show()   \n",
      "--------------------\n",
      "dataage['Gender' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(6,9))\n",
      "\n",
      "labels = [ujupyter_string, ujupyter_string, ujupyter_string]\n",
      "\n",
      "sizes = [2698,464,8]\n",
      "colors = [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "explode = (0.05,0,0)\n",
      "\n",
      "patches,l_text,p_text = plt.pie(sizes,explode=explode,labels=labels,colors=colors,\n",
      "                                labeldistance = 1.1,autopct = jupyter_string,shadow = False,\n",
      "                                startangle = 90,pctdistance = 0.6)\n",
      "\n",
      "for t in l_text:\n",
      "    t.set_size=(30)\n",
      "for t in p_text:\n",
      "    t.set_size=(20)\n",
      "\n",
      "plt.axis(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "decomposition = seasonal_decompose(week_timeAnalysisKSSuccessRate)\n",
      "\n",
      "trend = decomposition.trend\n",
      "seasonal = decomposition.seasonal\n",
      "residual = decomposition.resid\n",
      "\n",
      "plt.subplot(411)\n",
      "plt.plot(week_timeAnalysisKSSuccessRate, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(412)\n",
      "plt.plot(trend, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(413)\n",
      "plt.plot(seasonal,label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.subplot(414)\n",
      "plt.plot(residual, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.tight_layout()\n",
      "=====\n",
      "week_timeAnalysisKS_trend = week_timeAnalysisKS[[jupyter_string]].rolling(60).mean()\n",
      "\n",
      "\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_trend.index,\n",
      "          y=week_timeAnalysisKS_trend.countProj)]\n",
      "\n",
      "fig = dict(data=trend, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "--------------------\n",
      "week_timeAnalysisKS_trend = week_timeAnalysisKS[[jupyter_string,jupyter_string]].rolling(60).mean()\n",
      "\n",
      "\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_trend.index,\n",
      "          y=week_timeAnalysisKS_trend.countProj)]\n",
      "\n",
      "fig = dict(data=trend, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS_trend_success = week_timeAnalysisKSSuccess[[jupyter_string]].rolling(60).mean()\n",
      "week_timeAnalysisKS_trend_fail = week_timeAnalysisKSFail[[jupyter_string]].rolling(60).mean()\n",
      "\n",
      "week_timeAnalysisKS_trend_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_trend_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "ks_raw=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "=====\n",
      "FluTrend2=smf.ols(formula=jupyter_string,data=fluTrain)\n",
      "fitted2=FluTrend2.fit()\n",
      "fitted2.summary()\n",
      "--------------------\n",
      "week_timeAnalysisKS_trend_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_trend_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "=====\n",
      "trend = [go.Scatter(\n",
      "          x=week_timeAnalysisKS.index,\n",
      "          y=week_timeAnalysisKS.countProj)]\n",
      "\n",
      "\n",
      "week_timeAnalysisKS_seasonality = week_timeAnalysisKS[[jupyter_string]].diff()\n",
      "seasonality = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_seasonality.index,\n",
      "          y=week_timeAnalysisKS_seasonality.countProj)]\n",
      "\n",
      "fig = dict(data=seasonality, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "--------------------\n",
      "seasonality = [go.Scatter(\n",
      "          x=week_timeAnalysisKS_seasonality.index,\n",
      "          y=week_timeAnalysisKS_seasonality.countProj)]\n",
      "\n",
      "fig = dict(data=seasonality, layout=layout)\n",
      "plotly.offline.iplot(fig, filename = jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS_seasonality_success = week_timeAnalysisKSSuccess[[jupyter_string]].diff()\n",
      "week_timeAnalysisKS_seasonality_fail = week_timeAnalysisKSFail[[jupyter_string]].diff()\n",
      "\n",
      "week_timeAnalysisKS_seasonality_success.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "week_timeAnalysisKS_seasonality_fail.countProj.plot(title=jupyter_string, figsize=(30,20),label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "ks_raw=ks_raw.drop(['usd_pledged_real' <unk>,'usd_goal_real' <unk>],axis=1)\n",
      "=====\n",
      "ks=ks_raw.drop([\"ID\", \"goal\", \"pledged\", \"usd pledged\", \"currency\"], axis=1)\n",
      "\n",
      "\n",
      "ks.dtypes \n",
      "--------------------\n",
      "ks[\"launched\"] = pd.to_datetime(ks[\"launched\"])\n",
      "ks[\"deadline\"] = pd.to_datetime(ks[\"deadline\"])\n",
      "=====\n",
      "ks[\"launched\"] = pd.to_datetime(ks[\"launched\"], infer_datetime_format=True)\n",
      "ks[\"deadline\"] = pd.to_datetime(ks[\"deadline\"], infer_datetime_format=True)\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "closed=ks[\"deadline\"]<datetime.now()\n",
      "closed.value_counts()\n",
      "--------------------\n",
      "import pandas_profiling\n",
      "pandas_profiling.ProfileReport(ks)\n",
      "=====\n",
      "pandas_profiling.ProfileReport(ks)\n",
      "--------------------\n",
      "ks.info()\n",
      "=====\n",
      "ks.describe()\n",
      "--------------------\n",
      "ks.corr()\n",
      "=====\n",
      "ks = ks[(ks[\"launched\"].dt.year > 1970)]\n",
      "--------------------\n",
      "countries = pd.read_csv(jupyter_string)\n",
      "countries.head()\n",
      "=====\n",
      "country_mapping=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "country_mapping=country_mapping.set_index('country' <unk>)\n",
      "=====\n",
      "ks= pd.merge(ks,country_mapping, how=jupyter_string, left_on=\"country\", right_on=\"Code\" )\n",
      "\n",
      "\n",
      "\n",
      "ks= ks.drop([\"country\", \"Code\"], axis=1)\n",
      "ks=ks.rename(columns={jupyter_string: jupyter_string})\n",
      "ks.tail()\n",
      "--------------------\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_test.head()\n",
      "=====\n",
      "test[jupyter_string]=test.ILI.shift(2)\n",
      "test.ILI_lag2.head()\n",
      "--------------------\n",
      "ks = ks.dropna()\n",
      "print(ks.shape)\n",
      "=====\n",
      "ks = ks[(ks[jupyter_string].notna())]\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "ks.describe()\n",
      "--------------------\n",
      "ks.head()\n",
      "=====\n",
      "ks.head()\n",
      "--------------------\n",
      "df['project_status' <unk>].value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "sns.countplot(x=\"state\",data=ks, ax=ax,palette=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "ks = ks[ks.state != jupyter_string]\n",
      "=====\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "--------------------\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "ks= ks[ks.state != jupyter_string]\n",
      "=====\n",
      "ks.state=ks[\"state\"].replace({jupyter_string:jupyter_string})\n",
      "\n",
      "--------------------\n",
      "ks.state.value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "ax1=sns.countplot(x=\"state\",data=ks, ax=ax,palette=jupyter_string)\n",
      "\n",
      "for p in ax1.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+3000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "ax1=sns.countplot(x=\"main_category\",data=ks, palette=jupyter_string)\n",
      "\n",
      "for p in ax1.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+3000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax2=sns.countplot(x=\"main_category\",data=ks, ax=ax, order = ks['main_category' <<unk>>].value_counts().index)\n",
      "\n",
      "for p in ax2.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+1000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax2=sns.countplot(x=\"main_category\",data=ks, order = ks['main_category' <unk>].value_counts().index)\n",
      "\n",
      "for p in ax2.patches:\n",
      "   height = p.get_height()\n",
      "   ax.text(p.get_x()+p.get_width()/2.,height+1000 ,jupyter_string.format(height), ha=jupyter_string) \n",
      "=====\n",
      "ks.groupby(\"main_category\")[\"category\"].value_counts()\n",
      "--------------------\n",
      "df.groupby(\"main_category\")[\"backers\"].sum()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ksSortbyBacker = ks.groupby([\"main_category\"])['backers' <<unk>>].mean().reset_index().sort_values(\"backers\", ascending=False)\n",
      "ax2=sns.barplot(y=\"main_category\",x=\"backers\",data=ks,ci=None, ax=ax,order=ksSortbyBacker['main_category' <<unk>>])\n",
      "\n",
      "for p in ax2.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "\n",
      "\n",
      "--------------------\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-1)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-2)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-3)\n",
      "fluTest[jupyter_string] = fluTest[jupyter_string].shift(-4)\n",
      "=====\n",
      "test.head()\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ksSortbyFailed = ks.groupby([\"main_category\"])['failed' <unk>].mean().reset_index().sort_values(\"failed\", ascending=False)\n",
      "ax2=sns.barplot(y=\"main_category\",x=\"failed\",data=ks,ci=None, ax=ax,order=ksSortbyFailed['main_category' <unk>])\n",
      "\n",
      "for p in ax2.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "sns.countplot(x=\"main_category\",hue=\"state\",data=ks, ax=ax, palette=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "sns.countplot(x=\"main_category\",hue=\"state\",data=ks, ax=ax, palette=jupyter_string)\n",
      "=====\n",
      "mainCategory_byState = ks.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "success_rate_perCategory= mainCategory_byState.successful / (mainCategory_byState.successful+ mainCategory_byState.failed)\n",
      "success_rate_perCategory=success_rate_perCategory.sort_values(ascending=True)\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ax7=success_rate_perCategory.plot(kind=jupyter_string,ax=ax)\n",
      "for p in ax7.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width), va=jupyter_string)\n",
      "--------------------\n",
      "ks['country' <unk>].nunique()\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax4=sns.countplot(y=jupyter_string,data=ks,order = ks[jupyter_string].value_counts().index,ax=ax)\n",
      "\n",
      "\n",
      "for p in ax4.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "ax5=sns.countplot(y=jupyter_string,data=ks,order = ks[jupyter_string].value_counts().index)\n",
      "\n",
      "\n",
      "for p in ax5.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width))\n",
      "=====\n",
      "Country_byState = ks.groupby(jupyter_string).state.value_counts().unstack()\n",
      "success_rate_perCountry= Country_byState.successful / (Country_byState.successful + Country_byState.failed)\n",
      "success_rate_perCountry=success_rate_perCountry.sort_values(ascending=True)\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(15, 10)\n",
      "\n",
      "ax1=success_rate_perCountry.plot(kind=jupyter_string,ax=ax)\n",
      "for p in ax1.patches:\n",
      "   width = p.get_width()\n",
      "   ax.text(width, p.get_y()+p.get_height()/2.,jupyter_string.format(width), va=jupyter_string)\n",
      "--------------------\n",
      "ks.goal_per_day = ks.goal_per_day.astype(float)\n",
      "ks.us_real_goal = ks.us_real_goal.astype(float)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "B= plt.boxplot(ks.goal_per_day, sym=jupyter_string)\n",
      "\n",
      "print([item.get_ydata() for item in B[jupyter_string]])\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "fig.set_size_inches(10, 10)\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "B= plt.boxplot(ks.goal_per_day, sym=jupyter_string)\n",
      "\n",
      "print([item.get_ydata() for item in B[jupyter_string]])\n",
      "=====\n",
      "ks[[\"state\", jupyter_string]].groupby(\"state\").describe()\n",
      "--------------------\n",
      "ks[ks.goal_per_day < 70000].state.value_counts()\n",
      "=====\n",
      "PrjOver70k = ks[(ks[jupyter_string]>70000)].groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "print(PrjOver70k)\n",
      "--------------------\n",
      "PrjUnder70k = ks[(ks[jupyter_string]<70000)].groupby('main_category' <unk>).state.value_counts().unstack()\n",
      "print(PrjUnder70k)\n",
      "=====\n",
      "plt.figure(figsize=(20,20)) \n",
      "\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=ks[(ks[jupyter_string]<70000)], palette=jupyter_string,  hue=\"state\")\n",
      "--------------------\n",
      "plt.figure(figsize=(20,20)) \n",
      "\n",
      "sns.boxplot(x=\"usd_goal_real\", y=jupyter_string,data=ks, palette=jupyter_string,  hue=\"state\")\n",
      "=====\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "ks1 = ks[[jupyter_string,'usd_goal_real' <<unk>>, jupyter_string]]\n",
      "\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[\"usd_goal_real\"]])\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "ks1 = ks[[jupyter_string,'usd_goal_real' <unk>, jupyter_string]]\n",
      "\n",
      "plt.yscale(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.boxplot(data=ks1[[\"usd_goal_real\"]])\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "sns.boxplot(data=ks1[[jupyter_string]])\n",
      "=====\n",
      "plt.figure(figsize=(16,8))\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.violinplot(data=ks1[[jupyter_string]])\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "plt.yscale(jupyter_string)\n",
      "sns.violinplot(data=ks1[[\"usd_goal_real\"]])\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "sns.violinplot(data=ks1[[jupyter_string]])\n",
      "--------------------\n",
      "fluTest.fillna(method=jupyter_string,inplace=True)\n",
      "=====\n",
      "fluTrain.tail()\n",
      "--------------------\n",
      "ks1 = ks1[ks1.usd_goal_real != 0]\n",
      "ks1 = ks1[ks1.goal_per_day != 0]\n",
      "ks1 = ks1[ks1.project_length != 0]\n",
      "ks1 = ks1[ks1.usd_goal_real != 0]\n",
      "=====\n",
      "mainKS=ks[((ks[jupyter_string]<=1200) & (ks[jupyter_string]<=90))&(ks[\"usd_goal_real\"]<=38000) &(ks[jupyter_string]>0.0002)& (ks[jupyter_string]>=5) & (ks[\"usd_goal_real\"]>0.01)]\n",
      "ksUpperOutliers=ks[((ks[jupyter_string]>1200)| (ks[jupyter_string]>90) | (ks[\"usd_goal_real\"]>38000))]\n",
      "ksLowerOutliers=ks[(ks[jupyter_string]<=0.0002)| (ks[jupyter_string]<5)| (ks[\"usd_goal_real\"]<=0.01)]\n",
      "\n",
      "print(jupyter_string, len(ks.index))\n",
      "print(jupyter_string, len(mainKS.index))\n",
      "print(jupyter_string, len(ksUpperOutliers.index))\n",
      "print(jupyter_string, len(ksLowerOutliers.index))\n",
      "\n",
      "--------------------\n",
      "mainKS.describe()\n",
      "=====\n",
      "mainKS.describe()\n",
      "--------------------\n",
      "plt.hist(mainKS['goal' <unk>])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.rcParams[jupyter_string] = True\n",
      "sns.distplot(mainKS.goal_per_day, bins=80, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(y=jupyter_string,data=mainKS)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.rcParams[jupyter_string] = True\n",
      "sns.distplot(mainKS.goal_per_main, bins=80, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(y=jupyter_string,data=mainKS)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"country\", y=jupyter_string,data=mainKS) \n",
      "=====\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string,data=mainKS)\n",
      "plt.xticks(rotation=90)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string,data=mainKS)\n",
      "plt.xticks(rotation=90)\n",
      "=====\n",
      "mainKS[[\"main_category\", jupyter_string]].groupby(\"main_category\").describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "--------------------\n",
      "mainKS[[\"country\", jupyter_string]].groupby(\"country\").describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "=====\n",
      "mainKS[[jupyter_string, jupyter_string]].groupby(jupyter_string).describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "--------------------\n",
      "ksUpperOutliers[[jupyter_string, jupyter_string]].groupby(jupyter_string).describe().sort_values(by= (jupyter_string, jupyter_string), ascending =False)\n",
      "=====\n",
      "a = ksLowerOutliers.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "ksLowerOutliers_successrate_percat= a.successful / (a.successful + a.failed)\n",
      "ksLowerOutliers_successrate_percat=ksLowerOutliers_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "b = mainKS.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "mainKS_successrate_percat= b.successful / (b.successful  + b.failed)\n",
      "mainKS_successrate_percat=mainKS_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "c = ksUpperOutliers.groupby('main_category' <<unk>>).state.value_counts().unstack()\n",
      "ksUpperOutliers_successrate_percat= c.successful / (c.successful  + c.failed)\n",
      "ksUpperOutliers_successrate_percat=ksUpperOutliers_successrate_percat.sort_values(ascending=True)\n",
      "\n",
      "plt.figure(figsize=(25,10))\n",
      "plt.subplot(1,3,1)\n",
      "ksLowerOutliers_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "mainKS_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "ksUpperOutliers_successrate_percat.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(25,10))\n",
      "plt.subplot(1,3,1)\n",
      "ksLowerOutliers.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "mainKS.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "plt.subplot(1,3,3)\n",
      "ksUpperOutliers.goal_per_day.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,15))\n",
      "sns.boxplot(x=\"category\", y=jupyter_string,data=mainKS)\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.set(style=jupyter_string)\n",
      "ax=sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "\n",
      "handles, _ = ax.get_legend_handles_labels()\n",
      "ax.legend(handles, [jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "fluTrain.info()\n",
      "=====\n",
      "test.ILI_lag2[0]=fluTrain.ILI[415]\n",
      "test.ILI_lag2[1]=fluTrain.ILI[416]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.set(style=jupyter_string)\n",
      "ax=sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "\n",
      "handles, _ = ax.get_legend_handles_labels()\n",
      "ax.legend(handles, [jupyter_string, jupyter_string])\n",
      "=====\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.distplot(mainKS.project_length, bins=25, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.distplot(mainKS.project_status, bins=25, kde = False, hist_kws=dict(edgecolor=jupyter_string))\n",
      "=====\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True, label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True,label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True, label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "sns.distplot(mainKS[(mainKS[\"state\"]==jupyter_string)].project_length, bins=25, kde = True,label=jupyter_string, hist_kws=dict(edgecolor=jupyter_string))\n",
      "plt.legend(loc=jupyter_string)\n",
      "\n",
      "=====\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "=====\n",
      "plt.figure(figsize=(15,8))\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "sns.boxplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "plt.subplot(1,2,2)\n",
      "sns.violinplot(x=\"state\", y=jupyter_string,data=mainKS)\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string,data=mainKS)\n",
      "=====\n",
      "plt.figure(figsize=(20,20))\n",
      "sns.set(style=jupyter_string)\n",
      "sns.boxplot(x=\"main_category\", y=jupyter_string, hue=\"state\",data=mainKS, palette=jupyter_string)\n",
      "sns.despine(offset=10, trim=True)\n",
      "\n",
      "--------------------\n",
      "mainKS.groupby('main_category' <unk>)['main_category' <unk>].count().sort_values(ascending=False)\n",
      "=====\n",
      "mainKS[[\"state\", jupyter_string]].groupby(\"state\").describe()\n",
      "--------------------\n",
      "df.groupby('main_category' <unk>)['pledged_delta' <unk>].mean().sort_values(ascending=False).head(5)\n",
      "=====\n",
      "Top5=ks.sort_values(by=jupyter_string,ascending=False)\n",
      "Top5.head()\n",
      "--------------------\n",
      "Bottom5=ks.sort_values(by=jupyter_string)\n",
      "Bottom5.head()\n",
      "=====\n",
      "Bottom5=ks.sort_values(by=jupyter_string,ascending=False)\n",
      "Bottom5.tail()\n",
      "--------------------\n",
      "ks['backers_per_length' <unk>]=ks['backers' <unk>]/ks['backers' <unk>]\n",
      "ks.head()\n",
      "=====\n",
      "Top5=ks.sort_values(by=jupyter_string,ascending=False)\n",
      "Top5.head()\n",
      "--------------------\n",
      "df['launch_day' <unk>] = pd.to_datetime(df['launch_day' <unk>])\n",
      "df['launch_day' <unk>].head()\n",
      "=====\n",
      "mainKS[\"launched\"]=pd.DatetimeIndex(mainKS.launched).normalize()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from math import sqrt\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics\n",
      "=====\n",
      "A0=fitted2.params[0]\n",
      "Q0=fitted2.params[1]\n",
      "Q1=fitted2.params[2]\n",
      "test[jupyter_string]=np.exp(A0+Q0*(test.Queries)+Q1*(np.log(test.ILI_lag2)))\n",
      "--------------------\n",
      "mainKS.head()\n",
      "=====\n",
      "timeAnalysisKS=mainKS.groupby(\"launched\")[\"usd_pledged_real\",jupyter_string, \"backers\"].sum()\n",
      "\n",
      "timeAnalysisKSSuccess=mainKS[(mainKS[\"state\"]==jupyter_string)].groupby(\"launched\")[\"usd_pledged_real\",jupyter_string,\"backers\"].sum()\n",
      "timeAnalysisKSFail=mainKS[(mainKS[\"state\"]==jupyter_string)].groupby(\"launched\")[\"usd_pledged_real\",jupyter_string,\"backers\"].sum()\n",
      "--------------------\n",
      "timeAnalysisKSSuccess.resample(jupyter_string).countProj.plot(title=jupyter_string, figsize=(30,20), label=jupyter_string)\n",
      "timeAnalysisKSFail.resample(jupyter_string).countProj.plot(title=jupyter_string, figsize=(30,20), label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "=====\n",
      "week_timeAnalysisKS = pd.DataFrame()\n",
      "week_timeAnalysisKS[jupyter_string] = timeAnalysisKS.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKS[\"usd_pledged_real\"] = timeAnalysisKS.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKS[\"backers\"] = timeAnalysisKS.backers.resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "week_timeAnalysisKSSuccess = pd.DataFrame()\n",
      "week_timeAnalysisKSSuccess[jupyter_string] = timeAnalysisKSSuccess.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSSuccess[\"usd_pledged_real\"] = timeAnalysisKSSuccess.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSSuccess[\"backers\"] = timeAnalysisKSSuccess.backers.resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "week_timeAnalysisKSFail = pd.DataFrame()\n",
      "week_timeAnalysisKSFail[jupyter_string] = timeAnalysisKSFail.countProj.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSFail[\"usd_pledged_real\"] = timeAnalysisKSFail.usd_pledged_real.resample(jupyter_string).mean()\n",
      "week_timeAnalysisKSFail[\"backers\"] = timeAnalysisKSFail.backers.resample(jupyter_string).mean()\n",
      "--------------------\n",
      "students = [75,80,79,60]\n",
      "id = [1,2,3,4]\n",
      "marks = [75,80,79,60]\n",
      "df = pd.DataFrame({jupyter_string:students,jupyter_string:id,jupyter_string:marks})\n",
      "df\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df = df.fillna(0)\n",
      "df.head()\n",
      "=====\n",
      "df.fillna(value=0, inplace=True)\n",
      "df.head()\n",
      "--------------------\n",
      "df.head(10)\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.tail(10)\n",
      "=====\n",
      "df.tail(10)\n",
      "--------------------\n",
      "df[jupyter_string].value_counts()\n",
      "=====\n",
      "df[[jupyter_string,\"Grade\"]][df.Grade == jupyter_string].count()\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "df.groupby([jupyter_string])[jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "\n",
      "--------------------\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "--------------------\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "=====\n",
      "df[df.Grade == jupyter_string].mean()\n",
      "--------------------\n",
      "test[jupyter_string]=np.exp(A0+Q0*(test.Queries)+Q1*(np.log(test.ILI_lag2)))\n",
      "=====\n",
      "test.head()\n",
      "RMSE_test=math.sqrt(((test.predict_test-test.ILI)**2).mean())\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string, category=DeprecationWarning) \n",
      "\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "\n",
      "import keras \n",
      "from keras.models import Sequential \n",
      "from keras.layers import Dense\n",
      "\n",
      "\n",
      "df_train = pd.read_csv(jupyter_string)\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df = df_train.append(df_test , ignore_index = True)\n",
      "\n",
      "\n",
      "print(df_train.shape, df_test.shape, df_train.columns.values)\n",
      "--------------------\n",
      "df['Pclass' <unk>].value_counts()\n",
      "=====\n",
      "df['Pclass' <<unk>>].isnull().sum(axis=0)\n",
      "--------------------\n",
      "df['Pclass' <unk>].value_counts()\n",
      "=====\n",
      "df[['Pclass' <<unk>>, 'Survived' <<unk>>]].groupby(['Pclass' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "train_df[jupyter_string] = train_df['Name' <unk>].apply(lambda x: x.split(jupyter_string)[1].split(jupyter_string)[0].strip())\n",
      "test_df[jupyter_string] = test_df['Name' <unk>].apply(lambda x: x.split(jupyter_string)[1].split(jupyter_string)[0].strip())\n",
      "=====\n",
      "df.Name.head(10)\n",
      "--------------------\n",
      "df[jupyter_string] = df.Name.str.extract(jupyter_string, expand=False)\n",
      "=====\n",
      "df[jupyter_string] = df.Name.map( lambda x: x.split(jupyter_string)[1].split( jupyter_string )[0].strip())\n",
      "\n",
      "\n",
      "df[jupyter_string].value_counts()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].map({jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string})\n",
      "\n",
      "\n",
      "df[jupyter_string].value_counts()\n",
      "=====\n",
      "df[jupyter_string] = df[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "df[jupyter_string] = df[jupyter_string].replace([jupyter_string,jupyter_string,jupyter_string], jupyter_string)\n",
      "df.Title.loc[ (df.Title !=  jupyter_string) & (df.Title !=  jupyter_string) & (df.Title !=  jupyter_string) \n",
      "             & (df.Title !=  jupyter_string)] = jupyter_string\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "df[jupyter_string] = df[jupyter_string].replace([jupyter_string,jupyter_string,jupyter_string], jupyter_string)\n",
      "df[jupyter_string] = df[jupyter_string].replace([jupyter_string,jupyter_string,jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean()\n",
      "=====\n",
      "df[jupyter_string].value_counts()\n",
      "--------------------\n",
      "title_dummies = pd.get_dummies(df[jupyter_string], prefix=jupyter_string)\n",
      "df = pd.concat([df, title_dummies], axis=1)\n",
      "df = df.drop([jupyter_string], axis=1)\n",
      "=====\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string])], axis=1).drop(labels=['Name' <<unk>>], axis=1)\n",
      "--------------------\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string])], axis=1)\n",
      "=====\n",
      "df.Sex.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.Sex.value_counts()\n",
      "=====\n",
      "df[['Sex' <<unk>>, 'Survived' <<unk>>]].groupby(['Sex' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
      "=====\n",
      "train_index=[22,15,31,18,34,32,12,26,19,0,44,5,41,4,2,21,11,28,14,46,8,25,20,33,35,39,10,27,7,42]\n",
      "test_index=[x for x in list(range(len(X))) if x not in train_index]\n",
      "\n",
      "X_train=X[train_index,:] \n",
      "y_train=y[train_index] \n",
      "X_test=X[test_index,:] \n",
      "y_test=y[test_index] \n",
      "print(jupyter_string,len(train_index),len(test_index))\n",
      "--------------------\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "iris.head()\n",
      "=====\n",
      "iris_cols = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "iris = pd.read_csv(jupyter_string, header=None, names=iris_cols)\n",
      "--------------------\n",
      "sns.barplot(x='Sex' <unk>, y='Survived' <unk>, data=df)\n",
      "=====\n",
      "df.Sex = df.Sex.map({jupyter_string:0, jupyter_string:1})\n",
      "--------------------\n",
      "df.Age.fillna(df.Age.median(), inplace=True)\n",
      "=====\n",
      "df.Age.isnull().sum(axis=0)\n",
      "--------------------\n",
      "sns.barplot(x='SibSp' <unk>, y='Survived' <unk>, data=train)\n",
      "=====\n",
      "df.SibSp.isnull().sum(axis=0), df.Parch.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.SibSp.fillna(0, inplace=True)\n",
      "df.Parch.fillna(0, inplace=True)\n",
      "=====\n",
      "df[jupyter_string] = df['SibSp' <<unk>>] + df['Parch' <<unk>>] + 1\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "df[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean()\n",
      "=====\n",
      "df[jupyter_string].value_counts()\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].map({1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0, 24:0, 25:0, 26:0, 27:0, 28:0, 29:0, 30:0, 31:0, 32:0, 33:0, 34:0, 35:0, 36:0, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0, 46:0, 47:0, 48\n",
      "=====\n",
      "df.Family = df.Family.map(lambda x: 0 if x > 4 else x)\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "df[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean()\n",
      "=====\n",
      "df[jupyter_string].value_counts()\n",
      "--------------------\n",
      "df[jupyter_string] = df.Ticket.map(lambda x: x.split(jupyter_string)[0])\n",
      "df[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean()\n",
      "=====\n",
      "df.Ticket.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.Ticket.value_counts()\n",
      "=====\n",
      "df.Ticket.head(20)\n",
      "--------------------\n",
      "df.Ticket = df.Ticket.apply(lambda x: x[0])\n",
      "=====\n",
      "df.Ticket = df.Ticket.map(lambda x: x[0])\n",
      "\n",
      "\n",
      "df[['Ticket' <<unk>>, 'Survived' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.head()\n",
      "--------------------\n",
      "df.Ticket.head(20)\n",
      "=====\n",
      "df['Ticket' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df[['Pclass' <unk>, 'Survived' <unk>]].groupby(['Pclass' <unk>], as_index=False).mean()\n",
      "=====\n",
      "df[['Ticket' <<unk>>, 'Fare' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "df[['Pclass' <unk>, 'Fare' <unk>]].groupby(['Pclass' <unk>], as_index=False).mean()\n",
      "=====\n",
      "df[['Ticket' <<unk>>, 'Pclass' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "sns.distplot(train_df['Fare' <unk>], kde=False)\n",
      "=====\n",
      "df.Fare.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.groupby('Embarked' <unk>)['Fare' <unk>].describe()\n",
      "=====\n",
      "sns.boxplot('Pclass' <<unk>>,'Fare' <<unk>>,data=df)\n",
      "plt.ylim(0, 300) \n",
      "plt.show()\n",
      "--------------------\n",
      "sns.boxplot('Sex' madeupword0002,'Fare' <unk>,data=df)\n",
      "plt.ylim(0, 300) \n",
      "plt.show()\n",
      "=====\n",
      "df[['Pclass' <<unk>>, 'Fare' <<unk>>]].groupby(['Pclass' <<unk>>]).mean()\n",
      "--------------------\n",
      "df[(df['Pclass' <unk>] == 1) & (df['Ticket' <unk>] == 3) & (df['Embarked' <unk>] == S)]\n",
      "=====\n",
      "guess_Fare = df.Fare.loc[ (df.Ticket == jupyter_string) & (df.Pclass == 3) & (df.Embarked == jupyter_string)].median()\n",
      "df.Fare.fillna(guess_Fare , inplace=True)\n",
      "\n",
      "\n",
      "df[['Fare' <<unk>>, 'Survived' <<unk>>]].groupby(['Survived' <<unk>>],as_index=False).mean()\n",
      "--------------------\n",
      "df[['Pclass' <unk>, 'Survived' <unk>]].groupby(['Pclass' <unk>],as_index=False).mean()\n",
      "=====\n",
      "grid = sns.FacetGrid(df, hue='Survived' <<unk>>, size=4, aspect=1.5)\n",
      "grid.map(plt.hist, 'Fare' <<unk>>, alpha=.5, bins=range(0,210,10))\n",
      "grid.add_legend()\n",
      "plt.show()\n",
      "--------------------\n",
      "grid = sns.FacetGrid(df, hue='Survived' <unk>, size=4, aspect=1.5)\n",
      "grid.map(plt.hist, 'Fare' <unk>, alpha=.5, bins=range(0,210,10))\n",
      "grid.add_legend()\n",
      "plt.show()\n",
      "=====\n",
      "df[['Fare' <<unk>>, 'Survived' <<unk>>]].groupby(['Fare' <<unk>>],as_index=False).mean().plot.scatter('Fare' <<unk>>,'Survived' <<unk>>)\n",
      "plt.show()\n",
      "--------------------\n",
      "grid = sns.FacetGrid(df, hue='Survived' <unk>, size=4, aspect=1.5)\n",
      "grid.map(plt.hist, 'Pclass' <unk>, alpha=.5, bins=range(1,4))\n",
      "grid.add_legend()\n",
      "plt.show()\n",
      "=====\n",
      "df[jupyter_string] = pd.qcut(df.Fare,5,labels=[1,2,3,4,5]).astype(int)\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean()\n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.describe()\n",
      "--------------------\n",
      "df[jupyter_string] = pd.qcut(df.Cabin,4,labels=[1,2,3,4]).astype(int)\n",
      "\n",
      "\n",
      "df[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean()\n",
      "=====\n",
      "df.Cabin.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.Cabin.fillna(jupyter_string,inplace=True)\n",
      "df.Cabin.isnull().sum(axis=0)\n",
      "=====\n",
      "df = df.drop(labels=['Cabin' <<unk>>], axis=1)\n",
      "--------------------\n",
      "df['Embarked' <unk>].value_counts()\n",
      "=====\n",
      "df.Embarked.isnull().sum(axis=0)\n",
      "--------------------\n",
      "df.Embarked.value_counts()\n",
      "=====\n",
      "df.describe(include=[jupyter_string]) \n",
      "--------------------\n",
      "df.describe(include=[jupyter_string]) \n",
      "=====\n",
      "df.Embarked.fillna(jupyter_string , inplace=True )\n",
      "--------------------\n",
      "df.drop(['Embarked' <unk>], axis=1, inplace=True)\n",
      "=====\n",
      "df = df.drop(labels='Embarked' <<unk>>, axis=1)\n",
      "--------------------\n",
      "df['Age' <unk>].fillna(df['Age' <unk>].mean(), inplace=True)\n",
      "=====\n",
      "grid = sns.FacetGrid(df, col=jupyter_string, size=3, aspect=0.8, sharey=False)\n",
      "grid.map(plt.hist, 'Age' <<unk>>, alpha=.5, bins=range(0,105,5))\n",
      "plt.show()\n",
      "--------------------\n",
      "grid = sns.FacetGrid(df, col=jupyter_string, size=3, aspect=0.8, sharey=False)\n",
      "grid.map(plt.hist, 'Pclass' <unk>, alpha=.5, bins=range(1,4))\n",
      "plt.show()\n",
      "=====\n",
      "df[[jupyter_string, 'Age' <<unk>>]].groupby([jupyter_string]).mean()\n",
      "--------------------\n",
      "df[[jupyter_string, 'Age' <unk>]].groupby([jupyter_string]).median()\n",
      "=====\n",
      "df[[jupyter_string, 'Age' <<unk>>]].groupby([jupyter_string]).std()\n",
      "--------------------\n",
      "df['Age' <unk>].fillna(df['Age' <unk>].median(), inplace=True)\n",
      "=====\n",
      "df_sub = df[['Age' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,'SibSp' <<unk>>]]\n",
      "\n",
      "X_train  = df_sub.dropna().drop('Age' <<unk>>, axis=1)\n",
      "y_train  = df['Age' <<unk>>].dropna()\n",
      "X_test = df_sub.loc[np.isnan(df.Age)].drop('Age' <<unk>>, axis=1)\n",
      "\n",
      "regressor = RandomForestRegressor(n_estimators = 300)\n",
      "regressor.fit(X_train, y_train)\n",
      "y_pred = np.round(regressor.predict(X_test),1)\n",
      "df.Age.loc[df.Age.isnull()] = y_pred\n",
      "\n",
      "df.Age.isnull().sum(axis=0) \n",
      "--------------------\n",
      "iris.info()\n",
      "=====\n",
      "iris.isnull().sum()\n",
      "--------------------\n",
      "df_sub = df[['Age' <unk>,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,'Fare' <unk>]]\n",
      "\n",
      "X_train  = df_sub.dropna().drop('Age' <unk>, axis=1)\n",
      "y_train  = df['Age' <unk>].dropna()\n",
      "X_test = df_sub.loc[np.isnan(df.Age)].drop('Age' <unk>, axis=1)\n",
      "\n",
      "regressor = RandomForestRegressor(n_estimators = 300)\n",
      "regressor.fit(X_train, y_train)\n",
      "y_pred = np.round(regressor.predict(X_test),1)\n",
      "df.Age.loc[df.Age.isnull()] = y_pred\n",
      "\n",
      "df.Age.isnull().sum(axis=0) \n",
      "=====\n",
      "bins = [ 0, 4, 12, 18, 30, 50, 65, 100] \n",
      "age_index = (1,2,3,4,5,6,7) \n",
      "df[jupyter_string] = pd.cut(df.Age, bins, labels=age_index).astype(int)\n",
      "\n",
      "df[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string],as_index=False).mean()\n",
      "--------------------\n",
      "df.Ticket.value_counts()\n",
      "=====\n",
      "df[['Ticket' <<unk>>, 'Survived' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "df[['Ticket' <unk>, 'Survived' <unk>]].groupby(['Ticket' <unk>], as_index=False).mean()\n",
      "=====\n",
      "df['Ticket' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df['Ticket' <unk>] = df['Ticket' <unk>].map({1: jupyter_string, 2: jupyter_string, 3: jupyter_string, 4: jupyter_string})\n",
      "df['Ticket' <unk>].value_counts()\n",
      "=====\n",
      "df['Ticket' <<unk>>] = df['Ticket' <<unk>>].replace([jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string], jupyter_string)\n",
      "\n",
      "\n",
      "df[['Ticket' <<unk>>, 'Survived' <<unk>>]].groupby(['Ticket' <<unk>>], as_index=False).mean()\n",
      "--------------------\n",
      "df[['Ticket' <unk>, 'Survived' <unk>]].groupby(['Ticket' <unk>], as_index=False).mean()\n",
      "=====\n",
      "df = pd.get_dummies(df,columns=['Ticket' <<unk>>])\n",
      "--------------------\n",
      "df = df.drop(['PassengerId' <unk>, 'Name' <unk>, 'Ticket' <unk>, 'Cabin' <unk>, 'Embarked' <unk>], axis=1)\n",
      "=====\n",
      "df = df.drop(labels=['SibSp' <<unk>>,'Parch' <<unk>>,'Age' <<unk>>,'Fare' <<unk>>,jupyter_string], axis=1)\n",
      "y_train = df[0:891]['Survived' <<unk>>].values\n",
      "X_train = df[0:891].drop(['Survived' <<unk>>,'PassengerId' <<unk>>], axis=1).values\n",
      "X_test  = df[891:].drop(['Survived' <<unk>>,'PassengerId' <<unk>>], axis=1).values\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import Activation\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import Convolution2D\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.convolutional import MaxPooling2D\n",
      "\n",
      "=====\n",
      "model = Sequential()\n",
      "\n",
      "\n",
      "model.add(Dense(9, kernel_initializer = jupyter_string, activation = jupyter_string, input_dim = 17))\n",
      "model.add(Dense(9, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "model.add(Dense(5, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "model.add(Dense(1, kernel_initializer = jupyter_string, activation = jupyter_string))\n",
      "\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(optimizer = jupyter_string, loss = jupyter_string, metrics = [jupyter_string])\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train, batch_size = 32, epochs = 200)\n",
      "--------------------\n",
      "y_pred = model.predict(X_test)\n",
      "y_pred = (y_pred > 0.5)\n",
      "=====\n",
      "y_pred = model.predict(X_test)\n",
      "y_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n",
      "\n",
      "output = pd.DataFrame({'PassengerId' <<unk>>: df_test['PassengerId' <<unk>>], 'Survived' <<unk>>: y_final})\n",
      "output.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "iris.groupby('species' <unk>).mean()\n",
      "=====\n",
      "iris.groupby(jupyter_string).mean()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble\n",
      "=====\n",
      "train_all = pd.read_csv(jupyter_string)\n",
      "train_x = train_all.loc[:, \"Class\" : \"Fare\"]\n",
      "train_y = train_all.loc[:, \"Survived\"]\n",
      "\n",
      "train_x.head()\n",
      "--------------------\n",
      "test_all = pd.read_csv(jupyter_string)\n",
      "test_x = test_all.loc[:, \"Class\" : \"Fare\"]\n",
      "\n",
      "test_x.head()\n",
      "=====\n",
      "test_all = pd.read_csv(jupyter_string)\n",
      "test_x = test_all.loc[:, \"Class\" : \"Fare\"]\n",
      "test_y = test_all.loc[:, \"Survived\"]\n",
      "--------------------\n",
      "train_x = train_x.as_matrix()\n",
      "test_x = test_x.as_matrix()\n",
      "train_y = train_y.as_matrix()\n",
      "=====\n",
      "validate_all = pd.read_csv(jupyter_string)\n",
      "validate_x = validate_all.loc[:, \"Class\" : \"Fare\"]\n",
      "validate_y = validate_all.loc[:, \"Survived\"]\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(train_x)\n",
      "train_x = scaler.transform(train_x)\n",
      "validate_x = scaler.transform(validate_x)\n",
      "test_x = scaler.transform(test_x)\n",
      "=====\n",
      "df = df.drop(['Id' <<unk>>, 'Name' <<unk>>], axis=1)\n",
      "\n",
      "\n",
      "df = df.dropna(how=jupyter_string, axis=0)\n",
      "\n",
      "df = df.replace([jupyter_string,jupyter_string], [0,1])\n",
      "\n",
      "print(df.shape)\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_x, validate_x, train_y, validate_y = train_test_split(df.loc[:, \"Class\" : \"Fare\"], df.loc[:, \"Survived\"])\n",
      "\n",
      "print(train_x.shape, validate_x.shape, train_y.shape, validate_y.shape)\n",
      "=====\n",
      "train_all = df.iloc[0:300, :]\n",
      "export = pd.DataFrame(train_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "--------------------\n",
      "test_all = df.iloc[300:, :]\n",
      "export = pd.DataFrame(test_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "=====\n",
      "test_all = df.iloc[300:500, :]\n",
      "export = pd.DataFrame(test_all)\n",
      "export.to_csv(jupyter_string)\n",
      "\n",
      "print(export.shape)\n",
      "export.head()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "churn_df = pd.read_csv(jupyter_string)\n",
      "col_names = churn_df.columns.tolist()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(col_names)\n",
      "\n",
      "to_show = col_names[:6] + col_names[-6:]\n",
      "\n",
      "print(jupyter_string)\n",
      "churn_df[to_show].head(6)\n",
      "--------------------\n",
      "churn_df = churn_df.drop(['State' <unk>, 'Area Code' <unk>, 'Phone' <unk>], axis=1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "churn_df = churn_df.replace(jupyter_string, 1)\n",
      "=====\n",
      "to_drop = ['State' <<unk>>,'Area Code' <<unk>>,'Phone' <<unk>>,'Churn?' <<unk>>]\n",
      "X = churn_df.drop(to_drop,axis=1)\n",
      "\n",
      "\n",
      "\n",
      "churn_result = churn_df['Churn?' <<unk>>]\n",
      "y = np.where(churn_result == jupyter_string,1,0)\n",
      "\n",
      "\n",
      "\n",
      "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
      "X[yes_no_cols] = X[yes_no_cols] == jupyter_string\n",
      "X = X.as_matrix().astype(np.float)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "\n",
      "plt.scatter(X_train, y_train, color=jupyter_string)\n",
      "plt.plot(X_train, lr.predict_proba(X_train)[:, 1], color=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "x1 = np.linspace(-5,5,50)\n",
      "fx = 1/(1 + np.exp(-x1))\n",
      "\n",
      "plt.plot(x1, fx, c=jupyter_string, linewidth=2)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "x = np.linspace(-5,5,50)\n",
      "y = 1/(1 + np.exp(-x))\n",
      "\n",
      "plt.plot(x, y, c=jupyter_string, linewidth=2)\n",
      "plt.xlabel('x' <unk>)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "dat = pd.read_csv(jupyter_string)\n",
      "x2 = dat.x.reshape(214,1)\n",
      "y2 = dat.y.reshape(214,1)\n",
      "\n",
      "\n",
      "plt.scatter(x2, y2)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel('y' <<unk>>)\n",
      "\n",
      "--------------------\n",
      "iris.groupby([jupyter_string, jupyter_string]).mean()\n",
      "=====\n",
      "iris.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "dat = pd.read_csv(jupyter_string)\n",
      "x = dat.x.values.reshape(-1,1)\n",
      "y = dat.y.values.reshape(-1,1)\n",
      "\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
      "\n",
      "logreg = LogisticRegression()\n",
      "logreg.fit(x_train, y_train)\n",
      "\n",
      "y_pred = logreg.predict(x_test)\n",
      "\n",
      "print(classification_report(y_test,\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logreg = LogisticRegression(C=1e9)\n",
      "logreg.fit(x2, y2)\n",
      "hx = logreg.predict_proba(x2)[:,1]\n",
      "\n",
      "plt.scatter(x2, y2)\n",
      "plt.plot(x2, hx, color=jupyter_string)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel('y' <<unk>>)\n",
      "--------------------\n",
      "plt.plot(x2, y2, jupyter_string)\n",
      "plt.plot(x_bound, y_pred, jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(x2, y2)\n",
      "plt.plot(x2, hx, color=jupyter_string)\n",
      "plt.plot([x_bound,x_bound], [-0.2,1.2], color=jupyter_string)\n",
      "plt.xlabel('x' <<unk>>)\n",
      "plt.ylabel('y' <<unk>>)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "=====\n",
      "research = pd.read_csv(jupyter_string, sep=jupyter_string, header=0)\n",
      "research\n",
      "\n",
      "--------------------\n",
      "exprs = pd.read_csv(jupyter_string, index_col=0)\n",
      "sampleinfo = pd.read_csv(jupyter_string, index_col=0)\n",
      "=====\n",
      "exprs = pd.read_csv(jupyter_string,index_col=0)\n",
      "exprs.head()\n",
      "    \n",
      "--------------------\n",
      "sampleinfo = pd.read_csv(jupyter_string)\n",
      "sampleinfo.head()\n",
      "sampleinfo.shape\n",
      "=====\n",
      "sampleinfo = pd.read_csv(jupyter_string)\n",
      "sampleinfo.head()\n",
      "\n",
      "--------------------\n",
      "(exprs.columns == sampleinfo.filename).all()\n",
      "=====\n",
      "(exprs.columns == sampleinfo.filename).all()\n",
      "--------------------\n",
      "dfs[jupyter_string].head()\n",
      "=====\n",
      "dfs[jupyter_string].head()\n",
      "--------------------\n",
      "exprs.head()\n",
      "=====\n",
      "sampleinfo.head()\n",
      "--------------------\n",
      "sampleinfo[jupyter_string] = pd.to_datetime(sampleinfo[jupyter_string])\n",
      "sampleinfo.head()\n",
      "=====\n",
      "sampleinfo[jupyter_string]=pd.to_datetime(sampleinfo['date' <<unk>>])\n",
      "--------------------\n",
      "sampleinfo[jupyter_string]=sampleinfo[jupyter_string].dt.month\n",
      "sampleinfo[jupyter_string]=sampleinfo[jupyter_string].dt.year\n",
      "=====\n",
      "sampleinfo[jupyter_string] = sampleinfo[jupyter_string].apply(lambda x: x.year)\n",
      "sampleinfo[jupyter_string] = sampleinfo[jupyter_string].apply(lambda x: x.month)\n",
      "--------------------\n",
      "iris.plot(kind=jupyter_string, subplots=True)\n",
      "=====\n",
      "iris.boxplot(by=jupyter_string)\n",
      "--------------------\n",
      "sampleinfo.head()\n",
      "=====\n",
      "sampleinfo[jupyter_string].head()\n",
      "--------------------\n",
      "sampleinfoCEU = sampleinfo[sampleinfo.Ethnicity == jupyter_string]\n",
      "sampleinfoCEU.head()\n",
      "=====\n",
      "sampleinfoCEU = sampleinfo.loc[sampleinfo.ethnicity == jupyter_string]\n",
      "sampleinfoCEU.head()\n",
      "--------------------\n",
      "exprsCEU = exprs.loc[exprs.ethnicity == jupyter_string]\n",
      "exprsCEU.head()\n",
      "=====\n",
      "exprsCEU = exprs[sampleinfoCEU.filename]\n",
      "exprsCEU.head()\n",
      "--------------------\n",
      "exprsCEU = exprsCEU - exprsCEU.mean()\n",
      "exprsCEU.head()\n",
      "=====\n",
      "exprsCEU.mean(axis=1).head()\n",
      "--------------------\n",
      "exprsCEU.std(axis=1).head()\n",
      "=====\n",
      "exprsCEU_mn = exprsCEU.apply(lambda x: x - exprsCEU.mean(axis=1))\n",
      "exprsCEU_mn.head()\n",
      "--------------------\n",
      "plt.hist(PC1, bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(Vh[0,:],bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "plt.hist(Vh[1,:],bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(projection,bins=25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "plt.scatter(Vh[0,:],projection)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh[0,:])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "plt.scatter(sampleinfoCEU.elapsedInDays,Vh[0,:])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlim(0,170)\n",
      "plt.axvline(x=100,color=jupyter_string)\n",
      "--------------------\n",
      "election = pd.read_csv(jupyter_string)\n",
      "election.head()\n",
      "=====\n",
      "election = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "Data_train = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "Data = Data_train[Data_train['Subject' madeupword0412]==1]\n",
      "Data.head()\n",
      "--------------------\n",
      "election.head()\n",
      "=====\n",
      "election.head()\n",
      "--------------------\n",
      "election_nov2012 = election[election['Start Date' <unk>] >= jupyter_string]\n",
      "=====\n",
      "election[\"Start Date\"] = pd.to_datetime(election[\"Start Date\"])\n",
      "\n",
      "subset_election = election.loc[map(lambda x: x.month == 11 and x.year == 2012, election[\"Start Date\"])]\n",
      "subset_election.drop_duplicates(\"Pollster\",inplace=True)\n",
      "M = subset_election.shape[0]\n",
      "--------------------\n",
      "N = np.median([1,2,3,4,5])\n",
      "N\n",
      "=====\n",
      "N = election.loc[election[\"Start Date\"].apply(lambda x: x.month==11 and x.year == 2012),\"Number of Observations\"].median()\n",
      "--------------------\n",
      "np.random.seed(42)\n",
      "N = 1000\n",
      "p = 0.53\n",
      "samples = np.random.binomial(N, p, size=10000)\n",
      "samples.mean()\n",
      "=====\n",
      "simu_poll_sample= np.random.binomial(N,0.53,1000)\n",
      "\n",
      "--------------------\n",
      "plt.hist(simu_poll_sample)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "simu_result = pd.DataFrame(simu_poll_sample/N)\n",
      "simu_result.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "np.std(simu_poll_sample)\n",
      "=====\n",
      "std_single = simu_result.std()\n",
      "--------------------\n",
      "plt.hist(std_single)\n",
      "plt.show()\n",
      "=====\n",
      "import scipy.stats as stats\n",
      "stats.probplot(((simu_poll_sample - (simu_poll_sample).mean()) / simu_poll_sample.std()), dist=jupyter_string, plot = plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "simu_result = np.zeros(10000)\n",
      "simu_poll_sample = np.zeros(10000)\n",
      "for i in range(10000):\n",
      "    simu_poll_sample[i] = np.random.binomial(1, 0.5)\n",
      "    simu_result[i] = (simu_poll_sample[i] == 1).mean()\n",
      "=====\n",
      "simu_multi_poll_sample= np.random.binomial(N,0.53,(M,1000))\n",
      "--------------------\n",
      "sns.distplot(simu_multi_poll_mean)\n",
      "=====\n",
      "pd.DataFrame(simu_multi_poll_mean).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string % N)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.show\n",
      "--------------------\n",
      "simu_multi_poll_std = np.std(simu_multi_poll_mean)\n",
      "simu_multi_poll_std\n",
      "=====\n",
      "std_multi = simu_multi_poll_mean.std()\n",
      "--------------------\n",
      "X = Data.iloc[:,1:]\n",
      "y = Data.iloc[:,0]\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "X, y = Data.iloc[:,2:], Data.iloc[:,0]\n",
      "y=le.fit_transform(y)\n",
      "\n",
      "np.unique(y)\n",
      "--------------------\n",
      "np.sqrt(std_multi)\n",
      "=====\n",
      "stats.probplot((simu_multi_poll_mean - simu_multi_poll_mean.mean())/std_multi,dist=jupyter_string,plot=plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "std_cross_pull = (simu_multi_poll_sample/N).std(axis=0)\n",
      "--------------------\n",
      "plt.hist(std_cross_pull)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "pd.DataFrame(std_cross_pull).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "--------------------\n",
      "pd.DataFrame(std_cross_pull).plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "stats.probplot((std_cross_pull-std_cross_pull.mean())/std_cross_pull.std(),dist=jupyter_string,plot = plt)\n",
      "plt.show()\n",
      "--------------------\n",
      "subset_ratings.mean()\n",
      "=====\n",
      "subset_ratings.std()\n",
      "--------------------\n",
      "subset_ratings.mean()\n",
      "=====\n",
      "subset_ratings.mean()\n",
      "--------------------\n",
      "subset_ratings.std()\n",
      "=====\n",
      "np.mean(subset_ratings.std()<std_cross_pull)\n",
      "--------------------\n",
      "subset_election.head()\n",
      "=====\n",
      "subset_election[jupyter_string].head()\n",
      "--------------------\n",
      "plt.plot(dates.date2num(election[jupyter_string]), election[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax = plt.subplot(111)\n",
      "ax.plot_date(subset_election[\"Start Date\"],subset_election[jupyter_string])\n",
      "plt.axhline(y=0.039, linewidth=2, color = jupyter_string)\n",
      "ax.set_xlim([dt.datetime(2012,10,31,0,0),dt.datetime(2012,11,4,0,0)])\n",
      "ax.xaxis.set_major_formatter(dates.DateFormatter(jupyter_string))\n",
      "ax.set_ylim([-0.03,0.05])\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = plt.subplot(111)\n",
      "ax.plot_date(subset_election[\"Start Date\"],subset_election[jupyter_string])\n",
      "ax.set_xlim([dt.datetime(2012,10,31,0,0),dt.datetime(2012,11,4,0,0)])\n",
      "ax.xaxis.set_major_formatter(dates.DateFormatter(jupyter_string))\n",
      "ax.set_ylim([-0.03,0.05])\n",
      "plt.show()\n",
      "=====\n",
      "set_Poll = list(set(subset_election[\"Pollster\"]))\n",
      "index_map = {}\n",
      "for i,p in enumerate(set_Poll):\n",
      "    index_map[p] = i\n",
      "\n",
      "plt.scatter(map(lambda x: index_map[x], subset_election[\"Pollster\"]),subset_election[jupyter_string],c=map(lambda x: index_map[x], subset_election[\"Pollster\"]))\n",
      "plt.xticks(range(len(set_Poll)),set_Poll,rotation = 90)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "cov_mat = np.cov(X_train_std.T)\n",
      "eigen_vals,eigen_vecs = np.linalg.eig(cov_mat)\n",
      "eigen_vecs\n",
      "=====\n",
      "import numpy as np\n",
      "covariance_matrix = np.cov(X_train_std.T)\n",
      "eigen_vals,eigen_vecs = np.linalg.eig(covariance_matrix)\n",
      "\n",
      "sum_eigenvals = np.sum(eigen_vals)\n",
      "var_exp = np.real(eigen_vals)/np.real(sum_eigenvals)\n",
      "cumsum = np.cumsum(var_exp)\n",
      "n=var_exp.shape[0]\n",
      "--------------------\n",
      "obama_avg = np.mean(obama_polls)\n",
      "obama_std = np.std(obama_polls)\n",
      "print(obama_avg, obama_std)\n",
      "=====\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().mean()\n",
      "--------------------\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().std()\n",
      "=====\n",
      "subset_election.groupby(\"Pollster\")[jupyter_string].mean().std()\n",
      "--------------------\n",
      "model = Sequential()\n",
      "model.add(embedding_layer)\n",
      "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
      "model.add(Dense(1, activation=jupyter_string))\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model=Sequential()\n",
      "embedding_layer = pretrained_embedding_layer(words_to_vec,word_to_index)\n",
      "model.add(embedding_layer)\n",
      "model.add(LSTM(128, return_sequences=True))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(LSTM(128))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(5,activation=jupyter_string))\n",
      "model.add(Activation(jupyter_string))\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string,optimizer=jupyter_string,metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "train_dataframe=pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string])\n",
      "test_dataframe=pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string])\n",
      "--------------------\n",
      "from keras.utils import to_categorical\n",
      "Y_train=to_categorical(Y_train,5)\n",
      "Y_test=to_categorical(Y_test,5)\n",
      "=====\n",
      "Y_oh_train=to_categorical(Y_train,num_classes=5).astype(jupyter_string)\n",
      "Y_oh_test=to_categorical(Y_test,num_classes=5).astype(jupyter_string)\n",
      "--------------------\n",
      "plt.plot(k_range, k_scores)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "plt.plot(k_range, k_scores)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "X = pd.read_csv(jupyter_string, sep=jupyter_string, header=0, usecols=range(13), thousands=jupyter_string)\n",
      "Y = pd.read_csv(jupyter_string, sep=jupyter_string, header=0, usecols=[13], encoding=jupyter_string)\n",
      "Y = Y.replace({jupyter_string: 0}, regex=True)\n",
      "Y = Y.replace({jupyter_string: 1}, regex=True)\n",
      "Y = Y.replace({jupyter_string: 2}, regex=True)\n",
      "\n",
      "X = X.values.astype(np.float64)\n",
      "Y = Y.values[:,0].astype(np.float64)\n",
      "\n",
      "\n",
      "from sklearn import preprocessing\n",
      "\n",
      "X_NORMAL = preprocessing.normalize(X)\n",
      "X_STD = preprocessing.scale(X)\n",
      "X_NORMAL_STD = preprocessing.scale(X_NORMAL)\n",
      "\n",
      "--------------------\n",
      "U = U[:, :-1]\n",
      "V = V[:, :-1]\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(U)\n",
      "print(jupyter_string)\n",
      "print(V)\n",
      "=====\n",
      "s_rank = 12\n",
      "S = np.zeros(X.shape, dtype=complex)\n",
      "S[:s_rank, :s_rank] = np.diag(s)[:s_rank, :s_rank]\n",
      "\n",
      "X_SVD = np.dot(U, np.dot(S, V));\n",
      "X_SVD = np.real(X_SVD);\n",
      "--------------------\n",
      "X_ED = X_ED.drop([jupyter_string, jupyter_string, jupyter_string, jupyter_string], axis=1)\n",
      "=====\n",
      "used_columns = [1,6,10,11,12]\n",
      "X_EDITED = X[:, used_columns]\n",
      "--------------------\n",
      "model=sm.OLS(y=y_train,x=pd.DataFrame(X_train))\n",
      "result=model.fit()\n",
      "result.summary()\n",
      "=====\n",
      "result=ols(y=y_train,x=pd.DataFrame(X_train))\n",
      "\n",
      "R_2_IS=result.r2  \n",
      "OLS_coef=result.beta\n",
      "\n",
      "\n",
      "a=np.array(X_test)  \n",
      "b=np.array(result.beta) \n",
      "print(jupyter_string.format(b))\n",
      "c=np.sum(a*b[0:-1],axis=1)+b[-1] \n",
      "error=y_test-c \n",
      "R_2_OS=1-error.var()/y_test.var() \n",
      "print(jupyter_string.format(R_2_IS))\n",
      "print(jupyter_string.format(R_2_OS))\n",
      "--------------------\n",
      "tot = sum(var_exp)\n",
      "exp_var_exp = [(i/tot)*100 for i in sorted(var_exp, reverse=True)]\n",
      "cum_exp_var_exp = np.cumsum(exp_var_exp)\n",
      "=====\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set(style=jupyter_string)\n",
      "\n",
      "plt.figure(figsize=(5,4))\n",
      "sns.barplot(x=np.arange(1,21),y=var_exp[:20],palette=jupyter_string)\n",
      "\n",
      "plt.step(np.arange(0,20),cumsum[:20],where=jupyter_string,color=jupyter_string,label=jupyter_string)\n",
      "labels= plt.yticks()\n",
      "new_labels = [str(int(label*100))+jupyter_string for label in labels[0]]\n",
      "plt.yticks(labels[0], new_labels)\n",
      "plt.xlim(-1,20)\n",
      "plt.ylim(0,0.9)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.grid(axis=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "--------------------\n",
      "classifier = svm.SVC(kernel=jupyter_string, degree=3)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "y_pred = classifier.predict(X_test)\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "print(cm)\n",
      "=====\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=3, C=Penalty_C)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X_KPCA, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "classifier = SVC(kernel = jupyter_string, random_state = 0)\n",
      "classifier.fit(X_train, y_train)\n",
      "y_pred = classifier.predict(X_test)\n",
      "from sklearn.metrics import confusion_matrix\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "print(cm)\n",
      "=====\n",
      "svc = svm.SVC(kernel=jupyter_string, C=Penalty_C)\n",
      "accuracy_svc = cross_val_score(svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_svc)\n",
      "--------------------\n",
      "svc = svm.SVC(kernel=jupyter_string, C=Penalty_C)\n",
      "accuracy_svc = cross_val_score(svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_svc)\n",
      "=====\n",
      "rbf_svc = svm.SVC(kernel=jupyter_string, gamma=jupyter_string, C=Penalty_C)\n",
      "accuracy_rbf_svc = cross_val_score(rbf_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_rbf_svc)\n",
      "--------------------\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=2)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "=====\n",
      "poly_svc = svm.SVC(kernel=jupyter_string, degree=2, C=Penalty_C)\n",
      "accuracy_poly_svc = cross_val_score(poly_svc, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_poly_svc)\n",
      "--------------------\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "=====\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "model = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=1, random_state=0)\n",
      "accuracy_boost = cross_val_score(model, X, Y, cv=cross_val_k, scoring=jupyter_string).mean()\n",
      "print(jupyter_string, accuracy_boost)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "data = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "data.head()\n",
      "--------------------\n",
      "data[jupyter_string] = data[jupyter_string].apply(lambda x: 1 if x == jupyter_string else 0)\n",
      "data.head()\n",
      "=====\n",
      "data['Class' <<unk>>] = data.apply(lambda x: 1 if x['Class' <<unk>>]==jupyter_string else 0, axis=1)\n",
      "data.head()\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import\n",
      "=====\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "binary_vectorizer = CountVectorizer(binary=True,stop_words=jupyter_string,ngram_range=(1,2))\n",
      "X_train_binary = binary_vectorizer.fit_transform(X_train)\n",
      "X_test_binary = binary_vectorizer.transform(X_test)\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer(stop_words=jupyter_string,ngram_range=(1,2))\n",
      "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
      "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
      "X_train_binary.shape\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_binary, y_train)\n",
      "y_pred_lr = lr.predict(X_test_binary)\n",
      "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_lr)\n",
      "\n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(X_train_tfidf, y_train)\n",
      "y_pred_bnb = bnb.predict(X_test_tfidf)\n",
      "fpr_bnb, tpr_bnb, thresholds_bnb = roc_curve(y_test, y_pred_bnb)\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "clf = LogisticRegression()\n",
      "for train,test,feature_type in [(X_train_binary, X_test_binary, jupyter_string), (X_train_tfidf, X_test_tfidf, jupyter_string)]:\n",
      "    clf.fit(train, Y_train)\n",
      "    \n",
      "    predictions = clf.predict_proba(test)[:,1]\n",
      "    fpr, tpr, threshold = metrics.roc_curve(Y_test, predictions)\n",
      "    score = metrics.roc_auc_score(Y_test, predictions)\n",
      "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
      "    plt.plot(fpr, tpr, color=c, label=jupyter_string + feature_type + jupyter_string+ str(score))\n",
      "\n",
      "clf = BernoulliNB()\n",
      "for train, test, feature_type in [(X_train_binary, X_test_binary, jupyter_string), (X_train_tfidf, X_test_tfidf, jupyter_string)]:\n",
      "    clf.fit(train, Y_train)\n",
      "    \n",
      "    predictions = clf.predict_proba(test)[:,1]\n",
      "    fpr, tpr, threshold = metrics.roc_curve(Y_test, predictions)\n",
      "    score = metrics.roc_auc_score(Y_test, predictions)\n",
      "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
      "    plt.plot(fpr, tpr, color=c, label=jupyter_string + feature_type + jupyter_string+ str(score))\n",
      "\n",
      "plt.legend(loc=jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "import pprint as pprint\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "plt.style.use(jupyter_string)\n",
      "--------------------\n",
      "tot = sum(var_exp)\n",
      "exp_var_exp = [(i/tot)*100 for i in sorted(var_exp, reverse=True)]\n",
      "cum_exp_var_exp = np.cumsum(exp_var_exp)\n",
      "=====\n",
      "Pca_w = np.real(eigen_vecs[:,0:20])\n",
      "X_train_pca = X_train_std.dot(Pca_w)\n",
      "X_test_pca = X_test_std.dot(Pca_w)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "bb_df = pd.read_csv(jupyter_string)\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df.info()\n",
      "=====\n",
      "bb_df.describe()\n",
      "--------------------\n",
      "df_df['genre' <unk>] = df_df['genre' <unk>].apply(clean_genre)\n",
      "=====\n",
      "bb_df['genre' <<unk>>] = bb_df['genre' <<unk>>].apply(clean_genre)\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df['genre' <unk>].value_counts()\n",
      "=====\n",
      "bb_df['artist.inverted' <<unk>>].value_counts()\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "bb_df[jupyter_string] = bb_df['time' <<unk>>].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df[jupyter_string] = bb_df[jupyter_string].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "=====\n",
      "bb_df[jupyter_string] = bb_df['date.entered' <<unk>>].dt.month\n",
      "bb_df[jupyter_string] = bb_df['date.entered' <<unk>>].dt.year\n",
      "bb_df.head(3)\n",
      "--------------------\n",
      "bb_df[jupyter_string] = bb_df['time' <unk>].apply(get_duration)\n",
      "bb_df.head(3)\n",
      "=====\n",
      "pd.pivot_table(bb_df, index = [jupyter_string, jupyter_string], values = jupyter_string, aggfunc=jupyter_string)\n",
      "--------------------\n",
      "sns.distplot(bb_small_df[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "bb_small_df.hist(figsize=(15,15));\n",
      "--------------------\n",
      "small_df.hist(figsize=(15,15));\n",
      "=====\n",
      "bb_small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                     bins = 50,\\\n",
      "                                    fontsize = 15\\\n",
      "                                    ).set_title(jupyter_string, \\\n",
      "                                                fontsize = 20, y = 1.01)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                     bins = 50,\\\n",
      "                                    fontsize = 15\\\n",
      "                                    ).set_title(jupyter_string, \\\n",
      "                                                fontsize = 20, y = 1.01)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "bb_small_df[jupyter_string].plot(kind = jupyter_string, figsize = (15,7),\\\n",
      "                                 bins = 40, fontsize = 15\\\n",
      "                                ).set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "pca = PCA(n_components=20)\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "=====\n",
      "from sklearn.learning_curve import validation_curve\n",
      "param_range = np.logspace(-4,2,num=10)\n",
      "train_scores, test_scores = validation_curve(estimator=clf,X=X_train_pca,y=y_train,param_name=jupyter_string,\n",
      "                                             param_range=param_range,cv=10)\n",
      "\n",
      "train_mean = np.mean(train_scores,axis=1)\n",
      "train_std  = np.std(train_scores,axis=1)\n",
      "test_mean  = np.mean(test_scores,axis=1)\n",
      "test_std   = np.std(test_scores,axis=1)\n",
      "\n",
      "plt.plot(param_range,train_mean,color=jupyter_string,marker=jupyter_string,markersize=5,label=jupyter_string)\n",
      "plt.fill_between(param_range,train_mean+train_std,train_mean-train_std,color=jupyter_string,alpha=0.15)\n",
      "plt.plot(param_range,test_mean,color=jupyter_string,marker=jupyter_string,markersize=5,label=jupyter_string)\n",
      "plt.fill_between(param_range,test_mean+test_std,test_mean-test_std,color=jupyter_string,alpha=0.15)\n",
      "plt.grid()\n",
      "plt.xscale(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.ylim((0.9,1.02));\n",
      "--------------------\n",
      "plt.figure(figsize = (15,7))\n",
      "plt.title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.xlabel(jupyter_string, fontsize = 15)\n",
      "plt.ylabel(jupyter_string, fontsize = 15)\n",
      "plt.scatter(small_df[jupyter_string], small_df[jupyter_string])\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "sns.pairplot(data=bb_small_df)\n",
      "plt.rcParams[jupyter_string]=(15,15);\n",
      "--------------------\n",
      "plt.scatter(x=bb_small_df[jupyter_string], y=bb_small_df[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 22, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 22, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 0.5,  linestyle=jupyter_string, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14);\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 38, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 22, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.3)\n",
      "\n",
      "ax.axvline(x = 22, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 270, color = jupyter_string, alpha = 0.5)\n",
      "ax.axhline(y = 200, color = jupyter_string, alpha = 0.5)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 14);\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "unicorn = bb_df[(bb_df[jupyter_string] < 38) & (bb_df[jupyter_string] > 32)]\\\n",
      "[['artist.inverted' <<unk>>, 'genre' <<unk>>,'track' <<unk>>, jupyter_string, jupyter_string]]\n",
      "unicorn.sort_values(jupyter_string, inplace = True)\n",
      "\n",
      "\n",
      "new_col = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "unicorn.columns = new_col\n",
      "unicorn\n",
      "--------------------\n",
      "pd.pivot_table(bb_df, values = jupyter_string, index = [jupyter_string], columns = [jupyter_string], aggfunc = np.sum)\n",
      "=====\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.median, np.std, max, min])\n",
      "--------------------\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.median, np.std, max, min])\n",
      "=====\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.mean, np.median, np.std, max, min])\n",
      "--------------------\n",
      "pd.pivot_table(bb_small_df, index=[jupyter_string], \\\n",
      "               values= [jupyter_string, jupyter_string],\\\n",
      "              aggfunc = [np.mean, np.median, np.std, max, min])\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.1)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.1)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (10,8))\n",
      "\n",
      "ax = sns.boxplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings; warnings.simplefilter(jupyter_string)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (10,8))\n",
      "\n",
      "ax = sns.violinplot(data = bb_small_df, x = jupyter_string, y = jupyter_string,\\\n",
      "                order = [jupyter_string, jupyter_string])\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 14)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "import matplotlib.cm as cm\n",
      "from sklearn import linear_model\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "\n",
      "lm = linear_model.LinearRegression()\n",
      "\n",
      "X = bb_small_df[[jupyter_string]]\n",
      "y = bb_small_df[[jupyter_string]]\n",
      "\n",
      "lm.fit(X, y)\n",
      "predictions = lm.predict(X)\n",
      "\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
      "\n",
      "plt.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.9,\\\n",
      "          c = bb_small_df[jupyter_string], cmap=jupyter_string)\n",
      "\n",
      "plt.plot(X, predictions, c= jupyter_string, marker = jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 16)\n",
      "ax.set_xticklabels([0,20,40,60,80,100,120],fontsize = 14)\n",
      "\n",
      "ax.set_ylabel(jupyter_string, fontsize = 16)\n",
      "ax.set_yticklabels([-10,0,10,20,30,40,50,60], fontsize = 14)\n",
      "\n",
      "cb = plt.colorbar()\n",
      "cb.set_label(jupyter_string, rotation = 90, fontsize = 14)\n",
      "cb.set_ticks([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
      "cb.ax.tick_params(labelsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,9))\n",
      "\n",
      "ax = sns.violinplot(x=jupyter_string, y=jupyter_string, hue=jupyter_string,\\\n",
      "                     data=bb_small_df, split=True,\\\n",
      "                   order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_xticklabels([jupyter_string, jupyter_string, jupyter_string, jupyter_string], fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "plt.savefig(jupyter_string);\n",
      "=====\n",
      "lm = linear_model.LinearRegression()\n",
      "\n",
      "X = bb_small_df[[jupyter_string]]\n",
      "y = bb_small_df[[jupyter_string]]\n",
      "\n",
      "lm.fit(X, y)\n",
      "predictions = lm.predict(X)\n",
      "\n",
      "fig, ax = plt.subplots(1,1, figsize = (15,10))\n",
      "\n",
      "plt.scatter(x = bb_small_df[jupyter_string], y = bb_small_df[jupyter_string],\\\n",
      "                s = 200, alpha = 0.5,\\\n",
      "          c = bb_small_df[jupyter_string], cmap=jupyter_string)\n",
      "\n",
      "plt.plot(X, predictions, c= jupyter_string, marker = jupyter_string)\n",
      "\n",
      "\n",
      "ax.set_title(jupyter_string, fontsize = 20, y = 1.01)\n",
      "ax.set_xlabel(jupyter_string, fontsize = 16)\n",
      "ax.set_xticklabels([0,20,40,60,80,100,120],fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, fontsize = 16)\n",
      "ax.set_yticklabels([-10,0,10,20,30,40,50,60], fontsize = 14)\n",
      "\n",
      "cb = plt.colorbar()\n",
      "cb.set_label(jupyter_string, rotation = 90, fontsize = 14)\n",
      "cb.set_ticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
      "cb.ax.tick_params(labelsize = 14)\n",
      "\n",
      "plt.savefig(jupyter_string);\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "shuf_season_df = bb_small_df[[jupyter_string, jupyter_string]]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.hist(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "shuf_peak_df = bb_small_df[[jupyter_string, jupyter_string]]\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "rs = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "rs.head()\n",
      "=====\n",
      "pd.set_option(jupyter_string, 500)\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arima\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "import statsmodels.tsa.arima_model as arma\n",
      "=====\n",
      "rs['Borough' <<unk>>].value_counts()\n",
      "--------------------\n",
      "rs['Borough' <unk>].value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "rs['Borough' <<unk>>].value_counts().plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "fig = plt.figure(figsize=(20,18))\n",
      "for i,l in enumerate([jupyter_string, jupyter_string, jupyter_string, jupyter_string]):\n",
      "    ax = fig.add_subplot(3,2,i+1)\n",
      "    sns.pointplot(x=jupyter_string, y=l, hue=jupyter_string, data=results, ax=ax)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn import linear_model, preprocessing\n",
      "\n",
      "sstat = pd.read_csv(jupyter_string,index_col=1)\n",
      "ss = sstat.fillna(0)\n",
      "print(ss)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "=====\n",
      "ytr=ss['Pos' <<unk>>]\n",
      "xtr=ss[['FG' <<unk>>,'FGA' <<unk>>,'FG%' <<unk>>,'3P' <<unk>>,'3PA' <<unk>>,'3P%' <<unk>>,'2P' <<unk>>,'2PA' <<unk>>,'2P%' madeupword0002,'eFG%' <<unk>>,'FT' <<unk>>,'FTA' <<unk>>,'FT%' <<unk>>,'ORB' <<unk>>,'DRB' <<unk>>,'TRB' <<unk>>,'AST' <<unk>>,'STL' <<unk>>,'BLK' <<unk>>,'TOV' <<unk>>,'PF' <<unk>>,jupyter_string]]\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(xtr,ytr,test_size=0.2,random_state=42)\n",
      "=====\n",
      "logreg = linear_model.LogisticRegression(verbose=5, \n",
      "                                         solver=jupyter_string, max_iter=100)\n",
      "logreg.fit(xtr,ytr)\n",
      "--------------------\n",
      "ss[jupyter_string]=logreg.predict(ss[['FG' <unk>,'FGA' <unk>,'FG%' <unk>,'3P' <unk>,'3PA' <unk>,'3P%' <unk>,'2P' <unk>,'2PA' <unk>,'2P%' <unk>,'eFG%' <unk>]])\n",
      "=====\n",
      "sstat8 = pd.read_csv(jupyter_string,index_col=1)\n",
      "ss8 = sstat8.fillna(0)\n",
      "--------------------\n",
      "ss8[jupyter_string] = ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string] + ss8[jupyter_string]\n",
      "ss8\n",
      "=====\n",
      "yts=ss8['Pos' <<unk>>]\n",
      "xts=ss8[['FG' <<unk>>,'FGA' <<unk>>,'FG%' <<unk>>,'3P' <<unk>>,'3PA' <<unk>>,'3P%' <<unk>>,'2P' <<unk>>,'2PA' <<unk>>,'2P%' madeupword0002,'eFG%' <<unk>>,'FT' <<unk>>,'FTA' <<unk>>,'FT%' <<unk>>,'ORB' <<unk>>,'DRB' <<unk>>,'TRB' <<unk>>,'AST' <<unk>>,'STL' <<unk>>,'BLK' <<unk>>,'TOV' <<unk>>,'PF' <<unk>>,jupyter_string]]\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "xts_train, xts_test, yts_train, yts_test = train_test_split(xts, yts, test_size=0.33, random_state=42)\n",
      "=====\n",
      "yhat = logreg.predict(xtr)\n",
      "acc = np.mean(yhat == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "yhat = logreg.predict(xts)\n",
      "acc = np.mean(yhat == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "=====\n",
      "nprt = 10\n",
      "Ierr = np.where(ytr != yhat)[0]\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = jupyter_string.format(ytr[ind], yhat[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "plt.scatter(ytr, yhat)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "from sklearn.metrics import confusion_matrix\n",
      "C = confusion_matrix(ytr,yhat)\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr=='PF' <<unk>>)&(yhat=='PF' <<unk>>))[0]\n",
      "b=np.where((ytr=='PF' <<unk>>))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "\n",
      "Csum = np.sum(C,1)\n",
      "C = C / Csum[None,:]\n",
      "\n",
      "\n",
      "print(np.array_str(C, precision=3, suppress_small=True))\n",
      "plt.imshow(C, interpolation=jupyter_string)\n",
      "x=[0,1,2,3,4,5]\n",
      "y=[0,1,2,3,4,5]\n",
      "ygroup_labels = [jupyter_string, 'PF' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string]  \n",
      "xgroup_labels = [jupyter_string, 'PF' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string] \n",
      "plt.xticks(x, xgroup_labels, rotation=90)  \n",
      "plt.yticks(y, ygroup_labels, rotation=0)\n",
      "plt.colorbar()\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "--------------------\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat==jupyter_string))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "print(a.shape[0]/b.shape[0])\n",
      "a=np.where((ytr==jupyter_string)&(yhat=='PF' <unk>))[0]\n",
      "b=np.where((ytr==jupyter_string))[0]\n",
      "\n",
      "=====\n",
      "W = logreg.coef_\n",
      "x=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
      "y=[0,1,2,3,4,5]\n",
      "ygroup_labels = [jupyter_string, 'PF' <<unk>>,jupyter_string,jupyter_string,jupyter_string,jupyter_string]  \n",
      "xgroup_labels = ['FG' <<unk>>,'FGA' <<unk>>,'FG%' <<unk>>,'3P' <<unk>>,'3PA' <<unk>>,'3P%' <<unk>>,'2P' <<unk>>,'2PA' <<unk>>,'2P%' madeupword0002,'eFG%' <<unk>>,'FT' <<unk>>,'FTA' <<unk>>,'FT%' <<unk>>,'ORB' <<unk>>,'DRB' <<unk>>,'TRB' <<unk>>,'AST' <<unk>>,'STL' <<unk>>,'BLK' <<unk>>,'TOV' <<unk>>,'PF' <<unk>>,jupyter_string]  \n",
      "plt.xticks(x, xgroup_labels, rotation=90)  \n",
      "plt.yticks(y, ygroup_labels, rotation=0)  \n",
      "\n",
      "plt.imshow(W, interpolation=jupyter_string)\n",
      "plt.colorbar()\n",
      "--------------------\n",
      "svc.fit(X_train,y_train)\n",
      "=====\n",
      "svc.fit(xtr,ytr)\n",
      "yhat_tr = svc.predict(xtr)\n",
      "acc = np.mean(yhat_tr == ytr)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "N = 60  \n",
      "\n",
      "data = pd.read_csv(jupyter_string, index_col='Date' <<unk>>, parse_dates=['Date' <<unk>>]).iloc[:,:N]\n",
      "stock_list = data.columns\n",
      "\n",
      "print( jupyter_string.format(data.isna().sum().sum()) )\n",
      "print( jupyter_string.format(data.shape) )\n",
      "display(data.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if N <= 30:\n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "\n",
      "    ax = fig.add_subplot(121)\n",
      "    np.cumprod(1+data, axis=0).plot(ax=ax, title=jupyter_string)\n",
      "    np.cumprod(1+data.mean(axis=1)).plot(ax=ax, label=jupyter_string, color=jupyter_string)\n",
      "    ax.legend()\n",
      "\n",
      "    ax = fig.add_subplot(122)\n",
      "    data.plot(ax=ax, title=jupyter_string)\n",
      "\n",
      "    plt.show()\n",
      "--------------------\n",
      "yhat_te = svc.predict(xtest)\n",
      "acc = np.mean(yhat_te == ytest)\n",
      "print(jupyter_string.format(acc))\n",
      "=====\n",
      "Ierr = np.where(yhat_tr != ytr)[0]\n",
      "nprt = 2\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xtr.index[ind])  \n",
      "    title = jupyter_string.format(ytr[ind], yhat_tr[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "plt.scatter(ytr, yhat_tr)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "yhat_ts = svc.predict(xts)\n",
      "acc = np.mean(yhat_ts == yts)\n",
      "print(jupyter_string.format(acc))\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(yts, yhat_ts))\n",
      "=====\n",
      "Ierr = np.where(yhat_ts != yts)[0]\n",
      "nprt = 324\n",
      "print(jupyter_string,Ierr.shape[0])\n",
      "for i in range(nprt):              \n",
      "    ind = Ierr[i]    \n",
      "    print(xts.index[ind])  \n",
      "    title = jupyter_string.format(yts[ind], yhat_ts[ind])\n",
      "    print(title)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data_df.head()\n",
      "=====\n",
      "data_df.head()\n",
      "--------------------\n",
      "corrmat = data_df.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat)\n",
      "=====\n",
      "feature_df = data_df.drop(['Y1' <<unk>>, 'Y2' <<unk>>], axis=1)\n",
      "--------------------\n",
      "feature_df.head()\n",
      "=====\n",
      "y1_corrs = feature_df.corrwith(data_df.Y1)\n",
      "y1_corrs.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "y2_corrs = feature_df.corr()\n",
      "y2_corrs.plot(kind=jupyter_string)\n",
      "=====\n",
      "f_corrs = feature_df.corr()\n",
      "sns.heatmap(f_corrs, annot=True)\n",
      "--------------------\n",
      "model = sm.ols(formula=jupyter_string, data=feature_df)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "y1_model = sm.ols(data=data_df, \n",
      "                  formula=jupyter_string)\n",
      "y1_result = y1_model.fit()\n",
      "y1_result.summary()\n",
      "--------------------\n",
      "y2_model = sm.ols(data=data_df, \n",
      "                  formula=jupyter_string)\n",
      "y2_result = y2_model.fit()\n",
      "=====\n",
      "y2_corrs = feature_df.corrwith(data_df.Y2)\n",
      "y2_corrs.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd, numpy as np\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df\n",
      "--------------------\n",
      "sns.jointplot(x='Y2' <unk>, y='Y3' <unk>, data=data_df)\n",
      "=====\n",
      "y2_model = sm.ols(data=data_df, \n",
      "                  formula=jupyter_string)\n",
      "y2_result = y1_model.fit()\n",
      "y2_result.summary()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "model = KNeighborsClassifier(n_neighbors=5) \n",
      "model.fit(train_X, train_y)\n",
      "prediction = model.predict(test_X)\n",
      "print(jupyter_string, metrics.accuracy_score(prediction, test_y))\n",
      "=====\n",
      "a_index = list(range(1,11))\n",
      "a = pd.Series()\n",
      "for i in list(range(1,11)):\n",
      "    model = KNeighborsClassifier(n_neighbors=i)\n",
      "    model.fit(train_X, train_y)\n",
      "    prediction = model.predict(test_X)\n",
      "    a = a.append(pd.Series(metrics.accuracy_score(prediction, test_y)))\n",
      "plt.plot(a_index, a)\n",
      "x = [1,2,3,4,5,6,7,8,9,10]\n",
      "plt.xticks(x)\n",
      "--------------------\n",
      "iris.drop('Id' <unk>, axis=1, inplace=True)\n",
      "=====\n",
      "iris.drop(\"Id\", axis=1, inplace = True)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "petal = iris[['PetalLengthCm' madeupword0002,'PetalWidthCm' <<unk>>,'Species' <<unk>>]]\n",
      "sepal = iris[['SepalLengthCm' <<unk>>,'SepalWidthCm' <<unk>>,'Species' <<unk>>]]\n",
      "--------------------\n",
      "iris.head()\n",
      "=====\n",
      "fig = iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <<unk>>, y='SepalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "\n",
      "fig.set_xlabel(jupyter_string)\n",
      "fig.set_ylabel(jupyter_string)\n",
      "fig.set_title(jupyter_string)\n",
      "\n",
      "fig=plt.gcf()\n",
      "fig.set_size_inches(10, 7)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig = iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <unk>, y='SepalWidthCm' <unk>, color=jupyter_string, label=jupyter_string)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='SepalLengthCm' <unk>, y='SepalWidthCm' <unk>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "\n",
      "fig.set_xlabel(jupyter_string)\n",
      "fig.set_ylabel(jupyter_string)\n",
      "fig.set_title(jupyter_string)\n",
      "\n",
      "fig=plt.gcf()\n",
      "fig.set_size_inches(10, 7)\n",
      "plt.show()\n",
      "=====\n",
      "sns.FacetGrid(iris, hue='Species' <<unk>>, size=5)\\\n",
      "   .map(plt.scatter, 'SepalLengthCm' <<unk>>, 'SepalWidthCm' <<unk>>)\\\n",
      "   .add_legend()\n",
      "--------------------\n",
      "sns.FacetGrid(iris, hue='Species' <unk>, size=5)\\\n",
      "   .map(plt.scatter, 'SepalLengthCm' <unk>, 'SepalWidthCm' <unk>)\\\n",
      "   .add_legend()\n",
      "=====\n",
      "fig = iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='PetalLengthCm' madeupword0002, y='PetalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='PetalLengthCm' madeupword0002, y='PetalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "iris[iris.Species == jupyter_string].plot(kind=jupyter_string, x='PetalLengthCm' madeupword0002, y='PetalWidthCm' <<unk>>, color=jupyter_string, label=jupyter_string, ax=fig)\n",
      "\n",
      "fig.set_xlabel(jupyter_string)\n",
      "fig.set_ylabel(jupyter_string)\n",
      "fig.set_title(jupyter_string)\n",
      "\n",
      "fig=plt.gcf()\n",
      "fig.set_size_inches(10, 7)\n",
      "plt.show()\n",
      "--------------------\n",
      "iris_petal = pd.melt(iris, id_vars=['Species' <unk>], value_vars=['PetalLengthCm' <unk>,'PetalWidthCm' <unk>])\n",
      "=====\n",
      "train_p,test_p = train_test_split(petal, test_size=0.3, random_state=0) \n",
      "train_x_p = train_p[['PetalWidthCm' <<unk>>,'PetalLengthCm' madeupword0002]]\n",
      "train_y_p = train_p.Species\n",
      "\n",
      "test_x_p = test_p[['PetalWidthCm' <<unk>>,'PetalLengthCm' madeupword0002]]\n",
      "test_y_p = test_p.Species\n",
      "--------------------\n",
      "train_s,test_s = train_test_split(sepal, test_size=0.3, random_state=0) \n",
      "train_x_s = train_s[['SepalLengthCm' <unk>,'SepalWidthCm' <unk>]]\n",
      "train_y_s = train_s.Species\n",
      "\n",
      "test_x_s = test_s[['SepalLengthCm' <unk>,'SepalWidthCm' <unk>]]\n",
      "test_y_s = test_s.Species\n",
      "=====\n",
      "train_s,test_s = train_test_split(sepal, test_size=0.3, random_state=0) \n",
      "train_x_s = train_s[['SepalWidthCm' <<unk>>,'SepalLengthCm' <<unk>>]]\n",
      "train_y_s = train_s.Species\n",
      "\n",
      "test_x_s = test_s[['SepalWidthCm' <<unk>>,'SepalLengthCm' <<unk>>]]\n",
      "test_y_s = test_s.Species\n",
      "--------------------\n",
      "clf = tree.DecisionTreeClassifier(criterion=jupyter_string)\n",
      "train = TrainModel(clf, df, target=df.columns[-1])\n",
      "train.run()\n",
      "train.predict()\n",
      "display_clf(clf, feature_names=train.get_train_x_names(), class_names=train.get_train_y_names())\n",
      "save_png_clf(clf, clf.criterion)\n",
      "=====\n",
      "clf = tree.DecisionTreeClassifier(criterion=jupyter_string)\n",
      "train = TrainModel(clf, df, target=df.columns[-1])\n",
      "train.run()\n",
      "train.predict()\n",
      "display_clf(clf, feature_names=train.get_train_x_names(), class_names=train.get_train_y_names())\n",
      "save_png_clf(clf, clf.criterion)\n",
      "--------------------\n",
      "X = iris.drop(['Species' <unk>], axis=1) \n",
      "y = iris['Species' <unk>] \n",
      "=====\n",
      "plt.figure(figsize=(8,4))\n",
      "sns.heatmap(iris.corr(), annot=True, cmap=jupyter_string) \n",
      "plt.show()\n",
      "--------------------\n",
      "X_train = train[['SepalLengthCm' <unk>,'SepalWidthCm' <unk>,'PetalLengthCm' <unk>,'PetalWidthCm' <unk>]]\n",
      "y_train = train['Species' <unk>]\n",
      "\n",
      "X_test = test[['SepalLengthCm' <unk>,'SepalWidthCm' <unk>,'PetalLengthCm' <unk>,'PetalWidthCm' <unk>]]\n",
      "y_test = test['Species' <unk>]\n",
      "=====\n",
      "train_X = train[['SepalLengthCm' <<unk>>,'SepalWidthCm' <<unk>>,'PetalLengthCm' madeupword0002,'PetalWidthCm' <<unk>>]] \n",
      "train_y = train.Species \n",
      "\n",
      "test_X = test[['SepalLengthCm' <<unk>>,'SepalWidthCm' <<unk>>,'PetalLengthCm' madeupword0002,'PetalWidthCm' <<unk>>]] \n",
      "test_y = test.Species \n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier \n",
      "knn = KNeighborsClassifier(n_neighbors=1) \n",
      "knn.fit(train_X, train_y) \n",
      "predictions = knn.predict(test_X) \n",
      "=====\n",
      "train_X.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.distplot(df['Congruent' <unk>])\n",
      "sns.distplot(df['Incongruent' <unk>])\n",
      "=====\n",
      "sns.distplot(data['Congruent' <<unk>>], label=\"Congruent\");\n",
      "sns.distplot(data['Incongruent' <<unk>>], label=\"Incongruent\");\n",
      "plt.legend()\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "display(data.head())\n",
      "--------------------\n",
      "con = data['Congruent' <unk>]\n",
      "incong = data['Incongruent' <unk>]\n",
      "\n",
      "mean_con = con.mean()\n",
      "mean_incong = incong.mean()\n",
      "std_con = con.std()\n",
      "std_incong = incong.std()\n",
      "=====\n",
      "alpha = 0.05\n",
      "\n",
      "dof = len(data['Congruent' <<unk>>]) - 1\n",
      "\n",
      "data[jupyter_string] = data['Congruent' <<unk>>]-data['Incongruent' <<unk>>]\n",
      "\n",
      "\n",
      "diff_mean = data[jupyter_string].mean()\n",
      "diff_std = data[jupyter_string].std()\n",
      "diff_stderr = diff_std/len(data['Congruent' <<unk>>])**0.5\n",
      "t_crit = stats.t.ppf(1-alpha/2, dof)\n",
      "lower = diff_mean - t_crit*diff_stderr\n",
      "upper = diff_mean + t_crit*diff_stderr\n",
      "t_stat = (diff_mean-0)/(diff_std/len(data['Congruent' <<unk>>])**0.5)\n",
      "pvalue = stats.t.sf(np.abs(t_stat), dof)*2\n",
      "print(jupyter_string + jupyter_string % t_crit + jupyter_string + jupyter_string % t_crit)\n",
      "print(jupyter_string + jupyter_string % lower + jupyter_string + jupyter_string % upper + jupyter_string)\n",
      "print(jupyter_string + jupyter_string % t_stat)\n",
      "print(jupyter_string + jupyter_string % pvalue)\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "con_mean = data['Congruent' <<unk>>].mean()\n",
      "con_median = data['Congruent' <<unk>>].median()\n",
      "con_std = data['Congruent' <<unk>>].std()\n",
      "incon_mean = data['Incongruent' <<unk>>].mean()\n",
      "incon_median = data['Incongruent' <<unk>>].median()\n",
      "incon_std = data['Incongruent' <<unk>>].std()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "df.replace(jupyter_string, np.nan, inplace=True)\n",
      "=====\n",
      "df_pop1 = df.copy(deep=True)\n",
      "df_pop1 = df_pop1[df.A_Se != jupyter_string]\n",
      "df_pop1 = df_pop1[df.A_As != jupyter_string]\n",
      "\n",
      "\n",
      "df_pop2 = df.copy(deep=True)\n",
      "df_pop2 = df_pop2[df.A_As != jupyter_string]\n",
      "df_pop2 = df_pop2[df.A_Pb != jupyter_string]\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "df_pop1 = df_pop1.replace(jupyter_string, np.nan)\n",
      "=====\n",
      "df2 = df.replace(jupyter_string, 0)\n",
      "--------------------\n",
      "df_pop3 = df.copy(deep=True)\n",
      "df_pop3 = df_pop3[df_pop3.A_Se != jupyter_string]\n",
      "df_pop3 = df_pop3[df_pop3.A_As != jupyter_string]\n",
      "df_pop3 = df_pop3[df_pop3.A_Pb != jupyter_string]\n",
      "=====\n",
      "df2.head()\n",
      "--------------------\n",
      "df6 = df5.copy(deep=True)\n",
      "df6.head()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "df_pop2['A_As' madeupword0002] = df_pop2['A_As' madeupword0002].astype(float)\n",
      "print(df_pop2['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop2['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df3['A_As' madeupword0002] = df3['A_As' madeupword0002].astype(float)\n",
      "print(df3['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df4['A_As' madeupword0002] = df4['A_As' madeupword0002].astype(float)\n",
      "print(df4['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df4['A_As' madeupword0002].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df5['A_As' madeupword0002] = df5['A_As' madeupword0002].astype(float)\n",
      "print(df5['A_As' madeupword0002].describe())\n",
      "print(jupyter_string)\n",
      "print(df5['A_As' madeupword0002].median())\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "df_pop2['B_As' <unk>] = df_pop2['B_As' <unk>].astype(float)\n",
      "print(df_pop2['B_As' <unk>].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop2['B_As' <unk>].median())\n",
      "print(jupyter_string)\n",
      "df3['B_As' <unk>] = df3['B_As' <unk>].astype(float)\n",
      "print(df3['B_As' <unk>].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['B_As' <unk>].median())\n",
      "print(jupyter_string)\n",
      "df4['B_As' <unk>] = df4['B_As' <unk>].astype(float)\n",
      "print(df4['B_As' <unk>].describe())\n",
      "print(jupyter_string)\n",
      "print\n",
      "=====\n",
      "fig, ax = plt.subplots(figsize=(13, 8))\n",
      "\n",
      "\n",
      "plt.plot(df_pop2[jupyter_string].values, df_pop2['A_As' madeupword0002], jupyter_string)\n",
      "plt.plot(df3[jupyter_string].values, df3['A_As' madeupword0002].values, jupyter_string)\n",
      "plt.plot(df4[jupyter_string].values, df4['A_As' madeupword0002].values, jupyter_string)\n",
      "plt.plot(df5[jupyter_string].values, df5['A_As' madeupword0002].values, jupyter_string)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blue_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "red_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "green_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "yellow_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "\n",
      "\n",
      "plt.legend(handles=[blue_patch, red_patch, green_patch, yellow_patch])\n",
      "--------------------\n",
      "df_pop2['A_As' <unk>] = df_pop2['A_As' <unk>].astype(float)\n",
      "print(df_pop2['A_As' <unk>].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop2['A_As' <unk>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df3['A_As' <unk>] = df3['A_As' <unk>].astype(float)\n",
      "print(df3['A_As' <unk>].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['A_As' <unk>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df4['A_As' <unk>] = df4['A_As' <unk>].astype(float)\n",
      "print(df4['A_As' <unk>].describe())\n",
      "=====\n",
      "print(jupyter_string)\n",
      "df_pop2['A_Pb' <<unk>>] = df_pop2['A_Pb' <<unk>>].astype(float)\n",
      "print(df_pop2['A_Pb' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop2['A_Pb' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df3['A_Pb' <<unk>>] = df3['A_Pb' <<unk>>].astype(float)\n",
      "print(df3['A_Pb' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['A_Pb' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df4['A_Pb' <<unk>>] = df4['A_Pb' <<unk>>].astype(float)\n",
      "print(df4['A_Pb' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df4['A_Pb' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df5['A_Pb' <<unk>>] = df5['A_Pb' <<unk>>].astype(float)\n",
      "print(df5['A_Pb' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df5['A_Pb' <<unk>>].median())\n",
      "--------------------\n",
      "ax = plt.subplots(figsize=(13, 8))\n",
      "\n",
      "\n",
      "plt.plot(df_pop2[jupyter_string].values, df_pop2['A_Pb' <unk>], jupyter_string)\n",
      "plt.plot(df3[jupyter_string].values, df3['A_Pb' <unk>], jupyter_string)\n",
      "plt.plot(df4[jupyter_string].values, df4['A_Pb' <unk>], jupyter_string)\n",
      "plt.plot(df5[jupyter_string].values, df5['A_Pb' <unk>], jupyter_string)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blue_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "red_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "\n",
      "=====\n",
      "fig, ax = plt.subplots(figsize=(13, 8))\n",
      "\n",
      "\n",
      "plt.plot(df_pop2[jupyter_string].values, df_pop2['A_Pb' <<unk>>], jupyter_string)\n",
      "plt.plot(df3[jupyter_string].values, df3['A_Pb' <<unk>>].values, jupyter_string)\n",
      "plt.plot(df4[jupyter_string].values, df4['A_Pb' <<unk>>].values, jupyter_string)\n",
      "plt.plot(df5[jupyter_string].values, df5['A_Pb' <<unk>>].values, jupyter_string)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blue_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "red_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "green_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "yellow_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "\n",
      "\n",
      "plt.legend(handles=[blue_patch, red_patch, green_patch, yellow_patch])\n",
      "--------------------\n",
      "df_pop2['A_Pb' <unk>] = df_pop2['A_Pb' <unk>].astype(float)\n",
      "print(df_pop2['A_Pb' <unk>].describe())\n",
      "=====\n",
      "print(jupyter_string)\n",
      "df_pop1['A_Se' <<unk>>] = df_pop1['A_Se' <<unk>>].astype(float)\n",
      "print(df_pop1['A_Se' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df_pop1['A_Se' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df3['A_Se' <<unk>>] = df3['A_Se' <<unk>>].astype(float)\n",
      "print(df3['A_Se' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df3['A_Se' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df4['A_Se' <<unk>>] = df4['A_Se' <<unk>>].astype(float)\n",
      "print(df4['A_Se' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df4['A_Se' <<unk>>].median())\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "df5['A_Se' <<unk>>] = df5['A_Se' <<unk>>].astype(float)\n",
      "print(df5['A_Se' <<unk>>].describe())\n",
      "print(jupyter_string)\n",
      "print(df5['A_Se' <<unk>>].median())\n",
      "--------------------\n",
      "ax = plt.subplots(figsize=(13, 8))\n",
      "\n",
      "\n",
      "plt.plot(df_pop1[jupyter_string].values, df_pop1['A_Se' <unk>], jupyter_string)\n",
      "plt.plot(df3[jupyter_string].values, df3['A_Se' <unk>], jupyter_string)\n",
      "plt.plot(df4[jupyter_string].values, df4['A_Se' <unk>], jupyter_string)\n",
      "plt.plot(df5[jupyter_string].values, df5['A_Se' <unk>], jupyter_string)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blue_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "red_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "green_patch =\n",
      "=====\n",
      "fig, ax = plt.subplots(figsize=(13, 8))\n",
      "\n",
      "\n",
      "plt.plot(df_pop1[jupyter_string].values, df_pop1['A_Se' <<unk>>], jupyter_string)\n",
      "plt.plot(df3[jupyter_string].values, df3['A_Se' <<unk>>].values, jupyter_string)\n",
      "plt.plot(df4[jupyter_string].values, df4['A_Se' <<unk>>].values, jupyter_string)\n",
      "plt.plot(df5[jupyter_string].values, df5['A_Se' <<unk>>].values, jupyter_string)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blue_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "red_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "green_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "yellow_patch = mpatches.Patch(color=jupyter_string, label=jupyter_string)\n",
      "\n",
      "\n",
      "plt.legend(handles=[blue_patch, red_patch, green_patch, yellow_patch])\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "kmf.fit(df[jupyter_string], event_observed=df[jupyter_string])\n",
      "kmf.plot()\n",
      "plt.show()\n",
      "=====\n",
      "T = df_pop1[jupyter_string]\n",
      "C = df_pop1['A_Se' <<unk>>]\n",
      "kmf.fit(T, event_observed=C)\n",
      "--------------------\n",
      "import gmaps\n",
      "import gmaps.datasets\n",
      "\n",
      "gmaps.configure(api_key=jupyter_string)\n",
      "=====\n",
      "import os\n",
      "import gmaps\n",
      "import gmaps.datasets\n",
      "\n",
      "\n",
      "gmaps.configure(api_key=jupyter_string) \n",
      "--------------------\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop1[[\"Latitude\", \"Longitude\"]], weights=df_pop1[\"A_Se\"],\n",
      "    \n",
      "    max_intensity=5, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "from gmaps.display import Image\n",
      "Image(filename=jupyter_string)\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop2[[\"Latitude\", \"Longitude\"]], weights=df_pop2[\"A_As\"],\n",
      "    max_intensity=80, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "df_pop3 = pd.read_csv(jupyter_string)\n",
      "df_pop3.head()\n",
      "=====\n",
      "fig = gmaps.figure()\n",
      "heatmap_layer = gmaps.heatmap_layer(\n",
      "    df_pop2[[\"Latitude\", \"Longitude\"]], weights=df_pop2[\"A_Pb\"],\n",
      "    \n",
      "    max_intensity=110, point_radius=15.0\n",
      ")\n",
      "fig.add_layer(heatmap_layer)\n",
      "fig\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "cars = pd.read_csv(jupyter_string)\n",
      "cars.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "cars = pd.read_csv(jupyter_string)\n",
      "cars = cars.dropna()\n",
      "cars.shape\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "=====\n",
      "cars.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import make_scorer\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "\n",
      "param_dist = {jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_uniform(0, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "              jupyter_string: sp_randint(1, 100),\n",
      "=====\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string) \n",
      "\n",
      "\n",
      "\n",
      "grid_params = {\n",
      "    jupyter_string: [0.01,0.1,1.0,10,100],\n",
      "    jupyter_string:[False,True],\n",
      "    jupyter_string:[0.25,0.5,0.75],\n",
      "    jupyter_string:[500,1000,1500,2000]\n",
      "}\n",
      "\n",
      "\n",
      "grid_eNet = GridSearchCV(eCV,grid_params,cv = 5)\n",
      "grid_eNet.fit(polyX_train,y_train)\n",
      "\n",
      "print(grid_eNet.best_params_)\n",
      "--------------------\n",
      "from pandas.plotting import scatter_matrix\n",
      "scatter_matrix(cars)\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams \n",
      "from pandas.tools.plotting import scatter_matrix \n",
      "\n",
      "\n",
      "\n",
      "def MatplotLib_Params():\n",
      "    \n",
      "    rcParams[jupyter_string] = (8, 6)\n",
      "    \n",
      "    rcParams[jupyter_string] = 150\n",
      "    \n",
      "    rcParams[jupyter_string] = 14\n",
      "    \n",
      "MatplotLib_Params()\n",
      "cont = cars[[\"mpg\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\"]]\n",
      "ax = scatter_matrix(cont)\n",
      "plt.show()\n",
      "--------------------\n",
      "grid_eNet.best_estimator_\n",
      "=====\n",
      "plot_learning_curve(grid_eNet, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.scatter(cars[\"weight\"],cars[\"mpg\"])\n",
      "plt.xlabel(\"weight\")\n",
      "plt.ylabel(\"mpg\")\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(cars.weight,cars.mpg)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "random_params = {\n",
      "    jupyter_string:st.expon(scale=0.01),\n",
      "    jupyter_string:[False,True],\n",
      "    jupyter_string:[500,1000,1500,2000]\n",
      "}\n",
      "\n",
      "\n",
      "random_eNet = RandomizedSearchCV(eCV,random_params,cv = 5, n_iter = 50)\n",
      "random_eNet.fit(polyX_train,y_train)\n",
      "\n",
      "print(random_eNet.best_params_)\n",
      "=====\n",
      "plot_learning_curve(random_eNet, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "result=ols(y=y_train,x=pd.DataFrame(X_train))\n",
      "\n",
      "R_2_IS=result.r2  \n",
      "OLS_coef=result.beta\n",
      "\n",
      "\n",
      "a=np.array(X_test)  \n",
      "b=np.array(result.beta) \n",
      "print(jupyter_string.format(b))\n",
      "c=np.sum(a*b[0:-1],axis=1)+b[-1] \n",
      "error=y_test-c \n",
      "R_2_OS=1-error.var()/y_test.var() \n",
      "print(jupyter_string.format(R_2_OS))\n",
      "=====\n",
      "Number_variables=[]\n",
      "\n",
      "OLS_R_2_OS_F=[]\n",
      "OLS_R_2_IS_F=[]\n",
      "\n",
      "t=0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for j in range(len(X_train.T)): \n",
      "    \n",
      "    t+=1\n",
      "    Number_variables.append(t)\n",
      "\n",
      "    result=ols(y=y_train,x=pd.DataFrame(X_train[:,0:j+1]))\n",
      "    temp=X_test[:,0:j+1]\n",
      "\n",
      "    a=np.array(temp)\n",
      "    b=np.array(result.beta)\n",
      "    c=np.sum(a*b[0:-1],axis=1)+b[-1]\n",
      "\n",
      "    error=y_test-c\n",
      "    R_2=1-error.var()/y_test.var()\n",
      "    if R_2>0:\n",
      "        OLS_R_2_OS_F.append(R_2)\n",
      "    else:\n",
      "        OLS_R_2_OS_F.append(0)\n",
      "    \n",
      "    OLS_R_2_IS_F.append(result.r2)\n",
      "\n",
      "pylab.title(jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_OS_F,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_IS_F,jupyter_string,label=jupyter_string)\n",
      "\n",
      "\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.xlabel(jupyter_string)\n",
      "pylab.ylabel(jupyter_string)\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "selectedCols = [\n",
      "    'perGameStats_Age' <<unk>>,\n",
      "    'perGameStats_G' <<unk>>,\n",
      "    'perGameStats_GS' <<unk>>,\n",
      "    'perGameStats_MP' <<unk>>,\n",
      "    'per100Stats_FG' <<unk>>,\n",
      "    'per100Stats_FGA' <<unk>>,\n",
      "    'per100Stats_FGPerc' <<unk>>,\n",
      "    'per100Stats_3P' <<unk>>,\n",
      "    'per100Stats_3PA' <<unk>>,\n",
      "    'per100Stats_3PPerc' <<unk>>,\n",
      "    'per100Stats_2P' <<unk>>,\n",
      "    'per100Stats_2PA' madeupword0002,\n",
      "    'per100Stats_2PPerc' <<unk>>,\n",
      "    'per100Stats_FT' <<unk>>,\n",
      "    'per100Stats_FTA' <<unk>>,\n",
      "    'per100Stats_FTPerc' <<unk>>,\n",
      "    'per100Stats_ORB' <<unk>>,\n",
      "    'per100Stats_DRB' <<unk>>,\n",
      "    'per100Stats_TRB' <<unk>>,\n",
      "    'per100Stats_AST' <<unk>>,\n",
      "    'per100Stats_STL' <<unk>>,\n",
      "    'per100Stats_BLK' <<unk>>,\n",
      "    'per100Stats_TOV' <<unk>>,\n",
      "    'per100Stats_PF' <<unk>>,\n",
      "    'per100Stats_PTS' <<unk>>,\n",
      "    'per100Stats_ORtg' <<unk>>,\n",
      "    'per100Stats_DRtg' <<unk>>,\n",
      "    'advancedStats_PER' <<unk>>,\n",
      "    'advancedStats_TSPerc' <<unk>>,\n",
      "    'advancedStats_3PAr' <<unk>>,\n",
      "    'advancedStats_FTr' <<unk>>,\n",
      "    'advancedStats_ORBPerc' <<unk>>,\n",
      "    'advancedStats_DRBPerc' <<unk>>,\n",
      "    'advancedStats_TRBPerc' <<unk>>,\n",
      "    'advancedStats_ASTPerc' <<unk>>,\n",
      "    'advancedStats_STLPerc' <<unk>>,\n",
      "    'advancedStats_BLKPerc' <<unk>>,\n",
      "    'advancedStats_TOVPerc' <<unk>>,\n",
      "    'advancedStats_USGPerc' <<unk>>,\n",
      "    'advancedStats_OWS' <<unk>>,\n",
      "    'advancedStats_DWS' <<unk>>,\n",
      "    'advancedStats_WS' <<unk>>,\n",
      "    'advancedStats_WS48' <<unk>>,\n",
      "    'advancedStats_OBPM' <<unk>>,\n",
      "    'advancedStats_DBPM' <<unk>>,\n",
      "    'advancedStats_BPM' <<unk>>,\n",
      "    'advancedStats_VORP' <<unk>>,\n",
      "    'accolades_all_nba' <<unk>>\n",
      "]\n",
      "\n",
      "playerAggDfAllNbaAllStarInitFeatures = playerAggDfAllNbaAllStar[selectedCols]\n",
      "--------------------\n",
      "sns.regplot(cars.weight,cars.mpg)\n",
      "plt.show()\n",
      "=====\n",
      "import  seaborn as sns\n",
      "\n",
      "ax = sns.regplot(x = \"weight\", y = \"mpg\", data = cars)\n",
      "plt.show()\n",
      "--------------------\n",
      "ax = sns.regplot(x = \"weight\", y = \"mpg\", data = cars, order = 2)\n",
      "plt.show()\n",
      "=====\n",
      "import numpy as np\n",
      "\n",
      "z = np.polyfit(cars.weight,cars.mpg,12)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "--------------------\n",
      "z = np.polyfit(cars.weight,cars.mpg,12)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "=====\n",
      "z = np.polyfit(cars.weight,cars.mpg,2)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "w_new = np.linspace(min(cars.weight),max(cars.weight),5000)\n",
      "mpg_new = f(w_new)\n",
      "\n",
      "plt.plot(cars.weight,cars.mpg,jupyter_string, w_new, mpg_new)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X = cars.weight.values.reshape(-1,1)\n",
      "y = cars.mpg\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "print(y_train.shape)\n",
      "print(y_test.shape)\n",
      "=====\n",
      "def Normalize(cont):\n",
      "    mean = cont.mean()\n",
      "    std = cont.std()\n",
      "    return (cont - mean)/std\n",
      "\n",
      "\n",
      "cars_cont = cars.drop([\"model_year\",\"name\",\"origin\",\"cylinders\"],axis = 1)\n",
      "\n",
      "\n",
      "for column in cars_cont.columns:\n",
      "    cars_cont[column] = Normalize(cars_cont[column])\n",
      "\n",
      "\n",
      "cars[cars_cont.columns] = cars_cont\n",
      "cars.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X = cars_cont\n",
      "y = cars.mpg\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "print(y_train.shape)\n",
      "print(y_test.shape)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(cars.drop([\"mpg\"],axis = 1),cars.mpg, \n",
      "                                                 test_size = 0.23, random_state = 42 )\n",
      "\n",
      "\n",
      "X_train.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "model = LinearRegression()\n",
      "\n",
      "\n",
      "model.fit(X_train,y_train)\n",
      "\n",
      "\n",
      "y_pred = model.predict(X_test)\n",
      "=====\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "\n",
      "\n",
      "right_model = LinearRegression(fit_intercept = False,normalize = True)\n",
      "right_model.fit(X_train,y_train)\n",
      "\n",
      "print(jupyter_string + str(right_model.score(X_train,y_train)))\n",
      "print(jupyter_string + str(right_model.score(X_test,y_test)))\n",
      "\n",
      "\n",
      "over_model = LinearRegression(fit_intercept = False)\n",
      "Poly = PolynomialFeatures(4)\n",
      "polyX_train = Poly.fit_transform(X_train.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "polyX_test = Poly.fit_transform(X_test.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "\n",
      "polyX_train = np.c_[polyX_train,X_train[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "polyX_test= np.c_[polyX_test,X_test[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "\n",
      "over_model.fit(polyX_train,y_train)\n",
      "print(jupyter_string + str(over_model.score(polyX_train,y_train)))\n",
      "print(jupyter_string + str(over_model.score(polyX_test,y_test)))\n",
      "\n",
      "\n",
      "almost_model = LinearRegression(fit_intercept = False)\n",
      "Poly = PolynomialFeatures(2)\n",
      "polyX_train = Poly.fit_transform(X_train.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "polyX_test = Poly.fit_transform(X_test.drop([\"cylinders\",jupyter_string,\"origin\"],axis = 1))\n",
      "\n",
      "polyX_train = np.c_[polyX_train,X_train[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "polyX_test= np.c_[polyX_test,X_test[[\"cylinders\",jupyter_string,\"origin\"]].values]\n",
      "\n",
      "almost_model.fit(polyX_train,y_train)\n",
      "print(jupyter_string + str(almost_model.score(polyX_train,y_train)))\n",
      "print(jupyter_string + str(almost_model.score(polyX_test,y_test)))\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.model_selection import learning_curve\n",
      "\n",
      "title = jupyter_string\n",
      "\n",
      "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
      "\n",
      "estimator = LinearRegression()\n",
      "plot_learning_curve(estimator, title, X_train, y_train, (0.0, 1.01), cv=cv, n_jobs=4)\n",
      "\n",
      "plt.show()\n",
      "=====\n",
      "plot_learning_curve(right_model, jupyter_string, X_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import validation_curve\n",
      "\n",
      "\n",
      "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
      "train_scores, test_scores = validation_curve(\n",
      "    right_model, X_train, y_train, param_name=jupyter_string, param_range=param_range,\n",
      "    cv=10, scoring=jupyter_string)\n",
      "train_scores_mean = np.mean(train_scores, axis=1)\n",
      "train_scores_std = np.std(train_scores, axis=1)\n",
      "test_scores_mean = np.mean(test_scores, axis=1)\n",
      "test_scores_std = np.std(test_scores, axis=1)\n",
      "plt.grid()\n",
      "plt.fill_between(param_range,\n",
      "=====\n",
      "plot_learning_curve(almost_model, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "y_pred_lasso = lasso.predict(polyX_test)\n",
      "y_pred_ridge = ridge.predict(polyX_test)\n",
      "y_pred_eNet = eNet.predict(polyX_test)\n",
      "\n",
      "mse_lasso = mean_squared_error(y_test,y_pred_lasso)\n",
      "mse_ridge = mean_squared_error(y_test,y_pred_ridge)\n",
      "mse_eNet = mean_squared_error(y_test,y_pred_eNet)\n",
      "\n",
      "print(jupyter_string,mse_lasso)\n",
      "print(jupyter_string,mse_ridge)\n",
      "print(jupyter_string,mse_eNet)\n",
      "=====\n",
      "plot_learning_curve(lasso, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "plot_learning_curve(ridge, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "=====\n",
      "plot_learning_curve(ridge, jupyter_string, polyX_train, y_train, ylim=(0.7, 1.01))\n",
      "plt.show()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "x_train = playerAggDfAllNbaAllStarInitFeatures.ix[:, playerAggDfAllNbaAllStarInitFeatures.columns != 'accolades_all_nba' <<unk>>]\n",
      "y_train = playerAggDfAllNbaAllStarInitFeatures['accolades_all_nba' <<unk>>]\n",
      "--------------------\n",
      "stack.head()\n",
      "=====\n",
      "SES = pd.read_csv(jupyter_string)\n",
      "SES.set_index(jupyter_string, inplace = True)\n",
      "SES = SES.iloc[:, 2:12]\n",
      "SES.dropna(thresh = 10, inplace = True)\n",
      "ColNames = SES.iloc[0]\n",
      "listnames = ColNames.str.extract(jupyter_string).astype(int)\n",
      "SES.columns = listnames\n",
      "SES[listnames] = SES[listnames].replace({jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string:jupyter_string}, regex=True)\n",
      "\n",
      "--------------------\n",
      "stack.head()\n",
      "=====\n",
      "stack[jupyter_string].value_counts()\n",
      "--------------------\n",
      "crimes = pd.read_csv(jupyter_string)\n",
      "crimes.head()\n",
      "=====\n",
      "AAnn = pd.read_csv(jupyter_string)\n",
      "AUF2015= pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "SES.index = SES.swapaxes(1,0).columns.str.replace(jupyter_string,jupyter_string) \n",
      "\n",
      "\n",
      "crime_table = pd.merge(SES.swapaxes(1,0), AAnn, left_index = True, right_on = 'Council District' <<unk>>, how = jupyter_string)\n",
      "\n",
      "\n",
      "crime_agg = pd.DataFrame(crime_table.groupby('Council District' <<unk>>).count().loc[:, 'GO Primary Key' <<unk>>]).rename(columns = {'GO Primary Key' <<unk>> : jupyter_string})\n",
      "\n",
      "\n",
      "crime_table = pd.merge(crime_table, crime_agg, left_on = 'Council District' <<unk>>, right_index = True, how = jupyter_string)\n",
      "--------------------\n",
      "crime_table = pd.merge(crime_table, AAnn, left_index = True, right_on = 'Council District' <unk>, how = jupyter_string)\n",
      "\n",
      "\n",
      "crime_agg = pd.DataFrame(crime_table.groupby('Council District' <unk>).count().loc[:, 'GO Primary Key' <unk>]).rename(columns = {'GO Primary Key' <unk> : jupyter_string})\n",
      "\n",
      "\n",
      "crime_table = pd.merge(crime_table, crime_agg, left_on = 'Council District' <unk>, right_index = True, how = jupyter_string)\n",
      "=====\n",
      "stack = pd.merge(crime_table, AUF2015, left_on='GO Primary Key' <<unk>>, right_on=' Primary Key' <<unk>>, how=jupyter_string, indicator = True)\n",
      "stack.loc[ stack.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack.loc[ stack.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack.loc[ stack.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "\n",
      "stack.loc[:, jupyter_string] = 0\n",
      "stack.loc[ stack.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "\n",
      "stack.loc[:, jupyter_string] = 0\n",
      "stack.loc[ stack.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "--------------------\n",
      "import statsmodels.formula.api as smf\n",
      "model = smf.ols(formula=jupyter_string, data=stack)\n",
      "results = model.fit()\n",
      "results.summary()\n",
      "=====\n",
      "sesmodel = smf.ols(formula = jupyter_string, data = stack).fit()\n",
      "sesmodel.summary()\n",
      "--------------------\n",
      "model = smf.ols(formula = jupyter_string, data = stack).fit()\n",
      "model.summary()\n",
      "=====\n",
      "model = smf.probit(formula = jupyter_string , data = stack).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "stack[jupyter_string] = stack[jupyter_string].astype(jupyter_string)\n",
      "=====\n",
      "crime_types = pd.merge(pd.get_dummies(crime_table.loc[:,  'Highest NIBRS/UCR Offense Description' <<unk>>]), crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "--------------------\n",
      "crime_types = pd.merge(crime_types, crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "=====\n",
      "crime_types_agg = pd.DataFrame(crime_types.groupby('Council District' <<unk>>)[jupyter_string].transform(jupyter_string))\n",
      "for item in [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]:\n",
      "    crime_types_agg = pd.merge(pd.DataFrame(crime_types.groupby('Council District' <<unk>>)[item].transform(jupyter_string)), crime_types_agg, left_index = True, right_index = True, how = jupyter_string)\n",
      "\n",
      "\n",
      "crime_table_types = pd.merge(crime_types_agg, crime_table, left_index = True, right_index = True)\n",
      "--------------------\n",
      "crime_table_types = pd.merge(crime_table_types, crime_table, left_index = True, right_index = True, how = jupyter_string)\n",
      "=====\n",
      "stack_types = pd.merge(crime_table_types, AUF2015, left_on='GO Primary Key' <<unk>>, right_on=' Primary Key' <<unk>>, how=jupyter_string, indicator = True)\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] == jupyter_string  , jupyter_string] = jupyter_string\n",
      "\n",
      "stack_types.loc[:, jupyter_string] = 0\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "\n",
      "stack_types.loc[:, jupyter_string] = 0\n",
      "stack_types.loc[ stack_types.loc[:, jupyter_string] != jupyter_string , jupyter_string] = 1\n",
      "--------------------\n",
      "sns.heatmap(stack_types.corr(), annot = True, fmt = jupyter_string)\n",
      "=====\n",
      "stack_types[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]].corr()\n",
      "--------------------\n",
      "xgb_model = xgb.XGBClassifier()\n",
      "\n",
      "\n",
      "param_grid_search = {\n",
      "    jupyter_string: [5],\n",
      "    jupyter_string: [0.1],\n",
      "    jupyter_string: [0.8],\n",
      "    jupyter_string: [0.9],\n",
      "    jupyter_string: [1],\n",
      "    jupyter_string: [jupyter_string],\n",
      "    jupyter_string: [1],\n",
      "    jupyter_string: [10000]\n",
      "}\n",
      "\n",
      "fit_params = {\n",
      "    jupyter_string: 100,\n",
      "    jupyter_string: jupyter_string,\n",
      "    jupyter_string: [[x_test, y_test_encoded]],\n",
      "    jupyter_string: 500\n",
      "}\n",
      "\n",
      "\n",
      "clf = GridSearchCV(\n",
      "    xgb_model, \n",
      "    param_grid_search, \n",
      "    fit_params = fit_params,\n",
      "    n_jobs = -1, \n",
      "    cv = 5, \n",
      "    scoring = jupyter_string,\n",
      "    verbose = 2, \n",
      "    refit = True\n",
      "=====\n",
      "clf.fit(x_train, y_train_encoded)\n",
      "--------------------\n",
      "stack_types[[jupyter_string, jupyter_string, jupyter_string]].corr()\n",
      "=====\n",
      "pd.DataFrame(stack_types.loc[:, [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]].corrwith(stack_types.loc[:, jupyter_string]), columns = [jupyter_string]             )\n",
      "--------------------\n",
      "model_types[jupyter_string].summary()\n",
      "=====\n",
      "ENTER_CRIME_STRING_HERE = jupyter_string\n",
      "model_types[ENTER_CRIME_STRING_HERE].summary()\n",
      "--------------------\n",
      "districts = pd.read_csv(jupyter_string)\n",
      "districts.head()\n",
      "=====\n",
      "AAnn = pd.read_csv(jupyter_string)\n",
      "AUF2015= pd.read_csv(jupyter_string)\n",
      "districts = gpd.read_file(jupyter_string)\n",
      "--------------------\n",
      "districts.head()\n",
      "=====\n",
      "AUF2015.rename(columns={' Primary Key' <<unk>>: jupyter_string, ' Effect on Officer' <<unk>>: jupyter_string, jupyter_string:jupyter_string, 'Officer Yrs of Service' <<unk>>: jupyter_string}, inplace=True)\n",
      "AAnn.rename(columns={jupyter_string:jupyter_string, 'Council District' <<unk>>: jupyter_string}, inplace=True)\n",
      "\n",
      "\n",
      "AUF2015.columns = AUF2015.columns.str.replace(jupyter_string,jupyter_string)\n",
      "AAnn.columns = AAnn.columns.str.replace(jupyter_string,jupyter_string)\n",
      "\n",
      "\n",
      "AUF2015 = AUF2015.drop_duplicates(subset=jupyter_string, keep=jupyter_string, inplace = False)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(10,10))\n",
      "districts.plot(ax=ax, color=jupyter_string)\n",
      "AUF2015.plot(ax=ax, color=jupyter_string)\n",
      "AAnn.plot(ax=ax, color=jupyter_string)\n",
      "=====\n",
      "AAnnCount = AAnn.groupby(jupyter_string).count()\n",
      "\n",
      "\n",
      "AAnnCount.index = AAnnCount.index.map(lambda x: int(x))\n",
      "\n",
      "districts.set_index(districts[jupyter_string].astype(int), inplace = True)\n",
      "districts.index = districts.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "geo_merge = pd.merge(districts, pd.DataFrame(AAnnCount.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string )\n",
      "\n",
      "\n",
      "AAnnMap = geo_merge.plot(column = jupyter_string,legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "geo_merge[jupyter_string] = geo_merge[jupyter_string].apply(lambda x: x.representative_point().coords[:])\n",
      "geo_merge[jupyter_string] = [coords[0] for coords in geo_merge[jupyter_string]]\n",
      "\n",
      "for idx, row in geo_merge.iterrows():\n",
      "    AAnnMap.annotate(s = row[jupyter_string], xy = row[jupyter_string], horizontalalignment= jupyter_string, fontsize = 30, color = jupyter_string)\n",
      "\n",
      "AAnnMap.set_title(jupyter_string, fontsize = 30, loc = jupyter_string)\n",
      "AAnnMap.set_axis_off()\n",
      "--------------------\n",
      "force_count = force.groupby(jupyter_string).count()\n",
      "\n",
      "\n",
      "force_count.index = force_count.index.map(lambda x: int(x))\n",
      "\n",
      "force_count.set_index(force_count[jupyter_string].astype(int), inplace = True)\n",
      "force_count.index = force_count.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "geo_merge = pd.merge(districts, pd.DataFrame(force_count.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string )\n",
      "\n",
      "\n",
      "forceMap = geo_merge.plot(column = jupyter_string,legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "geo_merge[jupyter_string]\n",
      "=====\n",
      "AUF2015Count = AUF2015.groupby('CouncilDistrict' <<unk>>).count()\n",
      "\n",
      "\n",
      "AUF2015Count.index = AUF2015Count.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "AUF_merge = pd.merge(districts, pd.DataFrame(AUF2015Count.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string)\n",
      "\n",
      "\n",
      "AUFMap = AUF_merge.plot(column = jupyter_string, legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "AUF_merge[jupyter_string] = AUF_merge[jupyter_string].apply(lambda x: x.representative_point().coords[:])\n",
      "AUF_merge[jupyter_string] = [coords[0] for coords in AUF_merge[jupyter_string]]\n",
      "\n",
      "for idx, row in AUF_merge.iterrows():\n",
      "    AUFMap.annotate(s = row[jupyter_string], xy = row[jupyter_string], horizontalalignment= jupyter_string, fontsize = 30, color = jupyter_string)\n",
      "\n",
      "AUFMap.set_title(jupyter_string, fontsize = 30, loc = jupyter_string)\n",
      "AUFMap.set_axis_off()\n",
      "\n",
      "AUFMap.figure.savefig(jupyter_string)\n",
      "--------------------\n",
      "AUF2015Count = AUF2015.groupby('CouncilDistrict' <unk>).count()\n",
      "\n",
      "\n",
      "AUF2015Count.index = AUF2015Count.index.map(lambda x: int(x))\n",
      "\n",
      "\n",
      "AUF_merge = pd.merge(districts, pd.DataFrame(AUF2015Count.loc[:, jupyter_string]).rename(columns = {jupyter_string: jupyter_string}), left_index = True, right_index = True, how = jupyter_string)\n",
      "\n",
      "\n",
      "AUF_merge = AUF_merge.plot(column = jupyter_string, legend = True, cmap = jupyter_string , linewidth = 0, figsize = (20,20))\n",
      "\n",
      "\n",
      "AUF_merge[jupyter_string] = AUF_merge[jupyter_string].apply(lambda x: x.representative_point().coords[:])\n",
      "AUF_merge[jupyter_string]\n",
      "=====\n",
      "bigmerge = pd.merge(geo_merge, pd.DataFrame(AUF_merge.loc[:,jupyter_string].rename(columns = {jupyter_string: jupyter_string})),\\\n",
      "left_index = True, right_index = True, how = jupyter_string)\n",
      "bigmerge = bigmerge.rename(columns={bigmerge.columns[11]: jupyter_string})\n",
      "\n",
      "AAnnAUFPlot = bigmerge[[jupyter_string,jupyter_string]].plot(kind = jupyter_string, colormap = jupyter_string, figsize = (12, 4))\n",
      "\n",
      "AAnnAUFPlot.legend([jupyter_string, jupyter_string], loc = jupyter_string)\n",
      "AAnnAUFPlot.set_title(jupyter_string, fontsize = 12, y = 1)\n",
      "AAnnAUFPlot.set_xlabel(jupyter_string)\n",
      "AAnnAUFPlot.set_ylabel(jupyter_string)\n",
      "--------------------\n",
      "y_pred = pm.Poisson('y_pred' <unk>, mu=mu)\n",
      "y_est = pm.Poisson('y_est' <unk>, mu=mu)\n",
      "=====\n",
      "import json\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pymc3 as pm\n",
      "import scipy\n",
      "import scipy.stats as stats\n",
      "import statsmodels.api as sm\n",
      "import theano.tensor as tt\n",
      "\n",
      "from IPython.display import Image\n",
      "\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "colors = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "          jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "messages = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "varnames = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "          jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "x_lim = 60\n",
      "burnin = 50000\n",
      "\n",
      "y_pred = trace[burnin:].get_values(jupyter_string)\n",
      "mu_mean = trace[burnin:].get_values(jupyter_string).mean()\n",
      "\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype=jupyter_string, color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds' <<unk>>].values, range=[0, x_lim], bins=x_lim, histtype=jupyter_string)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype=jupyter_string, color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds' <unk>].values, range=[0, x_lim], bins=x_lim, histtype=jupyter_string)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "plt.tight_layout()\n",
      "=====\n",
      "fig = plt.figure(figsize=(10,5))\n",
      "fig.add_subplot(211)\n",
      "x_lim = 70\n",
      "mu = [15, 40]\n",
      "for i in np.arange(x_lim):\n",
      "    plt.bar(i, stats.poisson.pmf(mu[0], i), color=colors[3])\n",
      "    plt.bar(i, stats.poisson.pmf(mu[1], i), color=colors[4])\n",
      "    \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.legend([jupyter_string % mu[0],\n",
      "                jupyter_string % mu[1]])\n",
      "\n",
      "\n",
      "def get_n(mu, alpha):\n",
      "    return 1. / alpha * mu\n",
      "\n",
      "def get_p(mu, alpha):\n",
      "    return get_n(mu, alpha) / (get_n(mu, alpha) + mu)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "a = [2, 4]\n",
      "\n",
      "for i in np.arange(x_lim):\n",
      "    plt.bar(i, stats.nbinom.pmf(i, n=get_n(mu[0], a[0]), p=get_p(mu[0], a[0])), color=colors[3])\n",
      "    plt.bar(i, stats.nbinom.pmf(i, n=get_n(mu[1], a[1]), p=get_p(mu[1], a[1])), color=colors[4])\n",
      "\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.legend([jupyter_string % (mu[0], a[0]),\n",
      "                jupyter_string % (mu[1], a[1])])\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import LeaveOneYearOut\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "=====\n",
      "playerAggDfAllNbaAllStar.head()\n",
      "--------------------\n",
      "_ = pm.plot_posterior(trace[burnin:], varnames=[jupyter_string, jupyter_string])\n",
      "=====\n",
      "x_lim = 60\n",
      "y_pred = trace[burnin:].get_values(jupyter_string)\n",
      "\n",
      "fig = plt.figure(figsize=(10,6))\n",
      "fig.add_subplot(211)\n",
      "\n",
      "fig.add_subplot(211)\n",
      "\n",
      "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype=jupyter_string, color=colors[1])   \n",
      "_ = plt.xlim(1, x_lim)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "fig.add_subplot(212)\n",
      "\n",
      "_ = plt.hist(messages['time_delay_seconds' <<unk>>].values, range=[0, x_lim], bins=x_lim, histtype=jupyter_string)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "pm.forestplot(trace[burnin:], varnames=[jupyter_string])\n",
      "=====\n",
      "prob_pois = trace[burnin:][jupyter_string].mean()\n",
      "prob_nb = 1 - prob_pois\n",
      "BF = (prob_nb/prob_pois)*(prior_model_prob/(1-prior_model_prob))\n",
      "print(jupyter_string % BF)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import re\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk import word_tokenize\n",
      "import matplotlib.pyplot as plt\n",
      "import warnings\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "conversation_df = pd.read_csv(jupyter_string,encoding=jupyter_string, sep=jupyter_string,warn_bad_lines =False,header=None)\n",
      "lines_df = pd.read_csv(jupyter_string,sep=jupyter_string,error_bad_lines=False,warn_bad_lines =False,header=None)\n",
      "characters_df = pd.read_csv(jupyter_string,sep=jupyter_string,warn_bad_lines =False,error_bad_lines=False,header=None)\n",
      "\n",
      "--------------------\n",
      "dialogues.info()\n",
      "=====\n",
      "cul_sentence = jupyter_string.join(dialogues)\n",
      "nltk.download(jupyter_string)\n",
      "--------------------\n",
      "characters_df.head()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "characters_df.columns=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "characters_df.head(5)\n",
      "--------------------\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk import word_tokenize\n",
      "=====\n",
      "tknzr = TweetTokenizer()\n",
      "final_tokens = tknzr.tokenize(cul_sentence)\n",
      "fdist = nltk.FreqDist(final_tokens)\n",
      "fdist.plot(20,cumulative=False)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "conversations_df.columns=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "conversations_df.head(5)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "conversation_df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "conversation_df.head(5)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "lines_df = pd.read_csv(jupyter_string)\n",
      "lines_df.columns = [jupyter_string, jupyter_string]\n",
      "lines_df.head(5)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "lines_df.columns = [jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "lines_df.head(5)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf = TfidfVectorizer(ngram_range=(2,2))\n",
      "tfidf_matrix = tfidf.fit_transform(df[jupyter_string])\n",
      "tfidf_matrix.shape\n",
      "=====\n",
      "biCharWords = nltk.bigrams(char_final_tokens)\n",
      "biFdist = nltk.FreqDist(biCharWords)\n",
      "print(len(biFdist))\n",
      "biFdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "playerAggDfAllNbaAllStar = playerAggDfAllNbaAllStar.reset_index()\n",
      "playerAggDfAllNbaAllStar.head()\n",
      "=====\n",
      "playerAggDfAllNbaAllStarInitFeatures.head()\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "final_processable_df.head(5)\n",
      "=====\n",
      "merged_df = pd.merge(lines_df, characters_df, how=jupyter_string, on=[jupyter_string,jupyter_string,jupyter_string],\n",
      "         left_index=False, right_index=False, sort=True,\n",
      "         suffixes=(jupyter_string, jupyter_string), copy=True, indicator=False)\n",
      "merged_df.head()\n",
      "--------------------\n",
      "merged_df.info()\n",
      "=====\n",
      "merged_df.dropna(how=jupyter_string, inplace=True)\n",
      "--------------------\n",
      "triFdist = nltk.FreqDist(triCharWords)\n",
      "print(len(triFdist))\n",
      "triFdist.plot(20, cumulative=False)\n",
      "triFdist.most_common()[:5]\n",
      "=====\n",
      "triCharWords = nltk.trigrams(char_final_tokens)\n",
      "triFdist = nltk.FreqDist(triCharWords)\n",
      "triFdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "dialogues[dialogues.dialogue == jupyter_string].dialogue.iloc[0]\n",
      "=====\n",
      "char_cul_sentence = jupyter_string.join(char_dialogues)\n",
      "--------------------\n",
      "char_cul_sentence.split(jupyter_string)\n",
      "=====\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "tknzr = TweetTokenizer()\n",
      "char_final_tokens = tknzr.tokenize(char_cul_sentence)\n",
      "--------------------\n",
      "df = pd.DataFrame(ng, columns=[jupyter_string, jupyter_string])\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: len(x))\n",
      "df.head()\n",
      "=====\n",
      "fdist = nltk.FreqDist(ng)\n",
      "fdist.plot(20, cumulative=False)\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "reg_dftrain=pd.read_csv(jupyter_string)\n",
      "reg_dftest=pd.read_csv(jupyter_string)\n",
      "dftrain = pd.read_csv(jupyter_string)\n",
      "dftest = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "ax.plot(train_values[:,np.newaxis], np.array(Ytrain_actual), jupyter_string)\n",
      "ax.plot(train_values[:,np.newaxis], model.predict(train_values[:,np.newaxis]), jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(train_values, Ytrain_actual, color=jupyter_string)\n",
      "plt.plot(train_values, Ytrain_pred)\n",
      "--------------------\n",
      "plt.scatter(test_values, Ytest_actual, color=jupyter_string)\n",
      "plt.plot(test_values, Ytest_pred)\n",
      "=====\n",
      "plt.scatter(test_values, Ytest_actual, color=jupyter_string)\n",
      "plt.plot(test_values, Ytest_pred)\n",
      "--------------------\n",
      "base_dict_train.to_csv(jupyter_string)\n",
      "base_dict_test.to_csv(jupyter_string)\n",
      "=====\n",
      "train_base = pd.DataFrame.from_dict(base_dict_train)\n",
      "test_base = pd.DataFrame.from_dict(base_dict_test)\n",
      "train_base.to_csv(jupyter_string)\n",
      "test_base.to_csv(jupyter_string)\n",
      "--------------------\n",
      "playerAggDfAllNbaAllStarInitFeatures = playerAggDfAllNbaAllStarInitFeatures.reset_index(drop=True)\n",
      "=====\n",
      "playerAggDfAllNbaAllStarInitFeaturesFullSet = playerAggDfAllNbaAllStarInitFeatures.merge(\n",
      "    playerAggDfAllNbaAllStar[['season_start_year' <<unk>>, 'perGameStats_Player' <<unk>>, 'perGameStats_Pos' <<unk>>]],\n",
      "    how = jupyter_string,\n",
      "    left_index = True,\n",
      "    right_index = True\n",
      ")\n",
      "\n",
      "\n",
      "playerAggDfAllNbaAllStarInitFeaturesFullSet.tail()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "=====\n",
      "dftrain = reg_dftrain\n",
      "dftest = reg_dftest\n",
      "Ytrain_actual = dftrain['like_x' <<unk>>]\n",
      "Ytest_actual = dftest['like_x' <<unk>>]\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "=====\n",
      "mergedf = pd.concat([dftrain, dftest])\n",
      "avg_like = {}\n",
      "for iid in set(list(mergedf['iid' madeupword0186].values)):\n",
      "    avg_like[iid] = np.mean(mergedf[mergedf['iid' madeupword0186] == iid]['like_x' <<unk>>])\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.info()\n",
      "\n",
      "\n",
      "\n",
      "df2 = df[['Transaction_Id' <<unk>>,'Product_Name' <<unk>>]]\n",
      "df2.head(5)\n",
      "df = df2.drop_duplicates()\n",
      "df.head(5)\n",
      "--------------------\n",
      "transactions = df.groupby('Transaction_Id' <unk>)['Product_Name' <unk>].apply(list)\n",
      "transactions.head(5)\n",
      "=====\n",
      "transactions = df.groupby(['Transaction_Id' <<unk>>])['Product_Name' <<unk>>].apply(list)\n",
      "\n",
      "print(transactions.head(10))\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "result_df.sort_values(jupyter_string, ascending=False).head(20)\n",
      "=====\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=False)\n",
      "print(result_df.head(10))\n",
      "--------------------\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=True)\n",
      "print(result_df.head(10))\n",
      "=====\n",
      "result_df = result_df.sort_values(by=jupyter_string, ascending=False)\n",
      "print(result_df.head(10))\n",
      "--------------------\n",
      "my_dataframe = pd.read_csv(jupyter_string)\n",
      "test_dataframe = pd.read_csv(jupyter_string)\n",
      "my_head = my_dataframe.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "from pandas import Series,DataFrame\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import statsmodels.formula.api as sm\n",
      "\n",
      "my_dataframe=pd.read_csv(jupyter_string)\n",
      "test_dataframe=pd.read_csv(jupyter_string)\n",
      "my_head=my_dataframe.head()\n",
      "\n",
      "\n",
      "--------------------\n",
      "my_head.head()\n",
      "=====\n",
      "bitcoin_price_analysis = pd.read_csv(jupyter_string)\n",
      "bcktstdf=pd.read_csv(jupyter_string)\n",
      "tmp=bitcoin_price_analysis.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "my_plt = plt.plot(bcktstdf['date' <unk>], bcktstdf['btc_market_price' <unk>])\n",
      "my_infer = jupyter_string\n",
      "\n",
      "plt.show()\n",
      "=====\n",
      "my_infer=jupyter_string\n",
      "my_plt=my_dataframe[['Date' <<unk>>,'btc_market_price' <<unk>>]].set_index('Date' <<unk>>)\n",
      "--------------------\n",
      "my_infer=jupyter_string\n",
      "my_plt=my_dataframe[['Date' <unk>,'btc_market_price' <unk>]].set_index('Date' <unk>)\n",
      "=====\n",
      "tmp=bitcoin_price_analysis[['Date' <<unk>>,'btc_market_price' <<unk>>]].set_index('Date' <<unk>>)\n",
      "tmp.plot()\n",
      "--------------------\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "=====\n",
      "x_test_full.head()\n",
      "--------------------\n",
      "my_plt = tmp.plot(kind=jupyter_string)\n",
      "my_inf = jupyter_string\n",
      "my_inf\n",
      "=====\n",
      "my_inf=jupyter_string\n",
      "my_plt=sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "my_inf=jupyter_string\n",
      "my_plt=sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "=====\n",
      "g = sns.jointplot( \"btc_market_cap\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "my_plt = sns.jointplot( \"btc_total_bitcoins\", \"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "=====\n",
      "my_plt=sns.jointplot( \"btc_total_bitcoins\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "my_inf=jupyter_string\n",
      "--------------------\n",
      "my_plt=sns.jointplot( \"btc_market_cap\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "my_inf=jupyter_string\n",
      "=====\n",
      "g = sns.jointplot( \"btc_total_bitcoins\",\"btc_market_price\", data=bitcoin_price_analysis, color=jupyter_string, size=7)\n",
      "--------------------\n",
      "ms_columns = bitcoin_price_analysis.columns[bitcoin_price_analysis.isnull().any()]\n",
      "ms_columns\n",
      "=====\n",
      "ms_columns=my_dataframe.columns[df1.isnull().any()].tolist()\n",
      "--------------------\n",
      "ms_columns=dataframe.columns[dataframe.isnull().any()].tolist()\n",
      "ms_columns\n",
      "=====\n",
      "df1=bitcoin_price_analysis\n",
      "missing_columns=df1.columns[df1.isnull().any()].tolist()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.plot(df1['btc_total_bitcoins' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "my_inf=jupyter_string\n",
      "\n",
      "my_dataframe[['Date' <<unk>>,'btc_total_bitcoins' <<unk>>]].set_index('Date' <<unk>>).plot()\n",
      "--------------------\n",
      "my_inf=jupyter_string\n",
      "my_dataframe[['Date' <unk>,'btc_total_bitcoins' <unk>]].set_index('Date' <unk>).plot()\n",
      "=====\n",
      "df1[['Date' <<unk>>,'btc_total_bitcoins' <<unk>>]].set_index('Date' <<unk>>).plot()\n",
      "--------------------\n",
      "btc_total_bitcoins = btc_total_bitcoins.fillna(method=jupyter_string)\n",
      "btc_total_bitcoins.head()\n",
      "=====\n",
      "my_dataframe['btc_total_bitcoins' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "--------------------\n",
      "my_dataframe.head()\n",
      "=====\n",
      "df1['btc_total_bitcoins' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "=====\n",
      "all_nba_all_year_predicted_df['accolades_all_nba' <<unk>>].value_counts().plot(kind = jupyter_string)\n",
      "--------------------\n",
      "df1['btc_trade_volume' <unk>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_transaction_fees' <unk>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "=====\n",
      "my_dataframe['btc_trade_volume' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "my_dataframe['btc_transaction_fees' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "my_dataframe['btc_transaction_fees' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "my_dataframe['btc_blocks_size' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "my_dataframe['btc_median_confirmation_time' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "my_dataframe['btc_difficulty' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "\n",
      "--------------------\n",
      "my_dataframe.head()\n",
      "=====\n",
      "df1['btc_trade_volume' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_transaction_fees' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_blocks_size' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_median_confirmation_time' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "df1['btc_difficulty' <<unk>>].interpolate(method=jupyter_string,axis=0,inplace=True)\n",
      "\n",
      "--------------------\n",
      "my_dataframe['date' <unk>] = pd.to_datetime(my_dataframe['date' <unk>])\n",
      "my_dataframe.set_index('date' <unk>,inplace=True)\n",
      "=====\n",
      "import datetime as dt\n",
      "\n",
      "test_dataframe['Date' <<unk>>]=test_dataframe['Date' <<unk>>].astype(jupyter_string)\n",
      "test_dataframe['Date' <<unk>>] = pd.to_datetime(test_dataframe['Date' <<unk>>]).dt.strftime(jupyter_string)\n",
      "\n",
      "test_dataframe['Date' <<unk>>] = pd.to_datetime(test_dataframe['Date' <<unk>>])\n",
      "test_dataframe['Date' <<unk>>]=test_dataframe['Date' <<unk>>].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "my_dataframe['Date' <<unk>>]=my_dataframe['Date' <<unk>>].astype(jupyter_string)\n",
      "my_dataframe['Date' <<unk>>] = pd.to_datetime(my_dataframe['Date' <<unk>>]).dt.strftime(jupyter_string)\n",
      "\n",
      "my_dataframe['Date' <<unk>>] = pd.to_datetime(my_dataframe['Date' <<unk>>])\n",
      "my_dataframe['Date' <<unk>>]=my_dataframe['Date' <<unk>>].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "my_dataframe['Date' <<unk>>]\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.plot(my_dataframe['Date' <unk>],my_dataframe['btc_market_price' <unk>])\n",
      "plt.plot(my_dataframe['Date' <unk>],y_hat)\n",
      "plt.show()\n",
      "=====\n",
      "import datetime as dt\n",
      "\n",
      "bcktstdf['Date' <<unk>>]=bcktstdf['Date' <<unk>>].astype(jupyter_string)\n",
      "bcktstdf['Date' <<unk>>] = pd.to_datetime(bcktstdf['Date' <<unk>>]).dt.strftime(jupyter_string)\n",
      "\n",
      "bcktstdf['Date' <<unk>>] = pd.to_datetime(bcktstdf['Date' <<unk>>])\n",
      "bcktstdf['Date' <<unk>>]=bcktstdf['Date' <<unk>>].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "df1['Date' <<unk>>]=df1['Date' <<unk>>].astype(jupyter_string)\n",
      "df1['Date' <<unk>>] = pd.to_datetime(df1['Date' <<unk>>]).dt.strftime(jupyter_string)\n",
      "\n",
      "df1['Date' <<unk>>] = pd.to_datetime(df1['Date' <<unk>>])\n",
      "df1['Date' <<unk>>]=df1['Date' <<unk>>].map(dt.datetime.toordinal)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.\n",
      "=====\n",
      "lm = sm.ols(formula=jupyter_string, data=my_dataframe).fit()\n",
      "y_hat = lm.predict(my_dataframe[['Date' <<unk>>,'btc_n_orphaned_blocks' <<unk>>,'btc_total_bitcoins' <<unk>>, 'btc_market_cap' madeupword0002,'btc_trade_volume' <<unk>>, 'btc_blocks_size' <<unk>>, 'btc_avg_block_size' <<unk>>,'btc_n_transactions_per_block' <<unk>>, 'btc_median_confirmation_time' <<unk>>,'btc_hash_rate' <<unk>>, 'btc_difficulty' <<unk>>, 'btc_miners_revenue' <<unk>>, 'btc_transaction_fees' <<unk>>,'btc_cost_per_transaction' <<unk>>, 'btc_n_unique_addresses' <<unk>>, 'btc_n_transactions' <<unk>>, 'btc_n_transactions_total' <<unk>>, 'btc_n_transactions_excluding_popular' <<unk>>, 'btc_n_transactions_excluding_chains_longer_than_100' <<unk>>,'btc_estimated_transaction_volume_usd' <<unk>>,'btc_output_volume' <<unk>>, 'btc_estimated_transaction_volume' <<unk>>]])\n",
      "print(jupyter_string, mean_squared_error(my_dataframe['btc_market_price' <<unk>>].values, y_hat))\n",
      "print(lm.rsquared)\n",
      "lm.summary()\n",
      "--------------------\n",
      "lm = sm.ols(formula=jupyter_string, data=my_dataframe).fit()\n",
      "y_hat = lm.predict(my_dataframe[['Date' <unk>,'btc_n_orphaned_blocks' <unk>]])\n",
      "print(jupyter_string, mean_squared_error(my_dataframe['btc_market_price' <unk>].values, y_hat))\n",
      "print(lm.rsquared)\n",
      "lm.summary()\n",
      "=====\n",
      "lm = sm.ols(formula=jupyter_string, data=df1).fit()\n",
      "y_hat = lm.predict(df1[['Date' <<unk>>,'btc_n_orphaned_blocks' <<unk>>,'btc_total_bitcoins' <<unk>>, 'btc_market_cap' madeupword0002,'btc_trade_volume' <<unk>>, 'btc_blocks_size' <<unk>>, 'btc_avg_block_size' <<unk>>,'btc_n_transactions_per_block' <<unk>>, 'btc_median_confirmation_time' <<unk>>,'btc_hash_rate' <<unk>>, 'btc_difficulty' <<unk>>, 'btc_miners_revenue' <<unk>>, 'btc_transaction_fees' <<unk>>,'btc_cost_per_transaction' <<unk>>, 'btc_n_unique_addresses' <<unk>>, 'btc_n_transactions' <<unk>>, 'btc_n_transactions_total' <<unk>>, 'btc_n_transactions_excluding_popular' <<unk>>, 'btc_n_transactions_excluding_chains_longer_than_100' <<unk>>,'btc_estimated_transaction_volume_usd' <<unk>>,'btc_output_volume' <<unk>>, 'btc_estimated_transaction_volume' <<unk>>]])\n",
      "print(jupyter_string, mean_squared_error(df1['btc_market_price' <<unk>>].values, y_hat))\n",
      "print(lm.rsquared)\n",
      "lm.summary()\n",
      "--------------------\n",
      "y_hat = lm.predict(df1[['Date' <unk>,'btc_n_orphaned_blocks' <unk>,'btc_total_bitcoins' <unk>, 'btc_market_cap' <unk>,'btc_trade_volume' <unk>, 'btc_blocks_size' <unk>, 'btc_avg_block_size' <unk>,'btc_n_transactions_per_block' <unk>, 'btc_median_confirmation_time' <unk>,'btc_hash_rate' <unk>, 'btc_difficulty' <unk>, 'btc_miners_revenue' <unk>, 'btc_transaction_fees' <unk>,'btc_cost_per_transaction' <unk>, 'btc_n_unique_addresses' <unk>, 'btc_n_transactions\n",
      "=====\n",
      "testy_hat=lm.predict(test_dataframe[['Date' <<unk>>,'btc_n_orphaned_blocks' <<unk>>,'btc_total_bitcoins' <<unk>>, 'btc_market_cap' madeupword0002,'btc_trade_volume' <<unk>>, 'btc_blocks_size' <<unk>>, 'btc_avg_block_size' <<unk>>,'btc_n_transactions_per_block' <<unk>>, 'btc_median_confirmation_time' <<unk>>,'btc_hash_rate' <<unk>>, 'btc_difficulty' <<unk>>, 'btc_miners_revenue' <<unk>>, 'btc_transaction_fees' <<unk>>,'btc_cost_per_transaction' <<unk>>, 'btc_n_unique_addresses' <<unk>>, 'btc_n_transactions' <<unk>>, 'btc_n_transactions_total' <<unk>>, 'btc_n_transactions_excluding_popular' <<unk>>, 'btc_n_transactions_excluding_chains_longer_than_100' <<unk>>,'btc_estimated_transaction_volume_usd' <<unk>>,'btc_output_volume' <<unk>>, 'btc_estimated_transaction_volume' <<unk>>]])\n",
      "test_dataframe['btc_market_price' <<unk>>]=testy_hat\n",
      "test_dataframe\n",
      "--------------------\n",
      "test_dataframe.plot(x='Date' <unk>,y='btc_market_price' <unk>)\n",
      "=====\n",
      "testy_hat=lm.predict(bcktstdf[['Date' <<unk>>,'btc_total_bitcoins' <<unk>>, 'btc_market_cap' madeupword0002,'btc_trade_volume' <<unk>>, 'btc_blocks_size' <<unk>>, 'btc_avg_block_size' <<unk>>,'btc_n_transactions_per_block' <<unk>>, 'btc_median_confirmation_time' <<unk>>,'btc_hash_rate' <<unk>>, 'btc_difficulty' <<unk>>, 'btc_miners_revenue' <<unk>>, 'btc_transaction_fees' <<unk>>,'btc_cost_per_transaction' <<unk>>, 'btc_n_unique_addresses' <<unk>>, 'btc_n_transactions' <<unk>>, 'btc_n_transactions_total' <<unk>>, 'btc_n_transactions_excluding_popular' <<unk>>, 'btc_n_transactions_excluding_chains_longer_than_100' <<unk>>,'btc_estimated_transaction_volume_usd' <<unk>>,'btc_output_volume' <<unk>>, 'btc_estimated_transaction_volume' <<unk>>]])\n",
      "bcktstdf['btc_market_price' <<unk>>]=testy_hat\n",
      "bcktstdf\n",
      "--------------------\n",
      "X=bcktstdf[['btc_total_bitcoins' <unk>, 'btc_market_cap' <unk>,'btc_trade_volume' <unk>, 'btc_blocks_size' <unk>, 'btc_avg_block_size' <unk>,'btc_n_transactions_per_block' <unk>, 'btc_median_confirmation_time' <unk>,'btc_hash_rate' <unk>, 'btc_difficulty' <unk>, 'btc_miners_revenue' <unk>, 'btc_transaction_fees' <unk>,'btc_cost_per_transaction' <unk>, 'btc_n_unique_addresses' <unk>, 'btc_n_transactions' <unk>, 'btc_n_transactions_total' <unk>, 'btc_n\n",
      "=====\n",
      "my_dataframe.corr()\n",
      "\n",
      "--------------------\n",
      "sns.heatmap(my_dataframe.corr())\n",
      "=====\n",
      "df1.corr()\n",
      "--------------------\n",
      "all_nba_all_year_predicted_df['accolades_all_nba' <unk>].value_counts().plot(kind = jupyter_string)\n",
      "=====\n",
      "all_nba_all_year_predicted_df['accolades_all_nba' <<unk>>].value_counts()\n",
      "--------------------\n",
      "titanic = pd.read_csv(jupyter_string)\n",
      "titanic.head()\n",
      "=====\n",
      "titanic_df = pd.read_csv(jupyter_string)\n",
      "titanic_df.head()\n",
      "--------------------\n",
      "male_passengers = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]\n",
      "female_passengers = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]\n",
      "=====\n",
      "groupedby_Sex = titanic_df.groupby('Sex' madeupword0002,as_index = False)\n",
      "--------------------\n",
      "titanic_df = titanic_df.drop('Cabin' <unk>, axis=1)\n",
      "titanic_df.head()\n",
      "=====\n",
      "titanic_df.drop(['Cabin' <<unk>>],axis = 1,inplace = True)\n",
      "--------------------\n",
      "titanic_df.head()\n",
      "=====\n",
      "titanic_df.head()\n",
      "--------------------\n",
      "titanic_df.groupby('Survived' <unk>).count()\n",
      "=====\n",
      "groupedby_Survived = titanic_df.groupby('Survived' <<unk>>,as_index = False)\n",
      "survival_count = groupedby_Survived.count()\n",
      "print(jupyter_string+str(titanic_df.shape[0]))\n",
      "print(jupyter_string+str(survival_count.loc[1,'PassengerId' <<unk>>]))\n",
      "print(jupyter_string+str(survival_count.loc[0,'PassengerId' <<unk>>]))\n",
      "--------------------\n",
      "groupedby_Survived.head()\n",
      "=====\n",
      "percentage_survived = (survival_count.loc[1,'PassengerId' <<unk>>]/titanic_df.shape[0])*100\n",
      "percentage_not_survived = (survival_count.loc[0,'PassengerId' <<unk>>]/titanic_df.shape[0])*100\n",
      "sizes = [percentage_survived,percentage_not_survived]\n",
      "labels = ['Survived' <<unk>>,jupyter_string]\n",
      "plt.pie(sizes, labels=labels, autopct=jupyter_string, startangle=90)\n",
      "x = plt.title(jupyter_string)\n",
      "--------------------\n",
      "titanic_df.groupby('Pclass' <unk>)['PassengerId' <unk>].count()\n",
      "=====\n",
      "groupedby_Pclass = titanic_df.groupby('Pclass' <<unk>>,as_index = False)\n",
      "--------------------\n",
      "groupedby_Survived['Age' <unk>].apply(compute_age_stats)\n",
      "=====\n",
      "titanic_df['Age' <<unk>>].hist()\n",
      "plt.xlabel(\"Age\")\n",
      "plt.ylabel(jupyter_string)\n",
      "x = plt.title(jupyter_string)\n",
      "--------------------\n",
      "titanic_df['Pclass' <unk>].hist()\n",
      "plt.xlabel(\"Pclass\")\n",
      "plt.ylabel(jupyter_string)\n",
      "x = plt.title(jupyter_string)\n",
      "=====\n",
      "survived_group = groupedby_Survived.get_group(1)\n",
      "not_survived_group = groupedby_Survived.get_group(0)\n",
      "--------------------\n",
      "cart_data = df.groupby(['CustomerID' <unk>, 'InvoiceNo' <unk>]).Sales.sum().reset_index()\n",
      "\n",
      "\n",
      "cart_data.head()\n",
      "=====\n",
      "cart_data = df.groupby(['CustomerID' <<unk>> , 'InvoiceNo' <<unk>>]).Sales.agg( { jupyter_string : jupyter_string })\n",
      "\n",
      "\n",
      "cart_data.reset_index(inplace=True)\n",
      "\n",
      "\n",
      "cart_data.head()\n",
      "--------------------\n",
      "all_nba_all_year_predicted_df.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "all_nba_all_year_predicted_df[all_nba_all_year_predicted_df['accolades_all_nba' <<unk>>] == jupyter_string][jupyter_string].hist()\n",
      "--------------------\n",
      "agg_cart_data = cart_data.groupby('CustomerID' <unk>).cart_value.agg([jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "\n",
      "agg_cart_data.reset_index(inplace=True)\n",
      "\n",
      "\n",
      "agg_cart_data.head()\n",
      "=====\n",
      "agg_cart_data = cart_data.groupby('CustomerID' <<unk>>).cart_value.agg({jupyter_string:jupyter_string,\\\n",
      "                                                                 jupyter_string:jupyter_string,\\\n",
      "                                                                 jupyter_string:jupyter_string})\n",
      "\n",
      "\n",
      "\n",
      "agg_cart_data.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "customer_data = customer_data.set_index('CustomerID' <unk>)\n",
      "\n",
      "\n",
      "customer_data.head()\n",
      "=====\n",
      "customer_df = invoice_data.join([product_data, sales_data, agg_cart_data])\n",
      "\n",
      "\n",
      "customer_df.head()\n",
      "--------------------\n",
      "customer_df.to_csv(jupyter_string)\n",
      "=====\n",
      "customer_df.to_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head(10)\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df['Country' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize=(6,7))\n",
      "\n",
      "\n",
      "sns.countplot(y='Country' madeupword0002,data=df)\n",
      "plt.show()\n",
      "--------------------\n",
      "df.isnull().sum()\n",
      "=====\n",
      "df.isnull().sum()\n",
      "--------------------\n",
      "df = df.dropna(subset=['CustomerID' <unk>])\n",
      "=====\n",
      "df = df[df.CustomerID.notnull()]\n",
      "--------------------\n",
      "df.CustomerID = df.CustomerID.astype(int)\n",
      "df.CustomerID.head()\n",
      "=====\n",
      "df['CustomerID' <<unk>>] = df.CustomerID.astype(int)\n",
      "\n",
      "\n",
      "df.CustomerID.head()\n",
      "\n",
      "--------------------\n",
      "df[jupyter_string] = df['Sales' <unk>]\n",
      "\n",
      "\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string] = df.Quantity * df.UnitPrice\n",
      "\n",
      "\n",
      "df.Sales.head()\n",
      "\n",
      "--------------------\n",
      "Number_variables=[]\n",
      "\n",
      "OLS_R_2_OS_F=[]\n",
      "OLS_R_2_IS_F=[]\n",
      "\n",
      "t=0\n",
      "\n",
      "\n",
      "for j in range(len(X_train.T)): \n",
      "    \n",
      "    t+=1\n",
      "    Number_variables.append(t)\n",
      "\n",
      "    result=ols(y=y_train,x=pd.DataFrame(X_train[:,0:j+1]))\n",
      "    temp=X_test[:,0:j+1]\n",
      "\n",
      "    a=np.array(temp)\n",
      "    b=np.array(result.beta)\n",
      "    c=np.sum(a*b[0:-1],axis=1)+b[-1]\n",
      "\n",
      "    error=y_test-c\n",
      "    R_2=1-error.var()/y_test.var()\n",
      "    if R_2>0\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=1) \n",
      "\n",
      "Ridge.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "--------------------\n",
      "x_test_full[((x_test_full['accolades_all_nba' <unk>] == jupyter_string) & (x_test_full['season_start_year' <unk>] == 2006) & (x_test_full['perGameStats_Pos' <unk>].str.contains(jupyter_string))) | ((x_test_full['season_start_year' <unk>] == 2006) & (x_test_full['perGameStats_Player' <unk>] == jupyter_string))][cols_to_view].hist()\n",
      "=====\n",
      "x_test_full[(x_test_full['accolades_all_nba' <<unk>>] == jupyter_string) & (x_test_full[jupyter_string] == jupyter_string)][jupyter_string].hist()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, index_col=None)\n",
      "\n",
      "\n",
      "df.head()\n",
      "=====\n",
      "df.to_csv(jupyter_string,index=None)\n",
      "--------------------\n",
      "invoice_data = pd.read_csv(jupyter_string)\n",
      "invoice_data.head()\n",
      "=====\n",
      "invoice_data = df.groupby('CustomerID' <<unk>>).InvoiceNo.agg({ jupyter_string : jupyter_string })\n",
      "\n",
      "\n",
      "invoice_data.head()\n",
      "--------------------\n",
      "product_data = df.groupby('CustomerID' <unk>).StockCode.agg({ jupyter_string : jupyter_string })\n",
      "\n",
      "\n",
      "product_data.head()\n",
      "=====\n",
      "product_data = df.groupby('CustomerID' <<unk>>).StockCode.agg({jupyter_string:jupyter_string,jupyter_string:jupyter_string})\n",
      "\n",
      "\n",
      "product_data.head()\n",
      "--------------------\n",
      "sales_data = df.groupby('CustomerID' <unk>).Sales.agg({jupyter_string:jupyter_string,jupyter_string:jupyter_string})\n",
      "\n",
      "\n",
      "sales_data.head()\n",
      "=====\n",
      "sales_data = df.groupby('CustomerID' <<unk>>).Sales.agg({jupyter_string:jupyter_string,jupyter_string:jupyter_string})\n",
      "\n",
      "\n",
      "sales_data.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "full_df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "full_df.head()\n",
      "--------------------\n",
      "full_df.info()\n",
      "=====\n",
      "df = full_df[['Survived' <<unk>>, 'Sex' madeupword0002, 'Pclass' <<unk>>, 'Age' <<unk>>]]\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0,100)\n",
      "plt.xlim(0,100)\n",
      "plt.scatter(df[jupyter_string],df[jupyter_string])\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0,100)\n",
      "plt.xlim(0,100)\n",
      "plt.scatter(df[jupyter_string],df[jupyter_string])\n",
      "=====\n",
      "survivor_counts = df['Survived' <<unk>>].value_counts()\n",
      "survivor_counts.index = [jupyter_string, jupyter_string]\n",
      "survivor_counts.name = jupyter_string\n",
      "survivor_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "sex_counts = df['Sex' madeupword0002].value_counts()\n",
      "sex_counts.index = [jupyter_string, jupyter_string]\n",
      "sex_counts.name = jupyter_string\n",
      "sex_counts\n",
      "=====\n",
      "gender_counts = df['Sex' madeupword0002].value_counts()\n",
      "gender_counts.index = [jupyter_string, jupyter_string]\n",
      "gender_counts.name = jupyter_string\n",
      "gender_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "gender_counts = df['Sex' madeupword0002].value_counts()\n",
      "gender_counts.index = [jupyter_string, jupyter_string]\n",
      "gender_counts.name = jupyter_string\n",
      "gender_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "=====\n",
      "survivor_gender_counts = df[df['Survived' <<unk>>]==1]['Sex' madeupword0002].value_counts()\n",
      "survivor_gender_counts.index = [jupyter_string, jupyter_string]\n",
      "survivor_gender_counts.name = jupyter_string\n",
      "survivor_gender_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from IPython.display import display\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string, nrows=5, usecols=['LA1' <<unk>>,'LO1' <<unk>>,'EL1' <<unk>>,'LO1.1' <<unk>>,'LA1.1' <<unk>>,'EL1.1' <<unk>>,'VISIBLE' madeupword0002]).head()\n",
      "df['EL1' <<unk>>] = df['EL1' <<unk>>].round()\n",
      "df['EL1.1' <<unk>>] = df['EL1.1' <<unk>>].round()\n",
      "df['VISIBLE' madeupword0002] = df['VISIBLE' madeupword0002].round()\n",
      "display(df)\n",
      "--------------------\n",
      "survivor_age_counts = df[df['Survived' <unk>]==1]['Age' <unk>].value_counts()\n",
      "survivor_age_counts.index = [jupyter_string, jupyter_string]\n",
      "survivor_age_counts.name = jupyter_string\n",
      "survivor_age_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "=====\n",
      "grouped_by_pclass = df['Sex' madeupword0002].groupby(df['Pclass' <<unk>>])\n",
      "grouped_counts = grouped_by_pclass.value_counts()\n",
      "grouped_counts.name = jupyter_string\n",
      "grouped_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "sns.jointplot(x='Age' <unk>, y='Survived' <unk>, data=df)\n",
      "=====\n",
      "df_no_missing['Age' <<unk>>].plot(kind=jupyter_string, title=jupyter_string)\n",
      "--------------------\n",
      "df_no_missing['Age' <unk>].describe()\n",
      "=====\n",
      "df_no_missing[df_no_missing['Survived' <<unk>>]==1]['Age' <<unk>>].plot(kind=jupyter_string, title=jupyter_string)\n",
      "--------------------\n",
      "df_no_missing[df_no_missing['Survived' <unk>]==0]['Age' <unk>].plot(kind=jupyter_string, title=jupyter_string)\n",
      "=====\n",
      "df_no_missing[df_no_missing['Survived' <<unk>>]==1]['Age' <<unk>>].plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df_no_missing[df_no_missing['Survived' <unk>]==0]['Age' <unk>].plot(kind=jupyter_string)\n",
      "=====\n",
      "df_no_missing[df_no_missing['Survived' <<unk>>]==0]['Age' <<unk>>].plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df_no_missing[df_no_missing['Survived' <unk>]==0]['Pclass' <unk>].value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "pclass_counts = df['Pclass' <<unk>>].value_counts()\n",
      "pclass_counts.name = jupyter_string\n",
      "pclass_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "pclass_counts = df['Pclass' <unk>].value_counts()\n",
      "pclass_counts.name = jupyter_string\n",
      "pclass_counts.plot(kind=jupyter_string)\n",
      "=====\n",
      "survivor_pclass_counts = df[df['Survived' <<unk>>]==1]['Pclass' <<unk>>].value_counts()\n",
      "survivor_pclass_counts.name = jupyter_string\n",
      "survivor_pclass_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "pclass_counts = df['Pclass' <unk>].value_counts()\n",
      "pclass_counts.name = jupyter_string\n",
      "pclass_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "=====\n",
      "victim_pclass_counts = df[df['Survived' <<unk>>]==0]['Pclass' <<unk>>].value_counts()\n",
      "victim_pclass_counts.name = jupyter_string\n",
      "victim_pclass_counts.plot(kind=jupyter_string, autopct=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "SPX_data_dump = pd.read_csv(jupyter_string)\n",
      "print (jupyter_string.format(*SPX_data_dump.shape))\n",
      "print (jupyter_string)\n",
      "display(SPX_data_dump.head(n=5))\n",
      "--------------------\n",
      "SPX_data_dump.head()\n",
      "=====\n",
      "SPX_data_dump['Date' <<unk>>] = pd.to_datetime(SPX_data_dump.Date, format=jupyter_string)\n",
      "SPX_data_dump = SPX_data_dump.sort_values(by='Date' <<unk>>, ascending=1)\n",
      "SPX_data_dump.index = np.arange(rows)[::-1]\n",
      "display(SPX_data_dump.head(n=5))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train = pd.read_csv(jupyter_string)\n",
      "train.head(3)\n",
      "--------------------\n",
      "SPX_data_dump['Date' <unk>] = pd.to_datetime(SPX_data_dump.Date, format=jupyter_string)\n",
      "SPX_data_dump = SPX_data_dump.sort_values(by='Date' <unk>, ascending=1)\n",
      "SPX_data_dump.index = np.arange(rows)[::-1]\n",
      "display(SPX_data_dump.head(n=5))\n",
      "=====\n",
      "SPX_data_dump = SPX_data_dump.sort_index()\n",
      "display(SPX_data_dump.head(n=5))\n",
      "--------------------\n",
      "SPX_data_modified.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "SPX_data_modified.to_csv(jupyter_string)\n",
      "--------------------\n",
      "SPX_data_modified = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "X = SPX_data_modified.iloc[:,0:(cols_new-1)]\n",
      "Y = SPX_data_modified.iloc[:,(cols_new-1)]\n",
      "print(jupyter_string)\n",
      "display(X.head(n=5))\n",
      "print(jupyter_string)\n",
      "display(Y.head(n=5))\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "a = pd.concat([X,Y], axis=1)\n",
      "if pd.DataFrame.equals(a,SPX_data_modified):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
      "print(jupyter_string)\n",
      "print(X_train.shape)\n",
      "print(jupyter_string)\n",
      "print(X_test.shape)\n",
      "print(jupyter_string)\n",
      "print(Y_train.shape)\n",
      "print(jupyter_string)\n",
      "print(Y_test.shape)\n",
      "=====\n",
      "n = round(0.2*rows_new)\n",
      "X_test = X.iloc[0:n,:]\n",
      "Y_test = Y[0:n]\n",
      "\n",
      "X_cv = X.iloc[n:(2*n+1),:]\n",
      "Y_cv = Y[n:(2*n+1)]\n",
      "\n",
      "X_train = X.iloc[(2*n+1):,:]\n",
      "Y_train = Y[(2*n+1):]\n",
      "\n",
      "print(X_test.shape[0], X_cv.shape[0], X_train.shape[0])\n",
      "--------------------\n",
      "X_train = X_train.reset_index(drop=True)\n",
      "Y_train = Y_train.reset_index(drop=True)\n",
      "X_cv = X_cv.reset_index(drop=True)\n",
      "Y_cv = Y_cv.reset_index(drop=True)\n",
      "=====\n",
      "a = pd.concat([Y_test,Y_cv,Y_train])\n",
      "if pd.DataFrame.equals(a,Y):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "\n",
      "b = pd.concat([X_test,X_cv,X_train])\n",
      "if pd.DataFrame.equals(b,X):\n",
      "    print(jupyter_string)\n",
      "else:\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "=====\n",
      "from sklearn.metrics import f1_score, accuracy_score\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX' <<unk>>)\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX' <<unk>>)\n",
      "\n",
      "benchmark_Y_pred_train = np.empty(Y_train.shape[0])\n",
      "benchmark_Y_pred_train.fill(1)\n",
      "cashflows_train = (X_train.iloc[0,0],X_train.iloc[0,int(close_index[0])]),\\\n",
      "                  (X_train.iloc[X_train.shape[0]-1,0],-X_train.iloc[X_train.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.\\\n",
      "      format(accuracy_score(Y_train, benchmark_Y_pred_train)))\n",
      "print(jupyter_string.format(f1_score(Y_train, benchmark_Y_pred_train)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_train)))\n",
      "\n",
      "benchmark_Y_pred_cv = np.empty(Y_cv.shape[0])\n",
      "benchmark_Y_pred_cv.fill(1)\n",
      "cashflows_cv = (X_cv.iloc[0,0],X_cv.iloc[0,int(close_index[0])]),\\\n",
      "               (X_cv.iloc[X_cv.shape[0]-1,0],-X_cv.iloc[X_cv.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.\\\n",
      "      format(accuracy_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print(jupyter_string.format(f1_score(Y_cv, benchmark_Y_pred_cv)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_cv)))\n",
      "\n",
      "benchmark_Y_pred_test = np.empty(Y_test.shape[0])\n",
      "benchmark_Y_pred_test.fill(1)\n",
      "cashflows_test = (X_test.iloc[0,0],X_test.iloc[0,int(close_index[0])]),\\\n",
      "                 (X_test.iloc[X_test.shape[0]-1,0],-X_test.iloc[X_test.shape[0]-1,int(open_index[0])])\n",
      "print(jupyter_string.format(accuracy_score(Y_test, benchmark_Y_pred_test)))\n",
      "print(jupyter_string.format(f1_score(Y_test, benchmark_Y_pred_test)))\n",
      "print(jupyter_string.format(100*xirr(cashflows_test)))\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "import xgboost as x_gb\n",
      "\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "svm = SVC()\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(svm, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(rf, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "score_classifier(xgb, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "svm = SVC()\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "\n",
      "dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(dt, X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "\n",
      "=====\n",
      "max_depth_list = np.arange(1,11)\n",
      "min_samples_leaf_list = np.arange(1,11)\n",
      "criterion_list = (jupyter_string,jupyter_string)\n",
      "splitter_list = (jupyter_string,jupyter_string)\n",
      "\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "best_dt = dt\n",
      "best_dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_dt_Y_cv_pred = best_dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_dt_F1 = f1_score(Y_cv, best_dt_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for min_samples_leaf in min_samples_leaf_list:\n",
      "        for criterion in criterion_list:\n",
      "            for splitter in splitter_list:\n",
      "                dt = DecisionTreeClassifier(random_state=42, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
      "                dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
      "                if dt_F1 > best_dt_F1:\n",
      "                    best_dt = dt\n",
      "                    best_dt_F1 = dt_F1\n",
      "                    print(jupyter_string.\\\n",
      "                          format(max_depth, min_samples_leaf, criterion, splitter, dt_F1))\n",
      "\n",
      "print(best_dt)\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier(random_state=42)\n",
      "dt.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "dt_Y_cv_pred = dt.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "dt_F1 = f1_score(Y_cv, dt_Y_cv_pred)\n",
      "=====\n",
      "best_dt_feature_importance = pd.Series(best_dt.feature_importances_, index=\\\n",
      "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
      "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
      "print(best_dt_feature_importance.sort_values(ascending=False))\n",
      "--------------------\n",
      "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=jupyter_string)\n",
      "=====\n",
      "sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap = jupyter_string)\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "=====\n",
      "kernel_list = (jupyter_string, jupyter_string, jupyter_string, jupyter_string)\n",
      "C_list = np.arange(1,16)\n",
      "gamma_list = [1e-7*10**i for i in np.arange(1,7)]\n",
      "degree_list = np.arange(2,6)\n",
      "\n",
      "svm = SVC()\n",
      "best_svm = svm\n",
      "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_svm_Y_cv_pred = best_svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_svm_F1 = f1_score(Y_cv, best_svm_Y_cv_pred)\n",
      "for kernel in kernel_list:\n",
      "    for C in C_list:\n",
      "        for gamma in gamma_list:\n",
      "            for degree in degree_list:\n",
      "                svm = SVC(C=C, gamma=gamma, kernel=kernel, degree=degree)\n",
      "                svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                svm_Y_cv_pred = svm.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                svm_F1 = f1_score(Y_cv, svm_Y_cv_pred)\n",
      "                if svm_F1 > best_svm_F1:\n",
      "                    best_svm = svm\n",
      "                    best_svm_F1 = svm_F1\n",
      "                    print(jupyter_string.format(kernel, C, gamma, degree, svm_F1))\n",
      "\n",
      "print(best_svm)\n",
      "--------------------\n",
      "best_svm.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_svm_Y_test_pred = best_svm.predict(X_test.iloc[:,engineered_features_start_index:X_test.shape[1]])\n",
      "=====\n",
      "n_estimators_list = [1] + [i*10 for i in np.arange(1,11)]\n",
      "criterion_list = (jupyter_string,jupyter_string)\n",
      "max_depth_list = np.arange(1,7)\n",
      "min_samples_leaf_list = np.arange(1, 10, 3)\n",
      "max_features_list = (jupyter_string,jupyter_string,None)\n",
      "\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "best_rf = rf\n",
      "best_rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for min_samples_leaf in min_samples_leaf_list:\n",
      "        for criterion in criterion_list:\n",
      "            for n_estimators in n_estimators_list:\n",
      "                for max_features in max_features_list:\n",
      "                    rf = RandomForestClassifier(random_state=42,max_depth=max_depth, min_samples_leaf=min_samples_leaf,\\\n",
      "                                                criterion=criterion, n_estimators=n_estimators, max_features=max_features)\n",
      "                    rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                    rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                    rf_F1 = f1_score(Y_cv, rf_Y_cv_pred)\n",
      "                    if rf_F1 > best_rf_F1:\n",
      "                        best_rf = rf\n",
      "                        best_rf_F1 = rf_F1\n",
      "                        print(jupyter_string.\\\n",
      "                              format(max_depth, min_samples_leaf, criterion, n_estimators, max_features, rf_F1))\n",
      "\n",
      "print(best_rf)\n",
      "--------------------\n",
      "rf = RandomForestClassifier(random_state=42)\n",
      "rf.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "rf_Y_cv_pred = rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "rf_Y_test_pred = rf.predict(X_test.iloc[:,engineered_features_start_index:X_test.shape[1]])\n",
      "=====\n",
      "best_rf_feature_importance = pd.Series(best_rf.feature_importances_, index=\\\n",
      "                                  SPX_data_modified.columns.values[engineered_features_start_index:\\\n",
      "                                                                   (len(SPX_data_modified.columns.values)-1)])\n",
      "print(best_rf_feature_importance.sort_values(ascending=False))\n",
      "--------------------\n",
      "best_rf_feature_importance.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "n_estimators_list = [1, 10, 30, 50 ,100] + [i*250 for i in np.arange(1, 5)]\n",
      "max_depth_list = np.arange(1,11)\n",
      "reg_lambda_list = [0, 0.1, 0.5, 1, 2, 5]\n",
      "gamma_list = [0] + [10**i for i in np.arange(0,4)]\n",
      "\n",
      "xgb = x_gb.XGBClassifier()\n",
      "best_xgb = xgb\n",
      "best_xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_xgb_Y_cv_pred = best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
      "for max_depth in max_depth_list:\n",
      "    for n_estimators in n_estimators_list:\n",
      "        for reg_lambda in reg_lambda_list:\n",
      "            for gamma in gamma_list:\n",
      "                xgb = x_gb.XGBClassifier(max_depth=max_depth, n_estimators = n_estimators, reg_lambda=reg_lambda, gamma=gamma)\n",
      "                xgb.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "                xgb_Y_cv_pred = xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "                xgb_F1 = f1_score(Y_cv, xgb_Y_cv_pred)\n",
      "                if xgb_F1 > best_xgb_F1:\n",
      "                    best_xgb = xgb\n",
      "                    best_xgb_F1 = xgb_F1\n",
      "                    print(jupyter_string.\\\n",
      "                           format(max_depth, n_estimators, reg_lambda, gamma, xgb_F1))\n",
      "\n",
      "print(best_xgb)\n",
      "--------------------\n",
      "best_xgb_Y_cv_pred = best_xgb.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_xgb_F1 = f1_score(Y_cv, best_xgb_Y_cv_pred)\n",
      "=====\n",
      "close_index = np.where(SPX_data_modified.columns.values == 'Close_SPX' <<unk>>)\n",
      "open_index = np.where(SPX_data_modified.columns.values == 'Open_SPX' <<unk>>)\n",
      "date_index = np.where(SPX_data_modified.columns.values == 'Date' <<unk>>)\n",
      "\n",
      "def convert_label_to_cash_sign(label):\n",
      "    if label == 1:\n",
      "        cash_sign = -1\n",
      "    else:\n",
      "        cash_sign = 1\n",
      "    \n",
      "    return(cash_sign)\n",
      "\n",
      "def get_cashflows(best_clf, X):\n",
      "    Y_pred = best_clf.predict(X.iloc[:,engineered_features_start_index:X.shape[1]])\n",
      "    \n",
      "    \n",
      "    \n",
      "    length = len(Y_pred)\n",
      "    prev_cash_sign = convert_label_to_cash_sign(Y_pred[length-1])\n",
      "    net_position = -prev_cash_sign\n",
      "    cashflows = ((X.iloc[length-1, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                  X.iloc[length-1, int(open_index[0])]*prev_cash_sign),)\n",
      "\n",
      "    for i in np.arange(length-2, 1, -1):\n",
      "        cur_cash_sign = convert_label_to_cash_sign(Y_pred[i])\n",
      "        if cur_cash_sign != prev_cash_sign or net_position == 0:\n",
      "            net_position = net_position - cur_cash_sign\n",
      "            cashflows = (cashflows) + ((X.iloc[i, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                         X.iloc[i, int(open_index[0])]*cur_cash_sign),)\n",
      "        prev_cash_sign = cur_cash_sign\n",
      "\n",
      "    cur_cash_sign = convert_label_to_cash_sign(Y_pred[0])\n",
      "    \n",
      "    \n",
      "    \n",
      "    if net_position == 0:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),) + \\\n",
      "                                ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*cur_cash_sign),)\n",
      "    elif cur_cash_sign != prev_cash_sign:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 X.iloc[0, int(open_index[0])]*cur_cash_sign),)\n",
      "        net_position = net_position - cur_cash_sign\n",
      "    else:\n",
      "        cashflows = (cashflows) + ((X.iloc[0, int(date_index[0])].strftime(jupyter_string),\\\n",
      "                                 -X.iloc[0, int(close_index[0])]*prev_cash_sign),)\n",
      "        net_position = net_position + prev_cash_sign\n",
      "    \n",
      "    return(cashflows)\n",
      "\n",
      "best_dt_cv_cashflows = get_cashflows(best_dt, X_cv)\n",
      "best_svm_cv_cashflows = get_cashflows(best_svm, X_cv)\n",
      "best_rf_cv_cashflows = get_cashflows(best_rf, X_cv)\n",
      "best_xgb_cv_cashflows = get_cashflows(best_xgb, X_cv)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "=====\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "voting_list = (jupyter_string,jupyter_string)\n",
      "weights_list = [[2,1,3],[2,3,1],[1,2,3],[1,3,2],[3,1,2],[3,2,1],None]\n",
      "vot = VotingClassifier(estimators=[(jupyter_string, best_dt), (jupyter_string, best_xgb), (jupyter_string, best_rf)])\n",
      "best_vot = vot\n",
      "best_vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "best_vot_Y_cv_pred = best_vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_vot_F1 = f1_score(Y_cv, best_vot_Y_cv_pred)\n",
      "for voting in voting_list:\n",
      "    for weights in weights_list:        \n",
      "        vot = VotingClassifier(estimators=[(jupyter_string, best_dt), (jupyter_string, best_xgb), (jupyter_string, best_rf)], voting=voting, weights=weights)\n",
      "        vot.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "        vot_Y_cv_pred = vot.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "        vot_F1 = f1_score(Y_cv, vot_Y_cv_pred)        \n",
      "        if vot_F1 > best_vot_F1:\n",
      "            best_vot = vot\n",
      "            best_vot_F1 = vot_F1\n",
      "            print(jupyter_string.format(voting, weights, vot_F1))\n",
      "\n",
      "print(best_vot)\n",
      "--------------------\n",
      "best_rf_Y_cv_pred = best_rf.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "best_rf_accuracy = accuracy_score(Y_cv, best_rf_Y_cv_pred)\n",
      "best_rf_F1 = f1_score(Y_cv, best_rf_Y_cv_pred)\n",
      "best_rf_cashflows = get_cashflows(best_rf, X_cv)\n",
      "best_rf_XIRR = xirr(best_rf_cashflows)\n",
      "print(jupyter_string.format(best_rf_accuracy, \\\n",
      "                                                                                               best_rf_F1, 100*best_rf_XIRR))\n",
      "=====\n",
      "print(jupyter_string.format(best_rf))\n",
      "print((jupyter_string)*100)\n",
      "\n",
      "random_state_list = np.arange(1,101)\n",
      "F1_list = []\n",
      "XIRR_list = []\n",
      "for random_state in random_state_list:\n",
      "    rf_modified_random_state = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=jupyter_string,\n",
      "                            max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                            min_samples_leaf=7, min_samples_split=2,\n",
      "                            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
      "                            oob_score=False, random_state=random_state, verbose=0, warm_start=False)\n",
      "    rf_modified_random_state.fit(X_train.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train)\n",
      "    rfm_Y_cv_pred = rf_modified_random_state.predict(X_cv.iloc[:,engineered_features_start_index:X_cv.shape[1]])\n",
      "    rfm_F1 = f1_score(Y_cv, rfm_Y_cv_pred)\n",
      "    F1_list = F1_list + [rfm_F1]\n",
      "    rfm_cv_cashflows = get_cashflows(rf_modified_random_state, X_cv)\n",
      "    try:\n",
      "        rfm_XIRR = xirr(rfm_cv_cashflows)\n",
      "    except:\n",
      "        rfm_XIRR = np.nan\n",
      "    XIRR_list = XIRR_list + [rfm_XIRR]\n",
      "\n",
      "\n",
      "\n",
      "from itertools import compress\n",
      "XIRR_list = list(compress(XIRR_list, np.logical_not(np.isnan(XIRR_list))))\n",
      "\n",
      "f, axes = plt.subplots(2,1)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string.format(np.mean(F1_list), np.std(F1_list)))\n",
      "print(jupyter_string.\\\n",
      "      format(np.max(F1_list), int(np.where(F1_list == np.max(F1_list))[0]+1)))\n",
      "print(jupyter_string.format(np.min(F1_list), int(np.where(F1_list == np.min(F1_list))[0]+1)))\n",
      "print(jupyter_string)\n",
      "print(jupyter_string.format(100*np.mean(XIRR_list), 100*np.std(XIRR_list)))\n",
      "print(jupyter_string.\\\n",
      "      format(100*np.max(XIRR_list), int(np.where(XIRR_list == np.max(XIRR_list))[0]+1)))\n",
      "print(jupyter_string.\\\n",
      "      format(100*np.min(XIRR_list), int(np.where(XIRR_list == np.min(XIRR_list))[0]+1)))\n",
      "\n",
      "axes[0].axis([0, 100, 0.5, 0.7])\n",
      "axes[0].plot(F1_list)\n",
      "axes[0].set_ylabel(jupyter_string)\n",
      "\n",
      "axes[1].axis([0, 100, 0.18, 7.5])\n",
      "axes[1].plot(XIRR_list)\n",
      "axes[1].set_ylabel(jupyter_string)\n",
      "\n",
      "axes[1].set_xlabel(jupyter_string)\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "sns.distplot(F1_list)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import plotly\n",
      "plotly.tools.set_credentials_file(username=jupyter_string, api_key=jupyter_string)\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.tools import FigureFactory as FF\n",
      "import scipy\n",
      "\n",
      "x = F1_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "--------------------\n",
      "x = XIRR_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "=====\n",
      "x = XIRR_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.05,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "--------------------\n",
      "x = F1_list\n",
      "trace = go.Histogram(x=x,\n",
      "                     \n",
      "                     xbins=dict(start=np.min(x),\n",
      "                                size=0.005,\n",
      "                                end=np.max(x)),\n",
      "                     marker=dict(color=jupyter_string))\n",
      "\n",
      "layout = go.Layout(\n",
      "    title=jupyter_string\n",
      ")\n",
      "\n",
      "fig = go.Figure(data=go.Data([trace]), layout=layout)\n",
      "py.iplot(fig, filename=jupyter_string)\n",
      "=====\n",
      "X_train_new = pd.concat([X_cv, X_train])\n",
      "Y_train_new = pd.concat([Y_cv, Y_train])\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(best_rf, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
      "score_classifier(best_rf, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
      "best_rf_test_cashflows = get_cashflows(best_rf, X_test)\n",
      "print(jupyter_string.format(100*xirr(best_rf_test_cashflows)))\n",
      "\n",
      "print(jupyter_string)\n",
      "train_classifier(best_vot, X_train_new.iloc[:,engineered_features_start_index:X_train.shape[1]], Y_train_new)\n",
      "score_classifier(best_vot, X_test.iloc[:,engineered_features_start_index:X_cv.shape[1]], Y_test)\n",
      "best_vot_test_cashflows = get_cashflows(best_vot, X_test)\n",
      "print(jupyter_string.format(100*xirr(best_vot_test_cashflows)))\n",
      "--------------------\n",
      "train['Age' <unk>].fillna(train['Age' <unk>].median(), inplace=True)\n",
      "=====\n",
      "sns.set_style(jupyter_string)\n",
      "--------------------\n",
      "data = pandas.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pandas.DataFrame()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.DataFrame()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "data.tail()\n",
      "=====\n",
      "data.tail()\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "pd.date_range(jupyter_string, jupyter_string, freq=jupyter_string)\n",
      "=====\n",
      "times = pd.DatetimeIndex(data['starttime' <<unk>>])\n",
      "--------------------\n",
      "days = np.array([1,2,3,4,5,6])\n",
      "days\n",
      "=====\n",
      "import numpy as np\n",
      "np.exp(data[jupyter_string])\n",
      "--------------------\n",
      "rides.gender.value_counts()\n",
      "=====\n",
      "pd.value_counts(data['gender' <<unk>>])\n",
      "--------------------\n",
      "pd.value_counts(data['driver_gender' <unk>])\n",
      "=====\n",
      "pd.value_counts(data['birthyear' <<unk>>])\n",
      "--------------------\n",
      "sns.boxplot(x='Pclass' <unk>, y='Age' <unk>, hue='Survived' <unk>, data=train)\n",
      "=====\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Sex\", palette = jupyter_string)\n",
      "--------------------\n",
      "pd.value_counts(data['age' <unk>])\n",
      "=====\n",
      "pd.value_counts(data['birthyear' <<unk>>]).sort_index()\n",
      "--------------------\n",
      "pd.value_counts(data['birthyear' <unk>]).plot(kind=jupyter_string)\n",
      "=====\n",
      "pd.value_counts(2015 - data['birthyear' <<unk>>]).sort_index()\n",
      "--------------------\n",
      "pd.value_counts(data['tripduration' <unk>]).sort_index()\n",
      "=====\n",
      "pd.value_counts(times.dayofweek)\n",
      "--------------------\n",
      "pd.value_counts(times.dayofweek).sort_index()\n",
      "=====\n",
      "pd.value_counts(times.dayofweek, sort=False)\n",
      "--------------------\n",
      "pd.value_counts(times.dayofweek, sort=True)\n",
      "=====\n",
      "pd.value_counts(times.month)\n",
      "--------------------\n",
      "pd.value_counts(times.year)\n",
      "=====\n",
      "pd.value_counts(times.month, sort=False)\n",
      "--------------------\n",
      "df.groupby([df.index.hour, df.index.dayofweek]).size()\n",
      "=====\n",
      "pd.value_counts(times.hour)\n",
      "--------------------\n",
      "times.groupby('hour' <unk>).count()\n",
      "=====\n",
      "data.groupby(times.hour).count()\n",
      "--------------------\n",
      "data.groupby(times.hour).mean()\n",
      "=====\n",
      "data.groupby(times.hour)[jupyter_string].mean()\n",
      "--------------------\n",
      "data.groupby(times.hour)[jupyter_string].mean()\n",
      "=====\n",
      "data.groupby(['gender' <<unk>>])[jupyter_string].mean()\n",
      "--------------------\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Pclass\", palette = jupyter_string)\n",
      "=====\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Pclass\", palette = jupyter_string)\n",
      "--------------------\n",
      "data.groupby([jupyter_string, 'gender' <unk>])[jupyter_string].mean()\n",
      "=====\n",
      "grouped = data.groupby([times.hour, 'gender' <<unk>>])[jupyter_string].mean()\n",
      "grouped\n",
      "--------------------\n",
      "grouped.unstack()\n",
      "=====\n",
      "grouped.unstack()\n",
      "--------------------\n",
      "df.plot()\n",
      "=====\n",
      "data.groupby([times.hour, 'usertype' <<unk>>])[jupyter_string].mean().unstack().plot()\n",
      "--------------------\n",
      "data.groupby([times.hour, 'usertype' <unk>])['tripminutes' <unk>].mean().unstack().plot()\n",
      "=====\n",
      "data.plot.hist()\n",
      "--------------------\n",
      "data.plot.hist()\n",
      "=====\n",
      "data[jupyter_string].plot.hist(bins=100)\n",
      "--------------------\n",
      "data[jupyter_string].plot.hist(bins=100)\n",
      "=====\n",
      "plot = data[jupyter_string].plot.hist(bins=500)\n",
      "plot.set_xlim(0, 50)\n",
      "\n",
      "--------------------\n",
      "plt.scatter(Xtrain.iloc[:,0],Xtrain.iloc[:,1],c=ypred_train)\n",
      "plt.scatter(Xtest.iloc[:,0],Xtest.iloc[:,1],c=ypred_test)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "sns.lmplot(data=train,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "--------------------\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "=====\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.datasets import samples_generator\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "\n",
      "X,y = samples_generator.make_classification(n_informative=4,n_features=20,\n",
      "                                           n_redundant=0,random_state=5)\n",
      "X = pd.DataFrame(X); y = pd.DataFrame(y)\n",
      "X.head()\n",
      "--------------------\n",
      "plt.figure()\n",
      "sns.lmplot(data=test,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "plt.show()\n",
      "=====\n",
      "distances, indices = clf.kneighbors(Xtest)\n",
      "neighborpoints = train.iloc[np.unique(indices.ravel())]\n",
      "sns.lmplot(data=neighborpoints,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "plt.pcolormesh(x1grid,x2grid,output,cmap=matplotlib.cm.Pastel1,zorder=0)\n",
      "plt.title(jupyter_string)\n",
      "--------------------\n",
      "sns.countplot(x = \"Survived\", data = train, hue = \"Sex\", palette = jupyter_string)\n",
      "=====\n",
      "train['Age' <<unk>>].plot.hist(bins = 50)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "knn = KNeighborsRegressor(n_neighbors=3)\n",
      "knn.fit(Xtrain,ytrain)\n",
      "ypred_test = knn.predict(Xtest)\n",
      "print(jupyter_string,accuracy_score(ytest,ypred_test))\n",
      "=====\n",
      "amplitude = 10.; num_points=100\n",
      "X = amplitude*np.random.rand(num_points,1)-0.5*amplitude\n",
      "\n",
      "\n",
      "y = np.sinc(X).ravel(); y+=0.2*(0.5 - np.random.rand(y.size))\n",
      "X.shape\n",
      "y.shape\n",
      "data = pd.DataFrame({jupyter_string:X.ravel(),jupyter_string:y})\n",
      "\n",
      "\n",
      "sns.lmplot(data=data,x=jupyter_string,y=jupyter_string,fit_reg=False)\n",
      "--------------------\n",
      "plt.scatter(X,y)\n",
      "plt.plot(xgrid,yvals)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "plt.scatter(X,y,marker=jupyter_string,facecolors=jupyter_string,edgecolors=jupyter_string,s=50)\n",
      "plt.plot(xgrid,yvals,jupyter_string,linewidth=3.0)\n",
      "--------------------\n",
      "ratings = pd.read_json(jupyter_string)\n",
      "ratings.head()\n",
      "=====\n",
      "ratingsdf = pd.read_json(jupyter_string)\n",
      "ratingsdf.head(10)\n",
      "--------------------\n",
      "importances = pipe.feature_importances_\n",
      "std = np.std([tree.feature_importances_ for tree in pipe.estimators_],\n",
      "             axis=0)\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "for f in range(X.shape[1]):\n",
      "    print(jupyter_string % (f + 1, indices[f], importances[indices[f]]))\n",
      "\n",
      "plt.figure()\n",
      "plt.title(jupyter_string)\n",
      "plt.bar(range(X.shape[1]), importances[indices],\n",
      "       color=jupyter_string, yerr=std[indices], align=jupyter_string)\n",
      "plt.xticks(range(X.shape[1]), indices)\n",
      "plt.xlim([-1, X.shape[1]])\n",
      "plt.show()\n",
      "=====\n",
      "features_status = pipe.named_steps[jupyter_string].get_support()\n",
      "features = np.arange(1,21)\n",
      "print(jupyter_string,features[features_status])\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "import pandas as pd\n",
      "=====\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "\n",
      "X = pd.DataFrame( np.array([[1,1],[1,3],[2,2],[2.5,5],[3,1],[4,2],[2.,3.5],\n",
      "                            [3,3],[3.5,4] ]),columns=[jupyter_string,jupyter_string])\n",
      "X.head()\n",
      "--------------------\n",
      "neigh = NearestNeighbors(n_neighbors=3)\n",
      "neigh.fit(X)\n",
      "distances, indices = neigh.kneighbors(X)\n",
      "=====\n",
      "num_neighbors= 3\n",
      "\n",
      "\n",
      "input=[2.6,1.7]\n",
      "\n",
      "\n",
      "sns.lmplot(x=jupyter_string,y=jupyter_string,data=X,fit_reg=False)\n",
      "plt.plot(input[0],input[1],jupyter_string)\n",
      "--------------------\n",
      "user1=jupyter_string\n",
      "user2=jupyter_string\n",
      "print(jupyter_string, pearson_score(ratingsdf,user1,user2))\n",
      "=====\n",
      "ratingsdf = pd.read_json(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "\n",
      "knn = KNeighborsRegressor(n_neighbors=num_neighbors)\n",
      "\n",
      "\n",
      "knn.fit(X,y)\n",
      "=====\n",
      "knn = NearestNeighbors(n_neighbors=num_neighbors,algorithm=jupyter_string).fit(X)\n",
      "\n",
      "\n",
      "distances,indices = knn.kneighbors(input)\n",
      "print( indices)\n",
      "print(jupyter_string)\n",
      "knnX = X.iloc[indices[0]]\n",
      "knnX.head()\n",
      "--------------------\n",
      "plt.scatter(knnX[jupyter_string],knnX[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "sns.lmplot(x=jupyter_string,y=jupyter_string,data=X,fit_reg=False)\n",
      "plt.plot(input[0],input[1],jupyter_string)\n",
      "plt.scatter(knnX.x1,knnX.x2,marker=jupyter_string,s=200,color=jupyter_string,facecolors=jupyter_string,linewidth=3.0)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "data = pd.read_csv(jupyter_string,header=None)\n",
      "X = data.iloc[:,0:2].values\n",
      "y = data.iloc[:,2].values\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X,y)\n",
      "=====\n",
      "from sklearn import neighbors,datasets\n",
      "data = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string])\n",
      "display( data.head() )\n",
      "sns.lmplot(data=data,x=jupyter_string,y=jupyter_string,hue=jupyter_string,fit_reg=False)\n",
      "--------------------\n",
      "train['Fare' <unk>].plot.hist(bins = 50)\n",
      "=====\n",
      "train.info()\n",
      "--------------------\n",
      "similar_users = find_similar_users(ratings,jupyter_string,3)\n",
      "similar_users.head()\n",
      "=====\n",
      "user=jupyter_string\n",
      "scores = find_similar_users(ratingsdf,user,3)\n",
      "print(jupyter_string, user)\n",
      "scores.head()\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "n_neighbors = 10\n",
      "\n",
      "\n",
      "h = 0.01\n",
      "\n",
      "\n",
      "train,test = train_test_split(data,test_size=0.1, random_state=42)\n",
      "Xtrain = train.drop([jupyter_string],axis=1); ytrain=train.y\n",
      "Xtest  = test.drop([jupyter_string],axis=1); ytest = test.y\n",
      "clf = neighbors.KNeighborsClassifier(n_neighbors,weights=jupyter_string)\n",
      "clf.fit(Xtrain,ytrain)\n",
      "\n",
      "ypred_test = clf.predict(Xtest)\n",
      "ypred_train= clf.predict(Xtrain)\n",
      "\n",
      "x1max,x2max = Xtrain.max()\n",
      "x1min,x2min = Xtrain.min()\n",
      "x1grid,x2grid = np.meshgrid(np.arange(x1min,x1max,h),np.arange(x2min,x2max,h))\n",
      "output = clf.predict(np.c_[x1grid.ravel(),x2grid.ravel()])\n",
      "output = output.reshape(x1grid.shape)\n",
      "--------------------\n",
      "men = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]['Survived' <unk>]\n",
      "women = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]['Survived' <unk>]\n",
      "=====\n",
      "import scipy.stats as st\n",
      "\n",
      "st.norm.ppf(.05)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.info()\n",
      "=====\n",
      "pd_data = pd.read_csv(jupyter_string, dtype={'Survived' <<unk>>: bool, 'Pclass' <<unk>>: int, 'Age' <<unk>>: float, 'SibSp' <<unk>>: int, 'Parch' <<unk>>: int, 'Fare' <<unk>>: float}, na_values=[jupyter_string])\n",
      "\n",
      "\n",
      "pd_data.describe()\n",
      "--------------------\n",
      "pd_data.info()\n",
      "=====\n",
      "pd_data.count()\n",
      "--------------------\n",
      "titanic_df.groupby('Sex' madeupword0002)['Survived' <unk>].mean()\n",
      "=====\n",
      "st.norm.sf(abs(z))\n",
      "--------------------\n",
      "sns.pairplot(pd_data, hue='Survived' <unk>)\n",
      "=====\n",
      "newDF = pd_data[['Age' <<unk>>,'SibSp' <<unk>>,'Parch' <<unk>>,'Fare' <<unk>>,'Survived' <<unk>>]].dropna().copy()\n",
      "\n",
      "sns.pairplot(newDF, vars=['Age' <<unk>>,'SibSp' <<unk>>,'Parch' <<unk>>,'Fare' <<unk>>], hue='Survived' <<unk>>, palette=jupyter_string)\n",
      "--------------------\n",
      "newDF = pd_data[['Age' <unk>,'SibSp' <unk>,'Parch' <unk>,'Fare' <unk>]].dropna().copy()\n",
      "\n",
      "sns.pairplot(newDF, vars=['Age' <unk>,'SibSp' <unk>,'Parch' <unk>,'Fare' <unk>], hue='Survived' <unk>, palette=jupyter_string)\n",
      "=====\n",
      "pd_data.sample(n=5)\n",
      "--------------------\n",
      "sns.violinplot(x='Survived' <unk>, y='Age' <unk>, hue='Sex' madeupword0002, data=titanic_df, split=True)\n",
      "=====\n",
      "sns.set_style(jupyter_string)\n",
      "sns.plt.title(jupyter_string)\n",
      "ax = sns.violinplot(x=pd_data[\"Survived\"], y=pd_data[\"Age\"])\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn import svm\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "=====\n",
      "import numpy as np\n",
      "from sklearn import preprocessing, svm\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "df.drop(['Id' <<unk>>], 1, inplace=True)\n",
      "print(df.head())\n",
      "--------------------\n",
      "sns.countplot(x = 'Sex' madeupword0002, hue = 'Survived' <unk>, data = train)\n",
      "=====\n",
      "sns.countplot(x = \"SibSp\", data = train)\n",
      "--------------------\n",
      "plt.scatter(df['SepalLengthCm' <unk>], df['SepalWidthCm' <unk>])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(df['SepalLengthCm' <<unk>>], jupyter_string, df['SepalWidthCm' <<unk>>], jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(df[['SepalLengthCm' <unk>, 'SepalWidthCm' <unk>]], df['Species' <unk>], test_size=0.33, random_state=42)\n",
      "=====\n",
      "X = np.array(df.drop(['Species' <<unk>>], 1))\n",
      "y = np.array(df['Species' <<unk>>])\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
      "\n",
      "\n",
      "clf = svm.SVC(kernel=jupyter_string, decision_function_shape=jupyter_string)\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "accuracy = clf.score(X_test, y_test)\n",
      "print(accuracy)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "predict = np.array([2.2, 4.4, 1.3, 0.2])\n",
      "predict = predict.reshape(1, -1)\n",
      "prediction = clf.predict(predict)\n",
      "print(prediction)\n",
      "--------------------\n",
      "polykernel = svm.SVC(kernel=jupyter_string, degree=2)\n",
      "polykernel.fit(X_train, y_train)\n",
      "print(polykernel.score(X_test, y_test))\n",
      "=====\n",
      "polykernel = svm.SVC(kernel=jupyter_string, decision_function_shape=jupyter_string)\n",
      "polykernel.fit(X_train, y_train)\n",
      "\n",
      "polyaccuracy = clf.score(X_test, y_test)\n",
      "print(polyaccuracy)\n",
      "--------------------\n",
      "airports = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "airport = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "airport.sort_values('lat' <unk>, ascending=False).head(5)\n",
      "=====\n",
      "airport.sort_values('lat' <<unk>>, ascending = False).head(5)\n",
      "--------------------\n",
      "airports.sort_values('lon' <unk>, ascending = False).head(5)\n",
      "=====\n",
      "airport.sort_values('lon' <<unk>>, ascending = False).head(5)\n",
      "--------------------\n",
      "weather = pd.read_csv(jupyter_string)\n",
      "weather.head()\n",
      "=====\n",
      "weather = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "weather.sort_values(by='wind_speed' <unk>, ascending=False).head()\n",
      "=====\n",
      "weather[(weather['month' <<unk>>]==2) & (weather['day' <<unk>>]==12) & \n",
      "(weather['year' <<unk>>]==2013)].sort_values(by=['wind_speed' <<unk>>], ascending=False).head(5)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "lambdas=[0.001,0.01,0.1,1,10,100]\n",
      "R_2_OS_Ridge=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "plt.plot(lambdas,R_2_OS_Ridge)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "lambdas = np.linspace(-5,13,200)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "sns.countplot(x = \"Parch\", data = train)\n",
      "=====\n",
      "train[\"Fare\"].hist(bins =40)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "def ecdf(data):\n",
      "    x = np.sort(data)\n",
      "    y = 1.* np.arange(1,len(data)+1) / (len(data))\n",
      "    return x, y\n",
      "\n",
      "temp = np.array(df['temperature' <<unk>>])\n",
      "mu = np.mean(temp)\n",
      "sigma = np.std(temp)\n",
      "bs_samples = np.random.normal(mu,sigma,10000)\n",
      "\n",
      "x_theo, y_theo = ecdf(bs_samples)\n",
      "x ,y = ecdf(temp)\n",
      "\n",
      "\n",
      "_ = plt.plot(x_theo, y_theo)\n",
      "_ = plt.plot(x, y, marker=jupyter_string, linestyle=jupyter_string)\n",
      "plt.margins(0.02)\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "male_temp = df[df.gender == jupyter_string]['temperature' <unk>]\n",
      "female_temp = df[df.gender == jupyter_string]['temperature' <unk>]\n",
      "=====\n",
      "temp_male = np.array(df[df['gender' madeupword0002] == jupyter_string]['temperature' <<unk>>])\n",
      "temp_female = np.array(df[df['gender' madeupword0002] == jupyter_string]['temperature' <<unk>>])\n",
      "stats.ttest_ind(temp_male,temp_female, equal_var=False)\n",
      "--------------------\n",
      "dc_listings = pd.read_csv(jupyter_string)\n",
      "dc_listings.info()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "np.random.seed(1)\n",
      "\n",
      "dc_listings = pd.read_csv(jupyter_string)\n",
      "dc_listings = dc_listings.loc[np.random.permutation(len(dc_listings))]\n",
      "stripped_commas = dc_listings['price' <<unk>>].str.replace(jupyter_string, jupyter_string)\n",
      "stripped_dollars = stripped_commas.str.replace(jupyter_string, jupyter_string)\n",
      "dc_listings['price' <<unk>>] = stripped_dollars.astype(jupyter_string)\n",
      "--------------------\n",
      "dc_listings.info()\n",
      "=====\n",
      "dc_listings.head(1)\n",
      "--------------------\n",
      "dc_listings.info()\n",
      "=====\n",
      "dc_listings.info()\n",
      "--------------------\n",
      "dc_listings = dc_listings.drop(['room_type' <unk>, 'city' <unk>, 'state' <unk>, 'zipcode' <unk>, 'host_response_rate' <unk>, 'host_acceptance_rate' <unk>, 'host_listings_count' <unk>], axis=1)\n",
      "=====\n",
      "drop_columns = ['room_type' <<unk>>, 'city' <<unk>>, 'state' <<unk>>, 'latitude' <<unk>>, 'longitude' <<unk>>, 'zipcode' <<unk>>, 'host_response_rate' <<unk>>, 'host_acceptance_rate' <<unk>>, 'host_listings_count' <<unk>>]\n",
      "dc_listings = dc_listings.drop(drop_columns, axis=1)\n",
      "print(dc_listings.isnull().sum())\n",
      "--------------------\n",
      "drop_columns = ['cleaning_fee' <unk>, 'security_deposit' <unk>]\n",
      "dc_listings = dc_listings.drop(drop_columns, axis=1)\n",
      "print(dc_listings.isnull().sum())\n",
      "=====\n",
      "drop_cols = [\"cleaning_fee\", \"security_deposit\"]\n",
      "dc_listings = dc_listings.drop(drop_cols, axis=1)\n",
      "dc_listings.dropna(axis=0, inplace=True)\n",
      "dc_listings.isnull().sum()\n",
      "--------------------\n",
      "titanic_df['Age' <unk>] = titanic_df['Age' <unk>].fillna(titanic_df['Age' <unk>].mean())\n",
      "=====\n",
      "sns.boxplot(x = \"Pclass\", y = \"Age\", data = train)\n",
      "--------------------\n",
      "knn = KNeighborsRegressor(n_neighbors=5, algorithm=jupyter_string)\n",
      "\n",
      "features = ['accommodates' <unk>, 'bedrooms' <unk>, 'bathrooms' <unk>, 'number_of_reviews' <unk>]\n",
      "knn.fit(train_df[features], train_df.price)\n",
      "all_features_predictions = knn.predict(test_df[features])\n",
      "all_features_mse = mean_squared_error(all_features_predictions, test_df.price)\n",
      "all_features_rmse = all_features_mse ** (1/2)\n",
      "print(all_features_mse, all_features_rmse)\n",
      "=====\n",
      "features = list(train_df.columns)\n",
      "features.remove('price' <<unk>>)\n",
      "\n",
      "knn = KNeighborsRegressor(n_neighbors=5, algorithm=jupyter_string)\n",
      "knn.fit(train_df[features], train_df.price)\n",
      "all_features_predictions = knn.predict(test_df[features])\n",
      "all_features_mse = mean_squared_error(all_features_predictions, test_df.price)\n",
      "all_features_rmse = all_features_mse ** (1/2)\n",
      "print(all_features_mse, all_features_rmse)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head()\n",
      "--------------------\n",
      "titanic_data['Age' <unk>].fillna(titanic_data['Age' <unk>].mean(),inplace=True)\n",
      "=====\n",
      "mean_age = titanic_data['Age' <<unk>>].mean()\n",
      "titanic_data['Age' <<unk>>] = titanic_data['Age' <<unk>>].fillna(mean_age)\n",
      "--------------------\n",
      "titanic_data = titanic_data.drop(['Cabin' <unk>], axis=1)\n",
      "=====\n",
      "titanic_data = titanic_data.drop('Cabin' <<unk>>, axis=1)\n",
      "--------------------\n",
      "titanic_data = titanic_data.dropna()\n",
      "=====\n",
      "titanic_data = titanic_data.dropna()\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data = titanic_data.drop('PassengerId' <<unk>>, axis=1)\n",
      "--------------------\n",
      "titanic_data.head()\n",
      "=====\n",
      "titanic_data.head()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "sns.countplot(x='Pclass' <unk>, hue='Survived' <unk>, data=titanic_df)\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "sns.barplot(x='Pclass' <unk>, y='Survived' <unk>, hue='Survived' <unk>, data=titanic_df)\n",
      "plt.show()\n",
      "=====\n",
      "titanic_data['Sex' madeupword0002].value_counts()\n",
      "--------------------\n",
      "titanic_data['Pclass' <unk>].value_counts()\n",
      "=====\n",
      "bins = np.linspace(0, 80, 9)\n",
      "plt.hist(titanic_data['Age' <<unk>>],bins, alpha=0.5)\n",
      "plt.show()\n",
      "--------------------\n",
      "bins = np.linspace(0, 80, 9)\n",
      "plt.hist(titanic_data['Pclass' <unk>],bins, alpha=0.5)\n",
      "plt.show()\n",
      "=====\n",
      "titanic_data['Pclass' <<unk>>].value_counts()\n",
      "--------------------\n",
      "train['Age' <unk>] = train[['Age' <unk>, 'Pclass' <unk>]].apply(impute_age, axis = 1)\n",
      "=====\n",
      "train[[\"Age\", \"Pclass\"]].head(2)\n",
      "--------------------\n",
      "bins = np.linspace(0, 80, 9)\n",
      "plt.hist(titanic_data['Pclass' <unk>], bins, alpha=0.5)\n",
      "plt.show()\n",
      "=====\n",
      "data_by_class = titanic_data.groupby('Pclass' <<unk>>)\n",
      "--------------------\n",
      "titanic_data.groupby('Survived' <unk>)['Survived' <unk>].count()\n",
      "=====\n",
      "import seaborn as sns\n",
      "data_male = titanic_data.loc[lambda df: df.Sex == jupyter_string, :]\n",
      "data_female = titanic_data.loc[lambda df: df.Sex == jupyter_string, :]\n",
      "\n",
      "\n",
      "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
      "data_male['Survived' <<unk>>].value_counts().plot(kind= jupyter_string, \\\n",
      "                                          title=jupyter_string, ax=axes[0], \\\n",
      "                                          figsize=(8, 4),autopct=jupyter_string, \\\n",
      "                                          fontsize=10, \\\n",
      "                                          colors = [jupyter_string,jupyter_string],\\\n",
      "                                          legend=True)\n",
      "\n",
      "data_female['Survived' <<unk>>].value_counts().plot(kind= jupyter_string, \\\n",
      "                                          title=jupyter_string, ax=axes[1], \\\n",
      "                                          figsize=(8, 4),autopct=jupyter_string, \\\n",
      "                                          fontsize=10, \\\n",
      "                                          colors = [jupyter_string,jupyter_string],\\\n",
      "                                          legend=jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "''jupyter_string''\n",
      "--------------------\n",
      "from scipy import stats\n",
      "\n",
      "\n",
      "\n",
      "stats.ttest_ind(survived_female['SibSp' <unk>], non_survived_female['SibSp' <unk>])\n",
      "=====\n",
      "group_by_sib_female = data_female.groupby('SibSp' <<unk>>)\n",
      "\n",
      "group_by_sib_female['Survived' <<unk>>].mean().plot(kind = jupyter_string, color=jupyter_string, figsize=(4,4))\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "group_by_sib_male = data_male.groupby('SibSp' <unk>)\n",
      "\n",
      "group_by_sib_male['Survived' <unk>].mean().plot(kind = jupyter_string, color=jupyter_string, figsize=(4,4))\n",
      "plt.show()\n",
      "=====\n",
      "titanic_data.groupby(['Pclass' <<unk>>,'Sex' madeupword0002],as_index=False)[['Survived' <<unk>>]].mean()\n",
      "--------------------\n",
      "titanic_data.groupby(['Pclass' <unk>,'Sex' madeupword0002],as_index=False)[['Survived' <unk>]].mean().plot(kind = jupyter_string, color=jupyter_string, figsize=(4,4))\n",
      "plt.show()\n",
      "=====\n",
      "sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data_by_class, kind=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.factorplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data_by_class, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "def group_by_age(i):\n",
      "    return str(int(i/10)*10)+jupyter_string+str(int(i/10)*10+10)\n",
      "titanic_data[jupyter_string] = titanic_data['Age' <<unk>>].apply(group_by_age)\n",
      "--------------------\n",
      "sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=titanic_data, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data_by_age = titanic_data.groupby(['Pclass' <<unk>>,jupyter_string],as_index=False)[['Survived' <<unk>>]].mean()\n",
      "data_by_age\n",
      "--------------------\n",
      "sns.barplot(x='Pclass' <unk>,y='Survived' <unk>,hue='Sex' madeupword0002,data=data_by_age)\n",
      "=====\n",
      "sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=jupyter_string, data=data_by_age, kind=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.factorplot(x=\"Sex\", y=\"Survived\", hue=jupyter_string, data=data_by_age, kind=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "sib_count_survived_female = survived_female['SibSp' <<unk>>].value_counts()\n",
      "sib_count_no_survived_female = non_survived_female['SibSp' <<unk>>].value_counts()\n",
      "--------------------\n",
      "sib_count_survived_male = survived_male['SibSp' <unk>].value_counts()\n",
      "sib_count_no_survived_male = non_survived_male['SibSp' <unk>].value_counts()\n",
      "=====\n",
      "titanic_data['Survived' <<unk>>].value_counts()\n",
      "--------------------\n",
      "train.drop(\"Cabin\", axis = 1, inplace = True)\n",
      "=====\n",
      "train.drop('Cabin' <<unk>>, axis = 1, inplace = True)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "--------------------\n",
      "y_pred = nb_model.predict(X_test)\n",
      "=====\n",
      "y_pred = nb_model.predict(X_test)\n",
      "pred_summary = X_test.copy()\n",
      "pred_summary[y.name] = y_test\n",
      "pred_summary[jupyter_string] = y_pred\n",
      "pred_summary.head()\n",
      "--------------------\n",
      "X = data[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "y = data[jupyter_string]\n",
      "=====\n",
      "X = data[['Gender' <<unk>>, 'Age' <<unk>>, 'EstimatedSalary' <<unk>>]]\n",
      "y = data[\"Purchased\"]\n",
      "--------------------\n",
      "X = pd.get_dummies(X, columns=[\"Gender\"])\n",
      "=====\n",
      "X_dummies = pd.get_dummies(X[\"Gender\"], drop_first = True)\n",
      "X = pd.concat([X, X_dummies], axis = 1)\n",
      "X = X.drop([\"Gender\"], axis = 1)\n",
      "--------------------\n",
      "plt.hist(leaf_data.average-contrast)\n",
      "plt.show()\n",
      "=====\n",
      "leaf_data.average_contrast.plot(kind = jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "adult_data = pd.read_csv(jupyter_string, skipinitialspace=True)\n",
      "\n",
      "\n",
      "adult_data.columns = [c.replace(jupyter_string, jupyter_string) for c in adult_data.columns]\n",
      "\n",
      "\n",
      "\n",
      "adult_data.head()\n",
      "--------------------\n",
      "adult_data.groupby('marital-status' <unk>).mean()\n",
      "=====\n",
      "adult_data.groupby(jupyter_string)\n",
      "\n",
      "\n",
      "display(adult_data.groupby(jupyter_string).hours_per_week.mean())\n",
      "display(adult_data.groupby(jupyter_string).hours_per_week.median())\n",
      "display(adult_data.groupby(jupyter_string).hours_per_week.std())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.hist(adult_data.education)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "adult_data['education' <<unk>>].value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "df_data[jupyter_string] = df_data['native-country' <unk>].map(continent_dict)\n",
      "=====\n",
      "adult_data_copy = adult_data.copy()\n",
      "\n",
      "adult_data_copy[jupyter_string] = adult_data_copy[jupyter_string].map(continent_dict)\n",
      "adult_data_copy.head()\n",
      "--------------------\n",
      "adult_data_copy = adult_data.copy()\n",
      "\n",
      "adult_data_copy[jupyter_string] = adult_data_copy[jupyter_string].map(continent_dict)\n",
      "adult_data_copy.head()\n",
      "=====\n",
      "adult_data_copy.groupby(jupyter_string).age.mean().plot(kind = jupyter_string, color = jupyter_string, position = 1, width = 0.2, figsize = (7,7))\n",
      "adult_data_copy.groupby(jupyter_string).age.std().plot(kind = jupyter_string, color = jupyter_string, position = 0, width = 0.2)\n",
      "plt.legend([jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "leaf_data = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "leaf_data.columns = [c.replace(jupyter_string, jupyter_string) for c in leaf_data.columns]\n",
      "\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "leaf_data.eccentricity.fillna(leaf_data.eccentricity.median(), inplace=True)\n",
      "leaf_data.head()\n",
      "=====\n",
      "replace_val = leaf_data.eccentricity.dropna().mean()\n",
      "leaf_data['eccentricity' <<unk>>] = leaf_data.eccentricity.fillna(replace_val)\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "leaf_data[jupyter_string] = (leaf_data.eccentricity - leaf_data.eccentricity.mean()) / leaf_data.eccentricity.std()\n",
      "leaf_data.head()\n",
      "=====\n",
      "me = leaf_data.eccentricity.mean()\n",
      "sd = leaf_data.eccentricity.std()\n",
      "leaf_data['eccentricity' <<unk>>] = leaf_data['eccentricity' <<unk>>].apply(lambda x: (x - me) / (sd))\n",
      "leaf_data.head()\n",
      "--------------------\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity)\n",
      "plt.xlabel('smoothness' <<unk>>)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity, c=leaf_data.class)\n",
      "plt.xlabel('smoothness' <unk>)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "colors = {1: jupyter_string, 2: jupyter_string, 3: jupyter_string, 4: jupyter_string}\n",
      "plt.scatter(leaf_data.smoothness, leaf_data.eccentricity, c = leaf_data['class' <<unk>>].map(colors))\n",
      "plt.xlabel('smoothness' <<unk>>)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "''jupyter_string''\n",
      "--------------------\n",
      "leaf_data[['eccentricity' <unk>, 'smoothness' <unk>]].corr()\n",
      "=====\n",
      "leaf_data.smoothness.corr(leaf_data.eccentricity)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_mur=pd.read_csv(jupyter_string) \n",
      "df_ms=pd.read_csv(jupyter_string)  \n",
      "df_pop=pd.read_csv(jupyter_string, skiprows=(0,2)) \n",
      "                                                                        \n",
      "df_gdp=pd.read_csv(jupyter_string, skiprows=(0,2))  \n",
      "--------------------\n",
      "df_mur.head()\n",
      "=====\n",
      "df_mur=df_mur[[\"Country/Territory\",\"Number of homicides by firearm\",\"Average total all civilian firearms\"]]\n",
      "df_ms\n",
      "df_pop=df_pop[[jupyter_string,jupyter_string]]\n",
      "df_gdp=df_gdp[[jupyter_string,jupyter_string]]\n",
      "\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(df[jupyter_string], df[jupyter_string], \n",
      "            yerr = np.sqrt(df[jupyter_string]), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.GDP / 1e9, df['Number of mass shootings' <<unk>>])\n",
      "ax.errorbar(df.GDP / 1e9, df['Number of mass shootings' <<unk>>], \n",
      "            yerr = np.sqrt(df['Number of mass shootings' <<unk>>] * 1.0), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "\n",
      "--------------------\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.gdp / 1e9, df['Number of mass shootings' <unk>])\n",
      "ax.errorbar(df.gdp / 1e9, df['Number of mass shootings' <unk>], \n",
      "            yerr = np.sqrt(df['Number of mass shootings' <unk>] * 1.0), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.Population / 1e6, df['Number of mass shootings' <<unk>>],color=jupyter_string)\n",
      "ax.errorbar(df.Population / 1e6, df['Number of mass shootings' <<unk>>], xerr= np.sqrt(df.Population/ 1e6) ,\n",
      "            yerr = np.sqrt(df['Number of mass shootings' <<unk>>] * 1.0), fmt = jupyter_string,color=jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "\n",
      "--------------------\n",
      "train.dropna(inplace = True)\n",
      "=====\n",
      "train.dropna(inplace = True)\n",
      "--------------------\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.GDP / 1e9, df['Number of civilian firearms' <unk>])\n",
      "ax.errorbar(df.GDP / 1e9, df['Number of civilian firearms' <unk>], \n",
      "            yerr = np.sqrt(df['Number of civilian firearms' <unk>] * 1.0), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.Population / 1e6, df['Average total all civilian firearms' <<unk>>]/1e6,color=jupyter_string)\n",
      "\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "--------------------\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.Population / 1e6, df['Average total all civilian firearms' <unk>]/1e6,color=jupyter_string)\n",
      "\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.Population / 1e6, df['Average total all civilian firearms' <<unk>>]/1e6,color=jupyter_string)\n",
      "ax.errorbar(df.Population / 1e6,  df['Average total all civilian firearms' <<unk>>]/1e6, xerr= np.sqrt(df.Population/ 1e6) ,\n",
      "            yerr = np.sqrt( df['Average total all civilian firearms' <<unk>>]/1e6 * 1.0), fmt = jupyter_string,color=jupyter_string, alpha=0.3)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=18)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "\n",
      "--------------------\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "scatter = ax.scatter(df.Population / 1e6, df['Average total all civilian firearms' <unk>]/1e6,color=jupyter_string)\n",
      "\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=18)\n",
      "pl.show()\n",
      "print (jupyter_string + \n",
      "       jupyter_string + \n",
      "       jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "ax.hist(df[\"Average total all civilian firearms\"] / df[jupyter_string]);\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(df['Number of homicides by firearm' <unk>] / df[jupyter_string] * 1000)\n",
      "ax.set_xlabel(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(df['Number of homicides by firearm' madeupword0002] / df[jupyter_string] * 1000)\n",
      "ax.set_xlabel(jupyter_string);\n",
      "\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(df['Number of homicides by firearm' <unk>] / df[jupyter_string] * 1000)\n",
      "ax.set_xlabel(jupyter_string);\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(df['Number of homicides by firearm' madeupword0002] / df[jupyter_string] )\n",
      "ax.set_xlabel(jupyter_string);\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(df['Number of homicides by firearm' <unk>] / df[jupyter_string] )\n",
      "ax.set_xlabel(jupyter_string);\n",
      "=====\n",
      "fig = pl.figure(figsize=(10,5)) \n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "\n",
      "scatter = ax.scatter(df.GDP / 1e9, df['Number of homicides by firearm' madeupword0002])\n",
      "ax.errorbar(df.GDP / 1e9, df['Number of homicides by firearm' madeupword0002], \n",
      "            yerr = np.sqrt(df['Number of homicides by firearm' madeupword0002] * 1.0), fmt = jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel('Number of homicides by firearm' madeupword0002)\n",
      "ax.set_ylim()\n",
      "ax.set_title(jupyter_string,fontsize=20)\n",
      "pl.show()\n",
      "\n",
      "--------------------\n",
      "import statsmodels.formula.api as st\n",
      "import statsmodels as sm\n",
      "import seaborn as sns\n",
      "import statsmodels.graphics.regressionplots as plots\n",
      "plots.style.use(jupyter_string)\n",
      "=====\n",
      "df[\"Average total all civilian firearms\"].mean()\n",
      "--------------------\n",
      "model2.summary()\n",
      "=====\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "=====\n",
      "W1 = model.params[1]\n",
      "W2 = model.params[0] \n",
      "\n",
      "wmodel2 = model2.params[0]\n",
      "\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], W1*df[jupyter_string] + W2, c=jupyter_string,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], wmodel2*df[jupyter_string], c=jupyter_string,label=jupyter_string)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "plt.scatter(jupyter_string, jupyter_string, data = df,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], W1*df[jupyter_string] + W2, c=jupyter_string,label=jupyter_string)\n",
      "plt.plot(df[jupyter_string], wmodel2*df[jupyter_string], c=jupyter_string,label=jupyter_string)\n",
      "\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "=====\n",
      "sns.regplot(x=df.firearmspp, y=df.masshootingpp, data=df)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string, weight=jupyter_string, fontsize=14);\n",
      "--------------------\n",
      "sex = pd.get_dummies(titanic['Sex' madeupword0002],drop_first=True)\n",
      "embark = pd.get_dummies(titanic['Embarked' <unk>],drop_first=True)\n",
      "titanic = pd.concat([titanic,sex,embark],axis=1)\n",
      "=====\n",
      "sex = pd.get_dummies(train['Sex' madeupword0002], drop_first = True)\n",
      "--------------------\n",
      "import statsmodels.graphics.influence as influence\n",
      "influence.plot_influence(model)\n",
      "plt.show()\n",
      "=====\n",
      "statsmodels.graphics.regressionplots.influence_plot(model, alpha  = 0.05, criterion=jupyter_string);\n",
      "--------------------\n",
      "plt.scatter(X[:,0], X[:,1], c=y)\n",
      "plt.show()\n",
      "=====\n",
      "plot_points(X, y)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plot_line(-2.86, 2.0, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, header=None) \n",
      "X = np.array(data[[0, 1]])\n",
      "y = np.array(data[2])\n",
      "plot_points(X, y)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train_set = pd.read_csv(jupyter_string, header = None)\n",
      "test_set = pd.read_csv(jupyter_string , skiprows = 1, header = None)\n",
      "--------------------\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.head()\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.head()\n",
      "--------------------\n",
      "df_set = df_set.replace(jupyter_string, np.nan)\n",
      "df_set.head()\n",
      "=====\n",
      "train_set=train_set.replace(regex=jupyter_string,value=pd.np.nan).dropna(how=jupyter_string)\n",
      "train_set.head()\n",
      "--------------------\n",
      "hours_per_week=train_set['hours_per_week' <unk>]\n",
      "hours_per_week.head()\n",
      "=====\n",
      "sns.kdeplot(train_set.hours_per_week,shade=True)\n",
      "train_set.hours_per_week.describe()\n",
      "--------------------\n",
      "train_set[jupyter_string] = train_set.hours_per_week.apply(lambda x: 1 if x>=40 else 2 if x>=45 else 3 if x>=60 else 4 if x>=80 else 5)\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.hours_per_week=train_set.hours_per_week.astype(int)\n",
      "train_set.loc[train_set.hours_per_week < 40,jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week >= 40) & (train_set.hours_per_week <= 45),jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week > 45) & (train_set.hours_per_week <= 60),jupyter_string] = jupyter_string\n",
      "train_set.loc[(train_set.hours_per_week > 60) & (train_set.hours_per_week <= 80),jupyter_string] = jupyter_string\n",
      "train_set.loc[train_set.hours_per_week > 80,jupyter_string] = jupyter_string\n",
      "--------------------\n",
      "train_set.hours_per_week.value_counts()\n",
      "=====\n",
      "train_set[jupyter_string].head()\n",
      "--------------------\n",
      "embark = pd.get_dummies(train['Embarked' <unk>], drop_first = True)\n",
      "=====\n",
      "embark = pd.get_dummies(train['Embarked' <<unk>>], drop_first = True)\n",
      "--------------------\n",
      "train_set.native_country.unique()\n",
      "=====\n",
      "train_set.native_country.unique()\n",
      "\n",
      "--------------------\n",
      "train_set.native_county.unique()\n",
      "=====\n",
      "Asia_East = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string])\n",
      "Asia_Central = np.array([jupyter_string, jupyter_string])\n",
      "Central_America =  np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,  jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "South_America = np.array([jupyter_string, jupyter_string, jupyter_string,jupyter_string])\n",
      "Europe_West = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "Europe_East = np.array([jupyter_string, jupyter_string, jupyter_string])\n",
      "North_America = np.array([jupyter_string, jupyter_string,jupyter_string])\n",
      "--------------------\n",
      "train_set.native_region.unique()\n",
      "=====\n",
      "train_set.native_region.unique()\n",
      "\n",
      "--------------------\n",
      "train_set.capital_gain.unique()\n",
      "=====\n",
      "sns.kdeplot(train_set.capital_gain,shade=True)\n",
      "train_set.capital_gain.describe()\n",
      "--------------------\n",
      "train_set.capital_loss.describe()\n",
      "=====\n",
      "sns.kdeplot(train_set.capital_loss,shade=True)\n",
      "train_set.capital_loss.describe()\n",
      "--------------------\n",
      "train_set.capital_gain.fillna(train_set.capital_gain.mean(),inplace=True)\n",
      "train_set.capital_loss.fillna(train_set.capital_loss.mean(),inplace=True)\n",
      "=====\n",
      "capital_gain_mean = train_set.capital_gain.mean()\n",
      "capital_loss_mean = train_set.capital_loss.mean()\n",
      "\n",
      "--------------------\n",
      "train_set.head()\n",
      "=====\n",
      "train_set.capital_gain.describe()\n",
      "--------------------\n",
      "train_set.workclass.unique()\n",
      "=====\n",
      "train_set.workclass.describe()\n",
      "--------------------\n",
      "train_set.workclass = train_set.workclass.fillna(jupyter_string)\n",
      "=====\n",
      "train_set.workclass.unique()\n",
      "--------------------\n",
      "train_set.education.unique()\n",
      "=====\n",
      "workclass = pd.factorize(train_set.workclass)\n",
      "education = pd.factorize(train_set.education)\n",
      "hours_per_week_group = pd.factorize(train_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(train_set.marital_status)\n",
      "occupation = pd.factorize(train_set.occupation)\n",
      "relationship = pd.factorize(train_set.relationship)\n",
      "race = pd.factorize(train_set.race)\n",
      "sex = pd.factorize(train_set.sex)\n",
      "native_region = pd.factorize(train_set.native_region) \n",
      "wage_class = pd.factorize(train_set.wage_class)\n",
      "\n",
      "--------------------\n",
      "train = pd.concat([train,sex,embark], axis = 1)\n",
      "=====\n",
      "train = pd.concat([train, sex, embark], axis = 1)\n",
      "--------------------\n",
      "train_set = pd.concat([train_set,workclass,education,hours_per_week_group,marital_status,occupation,relationship,race,sex,native_region,wage_class],axis=1)\n",
      "=====\n",
      "X_train = pd.DataFrame({jupyter_string:train_set.age,jupyter_string:workclass[0],jupyter_string:train_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:train_set.capital_gain,jupyter_string:train_set.capital_loss,jupyter_string:hours_per_week_group[0],jupyter_string:native_region[0]})\n",
      "Y_train = wage_class[0]\n",
      "X_train.head()\n",
      "--------------------\n",
      "X_test = pd.DataFrame({jupyter_string:test_set.age,jupyter_string:test_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:native_region[0]})\n",
      "Y_test = wage_class[0]\n",
      "X_test.head()\n",
      "=====\n",
      "test_set=test_set.replace(regex=jupyter_string,value=pd.np.nan).dropna(how=jupyter_string) \n",
      "test_set.head()\n",
      "--------------------\n",
      "test_set['hours_per_week' <unk>] = test_set['hours_per_week' <unk>].astype(jupyter_string)\n",
      "=====\n",
      "test_set.hours_per_week=test_set.hours_per_week.astype(int)\n",
      "test_set.loc[test_set.hours_per_week < 40,jupyter_string] = jupyter_string\n",
      "test_set.loc[(test_set.hours_per_week >= 40) & (test_set.hours_per_week <= 45),jupyter_string] = jupyter_string\n",
      "test_set.loc[(test_set.hours_per_week > 45) & (test_set.hours_per_week <= 60),jupyter_string] = jupyter_string\n",
      "test_set.loc[(test_set.hours_per_week > 60) & (test_set.hours_per_week <= 80),jupyter_string] = jupyter_string\n",
      "test_set.loc[test_set.hours_per_week > 80,jupyter_string] = jupyter_string\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.native_region.unique()\n",
      "--------------------\n",
      "test_set.capital_gain.fillna(0,inplace=True)\n",
      "test_set.capital_loss.fillna(0,inplace=True)\n",
      "=====\n",
      "capital_gain_mean = test_set.capital_gain.mean()\n",
      "capital_loss_mean = test_set.capital_loss.mean()\n",
      "--------------------\n",
      "test_set.head()\n",
      "=====\n",
      "test_set.head()\n",
      "--------------------\n",
      "test_set.wage_class.unique()\n",
      "=====\n",
      "test_set.wage_class.head()\n",
      "--------------------\n",
      "test_set.wage_class = test_set.wage_class.str.replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "test_set.loc[test_set.wage_class == jupyter_string ,jupyter_string] = jupyter_string\n",
      "test_set.loc[test_set.wage_class == jupyter_string ,jupyter_string] = jupyter_string\n",
      "test_set.wage_class.head()\n",
      "--------------------\n",
      "train_set = pd.get_dummies(train_set)\n",
      "test_set = pd.get_dummies(test_set)\n",
      "=====\n",
      "workclass = pd.factorize(test_set.workclass)\n",
      "education = pd.factorize(test_set.education)\n",
      "hours_per_week_group = pd.factorize(test_set.hours_per_week_group) \n",
      "marital_status = pd.factorize(test_set.marital_status)\n",
      "occupation = pd.factorize(test_set.occupation)\n",
      "relationship = pd.factorize(test_set.relationship)\n",
      "race = pd.factorize(test_set.race)\n",
      "sex = pd.factorize(test_set.sex)\n",
      "native_region = pd.factorize(test_set.native_region) \n",
      "wage_class = pd.factorize(test_set.wage_class)\n",
      "--------------------\n",
      "test_set = pd.concat([test_set,workclass,education,hours_per_week_group,marital_status,occupation,relationship,race,sex,native_region,wage_class],axis=1)\n",
      "test_set.head()\n",
      "=====\n",
      "X_test = pd.DataFrame({jupyter_string:test_set.age,jupyter_string:workclass[0],jupyter_string:test_set.fnlwgt,jupyter_string:education[0],jupyter_string:hours_per_week_group[0],jupyter_string:marital_status[0],jupyter_string:occupation[0],jupyter_string:relationship[0],jupyter_string:race[0],jupyter_string:sex[0],jupyter_string:test_set.capital_gain,jupyter_string:test_set.capital_loss,jupyter_string:hours_per_week_group[0],jupyter_string:native_region[0]})\n",
      "Y_test = wage_class[0]\n",
      "X_test.head()\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.head(1)\n",
      "--------------------\n",
      "param_test1 = {\n",
      "    jupyter_string:range(3,10,2),\n",
      "    jupyter_string:range(2,10,2)\n",
      "}\n",
      "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
      " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
      " objective=jupyter_string, nthread=4, scale_pos_weight=1, seed=27), \n",
      " param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train,y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:range(3,10,2),\n",
      " jupyter_string:range(1,6,2)\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:range(3,10,2),\n",
      " jupyter_string:range(1,6,2)\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[4,5,6],\n",
      " jupyter_string:[1,2]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(max_depth=5, min_child_weight=2)\n",
      "model.fit(X_train, Y_train)\n",
      "Y_pred = model.predict(X_test)\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.8,\n",
      " colsample_bytree=0.8,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.8,\n",
      " colsample_bytree=0.8,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cv\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[i/10.0 for i in range(0,5)]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[i/10.0 for i in range(3,10)],\n",
      " jupyter_string:[i/10.0 for i in range(3,10)]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[i/10.0 for i in range(6,10)],\n",
      " jupyter_string:[i/10.0 for i in range(6,10)]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[i/10.0 for i in range(6,10)],\n",
      " jupyter_string:[i/10.0 for i in range(6,10)]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[i/100 for i in range(55,70,5)],\n",
      " jupyter_string:[i/100 for i in range(85,100,5)]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(colsample_bytree=0.85, subsample=0.65)\n",
      "model.fit(X_train, Y_train)\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.85,\n",
      " colsample_bytree=0.85,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "model = xgb.XGBClassifier(\n",
      " learning_rate =0.1,\n",
      " n_estimators=1000,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.85,\n",
      " colsample_bytree=0.85,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=\n",
      "=====\n",
      "param_test1 = {\n",
      " jupyter_string:[1e-5, 1e-2, 0.1, 1, 100]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test2 = {\n",
      " jupyter_string:[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
      "}\n",
      "\n",
      "gsearch2 = GridSearchCV(estimator = model, param_grid = param_test2, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch2.fit(X_train, Y_train)\n",
      "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
      "=====\n",
      "param_test1 = {\n",
      "jupyter_string:[0, 0.001, 0.005, 0.01, 0.05]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "--------------------\n",
      "param_test1 = {\n",
      " jupyter_string:[1e-5, 1e-2, 0.1, 1, 100]\n",
      "}\n",
      "\n",
      "gsearch1 = GridSearchCV(estimator = model, param_grid = param_test1, scoring=jupyter_string,n_jobs=4,iid=False, cv=5)\n",
      "gsearch1.fit(X_train, Y_train)\n",
      "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
      "=====\n",
      "model = xgb.XGBClassifier(learning_rate =0.01,\n",
      " n_estimators=5000,\n",
      " reg_alpha= 0.001,\n",
      " max_depth=5,\n",
      " min_child_weight=2,\n",
      " gamma=0,\n",
      " subsample=0.85,\n",
      " colsample_bytree=0.85,\n",
      " objective= jupyter_string,\n",
      " nthread=4,\n",
      " scale_pos_weight=1,\n",
      " seed=27)\n",
      "\n",
      "xgb_param = model.get_xgb_params()\n",
      "dtrain = xgb.DMatrix(data=X_train,label=Y_train)\n",
      "\n",
      "\n",
      "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=model.get_params()[jupyter_string], nfold=5,\n",
      "            metrics=jupyter_string, early_stopping_rounds=50)\n",
      "\n",
      "\n",
      "model.set_params(n_estimators=cvresult.shape[0])\n",
      "\n",
      "\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "\n",
      "Y_pred=model.predict(X_test)\n",
      "pred_prob = model.predict_proba(X_test)[:,1]\n",
      "\n",
      "print (jupyter_string % accuracy_score(Y_test, Y_pred))\n",
      "print (jupyter_string % roc_auc_score(Y_test, pred_prob))\n",
      "\n",
      "\n",
      "feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
      "feat_imp.plot(kind=jupyter_string, title=jupyter_string,figsize=(10,10),fontsize=14)\n",
      "pt.ylabel(jupyter_string)\n",
      "pt.show()\n",
      "--------------------\n",
      "lambdas = np.linspace(-5,13,200)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(2,X_train,y_train,lambdas,p=0.4,Graph=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "\n",
      "Ridge.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "--------------------\n",
      "train.drop([jupyter_string, jupyter_string, jupyter_string], axis = 1, inplace = True)\n",
      "train.head(1)\n",
      "=====\n",
      "train.drop(['Sex' madeupword0002, 'Embarked' <<unk>>, 'Name' <<unk>>, 'Ticket' <<unk>>], axis = 1, inplace = True)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string]\n",
      "data.head()\n",
      "=====\n",
      "import numpy\n",
      "\n",
      "def calc_entropy(column):\n",
      "    \"\"jupyter_string\"\"\n",
      "    \n",
      "    counts = numpy.bincount(column)\n",
      "    \n",
      "    \n",
      "    probabilities = counts / float(len(column))\n",
      "    \n",
      "    \n",
      "    entropy = 0\n",
      "    \n",
      "    \n",
      "    for prob in probabilities:\n",
      "        if prob > 0:\n",
      "            entropy += prob * math.log(prob, 2)\n",
      "    \n",
      "    return -entropy\n",
      "\n",
      "\n",
      "entropy = calc_entropy([1,1,0,0,1])\n",
      "print(jupyter_string, entropy)\n",
      "\n",
      "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
      "print(jupyter_string, information_gain)\n",
      "\n",
      "income_entropy = calc_entropy(income[\"high_income\"])\n",
      "\n",
      "median_age = income[\"age\"].median()\n",
      "\n",
      "left_split = income[income[\"age\"] <= median_age]\n",
      "right_split = income[income[\"age\"] > median_age]\n",
      "\n",
      "age_information_gain = income_entropy - ((float(left_split.shape[0]) / income.shape[0]) * calc_entropy(left_split[\"high_income\"]) + ((float(right_split.shape[0]) / income.shape[0]) * calc_entropy(right_split[\"high_income\"])))\n",
      "print(jupyter_string, age_information_gain)\n",
      "--------------------\n",
      "private_incomes = data[data['workclass' <unk>] == 4]\n",
      "public_incomes = data[data['workclass' <unk>] != 4]\n",
      "=====\n",
      "private_incomes = income[income[\"workclass\"] == 4]\n",
      "public_incomes = income[income[\"workclass\"] != 4]\n",
      "\n",
      "print(jupyter_string, private_incomes.shape)\n",
      "print(jupyter_string, public_incomes.shape)\n",
      "--------------------\n",
      "highest_gain = information_gains.index(max(information_gains))\n",
      "highest_gain\n",
      "=====\n",
      "def calc_information_gain(data, split_name, target_name):\n",
      "    \"\"jupyter_string\"\"\n",
      "    \n",
      "    original_entropy = calc_entropy(data[target_name])\n",
      "    \n",
      "    \n",
      "    column = data[split_name]\n",
      "    median = column.median()\n",
      "    \n",
      "    \n",
      "    left_split = data[column <= median]\n",
      "    right_split = data[column > median]\n",
      "    \n",
      "    \n",
      "    to_subtract = 0\n",
      "    for subset in [left_split, right_split]:\n",
      "        prob = float(subset.shape[0]) / data.shape[0]\n",
      "        to_subtract += prob * calc_entropy(subset[target_name])\n",
      "    \n",
      "    \n",
      "    return original_entropy - to_subtract\n",
      "\n",
      "\n",
      "age_information_gain = calc_information_gain(income, \"age\", \"high_income\")\n",
      "print(jupyter_string, age_information_gain)\n",
      "\n",
      "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
      "information_gains = []\n",
      "\n",
      "\n",
      "for col in columns:\n",
      "    information_gain = calc_information_gain(income, col, \"high_income\")\n",
      "    information_gains.append(information_gain)\n",
      "\n",
      "\n",
      "highest_gain_index = information_gains.index(max(information_gains))\n",
      "highest_gain = columns[highest_gain_index]\n",
      "\n",
      "print(jupyter_string, highest_gain_index)\n",
      "print(jupyter_string, highest_gain)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas\n",
      "\n",
      "\n",
      "income = pandas.read_csv(jupyter_string, index_col=False)\n",
      "income.head(5)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import scipy.stats as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "std = df[df['gender' <unk>]==jupyter_string]['temperature' <unk>].std()\n",
      "std\n",
      "=====\n",
      "stddev = math.sqrt((df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].std() * df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].std())/len(df[df['gender' madeupword0002]==jupyter_string]) + (df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].std() * df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].std())/len(df[df['gender' madeupword0002]==jupyter_string])); stddev\n",
      "--------------------\n",
      "plt.hist(df['temperature' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(df['temperature' <<unk>>], normed=True, bins=15)\n",
      "plt.show()\n",
      "--------------------\n",
      "sample_mean = df.temperature.mean()\n",
      "sample_mean\n",
      "=====\n",
      "df['temperature' <<unk>>].mean()\n",
      "--------------------\n",
      "temp_mean = df['temperature' <unk>].mean()\n",
      "temp_std = df['temperature' <unk>].std()\n",
      "temp_n = len(df['temperature' <unk>])\n",
      "temp_mean, temp_std, temp_n\n",
      "=====\n",
      "df['temperature' <<unk>>].std()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "=====\n",
      "X = train.drop('Survived' <<unk>>, axis = 1) \n",
      "y = train['Survived' <<unk>>]\n",
      "--------------------\n",
      "male_temp = df[df['gender' madeupword0002] == jupyter_string]['temperature' <unk>]\n",
      "female_temp = df[df['gender' madeupword0002] == jupyter_string]['temperature' <unk>]\n",
      "=====\n",
      "df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].mean()\n",
      "--------------------\n",
      "df[df['gender' <unk>]==jupyter_string]['temperature' <unk>].mean()\n",
      "=====\n",
      "df[df['gender' madeupword0002]==jupyter_string]['temperature' <<unk>>].mean()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import pprint\n",
      "\n",
      "filepath = jupyter_string\n",
      "df = pd.read_csv(filepath)\n",
      "pp = pprint.PrettyPrinter()\n",
      "\n",
      "print(jupyter_string)\n",
      "pp.pprint(df.head(1).T)\n",
      "\n",
      "--------------------\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], errors=jupyter_string)\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], errors=jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "categorical_vars = ['residence_area_type' <<unk>>,'sourcing_channel' <<unk>>]\n",
      "numerical_vars = list(set(list(df.columns)) - set(categorical_vars))\n",
      "\n",
      "sub_df = df[numerical_vars]\n",
      "sub_df['age_in_days' <<unk>>] = sub_df['age_in_days' <<unk>>] // 365\n",
      "--------------------\n",
      "sub_df[numerical_vars].hist(figsize=(20,20))\n",
      "plt.show()\n",
      "=====\n",
      "num_cols = 2 \n",
      "num_rows = len(numerical_vars) // 2 + 1\n",
      "\n",
      "fig = sub_df.hist(layout = (num_rows,num_cols), \n",
      "                    bins=30,\n",
      "                    figsize=(50,50),\n",
      "                   ylabelsize=50,\n",
      "                   xlabelsize=50)\n",
      "titles = [x.title.set_size(50) for x in fig.ravel()] \n",
      "\n",
      "--------------------\n",
      "corr = sub_df.corr()\n",
      "corr.style.background_gradient(cmap=plt.get_cmap(jupyter_string))\n",
      "=====\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def corr_matrix_generator(corr,drop_duplicates = True):\n",
      "    if drop_duplicates:    \n",
      "        mask = np.zeros_like(corr, dtype=np.bool)\n",
      "        mask[np.triu_indices_from(mask)] = True\n",
      "        \n",
      "    sns.set_style(style = jupyter_string)\n",
      "    f, ax = plt.subplots(figsize=(11, 9))\n",
      "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
      "    if drop_duplicates:\n",
      "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={jupyter_string: .5})\n",
      "    else:\n",
      "        sns.heatmap(corr, cmap=cmap, \n",
      "                square=True,\n",
      "                linewidth=.5, cbar_kws={jupyter_string: .5}, ax=ax)\n",
      "        \n",
      "        \n",
      "\n",
      "    \n",
      "corr = sub_df.corr()\n",
      "\n",
      "print(jupyter_string)\n",
      "print(sub_df['renewal' <<unk>>].value_counts())\n",
      "print(jupyter_string)\n",
      "corr_matrix_generator(corr)\n",
      "\n",
      "--------------------\n",
      "sub_df['renewal' <unk>].value_counts()\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import xgboost as xgb\n",
      "from sklearn import metrics,cross_validation\n",
      "from matplotlib import pyplot as plt\n",
      "import itertools\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "test_df = pd.read_csv(jupyter_string)\n",
      "train_df = pd.read_csv(jupyter_string)\n",
      "train_df.drop('renewal' <<unk>>,inplace=True,axis=1)\n",
      "assert(list(test_df.columns) ==  list(train_df.columns))\n",
      "\n",
      "train_df[jupyter_string] = 0\n",
      "test_df[jupyter_string] = 1\n",
      "\n",
      "combined_df = train_df.append(test_df)\n",
      "combined_df = combined_df.sample(frac=1).reset_index(drop=True) \n",
      "print(len(combined_df))\n",
      "print(combined_df.head(1).T)\n",
      "\n",
      "\n",
      "combined_df = dummy_encode(combined_df)\n",
      "combined_df = combined_df.drop('id' <<unk>>,axis=1)\n",
      "--------------------\n",
      "iris = sns.load_dataset(jupyter_string)\n",
      "iris.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, data=data)\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data)\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, data=data, fit_reg=False)\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data, fit_reg=False)\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, data=data, fit_reg=False, markers=jupyter_string)\n",
      "--------------------\n",
      "sns.heatmap(subset.corr())\n",
      "plt.show()\n",
      "=====\n",
      "cols = ['TotalConsmp' <<unk>>,'TempOutSide' <<unk>>,'Press_mm_hg' <<unk>>,'H_OutSide' <<unk>>,'Windspeed' <<unk>>,'Visibility' <<unk>>]\n",
      "cm = np.corrcoef(subset.values.T)\n",
      "sns.set(font_scale=1.5)\n",
      "plt.figure(figsize=(10,10))\n",
      "hm = sns.heatmap(cm,\n",
      "                 cbar=True,\n",
      "                 annot=True,\n",
      "                 square=True,\n",
      "                 fmt=jupyter_string,\n",
      "                 annot_kws= {jupyter_string: 10},\n",
      "                 yticklabels = cols,\n",
      "                 xticklabels=cols\n",
      "                )\n",
      "plt.show()\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data, fit_reg=False, markers=jupyter_string)\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, hue='species' madeupword0002, data=data, fit_reg=False)\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data, fit_reg=False)\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, col='species' madeupword0002, data=data, fit_reg=False)\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, col='species' <unk>, data=data, fit_reg=False, hue='species' <unk>)\n",
      "=====\n",
      "sns.jointplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, data=data)\n",
      "--------------------\n",
      "sns.jointplot(x='sepal_length' <unk>,y='sepal_width' <unk>, data=data, kind=jupyter_string)\n",
      "=====\n",
      "sns.pairplot(data, hue=\"species\")\n",
      "--------------------\n",
      "sns.pairplot(data, hue=\"species\", vars=[\"sepal_length\", \"sepal_width\"])\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, hue='species' madeupword0002, data=data, fit_reg=False, scatter_kws={jupyter_string:5})\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data, fit_reg=False, scatter_kws={jupyter_string:5})\n",
      "=====\n",
      "sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, hue='species' madeupword0002, data=data, fit_reg=False, \n",
      "             scatter_kws={jupyter_string:60*data['petal_width' <<unk>>]})\n",
      "--------------------\n",
      "sns.lmplot(x='sepal_length' <unk>,y='sepal_width' <unk>, hue='species' <unk>, data=data, fit_reg=False, \n",
      "             scatter_kws={jupyter_string:60*data['petal_width' <unk>]})\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "g = sns.lmplot(x='sepal_length' <<unk>>,y='sepal_width' <<unk>>, hue='species' madeupword0002, data=data, fit_reg=False, \n",
      "             scatter_kws={jupyter_string:60*data['petal_width' <<unk>>]})\n",
      "\n",
      "plt.annotate(jupyter_string, xy=(6, 3), xytext=(7, 4),arrowprops=dict(facecolor=jupyter_string, shrink=0.05))\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax = dta.groupby(\"education\").size().plot(kind=jupyter_string, figsize=(8, 8))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ylabel = ax.yaxis.get_label()\n",
      "ylabel.set_fontsize(24)\n",
      "\n",
      "\n",
      "labels = ax.yaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "\n",
      "\n",
      "labels = ax.xaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels]\n",
      "[label.set_rotation(-45) for label in labels];\n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(8, 8))\n",
      "\n",
      "\n",
      "sns.countplot(x=\"education\", hue=\"y\", data=dta, palette=jupyter_string)\n",
      "\n",
      "\n",
      "ylabel = ax.yaxis.get_label()\n",
      "ylabel.set_fontsize(24)\n",
      "\n",
      "\n",
      "labels = ax.yaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "\n",
      "\n",
      "labels = ax.xaxis.get_ticklabels()\n",
      "[label.set_fontsize(20) for label in labels];\n",
      "[label.set_rotation(-45) for label in labels];\n",
      "=====\n",
      "g = sns.factorplot(\"education_num\", \"hours_per_week\", hue=\"sex\", col=\"fifty_k\", data=dta)\n",
      "--------------------\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "pd.set_option(jupyter_string, 10)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "dta[dta.education_num <= 8].head()\n",
      "=====\n",
      "dta.groupby(\"work_class\").age.mean()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "dta = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "new_df_data = dta.iloc[:,:-1]\n",
      "new_df_labels = dta.iloc[:,-1]\n",
      "=====\n",
      "new_df_data = dta.iloc[:, 0:10]\n",
      "new_df_labels = dta.loc[:, 'fifty_k' <<unk>>]\n",
      "new_df_data.head()\n",
      "--------------------\n",
      "dta.head()\n",
      "=====\n",
      "dta.head()\n",
      "--------------------\n",
      "dta.info()\n",
      "=====\n",
      "dta.info()\n",
      "--------------------\n",
      "dta.describe()\n",
      "=====\n",
      "dta.describe()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string]\n",
      "df.head()\n",
      "=====\n",
      "new_df_transpose = new_df_data.transpose()\n",
      "\n",
      "data_into_dict = new_df_transpose.to_dict()\n",
      "census_data = [v for k, v in data_into_dict.items()]\n",
      "--------------------\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "vec = DictVectorizer()\n",
      "census_data_array = vec.fit_transform(census_data).toarray()\n",
      "=====\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "dv = DictVectorizer()\n",
      "transformed_data = dv.fit_transform(census_data).toarray()\n",
      "transformed_data\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(census_train, labels_train)\n",
      "=====\n",
      "knn = KNeighborsClassifier()\n",
      "knn.fit(census_train, labels_train)\n",
      "--------------------\n",
      "dta.groupby(\"work_class\").mean()\n",
      "=====\n",
      "dta.groupby(\"fifty_k\").education.describe()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig, ax = plt.subplots(5,5)\n",
      "for i, a in enumerate(ax.flatten()):\n",
      "    a.hist(trainlabels)\n",
      "plt.show()\n",
      "=====\n",
      "plt.hist(trainlabel)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(traindata)\n",
      "traindata = scaler.transform(traindata)\n",
      "testdata = scaler.transform(testdata)\n",
      "=====\n",
      "meanimage = traindata.mean(axis=0)\n",
      "--------------------\n",
      "traindata = traindata - meanimage\n",
      "testdata = testdata - meanimage\n",
      "=====\n",
      "plt.imshow(meanimage.reshape((28,28)))\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "meanimage = meanimage - meanimage.mean()\n",
      "plt.imshow(meanimage.reshape((28,28)))\n",
      "plt.show()\n",
      "=====\n",
      "traindata = np.subtract(traindata, meanimage)\n",
      "testdata = np.subtract(testdata, meanimage)\n",
      "--------------------\n",
      "ncomp = [30, 50, 100]\n",
      "reconstructed_data = []\n",
      "for i in ncomp:\n",
      "    reconstructed_data.append(reconstruct(X_train, X_train, i))\n",
      "reconstructed_data = np.array(reconstructed_data)\n",
      "=====\n",
      "train_reconst = np.add(traindata[:25,:], meanimage)\n",
      "plotdigits(train_reconst[20:], 1, 5)\n",
      "\n",
      "numcomp = [30, 50, 100]\n",
      "for num in numcomp:\n",
      "    \n",
      "    pca_num, recon_num = reconstruct(traindata, traindata[:25,:], num)\n",
      "    \n",
      "    recon_num = np.add(recon_num, meanimage)\n",
      "    \n",
      "    plotdigits(recon_num[20:], 1, 5)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "star_wars = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "\n",
      "star_wars = star_wars[star_wars['RespondentID' <<unk>>].notnull()]\n",
      "--------------------\n",
      "yes_no = {\n",
      "    \"Yes\": True,\n",
      "    \"No\": False\n",
      "}\n",
      "\n",
      "for col in [\n",
      "    \"Have you seen any of the 6 films in the Star Wars franchise?\",\n",
      "    \"Do you consider yourself to be a fan of the Star Wars film franchise?\"\n",
      "]:\n",
      "    star_wars[col] = star_wars[col].map(yes_no)\n",
      "\n",
      "star_wars.head()\n",
      "=====\n",
      "seen = 'Have you seen any of the 6 films in the Star Wars franchise?' <<unk>>\n",
      "fan = 'Do you consider yourself to be a fan of the Star Wars film franchise?' <<unk>>\n",
      "fan_st = 'Do you consider yourself to be a fan of the Star Trek franchise?' <<unk>>\n",
      "\n",
      "\n",
      "yes_no_map = {\n",
      "    jupyter_string: True,\n",
      "    jupyter_string: False\n",
      "}\n",
      "\n",
      "star_wars[seen] = star_wars[seen].map(yes_no_map)\n",
      "star_wars[fan] = star_wars[fan].map(yes_no_map)\n",
      "star_wars[fan_st] = star_wars[fan_st].map(yes_no_map)\n",
      "\n",
      "--------------------\n",
      "star_wars = star_wars.rename(columns={\n",
      "        \"Which of the following Star Wars films have you seen? Please select all that apply.\": jupyter_string,\n",
      "        \"Unnamed: 4\": jupyter_string,\n",
      "        \"Unnamed: 5\": jupyter_string,\n",
      "        \"Unnamed: 6\": jupyter_string,\n",
      "        \"Unnamed: 7\": jupyter_string,\n",
      "        \"Unnamed: 8\": jupyter_string\n",
      "        })\n",
      "star_wars.head()\n",
      "=====\n",
      "movie_name = {'Which of the following Star Wars films have you seen? Please select all that apply.' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 4' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 5' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 6' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 7' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 8' <<unk>>: jupyter_string}\n",
      "star_wars = star_wars.rename(columns={'Which of the following Star Wars films have you seen? Please select all that apply.' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 4' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 5' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 6' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 7' <<unk>>: jupyter_string,\n",
      "             'Unnamed: 8' <<unk>>: jupyter_string})\n",
      "--------------------\n",
      "star_wars = star_wars.rename(columns={\n",
      "        \"Which of the following Star Wars films have you seen? Please select all that apply.\": jupyter_string,\n",
      "        \"Unnamed: 4\": jupyter_string,\n",
      "        \"Unnamed: 5\": jupyter_string,\n",
      "        \"Unnamed: 6\": jupyter_string,\n",
      "        \"Unnamed: 7\": jupyter_string,\n",
      "        \"Unnamed: 8\": jupyter_string\n",
      "        })\n",
      "=====\n",
      "star_wars[star_wars.columns[9:15]] = star_wars[star_wars.columns[9:15]].astype(float)\n",
      "--------------------\n",
      "df.corr()\n",
      "=====\n",
      "from pandas.plotting import scatter_matrix\n",
      "subset = df[['TotalConsmp' <<unk>>,'TempOutSide' <<unk>>,'Press_mm_hg' <<unk>>,'H_OutSide' <<unk>>,'Windspeed' <<unk>>,'Visibility' <<unk>>]]\n",
      "scatter_matrix(subset,figsize=(10,10)) \n",
      "plt.show()\n",
      "--------------------\n",
      "ranking_std = star_wars[star_wars.columns[3:9]].std()\n",
      "ranking_std\n",
      "=====\n",
      "seen_sum = star_wars[star_wars.columns[3:9]].sum()\n",
      "seen_sum\n",
      "--------------------\n",
      "plt.bar(movie_names, seen_sum)\n",
      "plt.xticks(rotation = 90)\n",
      "=====\n",
      "plt.bar(movie_names, seen_sum)\n",
      "plt.xticks(rotation = 90)\n",
      "--------------------\n",
      "df_male = df[df['Gender' <unk>] == jupyter_string]\n",
      "df_female = df[df['Gender' <unk>] == jupyter_string]\n",
      "=====\n",
      "from textwrap import wrap\n",
      "movies =  [jupyter_string.join(wrap(l, 20)) for l in movie_names]\n",
      "males = star_wars[star_wars['Gender' <<unk>>] == jupyter_string]\n",
      "females = star_wars[star_wars['Gender' <<unk>>] == jupyter_string]\n",
      "\n",
      "ranking_males = males[males.columns[9:15]].mean()\n",
      "ranking_females = females[females.columns[9:15]].mean()\n",
      "seen_males = males[males.columns[3:9]].sum()\n",
      "seen_females = females[females.columns[3:9]].sum()\n",
      "\n",
      "plot_mf = [ranking_males, ranking_females, seen_males, seen_females]\n",
      "titles_mf = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "def plot_bar(var_ls, tit_ls):\n",
      "    \n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "    for i, each in enumerate(var_ls):\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        ax.bar(movies, each)\n",
      "        ax.tick_params(rotation = 90)\n",
      "        \n",
      "        ax.set_title(tit_ls[i])\n",
      "        for key,spine in ax.spines.items():\n",
      "                spine.set_visible(False)\n",
      "        if i == 0 or i == 1:\n",
      "            ax.get_xaxis().set_visible(False)\n",
      "        if i == 1 or i == 3:\n",
      "            ax.get_yaxis().set_visible(False)\n",
      "\n",
      "plot_bar(plot_mf,titles_mf)\n",
      "--------------------\n",
      "movies =  [jupyter_string.join(wrap(l, 20)) for l in movie_names]\n",
      "males = star_wars[star_wars['Gender' <unk>] == jupyter_string]\n",
      "females = star_wars[star_wars['Gender' <unk>] == jupyter_string]\n",
      "\n",
      "ranking_males = males[males.columns[3:9]].sum()\n",
      "ranking_females = females[females.columns[3:9]].sum()\n",
      "seen_males = males[males.columns[9:15]].sum()\n",
      "seen_females = females[females.columns[9:15]].sum()\n",
      "\n",
      "plot_mf = [ranking_males, ranking_females, seen_males, seen_females]\n",
      "titles_mf = [jupyter_string, jupyter_string\n",
      "=====\n",
      "fan_sw = 'Do you consider yourself to be a fan of the Star Wars film franchise?' <<unk>>\n",
      "\n",
      "sw_fan = star_wars[star_wars[fan_sw] == True]\n",
      "sw_not_fan = star_wars[star_wars[fan_sw] == False]\n",
      "\n",
      "\n",
      "ranking_swfans = sw_fan[sw_fan.columns[9:15]].mean()\n",
      "ranking_swNfans = sw_not_fan[sw_not_fan.columns[9:15]].mean()\n",
      "seen_swfans = sw_fan[sw_fan.columns[3:9]].sum()\n",
      "seen_swNfans = sw_not_fan[sw_not_fan.columns[3:9]].sum()\n",
      "\n",
      "plot_sw = [ranking_swfans, ranking_swNfans, seen_swfans,seen_swNfans]\n",
      "titles_sw = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "plot_bar(plot_sw,titles_sw)\n",
      "--------------------\n",
      "star_wars_fan = star_wars[star_wars[fan_sw] == True]\n",
      "star_wars_not_fan = star_wars[star_wars[fan_sw] == False]\n",
      "\n",
      "\n",
      "ranking_star_wars_fan = star_wars_fan[star_wars_fan.columns[3:9]].mean()\n",
      "ranking_star_wars_not_fan = star_wars_not_fan[star_wars_not_fan.columns[3:9]].mean()\n",
      "seen_star_wars_fan = star_wars_fan[star_wars_fan.columns[9:15]].sum()\n",
      "seen_star_wars_not_fan = star_wars_not_fan[star_wars_not_fan.columns[9:15]].sum()\n",
      "=====\n",
      "st_fan = star_wars[star_wars[fan_st] == True]\n",
      "st_not_fan = star_wars[star_wars[fan_st] == False]\n",
      "\n",
      "\n",
      "ranking_stfans = st_fan[st_fan.columns[9:15]].mean()\n",
      "ranking_stNfans = st_not_fan[st_not_fan.columns[9:15]].mean()\n",
      "seen_stfans = st_fan[st_fan.columns[3:9]].sum()\n",
      "seen_stNfans = st_not_fan[st_not_fan.columns[3:9]].sum()\n",
      "\n",
      "plot_st = [ranking_stfans, ranking_stNfans, seen_stfans,seen_stNfans]\n",
      "titles_st = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "plot_bar(plot_st,titles_st)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats as stats\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.stats.multicomp as multi\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "from scipy.stats import f\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.rcParams[jupyter_string] = 14\n",
      "\n",
      "x = np.linspace(0.01, 6, num=250)\n",
      "groups = 4\n",
      "samples = 327\n",
      "df1 = groups - 1\n",
      "df2 = samples - groups\n",
      "y = f.pdf(x, df1, df2)\n",
      "plt.plot(x, y)\n",
      "--------------------\n",
      "baseball = pd.read_csv(jupyter_string)\n",
      "baseball.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df = df.head(327)\n",
      "--------------------\n",
      "plt.scatter(outfield, des_hit)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "x = np.linspace(0.01, 6, num=250)\n",
      "groups = 4\n",
      "samples = outfield.count() + infield.count() + des_hit.count() + catcher.count()\n",
      "df1 = groups - 1\n",
      "df2 = samples - groups\n",
      "y = f.pdf(x, df1, df2)\n",
      "x_right = np.linspace(F_statistic, 6)\n",
      "y_right = f.pdf(x_right, df1, df2)\n",
      "plt.fill_between(x_right, 0, y_right, facecolor=jupyter_string)\n",
      "plt.plot(x, y, jupyter_string, lw=1)\n",
      "--------------------\n",
      "url = jupyter_string\n",
      "source = requests.get(url).text\n",
      "soup = BeautifulSoup(source, jupyter_string)\n",
      "=====\n",
      "post_url = jupyter_string\n",
      "posts = requests.get(post_url)\n",
      "parser = BeautifulSoup(posts.content,jupyter_string)\n",
      "table = parser.find_all(jupyter_string)[0] \n",
      "df = pd.read_html(str(table),header=0)[0]\n",
      "df.head(12)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, na_values = [jupyter_string])\n",
      "df.head()\n",
      "--------------------\n",
      "df['Brewery' <unk>].value_counts().head(10)\n",
      "=====\n",
      "df['Location' madeupword0002].describe()\n",
      "--------------------\n",
      "df['Location' <unk>].value_counts().head(10)\n",
      "=====\n",
      "df['Location' madeupword0002].value_counts().head(10)\n",
      "--------------------\n",
      "df['Size' <unk>].describe()\n",
      "=====\n",
      "df['Size' <<unk>>].describe()\n",
      "--------------------\n",
      "df['Size' <unk>].value_counts().head(10)\n",
      "=====\n",
      "df['Size' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df.ABV = df.ABV.str.replace(jupyter_string, jupyter_string)\n",
      "df.ABV = df.ABV.astype(float)\n",
      "=====\n",
      "df['ABV' <<unk>>].str.replace(jupyter_string, jupyter_string).head()\n",
      "--------------------\n",
      "df['ABV' <unk>] = df['ABV' <unk>].str.replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "df['ABV' <<unk>>] = df['ABV' <<unk>>].str.replace(jupyter_string, jupyter_string)\n",
      "--------------------\n",
      "df['ABV' <unk>].head()\n",
      "=====\n",
      "df['ABV' <<unk>>].astype(float).head()\n",
      "--------------------\n",
      "df['Location' <unk>].value_counts().head(10)\n",
      "=====\n",
      "df['Location' madeupword0002].value_counts().head(10)\n",
      "--------------------\n",
      "df[df['Location' <unk>] == jupyter_string].head()\n",
      "=====\n",
      "Brook_beer = df[df.Location == jupyter_string]\n",
      "Brook_beer\n",
      "--------------------\n",
      "df = df[df.Borough != jupyter_string]\n",
      "df.head(12)\n",
      "=====\n",
      "df = df[~df[jupyter_string].isin([jupyter_string])]\n",
      "df.reset_index(inplace=True,drop=True)\n",
      "df.head(12)\n",
      "--------------------\n",
      "brooklyn_brewery = Brooklyn_beer.Brewery.value_counts()\n",
      "brooklyn_brewery.head()\n",
      "=====\n",
      "Brook_beer['Brewery' <<unk>>].value_counts().head(1)\n",
      "--------------------\n",
      "brooklyn_beer['Style' <unk>].value_counts().head(5)\n",
      "=====\n",
      "Sixpoint = Brook_beer[Brook_beer.Brewery == jupyter_string]\n",
      "--------------------\n",
      "sixpoint['Style' <unk>].value_counts().head(5)\n",
      "=====\n",
      "Sixpoint['Style' <<unk>>].value_counts().head(5)\n",
      "--------------------\n",
      "NY_brewery = df[df['State' <unk>] == jupyter_string]\n",
      "NY_brewery['Brewery' <unk>].value_counts()\n",
      "=====\n",
      "NY_beer = df[df['Location' madeupword0002].str.contains(jupyter_string, na=False)]\n",
      "NY_beer\n",
      "--------------------\n",
      "NY_beer['Brewery' <unk>].value_counts().head(5)\n",
      "=====\n",
      "NY_beer['Brewery' <<unk>>].describe()\n",
      "--------------------\n",
      "df_Beer['IBU' <unk>].mean()\n",
      "=====\n",
      "df[\"IBUs\"].mean().astype(int)\n",
      "--------------------\n",
      "df.IBUs.hist(bins=10)\n",
      "plt.show()\n",
      "=====\n",
      "df[\"IBUs\"].hist(bins=30)\n",
      "--------------------\n",
      "df[df[\"IBUs\"] > df[\"IBUs\"].quantile(0.75)]\n",
      "=====\n",
      "df[\"IBUs\"].describe(percentiles=None, include=None, exclude=None)\n",
      "--------------------\n",
      "df.groupby('Style' <unk>).IBUs.median().sort_values(ascending=False).plot(kind=jupyter_string)\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean()\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].median()\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(20).plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df = df.groupby(['PostalCode' <unk>,'Borough' <unk>])['Neighborhood' <unk>].apply(lambda x: jupyter_string.join(x)).reset_index()\n",
      "df.head(12)\n",
      "=====\n",
      "df[jupyter_string].nunique()\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(20).plot(kind=jupyter_string)\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(20).plot(kind=jupyter_string)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().sort_values(ascending=False).head()\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(5)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().tail(5)\n",
      "=====\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(5)\n",
      "--------------------\n",
      "df.groupby(\"Style\")[\"IBUs\"].mean().plot(kind=jupyter_string, bins=20)\n",
      "=====\n",
      "choice_beer = df[(df['Style' <<unk>>] == jupyter_string) | (df['Style' <<unk>>] == jupyter_string) | (df['Style' <<unk>>] == jupyter_string)]\n",
      "--------------------\n",
      "choice_beer.groupby(\"Style\")[\"IBUs\"].mean().dropna().sort_values().head(20)\n",
      "=====\n",
      "choice_beer[\"IBUs\"].mean().astype(int)\n",
      "--------------------\n",
      "plt.hist(choice_beer[\"IBUs\"], bins=20)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "choice_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "--------------------\n",
      "choice_beer[choice_beer[\"Style\"] == jupyter_string][\"IBUs\"].mean()\n",
      "=====\n",
      "IPA_beer = df[df['Style' <<unk>>].str.contains(jupyter_string, na=False)]\n",
      "--------------------\n",
      "IPA_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "=====\n",
      "IPA_beer[\"IBUs\"].mean().astype(int)\n",
      "--------------------\n",
      "df[df['Style' <unk>].str.contains(jupyter_string, na=False)]\n",
      "=====\n",
      "IPA_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
      "fig.set_size_inches(12,5)\n",
      "ax1.hist(IPA_beer[\"IBUs\"])\n",
      "ax1.set_title(jupyter_string)\n",
      "ax1.set_xlabel(jupyter_string)\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "ax2.hist(IPB_beer[\"IBUs\"])\n",
      "ax2.set_title(jupyter_string)\n",
      "ax2.set_xlabel(jupyter_string)\n",
      "ax2.set_ylabel(jupyter_string)\n",
      "=====\n",
      "ax = choice_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"IBUs\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "--------------------\n",
      "df = pd.concat([df, pd.get_dummies(df[jupyter_string], prefix=jupyter_string)], axis=1)\n",
      "df.head()\n",
      "=====\n",
      "df = df.groupby([jupyter_string,jupyter_string])[jupyter_string].apply(jupyter_string.join).reset_index()\n",
      "df.head(12)\n",
      "--------------------\n",
      "ax = choice_beer[\"ABV\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"ABV\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "=====\n",
      "IPA_beer[jupyter_string].mean()\n",
      "--------------------\n",
      "ax = choice_beer[\"ABV\"].plot(kind=jupyter_string, bins=30)\n",
      "IPA_beer[\"ABV\"].plot(kind=jupyter_string, bins=30, ax=ax)\n",
      "=====\n",
      "wheat_beer = df[df['Style' <<unk>>].str.contains(jupyter_string, na=False)]\n",
      "wheat_beer[jupyter_string].mean()\n",
      "--------------------\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "clf = LogisticRegression()\n",
      "parameters = {jupyter_string: [0.001, 0.1, 1, 10, 100]}\n",
      "fitmodel = GridSearchCV(clf, param_grid=parameters, cv=5)\n",
      "fitmodel.fit(Xlr, ylr)\n",
      "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_\n",
      "=====\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "parameters = {jupyter_string:Cs}\n",
      "clf_gs = LogisticRegression()\n",
      "clf_gs_cv = GridSearchCV(clf_gs, param_grid=parameters, cv=5)\n",
      "clf_gs_cv.fit(Xlr, ylr)\n",
      "print(clf_gs_cv.best_params_, clf_gs_cv.best_score_)\n",
      "--------------------\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "parameters = {jupyter_string:Cs}\n",
      "clf = LogisticRegression()\n",
      "clf_gs = GridSearchCV(clf, param_grid=parameters, cv=5)\n",
      "clf_gs.fit(Xlr, ylr)\n",
      "print(clf_gs.best_params_, clf_gs.best_score_)\n",
      "=====\n",
      "y_pred_gs = clf_gs_cv.predict(Xtestlr)\n",
      "accuracy_score(y_pred_gs, ytestlr)\n",
      "--------------------\n",
      "dflog = pd.read_csv(jupyter_string)\n",
      "dflog.head()\n",
      "=====\n",
      "dflog = pd.read_csv(jupyter_string)\n",
      "dflog.head()\n",
      "--------------------\n",
      "plt.scatter(dflog.Weight, dflog.Height, c=[cm_bright.colors[i] for i in dflog.Gender==jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "_ = plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog.Weight, dflog.Height)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.xlabel('Weight' <<unk>>)\n",
      "_ = plt.ylabel('Height' <<unk>>)\n",
      "--------------------\n",
      "_ = plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog.Weight, dflog.Height, c=[cm_bright.colors[i] for i in dflog.Gender==jupyter_string])\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.xlabel('Weight' <unk>)\n",
      "_ = plt.ylabel('Height' <unk>)\n",
      "=====\n",
      "dflog_dummies = pd.get_dummies(dflog, drop_first = True)\n",
      "plt_gen_color = {0:(jupyter_string,jupyter_string), 1:(jupyter_string,jupyter_string)}\n",
      "patches = [mpl.patches.Patch(color=color, label=label) for color, label in plt_gen_color.values()]\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog_dummies.Weight, dflog_dummies.Height, c=[plt_gen_color[x] for x in dflog_dummies.Gender])\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.xlabel('Weight' <unk>)\n",
      "_ = plt.ylabel('Height' <unk>)\n",
      "_ = plt.legend(handles=patches)\n",
      "=====\n",
      "_ = plt.figure(figsize=(10,10))\n",
      "_ = plt.scatter(dflog_dummies.Weight, dflog_dummies.Height,\n",
      "                c = [plt_gen_color[i][0] for i in dflog_dummies.Gender_Male],\n",
      "                alpha = 0.1)\n",
      "_ = plt.legend(handles=patches, labels=[label for _, label in plt_gen_color.values()])\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.xlabel('Weight' <<unk>>)\n",
      "_ = plt.ylabel('Height' <<unk>>)\n",
      "--------------------\n",
      "h = lambda z: 1. / (1 + np.exp(-z))\n",
      "zs=np.arange(-5, 5, 0.1)\n",
      "plt.plot(zs, h(zs), alpha=0.5);\n",
      "=====\n",
      "h = lambda z: 1. / (1 + np.exp(-z))\n",
      "zs=np.arange(-5, 5, 0.1)\n",
      "plt.plot(zs, h(zs), alpha=0.5);\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data4=pd.read_csv(jupyter_string, low_memory=False)\n",
      "--------------------\n",
      "df = df[df.Borough != jupyter_string]\n",
      "=====\n",
      "df[jupyter_string] = df[jupyter_string].where(df[jupyter_string]==jupyter_string,df[jupyter_string])\n",
      "df.loc[df[jupyter_string] == jupyter_string]\n",
      "--------------------\n",
      "plt.figure()\n",
      "ax=plt.gca()\n",
      "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n",
      "=====\n",
      "plt.figure()\n",
      "ax=plt.gca()\n",
      "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n",
      "--------------------\n",
      "plt.figure()\n",
      "ax = plt.gca()\n",
      "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n",
      "=====\n",
      "plt.figure()\n",
      "ax = plt.gca()\n",
      "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "Xlr, Xtestlr, ylr, ytestlr = train_test_split(X, y)\n",
      "\n",
      "clf = LogisticRegression()\n",
      "clf.fit(Xlr, ylr)\n",
      "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n",
      "=====\n",
      "clf_best = LogisticRegression(C=0.1)\n",
      "clf_best.fit(Xlr, ylr)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "Xlr_train, Xlr_test, ylr_train, ylr_test = train_test_split(Xlr, ylr)\n",
      "clf = LogisticRegression()\n",
      "clf.fit(Xlr_train, ylr_train)\n",
      "print(accuracy_score(clf.predict(Xlr_test), ylr_test))\n",
      "=====\n",
      "y_pred_best = clf_best.predict(Xtestlr)\n",
      "accuracy_score(y_pred_best, ytestlr)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "import math\n",
      "\n",
      "import scipy\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "train.head()\n",
      "=====\n",
      "train_data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_data.head()\n",
      "=====\n",
      "train_data.head(5)\n",
      "--------------------\n",
      "df = pd.DataFrame()\n",
      "df[jupyter_string] = X_train[:,0]\n",
      "df[jupyter_string] = X_train[:,1]\n",
      "df[jupyter_string] = X_train[:,2]\n",
      "df[jupyter_string] = X_train[:,3]\n",
      "df[jupyter_string] = X_train[:,4]\n",
      "df[jupyter_string] = X_train[:,5]\n",
      "df[jupyter_string] = X_train[:,6]\n",
      "=====\n",
      "X_data_new = pd.DataFrame(X_data.iloc[:,:].apply(lambda row: features(row), axis = 1)) \n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "train_data['label' <unk>].unique()\n",
      "=====\n",
      "train_data[\"label\"].value_counts().sort_index()\n",
      "--------------------\n",
      "geodata = pd.read_csv(jupyter_string)\n",
      "geodata.head()\n",
      "=====\n",
      "geocode = pd.read_csv(jupyter_string)\n",
      "geocode.rename(columns={'Postal Code' madeupword0002: jupyter_string}, inplace=True)\n",
      "--------------------\n",
      "X = data.drop(\"label\", axis=1)\n",
      "y = data[\"label\"]\n",
      "=====\n",
      "X_data = train_data.iloc[:,1:]\n",
      "Y_data = train_data.iloc[:,0:1]\n",
      "--------------------\n",
      "X_train_mass = mass(X_train)\n",
      "X_test_mass = mass(X_test)\n",
      "=====\n",
      "X_data_new = pd.DataFrame(X_data.iloc[:,:].apply(lambda row: mass(row), axis = 1)) \n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "k_folds = 10\n",
      "for i in range(k_folds):\n",
      "    X_train, X_valid, Y_train, Y_valid = cross_validation.train_test_split(X_data, Y_data, test_size=0.3, random_state=i)\n",
      "    LDA_model.fit(X_train, Y_train.as_matrix().T.ravel())\n",
      "    print(LDA_model.score(X_valid, Y_valid))\n",
      "=====\n",
      "from sklearn.model_selection import cross_val_score\n",
      "Y_data_reshaped =  Y_data.as_matrix().reshape((Y_data.shape[0],))\n",
      "\n",
      "scores = cross_val_score(LDA_model, X_data, Y_data_reshaped, cv = 5, n_jobs = -1)\n",
      "print(scores)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "fig, axes = plt.subplots(3, 6, figsize=(12, 6),\n",
      "                         subplot_kw={jupyter_string: [], jupyter_string: []})\n",
      "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
      "axes = axes.ravel()\n",
      "im_len = len(X_train)\n",
      "print(im_len)\n",
      "for i in range(18):\n",
      "    \n",
      "    rand_index = random.randint(0,im_len)\n",
      "    image = X_train[rand_index]\n",
      "    axes[i].imshow(image)\n",
      "    axes[i].set_title((jupyter_string+str(y_train[rand_index])))\n",
      "--------------------\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "=====\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "def evaluate(X_data, y_data):\n",
      "    num_examples = len(X_data)\n",
      "    total_accuracy = 0\n",
      "    sess = tf.get_default_session()\n",
      "    for offset in range(0, num_examples, BATCH_SIZE):\n",
      "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
      "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
      "        total_accuracy += (accuracy * len(batch_x))\n",
      "    return total_accuracy / num_examples\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "new_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "passed = df[df['passed' <unk>] == 1]\n",
      "failed = df[df['passed' <unk>] == 0]\n",
      "=====\n",
      "v.to_frame()\n",
      "--------------------\n",
      "D.to_frame()\n",
      "=====\n",
      "D.detected()\n",
      "--------------------\n",
      "geodata = pd.merge(geodata, geocode, on=jupyter_string)\n",
      "=====\n",
      "com_df = df.merge(geocode, how=jupyter_string, on=jupyter_string )\n",
      "com_df.head(12)\n",
      "--------------------\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "=====\n",
      "data_PMN_300 = pd.read_csv(jupyter_string)\n",
      "data_AMN_300 = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data_PMN_300.head()\n",
      "=====\n",
      "data_PMN_300.tail()\n",
      "--------------------\n",
      "data_AMN_300.tail()\n",
      "=====\n",
      "data_AMN_300.tail()\n",
      "--------------------\n",
      "p_MgO_PMN = np.array(p_MgO_PMN)\n",
      "p_MgO_AMN = np.array(p_MgO_AMN)\n",
      "p_Pt_PMN = np.array(p_Pt_PMN)\n",
      "p_Au_AMN = np.array(p_Au_AMN)\n",
      "=====\n",
      "f, axarr = plt.subplots(2, 2, \\\n",
      "                    figsize=(8,6))\n",
      "ax = [axarr[0,0], axarr[0,1], axarr[1,0], axarr[1,1]]\n",
      "ms = 8; mew = 0.7\n",
      "label = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "for i in range(4):\n",
      "    ax[i].axhline(y=0, c=jupyter_string, ls=jupyter_string)\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            unp.nominal_values(p_Au_AMN[i]) - unp.nominal_values(p_MgO_AMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_AMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Au_AMN[i]),\\\n",
      "            fmt=jupyter_string, mec=jupyter_string, mew=mew, label = jupyter_string, \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].errorbar(unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            unp.nominal_values(p_Pt_PMN[i]) - unp.nominal_values(p_MgO_PMN[i]), \\\n",
      "            xerr = unp.std_devs(p_MgO_PMN[i]),\\\n",
      "            yerr = unp.std_devs(p_Pt_PMN[i]), \\\n",
      "            fmt=jupyter_string, mec=jupyter_string, mew=mew, label=jupyter_string, \\\n",
      "            ms=ms, capsize=0, lw=0.4)\n",
      "    ax[i].set_xlabel(jupyter_string); ax[i].set_ylabel(jupyter_string)\n",
      "    l = ax[i].legend(loc=3, numpoints = 1, fontsize = 10)\n",
      "    l.get_frame().set_linewidth(0.5)\n",
      "    plt.tight_layout(pad=0.4)\n",
      "    ax[i].set_ylim(-5.,3.); ax[i].set_xlim(0.,140.)\n",
      "    ax[i].set_xticks(ax[i].get_xticks()[::2]); ax[i].set_yticks(ax[i].get_yticks()[::2])\n",
      "    ax[i].text(0.08, 0.83,label[i], horizontalalignment=jupyter_string,\\\n",
      "            verticalalignment=jupyter_string, transform = ax[i].transAxes,\\\n",
      "              fontsize = 24)\n",
      "\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string, \\\n",
      "                        pad_inches=0.1)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_reader = pd.read_csv(jupyter_string,\n",
      "                        chunksize=10)\n",
      "\n",
      "print(next(df_reader))\n",
      "print(next(df_reader))\n",
      "--------------------\n",
      "df_reader = pd.read_csv(jupyter_string,\n",
      "                        chunksize=10)\n",
      "\n",
      "df_reader = pd.DataFrame()\n",
      "\n",
      "for df in df_reader:\n",
      "    df_reader = df_reader.append(df)\n",
      "\n",
      "df_reader.head()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=10)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "print(df_urb_pop.head())\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string]==jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "           df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "--------------------\n",
      "urb_pop_df = pd.DataFrame(pops_list, columns=[jupyter_string, jupyter_string])\n",
      "urb_pop_df.head()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "            df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "\n",
      "\n",
      "\n",
      "df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                        for tup in pops_list]\n",
      "\n",
      "\n",
      "df_pop_ceb.plot(kind=jupyter_string, \n",
      "                x=jupyter_string, \n",
      "                y=jupyter_string,\n",
      "                title=jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "df_urb_pop = next(urb_pop_reader)\n",
      "\n",
      "\n",
      "df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "pops = zip(df_pop_ceb[jupyter_string], \n",
      "            df_pop_ceb[jupyter_string])\n",
      "\n",
      "\n",
      "pops_list = list(pops)\n",
      "\n",
      "\n",
      "\n",
      "df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                        for tup in pops_list]\n",
      "\n",
      "\n",
      "df_pop_ceb.plot(kind=jupyter_string, \n",
      "                x=jupyter_string, \n",
      "                y=jupyter_string,\n",
      "                title=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "urb_pop_reader = pd.read_csv(jupyter_string,\n",
      "                             chunksize=1000)\n",
      "\n",
      "\n",
      "data = pd.DataFrame()\n",
      "\n",
      "\n",
      "for df_urb_pop in urb_pop_reader:\n",
      "    df_pop_ceb = df_urb_pop[df_urb_pop[jupyter_string] == jupyter_string]\n",
      "    \n",
      "    pops = zip(df_pop_ceb[jupyter_string],\n",
      "               df_pop_ceb[jupyter_string])\n",
      "    \n",
      "    pops_list = list(pops)\n",
      "    \n",
      "    \n",
      "    \n",
      "    df_pop_ceb[jupyter_string] = [int(tup[0] * tup[1]) \\\n",
      "                                            for tup in pops_list]\n",
      "    \n",
      "    \n",
      "    data = data.append(df_pop_ceb)\n",
      "    \n",
      "data.plot(kind=jupyter_string,\n",
      "          x=jupyter_string,\n",
      "          y=jupyter_string,\n",
      "          title=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "engine = create_engine(jupyter_string)\n",
      "conn = engine.connect()\n",
      "=====\n",
      "engine = create_engine(jupyter_string)\n",
      "conn = engine.connect()\n",
      "--------------------\n",
      "measurements = pd.read_csv(jupyter_string)\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "measurements = pd.read_csv(jupyter_string)\n",
      "measurements[\"date\"] = pd.to_datetime(measurements[\"date\"], format=jupyter_string)\n",
      "measurements.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns; sns.set();\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "stations.head()\n",
      "=====\n",
      "stations = pd.read_csv(jupyter_string)\n",
      "stations.head()\n",
      "--------------------\n",
      "plt.plot(patSatTrain.patSat,patSatTrain.patSat,jupyter_string)\n",
      "plt.plot(patSatTrain.patSat,mod1Result.predict(patSatTrain),jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.cla\n",
      "sg=sns.jointplot(x=jupyter_string, y=jupyter_string,marker=jupyter_string,data=predObsDF)\n",
      "x0, x1 = sg.ax_joint.get_xlim()\n",
      "y0, y1 = sg.ax_joint.get_ylim()\n",
      "lims = [max(x0, y0), min(x1, y1)]\n",
      "sg.ax_joint.plot(lims, lims, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)  \n",
      "\n",
      "--------------------\n",
      "pd.DataFrame(mod1Result.summary())\n",
      "=====\n",
      "from statsmodels.stats.outliers_influence import OLSInfluence\n",
      "influenceResults=OLSInfluence(mod1Result)\n",
      "influenceResults.summary_frame().head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "warnings.resetwarnings()  \n",
      "--------------------\n",
      "influenceResults.plot_influence()\n",
      "influenceResults.plot_leverage_resid2()\n",
      "plt.show()\n",
      "=====\n",
      "from statsmodels.graphics.regressionplots import influence_plot\n",
      "fig, ax = plt.subplots(figsize=(10,8))\n",
      "influence_plot(mod1Result,ax=ax,plot_alpha=0.01,alpha=0.001,fontsize=10)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "patSatDF=pd.read_csv(jupyter_string)  \n",
      "--------------------\n",
      "fig, ax = plt.subplots(figsize=(10,8))\n",
      "influence_plot(mod1Result,ax=ax,plot_alpha=0.01,alpha=0.001,fontsize=10)\n",
      "=====\n",
      "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
      "fig, ax = plt.subplots(figsize=(12,10))\n",
      "plot_leverage_resid2(mod1Result, ax = ax)\n",
      "--------------------\n",
      "patSatDF = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "patSatDF2=patSatDF[['patSat' <<unk>>,'q2' <<unk>>,'q6' <<unk>>,'ptCat' <<unk>>]]\n",
      "--------------------\n",
      "patSatDF2.to_html(jupyter_string)\n",
      "=====\n",
      "patSatProfile=profile.ProfileReport(patSatDF2)\n",
      "patSatProfile.to_file(jupyter_string)  \n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "seattleData = pd.read_csv(jupyter_string)\n",
      "seattleData.shape\n",
      "ind = pd.DatetimeIndex(seattleData['Date Reported' <<unk>>])\n",
      "seattleData[jupyter_string] = ind.dayofweek\n",
      "seattleData[jupyter_string] = ind.strftime(jupyter_string)\n",
      "seattleData[jupyter_string] = (ind.dayofweek>4)\n",
      "seattleData[jupyter_string] = ind.date.astype(jupyter_string)\n",
      "seattleData[jupyter_string] = ind.hour\n",
      "\n",
      "\n",
      "--------------------\n",
      "patSatDF2 = patSatDF2.sample(frac=1).reset_index(drop=True)\n",
      "=====\n",
      "np.random.seed(3)  \n",
      "\n",
      "patSatDF2=patSatDF2.assign(train=pd.Series(np.random.random(len(patSatDF2))<=0.80))\n",
      "patSatTrain=patSatDF2[patSatDF2.train==True]\n",
      "patSatTest=patSatDF2[patSatDF2.train!=True]\n",
      "print(jupyter_string,len(patSatTrain))\n",
      "print(jupyter_string,len(patSatTest))\n",
      "--------------------\n",
      "X.head()\n",
      "=====\n",
      "X.head()\n",
      "--------------------\n",
      "sns.regplot(y,mod1.predict(X))\n",
      "=====\n",
      "predObsDF=pd.DataFrame({jupyter_string:patSatTrain.patSat,jupyter_string:mod1Result.predict(patSatTrain)})\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, index_col=jupyter_string, parse_dates = True)\n",
      "df\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "names = df[jupyter_string]\n",
      "names\n",
      "--------------------\n",
      "names = pd.read_csv(jupyter_string)\n",
      "names.head()\n",
      "=====\n",
      "first_three = df.values[0:1,:] \n",
      "first_three\n",
      "--------------------\n",
      "first_three = first_three.flatten()\n",
      "first_three\n",
      "=====\n",
      "text = df.values[:,3:4]\n",
      "text.flatten()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.gray() \n",
      "plt.matshow(digits.images[0]) \n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r)\n",
      "plt.show()\n",
      "--------------------\n",
      "X = np.reshape(digits.data, (len(digits.data), 1))\n",
      "=====\n",
      "data = digits.images.reshape((digits.images.shape[0], -1))\n",
      "--------------------\n",
      "mean = np.mean(data, axis=0)\n",
      "std = np.std(data, axis=0)\n",
      "=====\n",
      "from sklearn import preprocessing\n",
      "preprocessing.scale(digits.data)\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string,data=seattleData,order=range(25),palette=jupyter_string)\n",
      "plt.xlabel(jupyter_string, fontsize=14)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "\n",
      "--------------------\n",
      "plt.scatter(iris_X_train[:,0], iris_X_train[:,1], c=iris_y_train)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "X = np.array([\n",
      "    [ .5],\n",
      "    [1]\n",
      "])\n",
      "\n",
      "y = [0.5, 1]\n",
      "\n",
      "test = np.c_[ 0, 2, 2].T\n",
      "\n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(X, y)\n",
      "\n",
      "plt.figure()\n",
      "np.random.seed(0)\n",
      "for _ in range(6):\n",
      "    this_X = 0.1*np.random.normal(size=(2, 1)) + X\n",
      "    regr.fit(this_X, y)\n",
      "    plt.plot(test, regr.predict(test))\n",
      "    plt.scatter(this_X, y, s=3)\n",
      "plt.show()\n",
      "--------------------\n",
      "math = pd.read_csv(jupyter_string)\n",
      "math.head()\n",
      "=====\n",
      "math = pd.read_csv(jupyter_string)\n",
      "math.head(10)\n",
      "--------------------\n",
      "por = pd.read_csv(jupyter_string)\n",
      "por.head(10)\n",
      "=====\n",
      "por = pd.read_csv(jupyter_string)\n",
      "por.head(10)\n",
      "--------------------\n",
      "students_math = students[students['gender' <unk>] == jupyter_string]\n",
      "students_por = students[students['gender' <unk>] == jupyter_string]\n",
      "=====\n",
      "math.head()\n",
      "--------------------\n",
      "por.head()\n",
      "=====\n",
      "por.head()\n",
      "--------------------\n",
      "demo = pd.read_csv(jupyter_string)\n",
      "demo.head()\n",
      "=====\n",
      "math.groupby(jupyter_string)[jupyter_string].hist()\n",
      "--------------------\n",
      "math.groupby(jupyter_string)[jupyter_string].hist()\n",
      "=====\n",
      "por.groupby(jupyter_string)[jupyter_string].hist()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "math.groupby(math.school)[[jupyter_string, jupyter_string]].count()\n",
      "--------------------\n",
      "math.groupby(jupyter_string)[jupyter_string].count()\n",
      "=====\n",
      "math_gp = math[math.school == jupyter_string]\n",
      "math_gp.groupby(math_gp.school)[[jupyter_string]].count()\n",
      "--------------------\n",
      "math_gp.groupby(math_gp.school)[[jupyter_string, jupyter_string]].count()\n",
      "=====\n",
      "math_ms = math[math.school == jupyter_string]\n",
      "math_ms.groupby(math_ms.school)[[jupyter_string]].count()\n",
      "--------------------\n",
      "sns.countplot(jupyter_string,data=seattleData,order=range(25),palette=jupyter_string)\n",
      "plt.xlabel(jupyter_string, fontsize=14)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "=====\n",
      "crimeCategorygroup = seattleData.groupby([jupyter_string,jupyter_string])\n",
      "crimeCategorygroup = crimeCategorygroup.size().reset_index()\n",
      "crimeCategorygroup.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "g = sns.FacetGrid(crimeCategorygroup, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "--------------------\n",
      "por_ms = por_ms.dropna()\n",
      "por_ms.head()\n",
      "=====\n",
      "math.info()\n",
      "math.isnull().sum()\n",
      "--------------------\n",
      "por.info()\n",
      "=====\n",
      "por.info()\n",
      "por.isnull().sum()\n",
      "--------------------\n",
      "math.head()\n",
      "=====\n",
      "sns.pairplot(math_gp)\n",
      "--------------------\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "lr = LogisticRegression()\n",
      "dt = DecisionTreeClassifier()\n",
      "rf = RandomForestClassifier()\n",
      "svm = SVC()\n",
      "nb = GaussianNB()\n",
      "\n",
      "eclf = VotingClassifier(estimators=[('lr' <unk>, lr), ('dt' <unk>, dt), ('rf' <unk>, rf), ('svm' <unk>, svm), ('nb' <unk>, nb)], voting=jupyter_string)\n",
      "eclf.fit(X_train, y_train)\n",
      "=====\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "df_test[jupyter_string] = df_test.apply(lambda x: 1 if x['Gender' <<unk>>] == jupyter_string else 0, axis=1)\n",
      "df_test[jupyter_string] = df_test.apply(lambda x: 1 if x['Gender' <<unk>>] == jupyter_string else 0, axis = 1)\n",
      "df_test[jupyter_string] = df_test.apply(lambda x: 1 if x['Gender' <<unk>>] != jupyter_string and x['Gender' <<unk>>] != jupyter_string else 0, axis = 1)\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "var_mod = ['Married' <<unk>>, 'Dependents' <<unk>>, 'Education' <<unk>>, 'Self_Employed' <<unk>>,'Property_Area' <<unk>>]\n",
      "le = LabelEncoder()\n",
      "for i in var_mod:\n",
      "    df_test[i] = le.fit_transform(df_test[i].astype(str))\n",
      "    \n",
      "df_test['Credit_History' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "df_test['LoanAmount' <<unk>>].fillna(np.mean(df['LoanAmount' <<unk>>]), inplace=True)\n",
      "df_test['Loan_Amount_Term' <<unk>>].fillna(360.0, inplace=True)\n",
      "df_test[jupyter_string] = np.log(df_test['ApplicantIncome' <<unk>>] + df_test['CoapplicantIncome' <<unk>>])\n",
      "df_test[jupyter_string] = np.log(df_test['LoanAmount' <<unk>>])\n",
      "df_test[jupyter_string] = np.log(df_test['Loan_Amount_Term' <<unk>>])\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import os\n",
      "os.getcwd()\n",
      "--------------------\n",
      "df_test[['Loan_ID' <unk>, 'Loan_Status' <unk>]].to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_test[['Loan_ID' <<unk>>, 'Loan_Status' <<unk>>]].to_csv(jupyter_string, sep=jupyter_string, index=False)\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df_test.head()\n",
      "=====\n",
      "df_test.head(10)\n",
      "--------------------\n",
      "g = sns.FacetGrid(seattleData, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "=====\n",
      "crimeCategorygroup = seattleData.groupby([jupyter_string,jupyter_string,\"Summarized Offense Description\"])\n",
      "\n",
      "crimeCategorygroup = crimeCategorygroup.size().reset_index()\n",
      "\n",
      "crimeCategorygroup.columns = [jupyter_string,jupyter_string,'Summarized Offense Description' <<unk>>,jupyter_string]\n",
      "\n",
      "\n",
      "\n",
      "select_number = pd.DataFrame({jupyter_string: {jupyter_string: 5, jupyter_string: 5, \n",
      "                                                   jupyter_string: 5,jupyter_string: 5,\n",
      "                                                    jupyter_string : 2}})\n",
      "\n",
      "CCG =  crimeCategorygroup.groupby([jupyter_string,\"Summarized Offense Description\"]).sum().reset_index()\n",
      "CCG = CCG[CCG[jupyter_string].isin([jupyter_string, jupyter_string])]\n",
      "CCG = CCG.groupby([jupyter_string]).apply(lambda dfg: (dfg.nlargest(4, jupyter_string))).reset_index(drop=True)\n",
      "\n",
      "crimeCategorygroup= crimeCategorygroup[crimeCategorygroup[\"Summarized Offense Description\"].isin(CCG[\"Summarized Offense Description\"])] \n",
      "\n",
      "\n",
      "g = sns.FacetGrid(crimeCategorygroup, row=jupyter_string,row_order =[jupyter_string,jupyter_string],\n",
      "                      hue=\"Summarized Offense Description\", size=5, aspect=2,sharey=False,sharex=False) \n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "\n",
      "g.set(xlabel=jupyter_string,\n",
      "      xticks = np.arange(0,25,1),\n",
      "      ylabel=jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.subplot(1,2,1)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "sns.boxplot(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "df['ApplicantIncome' <<unk>>].hist(bins=50)\n",
      "--------------------\n",
      "df['LoanAmount' <unk>].hist(bins=50)\n",
      "=====\n",
      "df.boxplot(column='ApplicantIncome' <<unk>>)\n",
      "--------------------\n",
      "df.boxplot(column='ApplicantIncome' <unk>, by='Education' <unk>)\n",
      "=====\n",
      "df['CoapplicantIncome' <<unk>>].hist(bins=50)\n",
      "--------------------\n",
      "df_test.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_test[['Loan_ID' <<unk>>, 'Loan_Status' <<unk>>]].to_csv(jupyter_string, sep=jupyter_string, index=False)\n",
      "--------------------\n",
      "df['Gender' <unk>].value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "df['Gender' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df['Married' <unk>].value_counts()\n",
      "=====\n",
      "df['Married' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df['ApplicantIncome_log' <unk>] = np.log(df.ApplicantIncome)\n",
      "df['CoapplicantIncome_log' <unk>] = np.log(df.CoapplicantIncome)\n",
      "=====\n",
      "df[jupyter_string] = np.log(df['ApplicantIncome' <<unk>>])\n",
      "--------------------\n",
      "df[jupyter_string] = np.log(df['LoanAmount' <unk>])\n",
      "=====\n",
      "df[jupyter_string] = df['CoapplicantIncome' <<unk>>].apply(lambda x: math.log(x) if x != 0 else x)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string].hist(bins=20)\n",
      "--------------------\n",
      "df[jupyter_string] = np.log(df['LoanAmount' <unk>])\n",
      "=====\n",
      "df[jupyter_string] = np.log(df['LoanAmount' <<unk>>])\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.yticks(rotation=jupyter_string)\n",
      "plt.scatter(crimes[jupyter_string], crimes[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.yticks(rotation=jupyter_string)\n",
      "plt.scatter(crimes[jupyter_string], crimes[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.set(style=jupyter_string, color_codes=True)\n",
      "districtGroup = seattleData.groupby('District/Sector' <<unk>>)\n",
      "districtGroup = districtGroup.size().sort_values( ascending=False).head(10)\n",
      "subDataDistrict = seattleData[seattleData['District/Sector' <<unk>>].isin( districtGroup.index)]\n",
      "\n",
      "subDataDistrictGroup = subDataDistrict.groupby([jupyter_string,\"District/Sector\"])\n",
      "subDataDistrictGroup = subDataDistrictGroup.size().reset_index()\n",
      "subDataDistrictGroup.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "g = sns.FacetGrid(subDataDistrictGroup, hue=jupyter_string, size=5, aspect=1.5)\n",
      "g.map(plt.plot, jupyter_string, jupyter_string).add_legend()\n",
      "g.ax.set(xlabel=jupyter_string,\n",
      "         xticks = np.arange(1,24,1),\n",
      "         ylabel=jupyter_string,\n",
      "         title=jupyter_string)\n",
      "g.fig.autofmt_xdate()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string].hist(bins=20)\n",
      "--------------------\n",
      "df.isnull().sum()\n",
      "=====\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "--------------------\n",
      "df.fillna(df.mean(), inplace = True)\n",
      "=====\n",
      "df[jupyter_string].fillna(np.mean(df[jupyter_string]), inplace=True)\n",
      "--------------------\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "=====\n",
      "df['Loan_Amount_Term' <<unk>>].fillna(360.0, inplace=True)\n",
      "--------------------\n",
      "df[jupyter_string] = np.log(df['ApplicantIncome' <unk>] + df['CoapplicantIncome' <unk>])\n",
      "=====\n",
      "df[jupyter_string] = np.log(df['ApplicantIncome' <<unk>>] + df['CoapplicantIncome' <<unk>>])\n",
      "--------------------\n",
      "df[jupyter_string] = df['Gender' <unk>].fillna(jupyter_string)\n",
      "df[jupyter_string] = df['Married' <unk>].fillna(jupyter_string)\n",
      "df[jupyter_string] = df['Dependents' <unk>].fillna(0)\n",
      "df[jupyter_string] = df['Self_Employed' <unk>].fillna(jupyter_string)\n",
      "df[jupyter_string] = df['Credit_History' <unk>].fillna(0)\n",
      "=====\n",
      "gender_marriege = pd.crosstab(df['Gender' <<unk>>], df['Married' <<unk>>])\n",
      "--------------------\n",
      "gender_marriege = gender_marriege.div(gender_marriege.sum(1).astype(float), axis=0)\n",
      "=====\n",
      "gender_marriege.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "--------------------\n",
      "gender_dependents = pd.crosstab(df['Gender' <unk>], df['Dependents' <unk>])\n",
      "gender_dependents.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "=====\n",
      "def checkStatus(x):\n",
      "    if (x['Married' <<unk>>] == jupyter_string):\n",
      "        return jupyter_string\n",
      "    return jupyter_string\n",
      "    \n",
      "df['Gender' <<unk>>].fillna(df[df['Gender' <<unk>>].isnull()].apply(checkStatus, axis=1), inplace=True)\n",
      "--------------------\n",
      "df['Credit_History' <unk>].fillna(1, inplace=True)\n",
      "=====\n",
      "df['Credit_History' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "df['Loan_Amount_Term' <unk>].fillna(360, inplace=True)\n",
      "=====\n",
      "df['Self_Employed' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "days = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "sns.countplot(x=jupyter_string,data=seattleData,palette=jupyter_string ,order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "\n",
      "--------------------\n",
      "df['Dependents' <unk>].fillna(-1, inplace=True)\n",
      "=====\n",
      "df['Dependents' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "df['Loan_Status' <unk>] = df['Loan_Status' <unk>].map({jupyter_string:0, jupyter_string:1})\n",
      "=====\n",
      "df = pandas.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head(10)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.apply(lambda x: sum(x.isnull()), axis = 0)\n",
      "--------------------\n",
      "df[jupyter_string] = df['Gender' <unk>].apply(lambda x: 1 if x == jupyter_string else 0)\n",
      "=====\n",
      "df[jupyter_string] = df.apply(lambda x: 1 if x['Gender' <<unk>>] == jupyter_string else 0, axis=1)\n",
      "df[jupyter_string] = df.apply(lambda x: 1 if x['Gender' <<unk>>] == jupyter_string else 0, axis = 1)\n",
      "df[jupyter_string] = df.apply(lambda x: 1 if x['Gender' <<unk>>] != jupyter_string and x['Gender' <<unk>>] != jupyter_string else 0, axis = 1)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "var_mod = ['Married' <<unk>>, 'Dependents' <<unk>>, 'Education' <<unk>>, 'Self_Employed' <<unk>>,'Property_Area' <<unk>>]\n",
      "le = LabelEncoder()\n",
      "for i in var_mod:\n",
      "    df[i] = le.fit_transform(df[i].astype(str))\n",
      "    \n",
      "df['Credit_History' <<unk>>].fillna(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "df['Loan_Status' <unk>].fillna(jupyter_string, inplace=True)\n",
      "=====\n",
      "df['LoanAmount' <<unk>>].fillna(np.mean(df['LoanAmount' <<unk>>]), inplace=True)\n",
      "--------------------\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "model = AdaBoostClassifier(n_estimators=100)\n",
      "predictor_var = ['Married' <unk>, 'Credit_History' <unk>, jupyter_string]\n",
      "classification_model(model, df, predictor_var,outcome_var)\n",
      "=====\n",
      "from sklearn import grid_search\n",
      "parameters = {jupyter_string:(jupyter_string, jupyter_string), jupyter_string:[1, 10]}\n",
      "\n",
      "param_grid = { \n",
      "    jupyter_string: [200, 700],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string]\n",
      "}\n",
      "\n",
      "rf = RandomForestClassifier()\n",
      "knn = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
      "knn_params = {\n",
      "    jupyter_string : [3, 39],\n",
      "    jupyter_string: [jupyter_string, jupyter_string]\n",
      "    \n",
      "}\n",
      "\n",
      "\n",
      "model = grid_search.GridSearchCV(knn, knn_params)\n",
      "\n",
      "predictor_var = ['Married' <<unk>>, 'Credit_History' <<unk>>,  jupyter_string, jupyter_string, jupyter_string]\n",
      "classification_model(model, df,predictor_var,outcome_var)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "=====\n",
      "gender_marriege = pd.crosstab(df['Gender' <<unk>>], df['Married' <<unk>>])\n",
      "--------------------\n",
      "gender_marriege.div(gender_marriege.sum(1).astype(float), axis=0).plot(kind=jupyter_string, stacked=True)\n",
      "=====\n",
      "gender_marriege.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "--------------------\n",
      "data4.head()\n",
      "=====\n",
      "data4.head()\n",
      "--------------------\n",
      "sns.countplot(x=jupyter_string,data=seattleData,palette=jupyter_string ,order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "=====\n",
      "sns.countplot(x=jupyter_string,data=seattleData,hue=jupyter_string ,\n",
      "              order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string),\n",
      "              hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "plt.xticks(rotation=45)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "\n",
      "--------------------\n",
      "gender_marriege = pd.crosstab(df['Gender' <unk>], df['Credit_History' <unk>])\n",
      "gender_marriege.plot(kind=jupyter_string, stacked=True, color=[jupyter_string, jupyter_string], grid=False)\n",
      "=====\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df_test[['Loan_ID' <unk>, 'Loan_Status' <unk>]].to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_test[['Loan_ID' <<unk>>, 'Loan_Status' <<unk>>]].to_csv(jupyter_string, sep=jupyter_string, index=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "airports.head()\n",
      "--------------------\n",
      "airports.info()\n",
      "=====\n",
      "airlines.head()\n",
      "--------------------\n",
      "airports.info()\n",
      "=====\n",
      "routes.head()\n",
      "--------------------\n",
      "routes = routes.set_index('id' <unk>)\n",
      "routes.head()\n",
      "=====\n",
      "routes = routes.merge(airports, left_on=jupyter_string, right_index=True, how=jupyter_string)\n",
      "routes = routes.merge(airports, left_on=jupyter_string, right_index=True, how=jupyter_string, suffixes=[jupyter_string, jupyter_string])\n",
      "routes = routes.merge(airlines, left_on=jupyter_string, right_index=True, how=jupyter_string, suffixes=(jupyter_string, jupyter_string))\n",
      "--------------------\n",
      "routes.head()\n",
      "=====\n",
      "routes.head()\n",
      "--------------------\n",
      "data[jupyter_string] = data.apply(lambda row: get_distance(row.latitude, row.longitude, row.pcode_lat, row.pcode_lon), axis=1)\n",
      "=====\n",
      "routes[jupyter_string] = get_distance(routes[jupyter_string], routes[jupyter_string],\n",
      "                                  routes[jupyter_string], routes[jupyter_string])\n",
      "routes.distance.describe()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "plt.show() \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np \n",
      "import scipy as sp \n",
      "import matplotlib as mpl \n",
      "import matplotlib.cm as cm \n",
      "import matplotlib.pyplot as plt \n",
      "import pandas as pd \n",
      "\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "import seaborn.apionly as sns \n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "mtcars = pd.read_csv(jupyter_string)\n",
      "mtcars.head()\n",
      "=====\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "days = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "offenseGroupdf = seattleData.groupby('Summarized Offense Description' <<unk>>)\n",
      "offenseGroup = offenseGroupdf.size().sort_values( ascending=False).head(10)\n",
      "subDataOffense = seattleData[seattleData['Summarized Offense Description' <<unk>>].isin( offenseGroup.index)]\n",
      "sns.countplot(x=\"Summarized Offense Description\",hue=jupyter_string, hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string),\n",
      "              data=subDataOffense);\n",
      "plt.xticks(rotation=70)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "\n",
      "--------------------\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "=====\n",
      "dfcars=pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "dfcars.rename(columns={'Unnamed: 0' <unk>:jupyter_string}, inplace=True)\n",
      "dfcars.head()\n",
      "=====\n",
      "dfcars=dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
      "dfcars.head()\n",
      "--------------------\n",
      "dfcars.rename(columns={\"Unnamed: 0\":\"name\"}, inplace=True)\n",
      "dfcars.head()\n",
      "=====\n",
      "dfcars = pd.read_csv(jupyter_string)\n",
      "dfcars.head()\n",
      "--------------------\n",
      "dfcars.info()\n",
      "=====\n",
      "dfcars.info()\n",
      "--------------------\n",
      "dfcars.name = dfcars.name.astype(jupyter_string)\n",
      "dfcars.info()\n",
      "=====\n",
      "different_values = [jupyter_string, 1, 2, 3]\n",
      "different_series = pd.Series(different_values)\n",
      "different_series\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "similar_values = [2, 3, 4]\n",
      "similar_series = pd.Series(similar_values)\n",
      "similar_series\n",
      "--------------------\n",
      "dfcars.to_csv(jupyter_string)\n",
      "=====\n",
      "dfcars.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "dfcars.describe()\n",
      "--------------------\n",
      "a = np.array([1,2,3,4,5])\n",
      "b = np.array([1,2,3,4,5])\n",
      "c = np.array([1,2,3,4,5])\n",
      "=====\n",
      "my_array = np.array([1,2,3], dtype=jupyter_string)\n",
      "my_array\n",
      "--------------------\n",
      "my_array.dtype\n",
      "=====\n",
      "my_array = np.array([1,2,3,4,5], dtype=jupyter_string)\n",
      "my_array\n",
      "--------------------\n",
      "districtGroupdf = seattleData.groupby('District' <unk>)\n",
      "districtGroup = districtGroupdf.size().sort_values( ascending=False).head(10)\n",
      "subDataDistrict = seattleData[seattleData['District' <unk>].isin(districtGroup.index)]\n",
      "sns.countplot(x=\"District\",hue=jupyter_string, hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string),\n",
      "              data=subDataDistrict);\n",
      "plt.xticks(rotation=70)\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "plt.title(jupyter_string,fontsize=18)\n",
      "=====\n",
      "sns.set(style=jupyter_string, color_codes=True)\n",
      "sns.countplot(x=\"District/Sector\", data=subDataDistrict, hue=jupyter_string,hue_order=(jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string));\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "color_palette = sns.color_palette()\n",
      "color_palette\n",
      "--------------------\n",
      "sns.palplot(color_palette)\n",
      "=====\n",
      "plt.show()\n",
      "sns.palplot(color_palette)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, index_col=0)\n",
      "df.head()\n",
      "=====\n",
      "dfcars.mpg.hist()\n",
      "plt.xlabel(\"mpg\");\n",
      "--------------------\n",
      "dfcars.mpg.hist(bins=20, color=jupyter_string)\n",
      "plt.xlabel(\"mpg\");\n",
      "=====\n",
      "plt.hist(dfcars.mpg, bins=15, alpha=0.5);\n",
      "plt.xlabel(\"mpg\");\n",
      "plt.title(jupyter_string);\n",
      "--------------------\n",
      "dfcars.mpg.values\n",
      "=====\n",
      "plt.hist(dfcars.mpg.values, bins=15, alpha=0.5);\n",
      "plt.xlim(5, 40)\n",
      "plt.ylim(0, 10)\n",
      "plt.xlabel(\"mpg\");\n",
      "plt.title(jupyter_string);\n",
      "--------------------\n",
      "with sns.plotting_context(jupyter_string): \n",
      "    plt.scatter(dfcars.wt, dfcars.mpg)\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg)\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string);\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string);\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(dfcars.wt, dfcars.mpg, jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string) \n",
      "--------------------\n",
      "dfcars = dfcars[dfcars.mpg < 20]\n",
      "=====\n",
      "np.sum(dfcars.mpg < 20)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "print(df.columns)\n",
      "\n",
      "--------------------\n",
      "total = df.shape[0]\n",
      "trues = df[df['mpg' <unk>] < 20].shape[0]\n",
      "print(jupyter_string.format(trues/total))\n",
      "=====\n",
      "np.mean(dfcars.mpg < 20)\n",
      "--------------------\n",
      "np.mean(dfcars.mpg < 20)\n",
      "=====\n",
      "(dfcars.mpg < 20).mean()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, parse_dates=['DATE' <<unk>>])\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "data = data.drop(['NAME' <unk>, 'LATITUDE' <unk>, 'LONGITUDE' <unk>, 'ELEVATION' <unk>], axis=1)\n",
      "=====\n",
      "clean_data=data.drop(['NAME' <<unk>>, 'LATITUDE' <<unk>>, 'LONGITUDE' <<unk>>, 'ELEVATION' <<unk>>, 'AWND' <<unk>>, 'AWND_ATTRIBUTES' madeupword0002, 'PRCP_ATTRIBUTES' <<unk>>,  'TAVG_ATTRIBUTES' <<unk>>, 'TMAX_ATTRIBUTES' <<unk>>, 'TMIN_ATTRIBUTES' <<unk>>], axis=1)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "ppd = pd.read_csv(jupyter_string)\n",
      "ppd_disc = pd.read_csv(jupyter_string)\n",
      "\n",
      "--------------------\n",
      "ppd.head()\n",
      "=====\n",
      "ppd.head()\n",
      "--------------------\n",
      "df = pd.merge(left=ppd, right=ppd_disc, how=jupyter_string, left_on=jupyter_string, right_on=jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "ppd = ppd.merge(ppd_disc, how=jupyter_string, on=\"cap_number\")\n",
      "--------------------\n",
      "ppd.head()\n",
      "=====\n",
      "ppd.head()\n",
      "--------------------\n",
      "ppd.describe()\n",
      "=====\n",
      "ppd[\"investigative_findings\"].value_counts()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.columns.to_series().groupby(df.dtypes).groups\n",
      "\n",
      "df['chas' <<unk>>] = pd.to_numeric(df['chas' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "df['dva_chasa' <<unk>>] = pd.to_numeric(df['dva_chasa' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "df['noch' madeupword0002] = pd.to_numeric(df['noch' madeupword0002], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['ves' <<unk>>] = pd.to_numeric(df['ves' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['rost' <<unk>>] = pd.to_numeric(df['rost' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['vozrast' <<unk>>] = pd.to_numeric(df['vozrast' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df['razmer' <<unk>>] = pd.to_numeric(df['razmer' <<unk>>], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "--------------------\n",
      "sns.countplot(x=\"investigative_findings\", data=ppd)\n",
      "=====\n",
      "ppd[\"disciplinary_findings\"].value_counts()\n",
      "--------------------\n",
      "ppd[jupyter_string].value_counts()\n",
      "=====\n",
      "ppd[\"allegations_investigated\"].value_counts()\n",
      "--------------------\n",
      "ppd[\"airport_code\"].value_counts()\n",
      "=====\n",
      "ppd[\"dist_occurrence\"].value_counts()\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "sns.countplot(x=\"dist_occurrence\", data=ppd, palette=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "guiltyfindings = ppd[ppd[\"disciplinary_findings\"] == jupyter_string]\n",
      "print(guiltyfindings)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "tcfindings = ppd[ppd[\"disciplinary_findings\"] == jupyter_string]\n",
      "print(tcfindings.head())\n",
      "--------------------\n",
      "tcfindings = tcfindings.reset_index(drop=True)\n",
      "print(tcfindings.head())\n",
      "=====\n",
      "tcfindings[\"cap_number\"].value_counts()\n",
      "--------------------\n",
      "tcfindings = tcfindings[tcfindings[\"cap_number\"] > 1]\n",
      "print(tcfindings.head())\n",
      "tcfindings[\"cap_number\"].value_counts()\n",
      "=====\n",
      "tcfindings[\"general_cap_classification\"].value_counts()\n",
      "--------------------\n",
      "tcfindings[jupyter_string] = tcfindings[\"general_cap_classification\"].apply(lambda x: 1 if x == jupyter_string else 0)\n",
      "tcfindings.head()\n",
      "=====\n",
      "tcfindings[\"allegations_investigated\"].value_counts()\n",
      "--------------------\n",
      "tcfindings[tcfindings[\"general_cap_classification\"] == jupyter_string][\"allegations_investigated\"].value_counts()\n",
      "=====\n",
      "tcfindings[\"po_sex\"].value_counts()\n",
      "--------------------\n",
      "complaints_by_officer = complaints.groupby(\"officer\").size().reset_index(name=jupyter_string)\n",
      "complaints_by_officer.head()\n",
      "=====\n",
      "ppd[jupyter_string] = ppd[[\"po_initials\",\"po_race\",\"po_sex\",\"dist_occurrence\"]].apply(lambda x: jupyter_string.join(x.astype(str)), axis=1)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "print(df.columns)\n",
      "df.columns.to_series().groupby(df.dtypes).groups\n",
      "=====\n",
      "back = df['vpopu' <<unk>>].copy()\n",
      "df = df.assign(backdoor=back)\n",
      "\n",
      "df['vpopu' <<unk>>][df['vpopu' <<unk>>] > 1] = 1\n",
      "\n",
      "\n",
      "    \n",
      "df[jupyter_string][0 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string][1 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df.chas.value_counts(normalize=False, sort=True,\n",
      "                         ascending=False, bins=None, dropna=True)\n",
      "\n",
      "df.vpopu\n",
      "print(df.head())\n",
      "--------------------\n",
      "ppd[jupyter_string] = ppd[[\"po_initials\",\"po_race\",\"po_sex\"]].apply(lambda x: jupyter_string.join(x.astype(str)), axis=1)\n",
      "=====\n",
      "ppd[jupyter_string].value_counts().head(25)\n",
      "--------------------\n",
      "ppd[jupyter_string].value_counts().tail(25)\n",
      "=====\n",
      "ppd[~ppd[jupyter_string].isin([jupyter_string])].value_counts().head(25)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head(3)\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "sns.distplot(df.temperature, bins=25)\n",
      "--------------------\n",
      "df.temperature.describe()\n",
      "=====\n",
      "print (jupyter_string).format(df.temperature.size)\n",
      "--------------------\n",
      "temp_mean = df.temperature.mean()\n",
      "temp_mean\n",
      "=====\n",
      "sample_mean =df.temperature.mean()\n",
      "sample_std =df.temperature.std()\n",
      "noise =(sample_std/ np.sqrt(130))\n",
      "z_score =(sample_mean-98.6)/noise\n",
      "print (jupyter_string)% z_score\n",
      "--------------------\n",
      "male_temp = df.temperature[df.gender == jupyter_string]\n",
      "female_temp = df.temperature[df.gender == jupyter_string]\n",
      "=====\n",
      "df_male = df[df.gender==jupyter_string]\n",
      "df_female = df[df.gender==jupyter_string]\n",
      "print (jupyter_string) % df_male.temperature.mean()\n",
      "print (jupyter_string) % df_female.temperature.mean()\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "binwidth = 0.025\n",
      "\n",
      "print (data.steering_angle[0:100])\n",
      "\n",
      "mysteer=data.steering_angle\n",
      "mysteer1 =mysteer[1:]\n",
      "\n",
      "\n",
      "steer_floats = [float(x) for x in mysteer1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "plt.hist(steer_floats,bins=np.arange(min(steer_floats), max(steer_floats) + binwidth, binwidth))\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams[jupyter_string] = (16, 9)\n",
      "plt.rcParams\n",
      "=====\n",
      "def brightness_shift(img, bright_value=None):\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
      "    \n",
      "    if bright_value:\n",
      "        img[:,:,2] += bright_value\n",
      "    else:\n",
      "        random_bright = .25 + np.random.uniform()\n",
      "        img[:,:,2] = img[:,:,2] * random_bright\n",
      "    \n",
      "    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
      "    return img\n",
      "\n",
      "\n",
      "img_in = cv2.imread(jupyter_string)\n",
      "\n",
      "img = brightness_shift(img_in, 200)\n",
      "\n",
      "pts1 = np.float32([[0,60],[0,120],[300,60],[300,120]])\n",
      "\n",
      "pts2 = np.float32([[0,120],[0,180],[300,120],[300,180]])\n",
      "\n",
      "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
      "dst = cv2.warpPerspective(img,M,(320,160))\n",
      "\n",
      "\n",
      "pts1 = np.float32([[0,60],[0,120],[300,60],[300,120]])\n",
      "\n",
      "pts2 = np.float32([[0,0],[0,60],[300,0],[300,60]])\n",
      "M2 = cv2.getPerspectiveTransform(pts1,pts2)\n",
      "dst2 = cv2.warpPerspective(img,M2,(320,160))\n",
      "\n",
      "\n",
      " \n",
      "plt.subplots(figsize=(24, 96))\n",
      "\n",
      "plt.subplot(141),plt.imshow(img_in),plt.title(jupyter_string)\n",
      "plt.subplot(142),plt.imshow(img),plt.title(jupyter_string)\n",
      "plt.subplot(143),plt.imshow(dst),plt.title(jupyter_string)\n",
      "plt.subplot(144),plt.imshow(dst2),plt.title(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm_notebook\n",
      "import time\n",
      "import shutil\n",
      "import os\n",
      "import random\n",
      "import cv2\n",
      "import math\n",
      "import json\n",
      "import seaborn as sns \n",
      "import tensorflow as tf\n",
      "\n",
      "import keras\n",
      "from keras.preprocessing.image import *\n",
      "from keras.models import Sequential, Model\n",
      "from keras.layers import Convolution2D, Flatten, MaxPooling2D, Lambda, ELU\n",
      "from keras.layers.core import Dense, Dropout, Activation\n",
      "from keras.optimizers import Adam\n",
      "from keras.callbacks import Callback\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.regularizers import l2\n",
      "\n",
      "from IPython.display import display \n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "seed = 42\n",
      "np.random.seed(seed)\n",
      "tf.set_random_seed(seed)\n",
      "tf.reset_default_graph()\n",
      "\n",
      "columns = ['center' madeupword0002, 'left' <<unk>>, 'right' <<unk>>, jupyter_string, 'throttle' <<unk>>, 'brake' <<unk>>, 'speed' <<unk>>]\n",
      "data = pd.read_csv(jupyter_string, names=columns)\n",
      "\n",
      "print(jupyter_string, columns, jupyter_string)\n",
      "print(jupyter_string, data.shape, jupyter_string)\n",
      "print(data.describe(), jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "--------------------\n",
      "back = df['vpopu' <unk>].copy()\n",
      "df = df.assign(backdoor=back)\n",
      "\n",
      "df['vpopu' <unk>][df['vpopu' <unk>] > 1] = 1\n",
      "\n",
      "\n",
      "    \n",
      "df[jupyter_string][0 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string][1 == df[jupyter_string]] = jupyter_string\n",
      "df[jupyter_string] = pd.to_numeric(df[jupyter_string], downcast=jupyter_string, errors=jupyter_string)\n",
      "\n",
      "df.chas.value_counts(normalize=False, sort=True,\n",
      "                         ascending=False, bins=None, dropna=True)\n",
      "\n",
      "df.vpopu\n",
      "=====\n",
      "plt.show()\n",
      "corrmat2 = df.corr()\n",
      "\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "sns.heatmap(corrmat2,linewidths=.1, annot=True, annot_kws={jupyter_string: 7})\n",
      "\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import seaborn as sns \n",
      "sns.set(color_codes=True)\n",
      "\n",
      "\n",
      "plt.show() \n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.to_csv(jupyter_string)\n",
      "=====\n",
      "df2 = pd.DataFrame({jupyter_string:range(10), jupyter_string: np.random.randint(10, 100, size=10)})\n",
      "--------------------\n",
      "df.head() \n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.tail()\n",
      "=====\n",
      "df.tail()\n",
      "--------------------\n",
      "df[jupyter_string] = df['Col One' <unk>] - df['Col Two' <unk>]\n",
      "df.head()\n",
      "=====\n",
      "df[jupyter_string] = df['Col One' <<unk>>] + jupyter_string +  df['Names' madeupword0002]\n",
      "df[jupyter_string] = df['Temp' <<unk>>].astype(int)\n",
      "df\n",
      "--------------------\n",
      "df.max()\n",
      "=====\n",
      "df['Temp' <<unk>>].min(), df[jupyter_string].max()\n",
      "--------------------\n",
      "df[jupyter_string].nunique()\n",
      "=====\n",
      "df['Date' <<unk>>].unique()\n",
      "\n",
      "--------------------\n",
      "df['Date' <unk>].value_counts()\n",
      "=====\n",
      "df['Date' <<unk>>].value_counts()\n",
      "--------------------\n",
      "df.rename(columns={'Unnamed: 0' <unk>: jupyter_string}, inplace=True)\n",
      "=====\n",
      "df.rename(columns={'Col One' <<unk>>: jupyter_string}, inplace = True)\n",
      "df\n",
      "--------------------\n",
      "corrmat = df.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "sns.heatmap(corrmat,linewidths=.1, annot=True, annot_kws={jupyter_string: 7})\n",
      "=====\n",
      "data = (df.chas.dropna())\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 12)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "ax.set_xticks(np.arange(0,31000,1000))\n",
      "\n",
      "ax.xaxis.labelpad = 20\n",
      "ax.yaxis.labelpad = 20\n",
      "\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "--------------------\n",
      "df.drop('Col One' <unk>, axis=1)\n",
      "=====\n",
      "df.drop('Names' madeupword0002, axis=1)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "neworder  = ['Time' <<unk>>, 'Date' <<unk>>, jupyter_string, 'Temp' <<unk>>, 'Names' madeupword0002, jupyter_string]\n",
      "df = df[neworder]\n",
      "df\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df.set_index('Names' madeupword0002)\n",
      "\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, index_col=jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "pd.read_csv(jupyter_string, index_col=['Names' madeupword0002])\n",
      "\n",
      "--------------------\n",
      "df[jupyter_string].head()\n",
      "=====\n",
      "df[jupyter_string].head() \n",
      "--------------------\n",
      "df.drop([jupyter_string, jupyter_string], axis=1, inplace=True)\n",
      "=====\n",
      "df.drop(['Time' <<unk>>, 'Date' <<unk>>], axis=1, inplace=True)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "pd.to_datetime(df[jupyter_string], format=jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string] = pd.to_datetime(df[jupyter_string])\n",
      "=====\n",
      "df[jupyter_string] = pd.to_datetime(df[jupyter_string])\n",
      "--------------------\n",
      "df.set_index(jupyter_string, inplace=True)\n",
      "df.head()\n",
      "=====\n",
      "df.set_index(jupyter_string, inplace=True)\n",
      "--------------------\n",
      "df[jupyter_string].resample(jupyter_string).mean()\n",
      "=====\n",
      "df.resample(jupyter_string)['Temp' <<unk>>].max()\n",
      "--------------------\n",
      "data = (df.price.dropna())\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 12)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "ax.set_xticks(np.arange(0,31000,1000))\n",
      "\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data = (df.chas.dropna()*0.018)\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 14)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.set_xticks(np.arange(0,550,50))\n",
      "ax.set_yticks(np.arange(0,550,50))\n",
      "ax.xaxis.labelpad = 20\n",
      "ax.yaxis.labelpad = 20\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, color = jupyter_string, edgecolor=jupyter_string, linewidth=1.2)\n",
      "plt.xticks(rotation=jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "--------------------\n",
      "df.resample(jupyter_string)['Temp' <unk>].mean()\n",
      "=====\n",
      "df.resample(jupyter_string)['Temp' <<unk>>].mean()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "=====\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "--------------------\n",
      "plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "=====\n",
      "pandas_plot = df.plot(jupyter_string, jupyter_string, color=jupyter_string)\n",
      "plt.title(jupyter_string);\n",
      "\n",
      "--------------------\n",
      "sns_plot = sns.regplot(x=jupyter_string, y=jupyter_string, data=df)\n",
      "plt.title(jupyter_string);\n",
      "=====\n",
      "seaborn_plot = sns.FacetGrid(df,  size=3, aspect= 2) \n",
      "seaborn_plot.map(plt.scatter, jupyter_string, jupyter_string)\n",
      "seaborn_plot.map(plt.plot, jupyter_string, jupyter_string)\n",
      "sns.plt.title(jupyter_string);\n",
      "--------------------\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "=====\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.close() \n",
      "--------------------\n",
      "mpl_plot = plt.plot(df.x, df.y, jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "fig = pandas_plot.get_figure()\n",
      "fig.savefig (jupyter_string)\n",
      "--------------------\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "seaborn_plot.savefig(jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 1], alpha=opacity, label=jupyter_string)\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 2], alpha=opacity, label=jupyter_string)\n",
      "plt.hist(titanic_df.Fare[titanic_df.Pclass == 3], alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "from scipy.stats import ttest_ind\n",
      "fare_from_q = titanic_data[titanic_data.Embarked == jupyter_string]\n",
      "fare_from_c = titanic_data[titanic_data.Embarked == jupyter_string]\n",
      "t_stat, p_value = ttest_ind(fare_from_q.Fare, fare_from_c.Fare)\n",
      "\n",
      "print(jupyter_string % (t_stat, p_value))\n",
      "--------------------\n",
      "t_stat, p_value = ttest_ind(fare_from_q.Fare, fare_from_c.Fare)\n",
      "\n",
      "print(jupyter_string % (t_stat, p_value))\n",
      "=====\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(fare_from_q.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.hist(fare_from_c.Fare, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head()\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "titanic_data = pd.read_csv(jupyter_string)\n",
      "titanic_data.head(5)\n",
      "--------------------\n",
      "data = (df.usd.dropna()*0.018)\n",
      "n, bins = np.histogram(data, 100)\n",
      "\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string, labelsize = 14)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.set_xticks(np.arange(0, 550,50))\n",
      "ax.set_yticks(np.arange(0, 550,50))\n",
      "ax.xaxis.labelpad = 20\n",
      "ax.yaxis.labelpad = 20\n",
      "ax.set_xlabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "ax.set_ylabel(jupyter_string, color = jupyter_string, fontsize = 14)\n",
      "plt.hist(data, bins=jupyter_string, color = jupyter_string,\n",
      "=====\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "ax.tick_params(axis=jupyter_string, colors=jupyter_string)\n",
      "\n",
      "sns.distplot( df.chas.dropna(), color=jupyter_string, label=jupyter_string)\n",
      "sns.plt.legend()\n",
      "\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data.info()\n",
      "--------------------\n",
      "titanic_data['Age' <unk>] = titanic_data['Age' <unk>].fillna(titanic_data['Age' <unk>].mean())\n",
      "=====\n",
      "titanic_data.Age = titanic_data.Age.fillna(np.mean(titanic_data.Age))\n",
      "--------------------\n",
      "titanic_data.info()\n",
      "=====\n",
      "titanic_data.info()\n",
      "--------------------\n",
      "titanic_data['Survived' <unk>].value_counts()\n",
      "=====\n",
      "survivors = titanic_data[titanic_data.Survived == 1]\n",
      "survivor_prob = (len(survivors) / len(titanic_data))\n",
      "print(jupyter_string + str(survivor_prob) + jupyter_string)\n",
      "--------------------\n",
      "men = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "men_prob = (len(men) / len(titanic_data))\n",
      "print(jupyter_string + str(men_prob) + jupyter_string)\n",
      "=====\n",
      "male_passenger = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "prob_male = (len(male_passenger) / len(titanic_data))\n",
      "print(jupyter_string + str(prob_male) + jupyter_string)\n",
      "--------------------\n",
      "herbourg_passenger = titanic_data[titanic_data.Sex == jupyter_string]\n",
      "prob_herbourg = (len(herbourg_passenger) / len(titanic_data))\n",
      "print(jupyter_string + str(prob_herbourg) + jupyter_string)\n",
      "=====\n",
      "c_port = survivors[survivors.Embarked == jupyter_string]\n",
      "prob_c = (len(c_port) / len(survivors))\n",
      "print(jupyter_string + str(prob_c) + jupyter_string)\n",
      "--------------------\n",
      "plt.hist(survivors.Age, bins = 25)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "all_ages = []\n",
      "age_mean = np.mean(titanic_data.Age)\n",
      "\n",
      "for i, k in enumerate(titanic_data.Age): \n",
      "    if round(k, 3) != round(age_mean, 3):\n",
      "        all_ages.append(k)\n",
      "\n",
      "H, edges = np.histogram(all_ages, bins=25)\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "ax.bar(edges[:-1], H / float(sum(H)), width=edges[1] - edges[0])\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.minorticks_on()\n",
      "plt.show()\n",
      "--------------------\n",
      "male_survivors = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]\n",
      "female_survivors = titanic_df[titanic_df['Sex' madeupword0002] == jupyter_string]\n",
      "=====\n",
      "from scipy.stats import ttest_ind\n",
      "\n",
      "survivors_male = survivors[(survivors.Sex == jupyter_string) & (round(survivors.Age,3) != round(age_mean, 3)) ]\n",
      "survivors_female = survivors[(survivors.Sex == jupyter_string) & (round(survivors.Age, 3) != round(age_mean, 3))]\n",
      "t_stat, p_value = ttest_ind(survivors_male.Age, survivors_female.Age)\n",
      "\n",
      "print(jupyter_string % (t_stat, p_value))\n",
      "--------------------\n",
      "sns.distplot(survivors_male.Age)\n",
      "sns.distplot(survivors_female.Age)\n",
      "=====\n",
      "plt.figure(figsize=(10, 4))\n",
      "opacity = 0.5\n",
      "\n",
      "plt.hist(survivors_male.Age, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.hist(survivors_female.Age, bins=np.arange(0, 90, 5), alpha=opacity, label=jupyter_string)\n",
      "plt.legend()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.io import show, output_notebook\n",
      "\n",
      "output_notebook()\n",
      "\n",
      "p = figure(plot_width=400, plot_height=400)\n",
      "\n",
      "\n",
      "p.circle([1, 2, 3, 4, 5], [6, 7, 2, 4, 5], size=20, color=jupyter_string, alpha=0.5)\n",
      "\n",
      "\n",
      "show(p)\n",
      "=====\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.io import show, output_notebook\n",
      "\n",
      "output_notebook()\n",
      "\n",
      "p = figure(plot_width=400, plot_height=400)\n",
      "\n",
      "\n",
      "p.square([1, 2, 3, 4, 5], [6, 7, 2, 4, 5], size=20, color=jupyter_string, alpha=0.5)\n",
      "\n",
      "\n",
      "show(p)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "leitor = pd.read_csv(jupyter_string)\n",
      "leitor.head()\n",
      "\n",
      "leitor.plot(kind = jupyter_string, x = [\"Year\"], y = [\"Biology\"], title = jupyter_string, color = jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(women_degrees['Year' <<unk>>], \n",
      "         women_degrees['Biology' <<unk>>], c=jupyter_string, label=jupyter_string)\n",
      "plt.plot(women_degrees['Year' <<unk>>], \n",
      "         100-women_degrees['Biology' <<unk>>], c=jupyter_string, label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "data4.info()\n",
      "=====\n",
      "y=data4['sale_price' madeupword0089]\n",
      "X=data4['gross_sq_feet' <<unk>>]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "from selenium.webdriver.support.ui import Select\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "driver=webdriver.Chrome()\n",
      "driver.get(jupyter_string)\n",
      "--------------------\n",
      "tow_trucks = pd.read_csv(jupyter_string)\n",
      "tow_trucks.head()\n",
      "=====\n",
      "df_add = pd.read_csv(jupyter_string)\n",
      "df_add\n",
      "--------------------\n",
      "df_add.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "pd.set_option(jupyter_string, -1)\n",
      "def get_url(row):\n",
      "    return jupyter_string.format(row[jupyter_string])\n",
      "df_add[jupyter_string] = df_add.apply(get_url, axis=1)\n",
      "df_add\n",
      "--------------------\n",
      "search_button = driver.find_element_by_id(jupyter_string)\n",
      "search_button.click()\n",
      "=====\n",
      "business_name = driver.find_elements_by_tag_name(jupyter_string)[5].text\n",
      "business_name.split(jupyter_string)\n",
      "\n",
      "\n",
      "owner = driver.find_elements_by_tag_name(jupyter_string)[7].text\n",
      "owner\n",
      "\n",
      "\n",
      "phone = driver.find_elements_by_tag_name(jupyter_string)[9].text\n",
      "phone\n",
      "\n",
      "\n",
      "status = driver.find_elements_by_tag_name(jupyter_string)[12].text\n",
      "status\n",
      "\n",
      "\n",
      "address_info = driver.find_elements_by_tag_name(jupyter_string)[14].text\n",
      "address = address_info.split(jupyter_string)[5:7]\n",
      "address\n",
      "--------------------\n",
      "df_add.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "df_add = df_add.apply(get_towing_info, axis=1).join(df)\n",
      "df_add\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df\n",
      "--------------------\n",
      "pd.set_option(jupyter_string, -1)\n",
      "=====\n",
      "pd.set_option(jupyter_string, -1)\n",
      "def get_url(row):\n",
      "    return jupyter_string.format(row[jupyter_string])\n",
      "df[jupyter_string] = df.apply(get_url, axis=1)\n",
      "df.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "def get_towing_info(row):\n",
      "    \n",
      "    \n",
      "    url = row[jupyter_string]\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        business_name = driver.find_elements_by_tag_name(jupyter_string)[5].text\n",
      "        business = business_name.split(jupyter_string)[1]\n",
      "        print(business)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        owner = driver.find_elements_by_tag_name(jupyter_string)[7].text\n",
      "        print(owner)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        phone = driver.find_elements_by_tag_name(jupyter_string)[9].text\n",
      "        print(phone)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        status = driver.find_elements_by_tag_name(jupyter_string)[12].text\n",
      "        print(status)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "    \n",
      "    try:\n",
      "        address_info = driver.find_elements_by_tag_name(jupyter_string)[14].text\n",
      "        address = address_info.split(jupyter_string)[5:7]\n",
      "        print(address)\n",
      "    except:\n",
      "        print(jupyter_string)\n",
      "\n",
      "df.apply(get_towing_info, axis=1)\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: x.split(jupyter_string)[0])\n",
      "df[jupyter_string] = df[jupyter_string].apply(lambda x: x.split(jupyter_string)[1])\n",
      "df.head()\n",
      "=====\n",
      "new_df = df.apply(get_towing_info, axis=1).join(df)\n",
      "new_df\n",
      "--------------------\n",
      "new_df.to_csv(jupyter_string)\n",
      "=====\n",
      "new_df.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "y=data4['sale_price' <unk>]\n",
      "X=data4['gross_sq_feet' <unk>]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "=====\n",
      "data4[jupyter_string]=np.log(data4['sale_price' madeupword0089]).round(decimals=3)\n",
      "data4[jupyter_string]=np.log(data4['gross_sq_feet' <<unk>>]).round(decimals=3)\n",
      "y=data4[jupyter_string]\n",
      "X=data4[jupyter_string]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "--------------------\n",
      "X = data.drop([jupyter_string], axis = 1)\n",
      "y = data[jupyter_string]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
      "=====\n",
      "Y1 = pd.DataFrame(data[jupyter_string]) \n",
      "Y2 = pd.DataFrame(data[jupyter_string]) \n",
      "Y3 = pd.DataFrame.join(Y1, Y2) \n",
      "\n",
      "\n",
      "X1 = data[col_names[2:]] \n",
      "X2 = data[col_names[5:]] \n",
      "X3 = data[col_names[2:5]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rs = 42\n",
      "\n",
      "ts1 = 0.3\n",
      "\n",
      "X_1_train, X_1_test, Y_1_train, Y_1_test = train_test_split(X1, Y1, test_size = ts1, random_state = rs)\n",
      "X_2_train, X_2_test, Y_2_train, Y_2_test = train_test_split(X2, Y2, test_size = ts1, random_state = rs)\n",
      "X_3_train, X_3_test, Y_3_train, Y_3_test = train_test_split(X3, Y3, test_size = ts1, random_state = rs)\n",
      "\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "lr = LinearRegression()\n",
      "lr.fit(X_1_train, Y_1_train)\n",
      "Y_1_pred = lr.predict(X_1_test)\n",
      "\n",
      "lr.fit(X_2_train, Y_2_train)\n",
      "Y_2_pred = lr.predict(X_2_test)\n",
      "\n",
      "lr.fit(X_3_train, Y_3_train)\n",
      "Y_3_pred = lr.predict(X_3_test)\n",
      "=====\n",
      "reg = linear_model.LinearRegression()\n",
      "reg1 = reg.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred1 = reg1.predict(X_1_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg1.coef_.T, index = X1.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % reg1.score(X_1_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,1], jupyter_string)\n",
      "--------------------\n",
      "reg = linear_model.LinearRegression()\n",
      "reg1 = reg.fit(X_2_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred1 = reg1.predict(X_2_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg1.coef_.T, index = X2.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred1))\n",
      "\n",
      "print(jupyter_string % reg.score(X_2_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred1[:,1],\n",
      "=====\n",
      "reg2 = linear_model.LinearRegression()\n",
      "reg2.fit(X_2_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred2 = reg2.predict(X_2_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg2.coef_.T, index = X2.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred2))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred2))\n",
      "\n",
      "print(jupyter_string % reg2.score(X_2_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred2[:,0],jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred2[:,1], jupyter_string)\n",
      "--------------------\n",
      "reg3 = linear_model.LinearRegression()\n",
      "reg3.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred3 = reg3.predict(X_3_test)\n",
      "\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % reg3.score(X_3_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,0],jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,1], jupyter_string)\n",
      "=====\n",
      "reg3 = linear_model.LinearRegression()\n",
      "reg3.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "y_pred3 = reg3.predict(X_3_test)\n",
      "\n",
      "\n",
      "print(jupyter_string, pd.DataFrame(reg3.coef_.T, index = X3.columns , columns = Y3.columns))\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % r2_score(Y_3_test, y_pred3))\n",
      "\n",
      "print(jupyter_string % reg3.score(X_3_test, Y_3_test))\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,0],jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred3[:,1], jupyter_string)\n",
      "\n",
      "--------------------\n",
      "reg4 = linear_model.LinearRegression()\n",
      "reg4.fit(X1_train, Y1_train)\n",
      "\n",
      "\n",
      "y_pred4 = reg4.predict(X1_test)\n",
      "\n",
      "\n",
      "print(jupyter_string\n",
      "      % mean_squared_error(Y1_test, y_pred4))\n",
      "\n",
      "print(jupyter_string % r2_score(Y1_test, y_pred4))\n",
      "\n",
      "print(jupyter_string % reg4.score(X1_test, Y1_test))\n",
      "\n",
      "\n",
      "plotSummary(Y1_test[jupyter_string], y_pred4[:,0],jupyter_string)\n",
      "plotSummary(Y1_test[jupyter_string], y_pred4[:,1], jupyter_string)\n",
      "=====\n",
      "model1 = sm.OLS(Y_1_train, X_1_train)\n",
      "results1 = model1.fit()\n",
      "\n",
      "\n",
      "predictions1 = results1.predict(X_1_test) \n",
      "\n",
      "print(results1.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions1,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model2 = sm.OLS(Y_2_train, X_2_train)\n",
      "results2 = model2.fit()\n",
      "\n",
      "\n",
      "predictions2 = results2.predict(X_2_test) \n",
      "\n",
      "print(results2.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions2,jupyter_string)\n",
      "=====\n",
      "model1a = sm.OLS(Y_2_train, X_1_train)\n",
      "results1a = model1a.fit()\n",
      "\n",
      "\n",
      "predictions1a = results1a.predict(X_1_test) \n",
      "\n",
      "print(results1a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions1a,jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string, results1a.resid.abs().sum())\n",
      "print(jupyter_string , results1a.ssr)\n",
      "print(jupyter_string, results1a.rsquared)\n",
      "\n",
      "print(jupyter_string, np.sqrt(mean_squared_error(Y_2_test, predictions1a)))\n",
      "print(jupyter_string, np.sqrt(mean_squared_error(Y_2_train, results1a.predict(X_1_train))))\n",
      "--------------------\n",
      "X2 = sm.add_constant(X2)\n",
      "model2 = sm.OLS(Y2, X2).fit()\n",
      "model2.summary()\n",
      "=====\n",
      "model2 = sm.OLS(Y_1_train, X_2_train)\n",
      "results2 = model2.fit()\n",
      "\n",
      "\n",
      "predictions2 = results2.predict(X_2_test) \n",
      "\n",
      "print(results2.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions2,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model3 = sm.OLS(Y_2_train, X_2_train)\n",
      "results3 = model3.fit()\n",
      "\n",
      "\n",
      "predictions3 = results3.predict(X_2_test) \n",
      "\n",
      "print(results3.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions3,jupyter_string)\n",
      "=====\n",
      "model2a = sm.OLS(Y_2_train, X_2_train)\n",
      "results2a = model2a.fit()\n",
      "\n",
      "\n",
      "predictions2a = results2a.predict(X_2_test) \n",
      "\n",
      "print(results2a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions2a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model3a = sm.OLS(Y_3_train, X_3_train)\n",
      "results3a = model3a.fit()\n",
      "\n",
      "\n",
      "predictions3a = results3a.predict(X_3_test) \n",
      "\n",
      "print(results3a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], predictions3a,jupyter_string)\n",
      "=====\n",
      "model3 = sm.OLS(Y_1_train, X_3_train)\n",
      "results3 = model3.fit()\n",
      "\n",
      "\n",
      "predictions3 = results3.predict(X_3_test) \n",
      "\n",
      "print(results3.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions3,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model4 = sm.OLS(Y_2_train, X_3_train)\n",
      "results4 = model4.fit()\n",
      "\n",
      "\n",
      "predictions4 = results4.predict(X_3_test) \n",
      "\n",
      "print(results4.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4,jupyter_string)\n",
      "=====\n",
      "model3a = sm.OLS(Y_2_train, X_3_train)\n",
      "results3a = model3a.fit()\n",
      "\n",
      "\n",
      "predictions3a = results3a.predict(X_3_test) \n",
      "\n",
      "print(results3a.summary())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions3a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "y=data4[jupyter_string]\n",
      "X=data4[jupyter_string]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "=====\n",
      "data4[jupyter_string]=np.log(data4['mean' madeupword0184]).round(decimals=3)\n",
      "y=data4[jupyter_string]\n",
      "X=data4[[jupyter_string,jupyter_string]]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "--------------------\n",
      "model3b = sm.GLM(Y_1_train, X_1_train)\n",
      "results3b = model3b.fit()\n",
      "\n",
      "\n",
      "predictions3b = results3b.predict(X_1_test) \n",
      "\n",
      "print(results3b.summary())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions3b,jupyter_string)\n",
      "=====\n",
      "model4 = sm.GLM(Y_1_train,X_1_train)\n",
      "results4 = model4.fit()\n",
      "\n",
      "\n",
      "predictions4 = results4.predict(X_1_test)\n",
      "\n",
      "print(results4.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions4,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model5 = sm.GLM(Y_2_train,X_2_train)\n",
      "results5 = model5.fit()\n",
      "\n",
      "\n",
      "predictions5 = results5.predict(X_2_test)\n",
      "\n",
      "print(results5.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5,jupyter_string)\n",
      "=====\n",
      "model4a = sm.GLM(Y_2_train,X_1_train)\n",
      "results4a = model4a.fit()\n",
      "\n",
      "\n",
      "predictions4a = results4a.predict(X_1_test)\n",
      "\n",
      "print(results4a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4a,jupyter_string)\n",
      "\n",
      "--------------------\n",
      "model4b = sm.GLM(Y_2_train,X_2_train)\n",
      "results4b = model4b.fit()\n",
      "\n",
      "\n",
      "predictions4b = results4b.predict(X_2_test)\n",
      "\n",
      "print(results4b.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions4b,jupyter_string)\n",
      "=====\n",
      "model5 = sm.GLM(Y_1_train,X_2_train)\n",
      "results5 = model5.fit()\n",
      "\n",
      "\n",
      "predictions5 = results5.predict(X_2_test)\n",
      "\n",
      "print(results5.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions5,jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "model6 = sm.GLM(Y_2_train,X_2_train)\n",
      "results6 = model6.fit()\n",
      "\n",
      "\n",
      "predictions6 = results6.predict(X_2_test)\n",
      "\n",
      "print(results6.summary2())\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions6,jupyter_string)\n",
      "=====\n",
      "model5a = sm.GLM(Y_2_train,X_2_train)\n",
      "results5a = model5a.fit()\n",
      "\n",
      "\n",
      "predictions5a = results5a.predict(X_2_test)\n",
      "\n",
      "print(results5a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5a,jupyter_string)\n",
      "\n",
      "\n",
      "--------------------\n",
      "model5b = sm.GLM(Y_2_train,X_2_train)\n",
      "results5b = model5b.fit()\n",
      "\n",
      "\n",
      "predictions5b = results5b.predict(X_2_test)\n",
      "\n",
      "print(results5b.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions5b,jupyter_string)\n",
      "=====\n",
      "model6a = sm.GLM(Y_1_train,X_3_train)\n",
      "results6a = model6a.fit()\n",
      "\n",
      "\n",
      "predictions6a = results6a.predict(X_3_test)\n",
      "\n",
      "print(results6a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_1_test[jupyter_string], predictions6a,jupyter_string)\n",
      "--------------------\n",
      "model7a = sm.GLM(Y_2_train,X_2_train)\n",
      "results7a = model7a.fit()\n",
      "\n",
      "\n",
      "predictions7a = results7a.predict(X_2_test)\n",
      "\n",
      "print(results7a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions7a,jupyter_string)\n",
      "=====\n",
      "model7a = sm.GLM(Y_2_train,X_3_train)\n",
      "results7a = model7a.fit()\n",
      "\n",
      "\n",
      "predictions7a = results7a.predict(X_3_test)\n",
      "\n",
      "print(results7a.summary2())\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], predictions7a,jupyter_string)\n",
      "--------------------\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics\n",
      "=====\n",
      "regr_1 = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_2 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "regr_1.fit(X_1_train, Y_3_train)\n",
      "regr_2.fit(X_1_train, Y_3_train)\n",
      "regr_3.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data1 = pd.DataFrame(regr_1.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data1[jupyter_string] = pd.Series(regr_2.feature_importances_.T)\n",
      "data1[jupyter_string] = pd.Series(regr_3.feature_importances_.T)\n",
      "data1.index = X_1_train.columns\n",
      "test1 = np.array([regr_1.max_depth, regr_2.max_depth, regr_3.max_depth]).reshape(1,3)\n",
      "model_info = pd.DataFrame(test1, columns = data1.columns, index = [jupyter_string])\n",
      "data1 = data1.append(model_info)\n",
      "print(data1)\n",
      "\n",
      "\n",
      "\n",
      "y_1 = regr_1.predict(X_1_test)\n",
      "y_2 = regr_2.predict(X_1_test)\n",
      "y_3 = regr_3.predict(X_1_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_1[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_2[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_2[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_3[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_3[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_1.score(X_1_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_2.score(X_1_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_3.score(X_1_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X1, Y3, cv=10))\n",
      "--------------------\n",
      "regr_1 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_2 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "regr_1.fit(X_1_train, Y_3_train)\n",
      "regr_2.fit(X_1_train, Y_3_train)\n",
      "regr_3.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data1 = pd.DataFrame(regr_1.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data1[jupyter_string] = pd.Series(regr_2.feature_importances_.T)\n",
      "data1[jupyter_string] = p\n",
      "=====\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "--------------------\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "=====\n",
      "regr_1a = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_2a = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3a = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "new_index = data1[data1[jupyter_string]>0.01].drop(jupyter_string, axis = 0).index\n",
      "X_1a_train = X_1_train[new_index]\n",
      "X_1a_test = X_1_test[new_index]\n",
      "X1a = X1[new_index]\n",
      "\n",
      "\n",
      "regr_1a.fit(X_1a_train, Y_3_train)\n",
      "regr_2a.fit(X_1a_train, Y_3_train)\n",
      "regr_3a.fit(X_1a_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data1a = pd.DataFrame(regr_1a.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data1a[jupyter_string] = pd.Series(regr_2a.feature_importances_.T)\n",
      "data1a[jupyter_string] = pd.Series(regr_3a.feature_importances_.T)\n",
      "data1a.index = X_1a_train.columns\n",
      "test1a = np.array([regr_1a.max_depth, regr_2a.max_depth, regr_3a.max_depth]).reshape(1,3)\n",
      "model_infoa = pd.DataFrame(test1a, columns = data1a.columns, index = [jupyter_string])\n",
      "data1a = data1a.append(model_infoa)\n",
      "print(data1a)\n",
      "\n",
      "\n",
      "\n",
      "y_1a = regr_1a.predict(X_1a_test)\n",
      "y_2a = regr_2a.predict(X_1a_test)\n",
      "y_3a = regr_3a.predict(X_1a_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_1a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_1a[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_2a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_2a[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_3a[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_3a[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_1a.score(X_1a_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_2a.score(X_1a_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_3a.score(X_1a_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X1a, Y3, cv=10))\n",
      "--------------------\n",
      "regr_1b = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_2b = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_3b = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "new_index = data2[data2[jupyter_string]>0.01].drop(jupyter_string, axis = 0).index\n",
      "X_1b_train = X_1_train[new_index]\n",
      "X_1b_test = X_1_test[new_index]\n",
      "X2 = X2[new_index]\n",
      "\n",
      "\n",
      "regr_1b.fit(X_1b_train, Y_3_train)\n",
      "regr_2b.fit(X_1b_train, Y_3_train)\n",
      "=====\n",
      "regr_4 = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_5 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_6 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "\n",
      "regr_4.fit(X_2_train, Y_3_train)\n",
      "regr_5.fit(X_2_train, Y_3_train)\n",
      "regr_6.fit(X_2_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data2 = pd.DataFrame(regr_4.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data2[jupyter_string] = pd.Series(regr_5.feature_importances_.T)\n",
      "data2[jupyter_string] = pd.Series(regr_6.feature_importances_.T)\n",
      "data2.index = X_2_train.columns\n",
      "test2 = np.array([regr_4.max_depth, regr_5.max_depth, regr_6.max_depth]).reshape(1,3)\n",
      "model_info2 = pd.DataFrame(test2, columns = data2.columns, index = [jupyter_string])\n",
      "data2 = data2.append(model_info2)\n",
      "print(data2)\n",
      "\n",
      "\n",
      "\n",
      "y_4 = regr_4.predict(X_2_test)\n",
      "y_5 = regr_5.predict(X_2_test)\n",
      "y_6 = regr_6.predict(X_2_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_4[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_4[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_5[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_5[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_6[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_6[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_4.score(X_2_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_5.score(X_2_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_6.score(X_2_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X2, Y3, cv=10))\n",
      "--------------------\n",
      "data4[jupyter_string]=np.sqrt(data4['mean' <unk>])\n",
      "y=data4[jupyter_string]\n",
      "X=data4[[jupyter_string,jupyter_string]]\n",
      "result=ols(y=y,x=X)\n",
      "print(jupyter_string)\n",
      "print(result.r2)\n",
      "result.summary_as_matrix.T\n",
      "=====\n",
      "list_comp=data4.columns[data4.columns.get_loc(\"Adopt A Basket\"):data4.columns.get_loc(\"X Ray Machine Equipment\")+1]\n",
      "list_comp=list(list_comp.values.tolist())\n",
      "\n",
      "\n",
      "Number_variables_L=[]\n",
      "OLS_R_2_L=[]\n",
      "depend_variable_L=[jupyter_string]\n",
      "t=0\n",
      "\n",
      "for j in ([jupyter_string]+[list_comp[i] for i in range(len(list_comp))]):\n",
      "    \n",
      "    t=t+1\n",
      "    Number_variables_L.append(t)\n",
      "    depend_variable_L.append(j)\n",
      "    X=data4[depend_variable_L]\n",
      "    y=data4[jupyter_string]\n",
      "    result=ols(y=y,x=X)\n",
      "    OLS_R_2_L.append(result.r2)\n",
      "    \n",
      "\n",
      "\n",
      "pylab.title(jupyter_string)\n",
      "pylab.plot(Number_variables_L,OLS_R_2_L,jupyter_string,label=jupyter_string)\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.xlabel(jupyter_string)\n",
      "pylab.ylabel(jupyter_string)\n",
      "pylab.show()\n",
      "\n",
      "\n",
      "--------------------\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_1a_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_1a_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "=====\n",
      "rmse_test = []\n",
      "R2_test = []\n",
      "for i in range(1,60):\n",
      "    \n",
      "    regressor_test = DecisionTreeRegressor(max_depth=i, min_samples_leaf=5)\n",
      "    regressor_test = regressor_test.fit(X_2_train, Y_3_train)\n",
      "    true = Y_3_test\n",
      "    predicted = regressor_test.predict(X_2_test)\n",
      "\n",
      "    rmse_test.append(np.sqrt(mean_squared_error(true, predicted)))\n",
      "    R2_test.append(r2_score(true, predicted))\n",
      "    \n",
      "\n",
      "index1 = [i for i in range(1,60)]\n",
      "plt.plot(index1, rmse_test)\n",
      "\n",
      "figure()\n",
      "plt.plot(index1, R2_test)\n",
      "--------------------\n",
      "regr_4 = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_5 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_6 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "\n",
      "regr_4.fit(X_3_train, Y_3_train)\n",
      "regr_5.fit(X_3_train, Y_3_train)\n",
      "regr_6.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data3 = pd.DataFrame(regr_4.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data3[jupyter_string] = pd.Series(regr_5.feature_importances_.T)\n",
      "data3[jupyter_string] =\n",
      "=====\n",
      "regr_7 = DecisionTreeRegressor(max_depth=10, min_samples_leaf=5)\n",
      "regr_8 = DecisionTreeRegressor(max_depth=20, min_samples_leaf=5)\n",
      "regr_9 = DecisionTreeRegressor(max_depth=30, min_samples_leaf=5)\n",
      "\n",
      "\n",
      "\n",
      "regr_7.fit(X_3_train, Y_3_train)\n",
      "regr_8.fit(X_3_train, Y_3_train)\n",
      "regr_9.fit(X_3_train, Y_3_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data3 = pd.DataFrame(regr_7.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data3[jupyter_string] = pd.Series(regr_8.feature_importances_.T)\n",
      "data3[jupyter_string] = pd.Series(regr_9.feature_importances_.T)\n",
      "data3.index = X_3_train.columns\n",
      "test3 = np.array([regr_7.max_depth, regr_8.max_depth, regr_9.max_depth]).reshape(1,3)\n",
      "model_info3 = pd.DataFrame(test3, columns = data3.columns, index = [jupyter_string])\n",
      "data3 = data3.append(model_info3)\n",
      "print(data3)\n",
      "\n",
      "\n",
      "\n",
      "y_7 = regr_7.predict(X_3_test)\n",
      "y_8 = regr_8.predict(X_3_test)\n",
      "y_9 = regr_9.predict(X_3_test)\n",
      "\n",
      "\n",
      "s = 25\n",
      "plotSummary(Y_3_test[jupyter_string], y_7[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_7[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_8[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_8[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_9[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_9[:,1], jupyter_string)\n",
      "print(jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string, regr_7.score(X_3_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_8.score(X_3_test,Y_3_test), jupyter_string)\n",
      "print(jupyter_string, regr_9.score(X_3_test,Y_3_test),jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "regressor = DecisionTreeRegressor(random_state=0,max_depth=30, min_samples_leaf=5)\n",
      "print( jupyter_string, regressor.max_depth, jupyter_string, cross_val_score(regressor, X3, Y3, cv=10))\n",
      "--------------------\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "regr_7 = RandomForestRegressor(n_estimators=10)\n",
      "regr_8 = RandomForestRegressor(n_estimators=20)\n",
      "regr_9 = RandomForestRegressor(n_estimators=30)\n",
      "\n",
      "\n",
      "\n",
      "regr_7.fit(X_7_train, Y_7_train)\n",
      "regr_8.fit(X_8_train, Y_8_train)\n",
      "regr_9.fit(X_9_train, Y_9_train)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data7 = pd.DataFrame(regr_7.feature_importances_.T, columns = [jupyter_string]) \n",
      "\n",
      "data7[jupyter_string] = pd.Series(regr_8.feature_importances_.T)\n",
      "data7[jupyter_string] = pd.Series(regr\n",
      "=====\n",
      "rs = 42\n",
      "regr_rf = RandomForestRegressor(max_depth = 6,  random_state=rs)\n",
      "regr_rf2 = RandomForestRegressor(max_depth = 8,  random_state=rs)\n",
      "regr_rf3 = RandomForestRegressor(max_depth = 10,  random_state=rs)\n",
      "regr_rf.fit(X_1_train, Y_3_train)\n",
      "regr_rf2.fit(X_1_train, Y_3_train)\n",
      "regr_rf3.fit(X_1_train, Y_3_train)\n",
      "\n",
      "\n",
      "predict_rf = regr_rf.predict(X_1_test)\n",
      "predict_rf2 = regr_rf2.predict(X_1_test)\n",
      "predict_rf3 = regr_rf3.predict(X_1_test)\n",
      "\n",
      "\n",
      "\n",
      "importance_rf_6 = pd.Series(regr_rf.feature_importances_, index= X_1_train.columns)\n",
      "importance_rf_8 = pd.Series(regr_rf2.feature_importances_, index= X_1_train.columns)\n",
      "importance_rf_10 = pd.Series(regr_rf3.feature_importances_, index = X_1_train.columns)\n",
      "importance_rf = pd.concat([importance_rf_6, importance_rf_8, importance_rf_10], axis = 1)\n",
      "importance_rf.columns = [jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "print(jupyter_string )\n",
      "print(importance_rf)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string , regr_rf.score(X_1_test, Y_3_test), jupyter_string)\n",
      "print(jupyter_string , regr_rf2.score(X_1_test, Y_3_test), jupyter_string)\n",
      "print(jupyter_string , regr_rf3.score(X_1_test, Y_3_test), jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf2[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf2[:,1], jupyter_string)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf3[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], predict_rf3[:,1], jupyter_string)\n",
      "--------------------\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble\n",
      "=====\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "\n",
      "rng = 42\n",
      "\n",
      "md = 10\n",
      "nest1 = 15\n",
      "nest2 = 50\n",
      "\n",
      "\n",
      "regr_1_AB = AdaBoostRegressor(DecisionTreeRegressor(max_depth = md), n_estimators=nest2, random_state=rng)\n",
      "regr_1_ABR = AdaBoostRegressor(DecisionTreeRegressor(max_depth = md),n_estimators=nest2, random_state=rng)\n",
      "\n",
      "model_AB = regr_1_AB.fit(X_1_train, Y_3_train[jupyter_string])\n",
      "model_AB_R = regr_1_ABR.fit(X_1_train, Y_3_train[jupyter_string])\n",
      "\n",
      "y_pred_AB1 = model_AB.predict(X_1_test)\n",
      "y_pred_AB1_R = model_AB_R.predict(X_1_test)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred_AB1, jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], y_pred_AB1_R, jupyter_string)\n",
      "\n",
      "print(jupyter_string, model_AB.score(X_1_test, Y_3_test[jupyter_string]))\n",
      "print(jupyter_string, model_AB_R.score(X_1_test, Y_3_test[jupyter_string]))\n",
      "\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import LSTM\n",
      "from keras.layers import Dropout\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import Convolution1D\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.convolutional import MaxPooling1D\n",
      "=====\n",
      "scaler = StandardScaler()  \n",
      "scaler.fit(X_1_train)\n",
      "X_1_traina = scaler.transform(X_1_train)  \n",
      "X_1_testa = scaler.transform(X_1_test)\n",
      "\n",
      "clf = MLPRegressor(solver=jupyter_string, alpha=1e-5,  random_state=1)\n",
      "clf = clf.fit(X_1_traina, Y_3_train)\n",
      "\n",
      "pred_nn_1 = clf.predict(X_1_testa)\n",
      "\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_1[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_1[:,1], jupyter_string)\n",
      "clf.loss_\n",
      "clf.hidden_layer_sizes\n",
      "\n",
      "print(jupyter_string, r2_score(Y_3_test, pred_nn_1))\n",
      "print(jupyter_string, r2_score(Y_3_test[jupyter_string], pred_nn_1[:,0]))\n",
      "print(jupyter_string, r2_score(Y_3_test[jupyter_string], pred_nn_1[:,1]))\n",
      "--------------------\n",
      "scaler = StandardScaler()  \n",
      "scaler.fit(X_2_train)\n",
      "X_2_traina = scaler.transform(X_2_train)  \n",
      "X_2_testa = scaler.transform(X_2_test)\n",
      "\n",
      "clf = MLPRegressor(solver=jupyter_string, alpha=1e-5,  random_state=1)\n",
      "clf = clf.fit(X_2_traina, Y_2_train)\n",
      "\n",
      "pred_nn_2 = clf.predict(X_2_testa)\n",
      "\n",
      "plotSummary(Y_2_test[jupyter_string], pred_nn_2[:,0], jupyter_string)\n",
      "plotSummary(Y_2_test[jupyter_string], pred_nn_2[:,1], jupyter_string)\n",
      "=====\n",
      "layer1 = 20\n",
      "layer2 = 20\n",
      "layer3 = 10\n",
      "clf2 = MLPRegressor(hidden_layer_sizes=(layer1,layer2, layer3), max_iter=10, alpha=1e-4, solver=jupyter_string, verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
      "\n",
      "\n",
      "clf2 = clf2.fit(X_1_traina, Y_3_train)\n",
      "print(jupyter_string % clf2.score(X_1_traina, Y_3_train))\n",
      "print(jupyter_string % clf2.score(X_1_testa, Y_3_test))\n",
      "\n",
      "print(jupyter_string, clf2.loss_)\n",
      "print(jupyter_string, clf2.n_iter_)\n",
      "print(jupyter_string, clf2.n_layers_)\n",
      "print(jupyter_string)\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "summary_nn1 = pd.DataFrame(clf2.coefs_[0], index = X_1_train.columns, columns = [(jupyter_string+ str(i)) for i in range(1,layer1+1)])\n",
      "summary_nn1a = pd.DataFrame(clf2.coefs_[1], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[1].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[1].shape[1]+1)])\n",
      "summary_nn1b = pd.DataFrame(clf2.coefs_[2], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[2].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[2].shape[1]+1)])\n",
      "summary_nn1c = pd.DataFrame(clf2.coefs_[3], index = [(jupyter_string + str(i)) for i in range(1,clf2.coefs_[3].shape[0]+1)], columns = [(jupyter_string+ str(i)) for i in range(1,clf2.coefs_[3].shape[1]+1)])\n",
      "\n",
      "pred_nn_2 = clf2.predict(X_1_testa)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_2[:,0], jupyter_string)\n",
      "plotSummary(Y_3_test[jupyter_string], pred_nn_2[:,1], jupyter_string)\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "print(summary_nn1)\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1a)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1b)\n",
      "\n",
      "print(jupyter_string)\n",
      "print(jupyter_string)\n",
      "print(summary_nn1c)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "countries = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "countries.head()\n",
      "=====\n",
      "countries.head(3)\n",
      "--------------------\n",
      "countries.tail(3)\n",
      "=====\n",
      "countries.tail(3)\n",
      "--------------------\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "ridge = Ridge(alpha=0.1)\n",
      "ridge.fit(X_train, y_train)\n",
      "y_pred = ridge.predict(X_test)\n",
      "print(jupyter_string, ridge.score(X_test, y_test))\n",
      "print(jupyter_string, mean_squared_error(y_test, y_pred))\n",
      "print(jupyter_string, r2_score(y_test, y_pred))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=1) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=1)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "--------------------\n",
      "countries.info()\n",
      "=====\n",
      "countries.describe()\n",
      "--------------------\n",
      "countries.tail(3)\n",
      "=====\n",
      "countries.tail(3)\n",
      "--------------------\n",
      "countries.info()\n",
      "=====\n",
      "countries['continent' madeupword0002].value_counts()\n",
      "--------------------\n",
      "countries['population' <unk>].mean()\n",
      "=====\n",
      "countries['population' <<unk>>].mean()\n",
      "--------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries.describe()\n",
      "--------------------\n",
      "countries.describe()\n",
      "=====\n",
      "countries['fertility' <<unk>>].cumsum()\n",
      "--------------------\n",
      "countries['fertility' <unk>].mean()\n",
      "=====\n",
      "countries.groupby('continent' madeupword0002)['population' <<unk>>].sum()\n",
      "--------------------\n",
      "countries.groupby('continent' <unk>)['life' <unk>].mean()\n",
      "=====\n",
      "countries.sort_values(by=['continent' madeupword0002, 'fertility' <<unk>>])\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "\n",
      "train_df = pd.read_csv(jupyter_string)\n",
      "test_df = pd.read_csv(jupyter_string)\n",
      "submission = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_df.landmark_id.value_counts().head(100)\n",
      "=====\n",
      "rank_number = 100 \n",
      "sampling_rate = 0.02 \n",
      "random_state = 17 \n",
      "\n",
      "landmarks=train_df.groupby(by='landmark_id' <<unk>>).count().loc[:,'id' <<unk>>]\n",
      "l = landmarks.sort_values(ascending=False)\n",
      "\n",
      "\n",
      "lmks = pd.concat([l, l/l.sum(), l.cumsum()/l.sum()], axis=1, ignore_index=True)\n",
      "lmks.columns=[jupyter_string, jupyter_string, jupyter_string]\n",
      "ranked = lmks[0:rank_number]\n",
      "\n",
      "train_ordered = train_df[train_df.landmark_id.isin(ranked.index)]\n",
      "sample_gby = train_ordered.groupby(by='landmark_id' <<unk>>).apply(lambda x: x.sample(frac=sampling_rate, random_state=random_state))\n",
      "sample_idx = sample_gby.index.levels[1]\n",
      "train_sample = train_df.iloc[sample_idx, :]\n",
      "\n",
      "\n",
      "train_sample.to_csv(jupyter_string, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
      "train_sample_df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=1) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=1)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS\n",
      "=====\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "train_df = pd.read_csv(jupyter_string)\n",
      "train_df.head()\n",
      "=====\n",
      "train_sample.head()\n",
      "--------------------\n",
      "df['landmark_id' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize = (14, 6))\n",
      "g = sns.countplot(x=\"landmark_id\", data=train_sample)\n",
      "g.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize = (14, 6))\n",
      "g = sns.countplot(x=\"landmark_id\", data=train_sample)\n",
      "g.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "=====\n",
      "train_sample.nunique()\n",
      "--------------------\n",
      "landmark_dist = train_sample['landmark_id' <unk>].value_counts() / train_sample.shape[0]\n",
      "plt.figure(figsize = (12, 5))\n",
      "plt.bar(range(100), landmark_dist)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "import cv2\n",
      "\n",
      "img = cv2.imread(jupyter_string)\n",
      "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
      "plt.imshow(cv_rgb)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize = (12, 5))\n",
      "fig = plt.bar(range(100), landmark_dist)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "img = cv2.imread(jupyter_string)\n",
      "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
      "plt.imshow(cv_rgb)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
      "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
      "=====\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "\n",
      "X = train_sample_df['id' <<unk>>]\n",
      "y = train_sample_df['landmark_id' <<unk>>]\n",
      "\n",
      "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=10)\n",
      "\n",
      "for train_id, test_id in sss.split(X, y):\n",
      "    X_train, X_tmp = X.iloc[train_id], X.iloc[test_id]\n",
      "    y_train, y_tmp = y.iloc[train_id], y.iloc[test_id]\n",
      "\n",
      "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=10)\n",
      "\n",
      "for train_id, test_id in sss1.split(X_tmp, y_tmp):\n",
      "    X_valid, X_test = X_tmp.iloc[train_id], X_tmp.iloc[test_id]\n",
      "    y_valid, y_test = y_tmp.iloc[train_id], y_tmp.iloc[test_id]\n",
      "--------------------\n",
      "train_sample_df['landmark_id' <unk>].value_counts()\n",
      "=====\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_train)\n",
      "h.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_test)\n",
      "h.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize = (14, 6))\n",
      "h = sns.countplot(x=y_test)\n",
      "h.set_title(jupyter_string, fontweight=jupyter_string, fontsize=12)\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import random\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "prob_id = np.array([(y_test == id).sum() / len(y_test) for id in y_test]) \n",
      "seed = [3, 10, 27, 31, 48, 55, 67, 95, 105, 117]\n",
      "\n",
      "expected_val_df = pd.DataFrame(columns=[jupyter_string, jupyter_string])\n",
      "\n",
      "for i in seed:\n",
      "    \n",
      "    random.seed(i)\n",
      "    randsample = random.sample(range(len(y_test)), 20)\n",
      "    \n",
      "    prob_id_montecarlo = prob_id[randsample]  \n",
      "    expected_val_df.loc[len(expected_val_df)] = expected_acc(prob_id_montecarlo)\n",
      "--------------------\n",
      "plt.imshow(train_tensors[0])\n",
      "=====\n",
      "plt.imshow(train_tensors[2])\n",
      "--------------------\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_r_optimal)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var\n",
      "=====\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_l_optimal=Regularization_fit_lambda(2,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "model = Sequential()\n",
      "model.add(Conv2D(32, (3, 3), activation=jupyter_string, input_shape=(32, 32, 3)))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(64, (3, 3), activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(128, (3, 3), activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D((2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten())\n",
      "=====\n",
      "input_shape = img_shape + (3,)\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(filters=16, kernel_size=4, padding=jupyter_string, activation=jupyter_string, input_shape=input_shape))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=32, kernel_size=3, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=64, kernel_size=3, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=128, kernel_size=2, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Conv2D(filters=256, kernel_size=2, padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "model.add(Flatten())\n",
      "model.add(Dense(1024, activation=jupyter_string))\n",
      "model.add(Dropout(0.3))\n",
      "model.add(Dense(512, activation=jupyter_string))\n",
      "model.add(Dropout(0.3))\n",
      "model.add(Dense(100, activation=jupyter_string))\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(optimizer=jupyter_string, loss=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "history = model.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=len(train_generator),\n",
      "    epochs=20,\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=len(validation_generator),\n",
      "    verbose=1\n",
      ")\n",
      "=====\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "epochs = 10\n",
      "\n",
      "checkpointer = ModelCheckpoint(filepath=jupyter_string, \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "hist = model.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "--------------------\n",
      "plt.plot(hist.history[jupyter_string])\n",
      "plt.plot(hist.history[jupyter_string])\n",
      "plt.legend([jupyter_string, jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "\n",
      "train_datagen = ImageDataGenerator(\n",
      "    rotation_range=40,\n",
      "    width_shift_range=0.2,\n",
      "    height_shift_range=0.2,\n",
      "    shear_range=0.2,\n",
      "    zoom_range=0.2,\n",
      "    horizontal_flip=True,\n",
      "    fill_mode=jupyter_string\n",
      ")\n",
      "\n",
      "train_generator = train_datagen.flow(train_tensors, train_target, batch_size=32)\n",
      "=====\n",
      "train_datagen = ImageDataGenerator(\n",
      "    rotation_range=45, \n",
      "    width_shift_range=0.2, \n",
      "    height_shift_range=0.2, \n",
      "    zoom_range=0.3)\n",
      "\n",
      "valid_datagen = ImageDataGenerator()\n",
      "\n",
      "train_generator = train_datagen.flow(train_tensors, train_target, batch_size= 128)\n",
      "valid_generator = valid_datagen.flow(valid_tensors, valid_target, batch_size= 128)\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Activation, Dropout, Flatten\n",
      "from keras.layers import Convolution2D, MaxPooling2D\n",
      "from keras.optimizers import SGD\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Convolution2D(32, 3, 3, border_mode=jupyter_string, border_mode=jupyter_string, input_shape=(32, 32, 3)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(32, 3, 3))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Convolution2D(64, 3, 3, border_mode=jupyter_string, border_mode=jupyter_string))\n",
      "\n",
      "=====\n",
      "epochs_aug = 50\n",
      "\n",
      "hist_aug = model.fit_generator(train_generator, steps_per_epoch=6480//128, epochs=epochs_aug,\n",
      "                    validation_data=valid_generator, validation_steps=805//128,\n",
      "                    callbacks=[checkpointer], verbose=1)\n",
      "--------------------\n",
      "plt.plot(hist_aug.history[jupyter_string])\n",
      "plt.plot(hist_aug.history[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.legend([jupyter_string, jupyter_string], loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "landmark_prob = [np.amax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "model.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "landmark_prob = [np.amax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "=====\n",
      "sort_id = np.argsort(landmark_prob)[::-1]\n",
      "landmark_pred_sorted = [landmark_pred[sid] for sid in sort_id]\n",
      "test_target_sorted = [test_target[sid] for sid in sort_id]\n",
      "--------------------\n",
      "model = Sequential()\n",
      "model.add(Conv2D(32, (3, 3), padding=jupyter_string, activation=jupyter_string, input_shape=input_shape))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(64, (3, 3), padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(128, (3, 3), padding=jupyter_string, activation=jupyter_string))\n",
      "model.add(BatchNormalization())\n",
      "model.add(MaxPooling2D(pool_size=(2, 2\n",
      "=====\n",
      "modelBN = Sequential()\n",
      "\n",
      "modelBN.add(Conv2D(filters=16, kernel_size=4, padding=jupyter_string, use_bias=False, input_shape=input_shape))\n",
      "modelBN.add(Activation(jupyter_string))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=32, kernel_size=3, padding=jupyter_string, use_bias=False))\n",
      "modelBN.add(Activation(jupyter_string))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=64, kernel_size=3, padding=jupyter_string, use_bias=False))\n",
      "modelBN.add(Activation(jupyter_string))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Conv2D(filters=128, kernel_size=2, padding=jupyter_string, use_bias=False))\n",
      "modelBN.add(Activation(jupyter_string))\n",
      "modelBN.add(BatchNormalization())\n",
      "modelBN.add(MaxPooling2D(pool_size=2))\n",
      "\n",
      "modelBN.add(Flatten())\n",
      "modelBN.add(Dense(512, activation=jupyter_string))\n",
      "modelBN.add(Dropout(0.3))\n",
      "modelBN.add(Dense(100, activation=jupyter_string))\n",
      "\n",
      "modelBN.summary()\n",
      "--------------------\n",
      "modelBN.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "adam = optimizers.Adam(lr=0.02)\n",
      "modelBN.compile(optimizer=jupyter_string, loss=jupyter_string, metrics=[jupyter_string])\n",
      "--------------------\n",
      "lambdas = np.linspace(-15,1,100)\n",
      "lambdas=[math.exp(i) for i in lambdas]\n",
      "lambda_l_optimal=Regularization_fit_lambda(3,X_train,y_train,lambdas,p=0.4,Graph=True,logl=True)\n",
      "print(jupyter_string.format(lambda_l_optimal))\n",
      "=====\n",
      "Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal) \n",
      "Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_l_optimal)\n",
      "\n",
      "X_train=np.matrix(data_train[depend_variable])\n",
      "y_train=np.array(data_train[jupyter_string])\n",
      "X_test=np.matrix(data_test[depend_variable])\n",
      "y_test=np.array(data_test[jupyter_string])\n",
      "Ridge.fit(X_train,y_train)\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Ridge.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "\n",
      "Ridge_coef=Ridge.coef_\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "    \n",
      "\n",
      "p_OS=Ridge.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_IS_Ridge))\n",
      "print(jupyter_string.format(R_2_OS_Ridge))\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "--------------------\n",
      "historyBN = modelBN.fit_generator(\n",
      "    train_generator,\n",
      "    steps_per_epoch=nb_train_samples // batch_size,\n",
      "    epochs=nb_epochs,\n",
      "    validation_data=validation_generator,\n",
      "    validation_steps=nb_validation_samples // batch_size\n",
      ")\n",
      "=====\n",
      "checkpointer = ModelCheckpoint(filepath=jupyter_string, \n",
      "                               verbose=1, save_best_only=True)\n",
      "\n",
      "epochs_batch = 15\n",
      "\n",
      "hist_BN = modelBN.fit(train_tensors, train_target, \n",
      "          validation_data=(valid_tensors, valid_target),\n",
      "          epochs=epochs_batch, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
      "--------------------\n",
      "plt.plot(hist_BN.history[jupyter_string])\n",
      "plt.plot(hist_BN.history[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.legend([jupyter_string, jupyter_string], loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "modelBN.load_weights(jupyter_string)\n",
      "\n",
      "landmark_pred = [np.argmax(modelBN.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
      "test_accuracy = np.sum(np.array(landmark_pred) == np.argmax(test_target, axis=1)) / len(landmark_pred)\n",
      "print(jupyter_string, test_accuracy)\n",
      "--------------------\n",
      "plt.plot(hist_BaseCNN[jupyter_string], hist_BaseCNN[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "hist_BaseCNN_df = pd.DataFrame(hist_BaseCNN)\n",
      "hist_BaseCNN_df.columns = [jupyter_string, jupyter_string, jupyter_string,jupyter_string]\n",
      "hist_BaseCNN_df\n",
      "--------------------\n",
      "plt.plot(hist_BaseCNN_df[jupyter_string], hist_BaseCNN_df[jupyter_string])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "ax1 = hist_BaseCNN_df.plot(marker=jupyter_string)\n",
      "ax1.set_xlabel(jupyter_string)\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "plt.savefig(jupyter_string)\n",
      "--------------------\n",
      "titanic = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "titanic = pd.read_csv(jupyter_string)      \n",
      "char_cabin = titanic[\"Cabin\"].astype(str)    \n",
      "new_Cabin = np.array([cabin[0] for cabin in char_cabin]) \n",
      "titanic[\"Cabin\"] = pd.Categorical(new_Cabin)  \n",
      "--------------------\n",
      "pd.crosstab(titanic[\"Survived\"], titanic[\"Pclass\"])\n",
      "=====\n",
      "my_tab = pd.crosstab(index=titanic[\"Survived\"],     \n",
      "                              columns=jupyter_string)      \n",
      "\n",
      "my_tab\n",
      "--------------------\n",
      "my_tab = pd.crosstab(index=titanic[\"Survived\"],     \n",
      "                              columns=jupyter_string)      \n",
      "\n",
      "my_tab\n",
      "=====\n",
      "pd.crosstab(index=titanic[\"Pclass\"],        \n",
      "                      columns=jupyter_string)      \n",
      "--------------------\n",
      "pd.crosstab(index=titanic[\"Sex\"],        \n",
      "                      columns=jupyter_string)      \n",
      "=====\n",
      "pd.crosstab(index=titanic[\"Sex\"],        \n",
      "                      columns=jupyter_string)      \n",
      "--------------------\n",
      "pd.crosstab(index=titanic[\"Embarked\"],        \n",
      "                      columns=jupyter_string)      \n",
      "=====\n",
      "cabin_tab = pd.crosstab(index=titanic[\"Cabin\"],        \n",
      "                        columns=jupyter_string)               \n",
      "\n",
      "cabin_tab \n",
      "--------------------\n",
      "pd.crosstab(titanic_df.Pclass, titanic_df.Survived)\n",
      "=====\n",
      "survived_sex = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                           columns=titanic[\"Sex\"])\n",
      "\n",
      "survived_sex.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_sex\n",
      "--------------------\n",
      "plt.plot(lambdas,R_2_IS_Ridge,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_IS_Lasso,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_OS_Ridge,label=jupyter_string)\n",
      "plt.plot(lambdas,R_2_OS_Lasso,label=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "Number_variables=[]\n",
      "OLS_R_2_IS=[] \n",
      "Ridge_R_2_IS=[]\n",
      "Lasso_R_2_IS=[]\n",
      "OLS_R_2_OS=[] \n",
      "Ridge_R_2_OS=[]\n",
      "Lasso_R_2_OS=[]\n",
      "depend_variable=[jupyter_string,jupyter_string]\n",
      "t=0\n",
      "\n",
      "\n",
      "for j in ([list_comp[i] for i in range(var)]):\n",
      "\n",
      "    t+=1\n",
      "    Number_variables.append(t)\n",
      "\n",
      "    depend_variable.append(j)\n",
      "\n",
      "\n",
      "    X=data_train[depend_variable]\n",
      "    y=data_train[jupyter_string]\n",
      "    result=ols(y=y,x=X)\n",
      "    OLS_R_2_IS.append(result.r2)\n",
      "    \n",
      "    X=np.matrix(data_train[depend_variable])\n",
      "    y=np.array(data_train[jupyter_string])\n",
      "    Ridge=linear_model.Ridge(fit_intercept=True,alpha=lambda_r_optimal)\n",
      "    Lasso=linear_model.Lasso(fit_intercept=True,alpha=lambda_l_optimal)\n",
      "    Ridge.fit(X,y)\n",
      "    Lasso.fit(X,y)\n",
      "    p_IS=Ridge.predict(X)\n",
      "    err_IS=p_IS-y\n",
      "    R_2_IS_Ridge=1-np.var(err_IS)/np.var(y)\n",
      "    p_IS=Lasso.predict(X)\n",
      "    err_IS=p_IS-y\n",
      "    R_2_IS_Lasso=1-np.var(err_IS)/np.var(y)\n",
      "    Ridge_R_2_IS.append(R_2_IS_Ridge)\n",
      "    Lasso_R_2_IS.append(R_2_IS_Lasso)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    temp=data_test[depend_variable]\n",
      "\n",
      "    a=np.array(temp)\n",
      "    b=np.array(result.beta)\n",
      "    c=np.sum(a*b[0:-1],axis=1)+b[-1]\n",
      "    \n",
      "    X_test=np.matrix(data_test[depend_variable])\n",
      "    y_test=np.array(data_test[jupyter_string])\n",
      "\n",
      "    error=data_test[jupyter_string]-c\n",
      "    R_2=1-error.var()/data_test[jupyter_string].var()\n",
      "    if R_2>0:\n",
      "        OLS_R_2_OS.append(R_2)\n",
      "    else:\n",
      "        OLS_R_2_OS.append(0)\n",
      "        \n",
      "    p_OS=Ridge.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
      "\n",
      "    p_OS=Lasso.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test) \n",
      "    Ridge_R_2_OS.append(R_2_OS_Ridge)\n",
      "    Lasso_R_2_OS.append(R_2_OS_Lasso)\n",
      "\n",
      "\n",
      "pylab.title(jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,OLS_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Ridge_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Ridge_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Lasso_R_2_IS,jupyter_string,label=jupyter_string)\n",
      "pylab.plot(Number_variables,Lasso_R_2_OS,jupyter_string,label=jupyter_string)\n",
      "pylab.legend(loc=jupyter_string)\n",
      "pylab.xlabel(jupyter_string)\n",
      "pylab.ylabel(jupyter_string)\n",
      "pylab.draw()\n",
      "--------------------\n",
      "survived_sex.div(survived_sex.sum(1).astype(float), axis=0)\n",
      "=====\n",
      "survived_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                            columns=titanic[\"Pclass\"])\n",
      "\n",
      "survived_class.columns = [jupyter_string,jupyter_string,jupyter_string]\n",
      "survived_class.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_class\n",
      "--------------------\n",
      "survived_sex = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                           columns=titanic[\"Sex\"], margins=True)\n",
      "\n",
      "survived_sex.index= [jupyter_string,jupyter_string]\n",
      "\n",
      "survived_sex\n",
      "=====\n",
      "survived_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                            columns=titanic[\"Pclass\"],\n",
      "                             margins=True)   \n",
      "\n",
      "survived_class.columns = [jupyter_string,jupyter_string,jupyter_string,jupyter_string]\n",
      "survived_class.index= [jupyter_string,jupyter_string,jupyter_string]\n",
      "\n",
      "survived_class\n",
      "--------------------\n",
      "survived_class.div(survived_class.ix[jupyter_string], axis=0)\n",
      "=====\n",
      "survived_class.div(survived_class[jupyter_string],\n",
      "                   axis=jupyter_string)\n",
      "--------------------\n",
      "survival = pd.crosstab(index=titanic_df.survived, columns=titanic_df.sex)\n",
      "survival\n",
      "=====\n",
      "surv_sex_class = pd.crosstab(index=titanic[\"Survived\"], \n",
      "                             columns=[titanic[\"Pclass\"],\n",
      "                                      titanic[\"Sex\"]],\n",
      "                             margins=True)   \n",
      "\n",
      "surv_sex_class\n",
      "--------------------\n",
      "X_test, y_test = problem.get_test_data()\n",
      "=====\n",
      "train_is, test_is = list(problem.get_cv(X_train, y_train))[0]\n",
      "print(len(train_is), len(test_is))\n",
      "--------------------\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "from ipywidgets import interact, interactive, fixed, interact_manual\n",
      "import ipywidgets as widgets\n",
      "\n",
      "=====\n",
      "years = np.arange(850, 2005)\n",
      "def plot_grid(ens, year, var):\n",
      "    ti = np.where(year == years)[0][0]\n",
      "    fig = plt.figure(figsize=(10, 5))\n",
      "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
      "    ax.coastlines()\n",
      "    min_val = train_X[var].min()\n",
      "    max_val = train_X[var].max()\n",
      "    cont = ax.contourf(train_X[jupyter_string] - 180, train_X[jupyter_string], \n",
      "                       train_X[var].sel(ens=ens, time=train_X[jupyter_string].values[ti]),\n",
      "                       np.linspace(min_val, max_val, 20))\n",
      "    ax.set_title(var + jupyter_string + jupyter_string.format(year, ens))\n",
      "    plt.colorbar(cont)\n",
      "interact(plot_grid, ens=[0, 1, 2, 3], year=SelectionSlider(options=years.tolist()), \n",
      "         var=data_vars)\n",
      "--------------------\n",
      "years = np.arange(850, 2005)\n",
      "def plot_grid(ens, year, var):\n",
      "    ti = np.where(year == years)[0][0]\n",
      "    fig = plt.figure(figsize=(10, 5))\n",
      "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
      "    ax.coastlines()\n",
      "    min_val = train_X[var].min()\n",
      "    max_val = train_X[var].max()\n",
      "    cont = ax.contourf(train_X[jupyter_string] - 180, train_X[jupyter_string], \n",
      "                       train_X[var].sel(ens=ens, time=train_X[jupyter_string].values[ti]),\n",
      "                       np.linspace(min_val, max_val, 20))\n",
      "    ax.set_title(var + jupyter_string +\n",
      "=====\n",
      "train_X_anomalies = xr.merge([(train_X[var] - train_X[var].mean(axis=0)) / (train_X[var].std(axis=0)) for var in data_vars])\n",
      "--------------------\n",
      "from IPython.core.interactiveshell import InteractiveShell\n",
      "InteractiveShell.ast_node_interactivity = jupyter_string\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(8, 5))\n",
      "participant_list = np.array([8, 13, 15, 15])\n",
      "categories = np.array([jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "plt.bar(np.arange(4), participant_list / 21)\n",
      "plt.xticks(np.arange(4), categories, fontsize=12)\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string, fontsize=14)\n",
      "plt.savefig(jupyter_string, bbox_inches=jupyter_string, dpi=200)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 5))\n",
      "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
      "ax.coastlines()\n",
      "min_val = -5\n",
      "max_val = 5\n",
      "cont = ax.contourf(train_X_anomalies[jupyter_string] - 180, train_X_anomalies[jupyter_string], \n",
      "               train_X_anomalies[jupyter_string], cmap=jupyter_string)\n",
      "ax.set_title(jupyter_string)\n",
      "plt.colorbar(cont)\n",
      "interact(plot_anomaly, ens=[0, 1, 2, 3], year=SelectionSlider(options=years.tolist()), \n",
      "         var=data_vars)\n",
      "=====\n",
      "rain_data = pd.read_csv(jupyter_string, index_col=\"Year\")\n",
      "rain_data.rolling(25).mean().plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "rain_data = pd.read_csv(jupyter_string, index_col=\"Year\")\n",
      "rain_data.rolling(25).mean().plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "rain_data.hist(bins=np.arange(0, 1600, 100), figsize=(10, 5))\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings\n",
      "warnings.simplefilter(action = jupyter_string, category = FutureWarning)\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn.metrics import mean_absolute_error as mae\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams, style\n",
      "style.use(jupyter_string)\n",
      "--------------------\n",
      "rain_data.plot(figsize=(15, 5))\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "lags = np.arange(1, 20)\n",
      "autocorr = np.zeros((rain_data.columns.size, lags.size))\n",
      "plt.figure(figsize=(8, 5))\n",
      "for c, col in enumerate(rain_data.columns):\n",
      "    autocorr[c] = np.array([rain_data[col].autocorr(l) for l in range(1, 20)])\n",
      "    plt.plot(lags, np.abs(autocorr[c]), label=col)\n",
      "plt.xticks(lags)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.legend(loc=0)\n",
      "--------------------\n",
      "clf = Classifier()\n",
      "clf.fit(train_X.sel(lat=train_X[jupyter_string] > -30), train_y)\n",
      "=====\n",
      "fe = FeatureExtractor()\n",
      "fe.fit(train_X, train_y)\n",
      "X = fe.transform(train_X)\n",
      "cls = Classifier()\n",
      "cls.fit(X, train_y)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "X_transformed = pca.transform(X)\n",
      "\n",
      "plt.scatter(X_transformed[:,0], X_transformed[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "coefs = cls.clf.coef_[0]\n",
      "coef_rankings = np.argsort(np.abs(coefs))[::-1]\n",
      "fig, axes = plt.subplots(3, 3, figsize=(16, 9), \n",
      "                         subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=180)))\n",
      "axef = axes.ravel()\n",
      "for c, coef_rank in enumerate(coef_rankings[:9]):\n",
      "    c_var = data_vars[int(np.floor(coef_rank / fe.num_comps))]\n",
      "    c_comp = coef_rank % fe.num_comps\n",
      "    comp_vals = fe.pca[c_var].components_[c_comp]\n",
      "    axef[c].coastlines()\n",
      "    axef[c].contourf(train_X[jupyter_string] - 180, \n",
      "                     train_X[jupyter_string], \n",
      "                     fe.pca[c_var].components_[c_comp].reshape(train_X[c_var].shape[1:]),\n",
      "                     np.linspace(-0.04, 0.04, 11), cmap=jupyter_string)\n",
      "    axef[c].set_title(jupyter_string.format(c_var, c_comp, coefs[coef_rank]))\n",
      "--------------------\n",
      "ratings = pd.read_csv(jupyter_string, sep=jupyter_string, names=column_names)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string,sep = jupyter_string,names=column_names)\n",
      "--------------------\n",
      "df['title' <unk>].head()\n",
      "=====\n",
      "film_titles = pd.read_csv(jupyter_string)\n",
      "film_titles.head()\n",
      "--------------------\n",
      "df = pd.merge(df, film_titles, on='id' <unk>)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.merge(df,film_titles,on= jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "df.groupby(jupyter_string)[jupyter_string].count().sort_values(ascending=False).head()\n",
      "=====\n",
      "sb.set_style(jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,6))\n",
      "sb.countplot(x='Survived' <unk>, data=train)\n",
      "=====\n",
      "df.groupby(jupyter_string)[jupyter_string].mean()\n",
      "--------------------\n",
      "plt.hist(ratings[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "=====\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "--------------------\n",
      "plt.plot(women_degrees['Year' <unk>], \n",
      "         women_degrees['Biology' <unk>], c=jupyter_string, label=jupyter_string)\n",
      "plt.plot(women_degrees['Year' <unk>], \n",
      "         100-women_degrees['Biology' <unk>], c=jupyter_string, label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.show()\n",
      "women_degrees[jupyter_string] = 100-women_degrees['Biology' <<unk>>]\n",
      "women_degrees.plot(kind=jupyter_string,x='Year' <<unk>>,y=['Biology' <<unk>>,jupyter_string],\n",
      "                   title=jupyter_string,\n",
      "                   color=[jupyter_string,jupyter_string]).\\\n",
      "                        legend(loc=jupyter_string,\n",
      "                               labels=[jupyter_string,jupyter_string])\n",
      "--------------------\n",
      "gms_all = pd.read_csv(jupyter_string)\n",
      "gms_10 = pd.read_csv(jupyter_string)\n",
      "gms_exc_14 = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "gms_all = pd.read_csv(jupyter_string)\n",
      "\n",
      "gms_10 = gms_all.loc[gms_all['key' <<unk>>]==jupyter_string]\n",
      "gms_14 = gms_all.loc[gms_all['key' <<unk>>]==jupyter_string]\n",
      "gms_exc_14 = gms_all[gms_all['key' <<unk>>]!=jupyter_string]\n",
      "display(gms_10.describe())\n",
      "display(gms_14.describe())\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "ratings[jupyter_string].hist(bins=70)\n",
      "=====\n",
      "sb.jointplot(x=jupyter_string,y=jupyter_string,data=ratings,alpha=0.5)\n",
      "--------------------\n",
      "moviemat = df.pivot_table(index='user_id' <unk>,columns='title' <unk>,values='rating' <unk>)\n",
      "moviemat.head()\n",
      "=====\n",
      "filmmat = df.pivot_table(index=jupyter_string,columns=jupyter_string,values=jupyter_string)\n",
      "filmmat.head()\n",
      "--------------------\n",
      "ratings.sort_values(jupyter_string,ascending=False).head(10)\n",
      "=====\n",
      "ratings.sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "starwars_user_ratings = ratings[ratings.title == jupyter_string]\n",
      "animation_user_ratings = ratings[ratings.title == jupyter_string]\n",
      "=====\n",
      "starwars_user_rating = filmmat[jupyter_string]\n",
      "toystory_user_rating = filmmat[jupyter_string]\n",
      "\n",
      "starwars_user_rating.head()\n",
      "--------------------\n",
      "similar_to_starwars = filmmat.corrwith(starwars_user_rating)\n",
      "similar_to_animation = filmmat.corrwith(animation_user_rating)\n",
      "=====\n",
      "similar_to_starwars = filmmat.corrwith(starwars_user_rating)\n",
      "similar_to_toystory = filmmat.corrwith(toystory_user_rating)\n",
      "--------------------\n",
      "corr_starwars = pd.DataFrame(similar_to_starwars,columns=[jupyter_string])\n",
      "corr_starwars.dropna(inplace=True)\n",
      "=====\n",
      "corr_starwars = pd.DataFrame(similar_to_starwars,columns=[jupyter_string])\n",
      "corr_starwars.dropna(inplace=True)\n",
      "corr_starwars.head()\n",
      "--------------------\n",
      "corr_starwars.sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "corr_starwars.sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "corr_starwars = corr_starwars.join(ratings[jupyter_string])\n",
      "corr_starwars.head()\n",
      "=====\n",
      "corr_starwars =  corr_starwars.join(ratings[jupyter_string])\n",
      "corr_starwars.head()\n",
      "--------------------\n",
      "corr_starwars[corr_starwars[jupyter_string]>100].sort_values(jupyter_string,ascending=False).head()\n",
      "=====\n",
      "corr_starwars[corr_starwars[jupyter_string] > 100].sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "corr_toystory = pd.DataFrame(similar_to_toystory,columns=[jupyter_string])\n",
      "corr_toystory.dropna(inplace=True)\n",
      "=====\n",
      "corr_toystory = pd.DataFrame(similar_to_toystory,columns=[jupyter_string])\n",
      "corr_toystory.dropna(inplace=True)\n",
      "corr_toystory = corr_toystory.join(ratings[jupyter_string])\n",
      "corr_toystory[corr_toystory[jupyter_string] > 100].sort_values(jupyter_string,ascending=False).head(10)\n",
      "--------------------\n",
      "gms_10.head()\n",
      "=====\n",
      "rcParams[jupyter_string] = 8,8\n",
      "gls = pd.concat([gms_all.rename(columns={'score1' <<unk>>:jupyter_string}),\n",
      "                gms_all.rename(columns={'score2' <<unk>>:jupyter_string,'team1' <<unk>>:'team2' <<unk>>,'team2' <<unk>>:'team1' <<unk>>})])\n",
      "gls_groups = gls.groupby('key' <<unk>>)[jupyter_string]\n",
      "gls_means = gls_groups.mean().rename(jupyter_string)\n",
      "gls_stds = gls_groups.std().rename(jupyter_string)\n",
      "std_mean_df = pd.concat([gls_means, gls_stds],axis=1)\n",
      "plt.xlim(0, 2.8)\n",
      "plt.ylim(0, 2.8)\n",
      "g=sns.regplot(x=jupyter_string,y=jupyter_string,data=std_mean_df).set_title(''jupyter_string'')\n",
      "sns.despine()\n",
      "plt.tick_params(bottom=False,left=False)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "g.axes.set_title(r''jupyter_string'',ha=jupyter_string,position=(0,1),size=13)\n",
      "plt.show()\n",
      "--------------------\n",
      "milk = pd.read_csv(jupyter_string,index_col='Month' <unk>)\n",
      "=====\n",
      "milk = pd.read_csv(jupyter_string, index_col='Month' <<unk>>)\n",
      "milk.columns\n",
      "--------------------\n",
      "milk.head()\n",
      "=====\n",
      "milk.index = pd.to_datetime(milk.index, format=jupyter_string, errors=jupyter_string)\n",
      "milk.head()\n",
      "--------------------\n",
      "milk.plot()\n",
      "=====\n",
      "plt.figure()\n",
      "plt.plot(milk)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "=====\n",
      "train_set = milk.head(156)\n",
      "test_set = milk.tail(12)\n",
      "--------------------\n",
      "test_set.plot(x=jupyter_string,y=jupyter_string,kind=jupyter_string)\n",
      "=====\n",
      "test_set.plot()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dji_2011 = pd.read_csv(jupyter_string)\n",
      "print(dji_2011.dtypes)\n",
      "dji_2011.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "jnj_2011 = dji_2011.loc[lambda df: dji_2011['stock' <<unk>>] == jupyter_string]\n",
      "print(jnj_2011.dtypes)\n",
      "jnj_2011.head()\n",
      "--------------------\n",
      "jnj_2011_df = pd.DataFrame(jnj_2011_dict)\n",
      "jnj_2011_df.head()\n",
      "=====\n",
      "plt.figure(figsize=(12, 14))\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['open' <<unk>>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['high' madeupword0002], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['low' <<unk>>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011_dict['close' <<unk>>], label=jupyter_string)\n",
      "plt.ylim([57, 68])\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.title(jupyter_string,fontsize=20)\n",
      "plt.yticks(np.arange(57, 69, 1))\n",
      "plt.legend()\n",
      "\n",
      "plt.subplot(2, 1, 2)\n",
      "plt.plot(jnj_2011_dict['date' <<unk>>], jnj_2011['volume' <<unk>>])\n",
      "plt.ylabel(jupyter_string, fontsize=14)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(12, 14))\n",
      "plt.subplot(2, 1, 1)\n",
      "plt.plot(jnj_2011_dict['date' <unk>], jnj_2011_dict['open' <unk>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <unk>], jnj_2011_dict['high' <unk>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <unk>], jnj_2011_dict['low' <unk>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <unk>], jnj_2011_dict['close' <unk>], label=jupyter_string)\n",
      "plt.plot(jnj_2011_dict['date' <unk>], jnj_2011_dict['volume' <unk>\n",
      "=====\n",
      "plt.figure(figsize=(12, 7))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.boxplot(jnj_2011['days_to_next_dividend' <<unk>>])\n",
      "plt.ylabel(jupyter_string, fontsize=13)\n",
      "plt.yticks(np.arange(0, 100, 10))\n",
      "plt.title(jupyter_string, fontsize=14)\n",
      "plt.xlabel(jupyter_string, fontsize=13)\n",
      "\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.boxplot(jnj_2011['percent_return_next_dividend' <<unk>>])\n",
      "plt.ylabel(jupyter_string, fontsize=13)\n",
      "plt.yticks(np.arange(0.85, 1, 0.01))\n",
      "plt.title(jupyter_string, fontsize=14)\n",
      "plt.xlabel(jupyter_string, fontsize=13)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(12, 7))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.scatter(jnj_2011['days_to_next_dividend' <unk>], jnj_2011['percent_return_next_dividend' <unk>])\n",
      "plt.ylabel(jupyter_string, fontsize=13)\n",
      "plt.xlabel(jupyter_string, fontsize=13)\n",
      "plt.title(jupyter_string, fontsize=14)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(18, 8))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.scatter(x=jnj_2011['percent_change_price' <<unk>>].drop(554), y=jnj_2011['percent_change_volume_over_last_wk' <<unk>>].drop(168), s=40)\n",
      "plt.ylabel(jupyter_string, fontsize=10)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.title(jupyter_string, fontsize=12)\n",
      "\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.scatter(x=jnj_2011['percent_change_price' <<unk>>].drop(554), y=jnj_2011['volume' <<unk>>].drop(168), s=30)\n",
      "plt.ylabel(jupyter_string, fontsize=10)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.title(jupyter_string, fontsize=12)\n",
      "plt.show()\n",
      "--------------------\n",
      "rcParams[jupyter_string] = 8,8\n",
      "gls = pd.concat([gms_all.rename(columns={'score1' <unk>:jupyter_string}),\n",
      "                gms_all.rename(columns={'score2' <unk>:jupyter_string,'team1' <unk>:'team2' <unk>,'team2' <unk>:'team1' <unk>})])\n",
      "gls_groups = gls.groupby('key' <unk>)\n",
      "gls_means = gls_groups.mean().rename(jupyter_string)\n",
      "gls_stds = gls_groups.std().rename(jupyter_string)\n",
      "std_mean_df = pd.concat([gls_means, gls_stds],axis=1)\n",
      "plt.xlim(0, 2.8)\n",
      "plt.ylim(0, 2.8)\n",
      "=====\n",
      "gls_10 = pd.concat([gms_10.rename(columns={'score1' <<unk>>:jupyter_string}),\n",
      "                    gms_10.rename(columns={'score2' <<unk>>:jupyter_string,'team1' <<unk>>:'team2' <<unk>>,'team2' <<unk>>:'team1' <<unk>>})])\n",
      "poisson_model = smf.glm(formula=jupyter_string, data=gls_10, \n",
      "                        family=sm.families.Poisson()).fit()\n",
      "poisson_model.summary()\n",
      "--------------------\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sb\n",
      "from matplotlib import pyplot as plt\n",
      "from collections import Counter\n",
      "from __future__ import division\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train.head()\n",
      "=====\n",
      "train.info()\n",
      "--------------------\n",
      "train.describe()\n",
      "=====\n",
      "train.describe()\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,3))\n",
      "--------------------\n",
      "plt.figure(figsize=(16,6))\n",
      "plt.subplot(1,2,1)\n",
      "sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\n",
      "plt.subplot(1,2,2)\n",
      "sns.barplot(x=\"Pclass\",y=\"Survived\",data=train)\n",
      "=====\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "--------------------\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train.groupby([\"Pclass\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "--------------------\n",
      "train.groupby([\"Sex\",\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).apply(lambda x: 100*x/float(x.sum()),axis=1)\n",
      "=====\n",
      "train[jupyter_string] = pd.cut(train[\"Age\"],bins=range(0,90,10),precision=1)\n",
      "train.groupby([jupyter_string,\"Survived\"]).count()[\"PassengerId\"].unstack([\"Survived\"]).plot.bar(stacked=True,color=[jupyter_string,[jupyter_string]])\n",
      "train = train.drop(jupyter_string,axis=1)\n",
      "--------------------\n",
      "train[\"Name\"] = train[\"Name\"].apply(lambda x: x.split(jupyter_string)[0])\n",
      "train[\"Ticket\"] = train[\"Ticket\"].apply(lambda x: x.split(jupyter_string)[1])\n",
      "=====\n",
      "ctrain = train.drop([\"Name\",\"Ticket\"],axis=1).replace({\"Sex\": {jupyter_string: 1, jupyter_string:0}})\n",
      "ctrain[\"Cabin\"]=ctrain[\"Cabin\"].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
      "def embarked_letters_to_integers(letter):\n",
      "    if letter == jupyter_string:\n",
      "        return 0\n",
      "    elif letter == jupyter_string:\n",
      "        return 1\n",
      "    elif letter == jupyter_string:\n",
      "        return 2\n",
      "ctrain[\"Embarked\"] = ctrain[\"Embarked\"].apply(embarked_letters_to_integers)\n",
      "--------------------\n",
      "from scipy.stats import poisson\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "from scipy.stats import binom\n",
      "=====\n",
      "pd.set_option(jupyter_string, None)\n",
      "def pred_team_gls(model, team1, team2, gls_df, have_teams=None):\n",
      "    if have_teams is None: \n",
      "        have_teams = set(gls_df['team1' <<unk>>])|set(gls_df['team2' <<unk>>])\n",
      "    if team1 in have_teams: \n",
      "        if team2 in have_teams:\n",
      "            pred = model.predict(pd.DataFrame(data={'team1' <<unk>>: team1, 'team2' <<unk>>: team2},index=[0])).values[0]\n",
      "        else:\n",
      "            team1_gls = pd.concat([gls_df.loc[gls_df['team1' <<unk>>]==team1,'score1' <<unk>>],\n",
      "                                gls_df.loc[gls_df['team2' <<unk>>]==team1,'score2' <<unk>>]])\n",
      "            pred = team1_gls.mean()\n",
      "    else:\n",
      "        if team2 in have_teams:\n",
      "            team2_gls = pd.concat([gls_df.loc[gls_df['team1' <<unk>>]==team2,'score1' <<unk>>],\n",
      "                                gls_df.loc[gls_df['team2' <<unk>>]==team2,'score2' <<unk>>]])\n",
      "            pred = team2_gls.mean()        \n",
      "        else:\n",
      "            pred = gls_df[jupyter_string].mean()\n",
      "    return np.round(pred,8)\n",
      "\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(poisson_model, r['team1' <<unk>>],r['team2' <<unk>>], gls_10), axis=1)\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(poisson_model, r['team2' <<unk>>],r['team1' <<unk>>], gls_10), axis=1)\n",
      "pred_14_df = gms_14[['score1' <<unk>>, 'score2' <<unk>>, 'team1' <<unk>>, 'team2' <<unk>>, jupyter_string, jupyter_string]]\n",
      "pred_14_df.head()\n",
      "--------------------\n",
      "ctrain = ctrain.drop([\"Name\",\"Ticket\"],axis=1).replace({\"Sex\": {jupyter_string: 1, jupyter_string:0}})\n",
      "def embarked_letters_to_integers(letter):\n",
      "    if letter == jupyter_string:\n",
      "        return 0\n",
      "    elif letter == jupyter_string:\n",
      "        return 1\n",
      "    elif letter == jupyter_string:\n",
      "        return 2\n",
      "    elif letter == jupyter_string:\n",
      "        return 3\n",
      "    elif letter == jupyter_string:\n",
      "        return 4\n",
      "ctrain[\"Embarked\"] = ctrain[\"Embarked\"].apply(embarked_letters_to_integers)\n",
      "=====\n",
      "train[jupyter_string] = train[\"Sex\"].map({jupyter_string:0,jupyter_string:1}).astype(int)\n",
      "train[jupyter_string] = train[\"Embarked\"].map({jupyter_string:0,jupyter_string:1,jupyter_string:2}).fillna(0).astype(int)\n",
      "train[jupyter_string] = train[\"Cabin\"].map(lambda x: 0 if pd.isnull(x) else 1)\n",
      "--------------------\n",
      "train[jupyter_string] = train.groupby([\"Sex\", \"Pclass\"])[\"Age\"].transform(jupyter_string)\n",
      "train[jupyter_string] = train.groupby([\"Pclass\"])[\"Age\"].transform(jupyter_string)\n",
      "=====\n",
      "mean_age_per_sex_class = train.groupby([\"Sex\",\"Pclass\"]).mean()[\"Age\"]\n",
      "mean_age_per_sex_class.unstack(\"Pclass\").plot.bar()\n",
      "--------------------\n",
      "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\n",
      "test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())\n",
      "=====\n",
      "train[jupyter_string] = train[\"Age\"]\n",
      "train[jupyter_string] = pd.isnull(train[\"Age\"]).astype(int)\n",
      "for sex in [jupyter_string,jupyter_string]:\n",
      "    for pclass in [1,2,3]:\n",
      "        train.loc[train.Age.isnull() & (train.Sex==sex) & (train.Pclass==pclass),jupyter_string] = mean_age_per_sex_class[sex][pclass]\n",
      "--------------------\n",
      "train.info()\n",
      "=====\n",
      "train.describe().round(2)\n",
      "--------------------\n",
      "df_train[jupyter_string] = df_train['SibSp' <unk>] + df_train['Parch' <unk>]\n",
      "df_test[jupyter_string] = df_test['SibSp' <unk>] + df_test['Parch' <unk>]\n",
      "=====\n",
      "train[jupyter_string] = train[\"SibSp\"] + train[\"Parch\"]\n",
      "train[[jupyter_string,\"SibSp\",\"Parch\"]].head(10)\n",
      "--------------------\n",
      "test[jupyter_string] = test[\"SibSp\"] + test[\"Parch\"]\n",
      "test[[jupyter_string,\"SibSp\",\"Parch\"]].head(10)\n",
      "=====\n",
      "train[jupyter_string] = train[jupyter_string] * train[\"Pclass\"]\n",
      "train[[jupyter_string,jupyter_string,\"Pclass\"]].head(10)\n",
      "--------------------\n",
      "train = train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
      "test = test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
      "=====\n",
      "text_info_columns = train.dtypes[train.dtypes.map(lambda x: x==jupyter_string)].index.tolist()\n",
      "train = train.drop(text_info_columns,axis=1)\n",
      "text_info_columns\n",
      "--------------------\n",
      "train = train.drop(['Age' <unk>],axis=1)\n",
      "=====\n",
      "train = train.drop(\"Age\",axis=1)\n",
      "--------------------\n",
      "train.info()\n",
      "=====\n",
      "train.info()\n",
      "--------------------\n",
      "train.hist(figsize=(20,20))\n",
      "plt.show()\n",
      "=====\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,6));\n",
      "--------------------\n",
      "gms_14[jupyter_string] = gms_14['score1' <unk>] + gms_14['score2' <unk>]\n",
      "gms_14.head()\n",
      "=====\n",
      "nbinom_model = smf.glm(formula=jupyter_string, data=gls_10, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14.loc[:,jupyter_string] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team1' <<unk>>],r['team2' <<unk>>], gls_10), axis=1)\n",
      "gms_14.loc[:,jupyter_string] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team2' <<unk>>],r['team1' <<unk>>], gls_10), axis=1)\n",
      "\n",
      "evaluate_predictions(gms_14)\n",
      "--------------------\n",
      "train.drop([\"PassengerId\"],axis=1).hist(figsize=(16,6),layout=(2,6));\n",
      "=====\n",
      "train.Fare.hist(range=(0,100));plt.title(jupyter_string)\n",
      "--------------------\n",
      "model = MultiClassLogisticRegressorPurePython(lr=0.001, n_iter=100)\n",
      "model.fit(X_train, y_train)\n",
      "y_pred = model.predict(X_test)\n",
      "print(jupyter_string, accuracy_score(y_test, y_pred))\n",
      "=====\n",
      "model_3 = MultiClassLogisticRegressorPurePython(n_iter=1000, lr=0.1)\n",
      "model_3.fit(X_train, y_train)\n",
      "plt.plot(model_3.cost_)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from __future__ import print_function \n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "names = [jupyter_string]\n",
      "feature_names = map(str, range(784))\n",
      "names.extend(feature_names)\n",
      "\n",
      "df_mnist_train = pd.read_csv(jupyter_string, header=None, nrows=1000, names=names)\n",
      "df_mnist_test = pd.read_csv(jupyter_string, header=None, nrows=100, names=names)\n",
      "\n",
      "\n",
      "max_pixel =  255\n",
      "df_mnist_train.iloc[:, 1:] /= max_pixel\n",
      "df_mnist_test.iloc[:, 1:] /= max_pixel\n",
      "\n",
      "print(df_mnist_train.shape, df_mnist_test.shape)\n",
      "--------------------\n",
      "import tensorflow as tf\n",
      "\n",
      "X = tf.placeholder(tf.float32, [None, 2])\n",
      "y = tf.placeholder(tf.int32, [None])\n",
      "\n",
      "W = tf.Variable(tf.zeros([2, 1]))\n",
      "b = tf.Variable(tf.zeros([1]))\n",
      "\n",
      "y_pred = tf.nn.softmax(tf.matmul(X, W) + b)\n",
      "\n",
      "cross_entropy = -tf.reduce_sum(y_*tf.log(y_pred))\n",
      "\n",
      "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
      "\n",
      "sess = tf.Session()\n",
      "sess.run(tf.global_variables_initializer())\n",
      "\n",
      "for i in range(1000):\n",
      "    sess.run(train_step\n",
      "=====\n",
      "import tensorflow as tf\n",
      "tf.logging.set_verbosity(tf.logging.ERROR)\n",
      "\n",
      "\n",
      "def input_fn(x, y):\n",
      "    feature_cols = {name: tf.constant(x[name].values) for name in x.columns}\n",
      "    label = tf.constant(y.values)\n",
      "    return feature_cols, label\n",
      "\n",
      "\n",
      "\n",
      "feat_cols = [tf.contrib.layers.real_valued_column(name) for name in X_train.columns]\n",
      "\n",
      "\n",
      "model_2 = tf.contrib.learn.LinearClassifier(feature_columns=feat_cols, n_classes=10) \n",
      "model_2.fit(input_fn=lambda: input_fn(X_train, y_train), steps=100)\n",
      "\n",
      "\n",
      "train_acc = model_2.evaluate(input_fn=lambda: input_fn(X_train, y_train), steps=1)[jupyter_string]\n",
      "test_acc = model_2.evaluate(input_fn=lambda: input_fn(X_test, y_test), steps=1)[jupyter_string]\n",
      "print(jupyter_string.format(train_acc))\n",
      "print(jupyter_string.format(test_acc))\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "X_filled_mice = fancyimpute.MICE().complete(np.asarray(carmpg.ix[:, carmpg.columns != 'Auto' <<unk>>]))\n",
      "--------------------\n",
      "carmpg_filled = pd.DataFrame(X_filled_mice)\n",
      "carmpg_filled.columns = carmpg.columns\n",
      "carmpg_filled.head()\n",
      "=====\n",
      "X_filled_mice = pd.DataFrame(X_filled_mice)\n",
      "X_filled_mice.columns = ['MPG' <<unk>>,'CYLINDERS' madeupword0002,'SIZE' <<unk>>,'HP' <<unk>>,'WEIGHT' <<unk>>,'ACCEL' <<unk>>,'ENG_TYPE' <<unk>>]\n",
      "X_filled_mice.insert(0, 'Auto' <<unk>>, carmpg['Auto' <<unk>>])\n",
      "\n",
      "X_filled_mice\n",
      "--------------------\n",
      "X_filled_mice = pd.DataFrame(X_filled_mice)\n",
      "X_filled_mice.columns = ['MPG' <unk>,'CYLINDERS' <unk>,'SIZE' <unk>,'HP' <unk>,'WEIGHT' <unk>,'ACCEL' <unk>,'ENG_TYPE' <unk>]\n",
      "X_filled_mice.insert(0, 'Auto' <unk>, carmpg['Auto' <unk>])\n",
      "X_filled_mice\n",
      "=====\n",
      "X = X_filled_mice[['CYLINDERS' madeupword0002,'SIZE' <<unk>>,'HP' <<unk>>,'WEIGHT' <<unk>>,'ACCEL' <<unk>>,'ENG_TYPE' <<unk>>]]\n",
      "y = X_filled_mice['MPG' <<unk>>]\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "carmpg = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "carmpg.info()\n",
      "=====\n",
      "carmpg.info()\n",
      "--------------------\n",
      "df = df.dropna()\n",
      "df.head()\n",
      "=====\n",
      "carmpg_listwise = carmpg.dropna(axis=0)\n",
      "--------------------\n",
      "nbinom_model = smf.glm(formula=jupyter_string, data=gls_14, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14.loc[:,jupyter_string] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team1' <unk>],r['team2' <unk>], gls_14), axis=1)\n",
      "gms_14.loc[:,jupyter_string] = gms_14.apply(lambda r: pred_team_gls(nbinom_model, r['team2' <unk>],r['team1' <unk>], gls_14), axis=1)\n",
      "\n",
      "evaluate_predictions(gms_14)\n",
      "=====\n",
      "gls_exc_14 = pd.concat([gms_exc_14.rename(columns={'score1' <<unk>>:jupyter_string}),\n",
      "                    gms_exc_14.rename(columns={'score2' <<unk>>:jupyter_string,'team1' <<unk>>:'team2' <<unk>>,'team2' <<unk>>:'team1' <<unk>>})])\n",
      "regr = smf.glm(formula=jupyter_string, data=gls_exc_14, \n",
      "                        family=sm.families.Poisson()).fit()\n",
      "display(regr.summary())\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team1' <<unk>>],r['team2' <<unk>>], gls_exc_14), axis=1)\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team2' <<unk>>],r['team1' <<unk>>], gls_exc_14), axis=1)\n",
      "--------------------\n",
      "carmpg_listwise.info()\n",
      "=====\n",
      "carmpg_listwise.info()\n",
      "--------------------\n",
      "X = df[['CYLINDERS' <unk>, 'SIZE' <unk>, 'HP' <unk>, 'WEIGHT' <unk>, 'ACCEL' <unk>, 'ENG_TYPE' <unk>]]\n",
      "y = df['MPG' <unk>]\n",
      "=====\n",
      "X = carmpg_listwise[['CYLINDERS' madeupword0002,'SIZE' <<unk>>,'HP' <<unk>>,'WEIGHT' <<unk>>,'ACCEL' <<unk>>,'ENG_TYPE' <<unk>>]]\n",
      "y = carmpg_listwise['MPG' <<unk>>]\n",
      "--------------------\n",
      "dataset = pd.read_csv(jupyter_string)\n",
      "X = dataset.iloc[:, :-1].values\n",
      "y = dataset.iloc[:, 4].values\n",
      "=====\n",
      "training_set = pd.read_csv(jupyter_string)\n",
      "training_set = training_set.iloc[:, 1:2].values\n",
      "training_set\n",
      "\n",
      "--------------------\n",
      "X_train = training_set[0:training_set.shape[0], 1:]\n",
      "y_train = training_set[0:training_set.shape[0], 0]\n",
      "=====\n",
      "X_train = training_set[0:1257]\n",
      "y_train = training_set[1:1258]\n",
      "--------------------\n",
      "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
      "=====\n",
      "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
      "X_train.shape\n",
      "--------------------\n",
      "regressor = Sequential()\n",
      "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
      "regressor.add(LSTM(units = 50, return_sequences = True))\n",
      "regressor.add(LSTM(units = 50))\n",
      "regressor.add(Dense(units = 1))\n",
      "=====\n",
      "regressor = Sequential()\n",
      "\n",
      "\n",
      "regressor.add(LSTM(units = 4, activation = jupyter_string, input_shape = (None, 1)))\n",
      "\n",
      "\n",
      "regressor.add(Dense(units = 1))\n",
      "\n",
      "\n",
      "regressor.compile(optimizer = jupyter_string, loss = jupyter_string)\n",
      "--------------------\n",
      "dataset_test = pd.read_csv(jupyter_string)\n",
      "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
      "=====\n",
      "test_set = pd.read_csv(jupyter_string)\n",
      "real_stock_price = test_set.iloc[:, 1:2].values\n",
      "real_stock_price\n",
      "--------------------\n",
      "predicted_stock_price = regressor.predict(X_test)\n",
      "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
      "predicted_stock_price\n",
      "=====\n",
      "inputs = real_stock_price\n",
      "inputs = sc.transform(inputs)\n",
      "inputs = np.reshape(inputs, (20, 1, 1))\n",
      "predicted_stock_price = regressor.predict(inputs)\n",
      "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
      "--------------------\n",
      "plt.plot(real_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.plot(predicted_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(real_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.plot(predicted_stock_price, color = jupyter_string, label = jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "from pandas import *\n",
      "pd.options.mode.chained_assignment = None  \n",
      "\n",
      "population_f = pd.read_csv(jupyter_string, index_col=0)\n",
      "population_m = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "tests_f_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_f_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_m_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "tests_m_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "diagnoses_f_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_f_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_m_max = pd.read_csv(jupyter_string, index_col=0)\n",
      "diagnoses_m_min = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "--------------------\n",
      "neg_binomial = NegativeBinomialNB()\n",
      "neg_binomial.fit(X_train, y_train)\n",
      "evaluate_predictions(neg_binomial)\n",
      "=====\n",
      "regr = smf.glm(formula=jupyter_string, data=gls_exc_14, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team1' <<unk>>],r['team2' <<unk>>], gls_exc_14), axis=1)\n",
      "gms_14[jupyter_string] = gms_14.apply(lambda r: pred_team_gls(regr, r['team2' <<unk>>],r['team1' <<unk>>], gls_exc_14), axis=1)\n",
      "evaluate_predictions(gms_14)\n",
      "--------------------\n",
      "tests_f_max = tests_f_max.dropna()\n",
      "tests_f_min = tests_f_min.dropna()\n",
      "tests_m_max = tests_m_max.dropna()\n",
      "tests_m_min = tests_m_min.dropna()\n",
      "=====\n",
      "import matplotlib\n",
      "matplotlib.use(jupyter_string)\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid.anchored_artists import AnchoredText\n",
      "from matplotlib.patches import Polygon\n",
      "--------------------\n",
      "fig = plt.figure(figsize = (14, 8.5))\n",
      "plt.rc(jupyter_string, dpi=200) \n",
      "\n",
      "\n",
      "ax1 = fig.add_subplot(221)\n",
      "ax1.boxplot(100.0*transpose(prev_m_min_diff_2[:,0,:]), positions=array(range(2001,2016)) - 0.6, \n",
      "            whis = [2.5, 97.5], showfliers=False, whisaps=False,\n",
      "            boxprops = {jupyter_string:jupyter_string, jupyter_string:jupyter_string},\n",
      "            whiskerprops = {jupyter_string:jupyter_string,jupyter_string:jupyter_string},\n",
      "            medianprops = {jupyter_string:jupyter_string,jupyter_string:jupyter_string},\n",
      "            patch_artist = True, sym=jupyter_string, widths=0.2)\n",
      "ax1.boxplot(100.0*transpose(prev_m_max_diff\n",
      "=====\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "tax = pd.read_csv(jupyter_string,encoding=jupyter_string,usecols=[0,1,2,3,4,5,6])\n",
      "--------------------\n",
      "tax.head()\n",
      "=====\n",
      "tax.head()\n",
      "--------------------\n",
      "tax.info()\n",
      "=====\n",
      "tax.info()\n",
      "--------------------\n",
      "tax['Prov/Terr' <unk>] = pd.to_numeric(tax['Prov/Terr' <unk>], errors=jupyter_string)\n",
      "=====\n",
      "tax['Prov/Terr' <<unk>>].unique()\n",
      "--------------------\n",
      "tax = tax[tax['Prov/Terr' <unk>] != jupyter_string]\n",
      "=====\n",
      "tax = tax[tax['Prov/Terr' <<unk>>] != jupyter_string]\n",
      "--------------------\n",
      "tax['Prov/Terr' <unk>].unique()\n",
      "=====\n",
      "tax['Prov/Terr' <<unk>>].unique()\n",
      "--------------------\n",
      "tax.groupby('Prov/Terr' <unk>)['Total' <unk>].sum().sort_values(ascending=False).head(1)\n",
      "=====\n",
      "tax.groupby('Prov/Terr' <<unk>>)['Total Income' <<unk>>].sum().sort_values().plot(kind=jupyter_string)\n",
      "--------------------\n",
      "gls_exc_18 = pd.read_csv(jupyter_string)\n",
      "gls_exc_18[jupyter_string] = gls_exc_18.apply(lambda r: pred_team_gls(regr, r['team1' <unk>],r['team2' <unk>], gls_exc_18), axis=1)\n",
      "=====\n",
      "gls_all = pd.concat([gms_all.rename(columns={'score1' <<unk>>:jupyter_string}),\n",
      "                    gms_all.rename(columns={'score2' <<unk>>:jupyter_string,'team1' <<unk>>:'team2' <<unk>>,'team2' <<unk>>:'team1' <<unk>>})])\n",
      "pred_df = pd.read_csv(jupyter_string)\n",
      "regr_all = smf.glm(formula=jupyter_string, data=gls_all, \n",
      "                        family=sm.families.NegativeBinomial()).fit()\n",
      "pred_df[jupyter_string] = pred_df.apply(lambda r: \n",
      "    pred_team_gls(regr_all, r['team1' <<unk>>],r['team2' <<unk>>], gls_all), axis=1)\n",
      "pred_df[jupyter_string] = pred_df.apply(lambda r: \n",
      "    pred_team_gls(regr_all, r['team2' <<unk>>],r['team1' <<unk>>], gls_all), axis=1)\n",
      "pred_df[jupyter_string] = np.round(pred_df[jupyter_string],0)\n",
      "pred_df[jupyter_string] = np.round(pred_df[jupyter_string],0)\n",
      "pred_df\n",
      "--------------------\n",
      "df.groupby('District' <unk>)['Total Income' <unk>].sum().sort_values().plot(kind=jupyter_string)\n",
      "=====\n",
      "tax.plot(kind=jupyter_string,x='Total Income' <<unk>>,y='Total' <<unk>>)\n",
      "--------------------\n",
      "tax[jupyter_string] = tax['Region' <unk>].map({1: jupyter_string, 2: jupyter_string, 3: jupyter_string, 4: jupyter_string, 5: jupyter_string, 6: jupyter_string})\n",
      "=====\n",
      "tax[jupyter_string] = tax['Prov/Terr' <<unk>>].map(lambda code : int(code[0]))\n",
      "--------------------\n",
      "tax.head()\n",
      "=====\n",
      "tax.plot(kind=jupyter_string,x='Total Income' <<unk>>,y='Total' <<unk>>,c=jupyter_string,figsize=(15,10),colormap=jupyter_string,s=100,alpha=0.7)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from datetime import datetime as dt \n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "plt.show()\n",
      "\n",
      "sphist = pd.read_csv(jupyter_string)\n",
      "sphist.shape\n",
      "--------------------\n",
      "sphist.head()\n",
      "=====\n",
      "sphist.head(4)\n",
      "--------------------\n",
      "sphist.tail(4)\n",
      "=====\n",
      "sphist.Date = pd.to_datetime(sphist.Date)\n",
      "sphist = sphist.sort_values(by = 'Date' <<unk>>, ascending=False)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "rf = RandomForestClassifier()\n",
      "rf.fit(X_train, y_train)\n",
      "=====\n",
      "fitted_logreg = LogisticRegressionCV().fit(predictions_tune, y_tune)\n",
      "print(jupyter_string.format(fitted_logreg.coef_.shape[1]))\n",
      "\n",
      "\n",
      "y_hat = fitted_logreg.predict(predictions_test)\n",
      "\n",
      "print(jupyter_string, accuracy_score(y_test, y_hat))\n",
      "\n",
      "\n",
      "--------------------\n",
      "logreg = LogisticRegressionCV().fit(predictions_tune, y_tune)\n",
      "print(jupyter_string.format(logreg.coef_.shape[1]))\n",
      "\n",
      "\n",
      "y_hat = fitted_logreg.predict(predictions_test)\n",
      "\n",
      "print(jupyter_string, accuracy_score(y_test, y_hat))\n",
      "=====\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "for cur_depth in range(1,8):\n",
      "    model = DecisionTreeClassifier(max_depth = cur_depth)\n",
      "    scores = cross_val_score(model, predictions_tune, y_tune, cv=5)\n",
      "    print(jupyter_string.format(np.mean(scores), np.std(scores)))\n",
      "\n",
      "DecisionTreeClassifier(max_depth=4).fit(predictions_tune, y_tune).score(predictions_test, y_test)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.wrappers.scikit_learn import KerasClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import average_precision_score\n",
      "\n",
      "=====\n",
      "plt.show()\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_train = pd.read_csv(jupyter_string, index_col=0)\n",
      "df_test = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "data = pandas.read_csv(jupyter_string, header=None)\n",
      "data.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "data\n",
      "=====\n",
      "data = pandas.read_csv(jupyter_string)\n",
      "data.head()\n",
      "targets = pandas.DataFrame(data.TARGET.value_counts())\n",
      "targets[jupyter_string] = 100*targets['TARGET' madeupword0322]/data.shape[0]\n",
      "targets\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "x = np.arange(0, 2*np.pi, 0.1)\n",
      "y = np.sin(x) + 0.1*np.random.normal(size=x.shape[0])\n",
      "--------------------\n",
      "plt.plot(x, estgb.predict(x.reshape(-1,1)), label=jupyter_string)\n",
      "plt.plot(x, y, label=jupyter_string)\n",
      "plt.legend();\n",
      "=====\n",
      "display_iters = [0, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 400, 500]\n",
      "\n",
      "\n",
      "\n",
      "import time\n",
      "from IPython import display\n",
      "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
      "ax[0].plot(x, y, jupyter_string);\n",
      "ax[0].set_color_cycle([plt.cm.viridis(i) for i in np.linspace(0, 1, len(display_iters))])\n",
      "sleep_time = 2\n",
      "\n",
      "\n",
      "overall_predictions = list(estgb.staged_predict(x.reshape(-1,1)))\n",
      "overall_predictions = [np.mean(y)*np.ones_like(x)] + overall_predictions\n",
      "\n",
      "\n",
      "for i in display_iters:\n",
      "    \n",
      "    \n",
      "    cur_overall_prediction = overall_predictions[i]\n",
      "    ax[0].plot(x, cur_overall_prediction, alpha=0.7, label=str(i), lw=2)\n",
      "    ax[0].legend()\n",
      "    \n",
      "    \n",
      "    resid = y - cur_overall_prediction\n",
      "    ax[1].cla()\n",
      "    ax[1].scatter(x,resid, label=jupyter_string)\n",
      "    ax[1].axhline(0)\n",
      "    \n",
      "    \n",
      "    if i <=5:\n",
      "        cur_est = estgb.estimators_[i,0]\n",
      "        cur_prediction = cur_est.predict(x.reshape(-1,1))\n",
      "        ax[1].plot(x, cur_prediction, color=jupyter_string, label=jupyter_string)\n",
      "    else:\n",
      "        \n",
      "        sleep_time = sleep_time/2\n",
      "    ax[1].legend()\n",
      "    \n",
      "    \n",
      "    display.display(fig)\n",
      "    display.clear_output(wait=True)\n",
      "    time.sleep(sleep_time)\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import make_moons\n",
      "\n",
      "X, y = make_moons(n_samples=100, noise=0.25, random_state=1)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators=10, random_state=1)\n",
      "forest.fit(X_train, y_train)\n",
      "forest.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "RandomForestClassifier(100).fit(x_train,y_train).score(x_test,y_test)\n",
      "--------------------\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import\n",
      "=====\n",
      "models = np.load(jupyter_string, encoding = jupyter_string)\n",
      "--------------------\n",
      "model.fit(x_train, y_train_cat, epochs=5, batch_size=32, validation_split = .2)\n",
      "=====\n",
      "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
      "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegressionCV\n",
      "\n",
      "LR_score = LogisticRegressionCV().fit(x_train, y_train).score(x_test,y_test)\n",
      "\n",
      "scores = []\n",
      "for cur_model in models:\n",
      "    scores.append(cur_model.score(x_test,y_test))\n",
      "    \n",
      "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
      "ax.hist(scores,20, label=jupyter_string);\n",
      "\n",
      "\n",
      "ax.axvline(LR_score, color=jupyter_string,label=jupyter_string)\n",
      "ax.set_xlabel(jupyter_string, fontsize=24) \n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.legend(loc=jupyter_string, fontsize=24)\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "predictions_tune = pd.read_csv(jupyter_string, index_col=0)\n",
      "predictions_test = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "predictions_tune.head()\n",
      "--------------------\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(64, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(10, activation=jupyter_string))\n",
      "=====\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "scaler = MinMaxScaler().fit(x_train_flat)\n",
      "x_train_scaled = scaler.transform(x_train_flat)\n",
      "x_test_scaled = scaler.transform(x_test_flat)\n",
      "--------------------\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "=====\n",
      "from keras.datasets import mnist\n",
      "from keras.utils import to_categorical\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "\n",
      "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "\n",
      "\n",
      "y_train_cat = to_categorical(y_train, num_classes=10)\n",
      "y_test_cat  = to_categorical(y_test, num_classes=10)\n",
      "\n",
      "\n",
      "x_train_flat = x_train.reshape(x_train.shape[0],-1)\n",
      "x_test_flat = x_test.reshape(x_test.shape[0],-1)\n",
      "\n",
      "\n",
      "scaler = MinMaxScaler().fit(x_train_flat)\n",
      "x_train_scaled = scaler.transform(x_train_flat)\n",
      "x_test_scaled = scaler.transform(x_test_flat)\n",
      "\n",
      "--------------------\n",
      "removeConstantColumns(data)\n",
      "=====\n",
      "removeDuplicates(data)\n",
      "removeConstantColumns(data)\n",
      "data.describe()\n",
      "--------------------\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
      "model.add(keras.layers.Dense(300, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(100, activation=jupyter_string))\n",
      "model.add(keras.layers.Dense(10, activation=jupyter_string))\n",
      "=====\n",
      "model = Sequential([\n",
      "    Dense(500, input_shape=(784,), activation=jupyter_string),\n",
      "    Dense(100, activation=jupyter_string),\n",
      "    Dense(50, activation=jupyter_string),\n",
      "    Dense(10, activation=jupyter_string)\n",
      "])\n",
      "\n",
      "model.compile(loss=jupyter_string, optimizer=jupyter_string, metrics=[jupyter_string])\n",
      "\n",
      "\n",
      "model.summary()\n",
      "--------------------\n",
      "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
      "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
      "\n",
      "print(x_train.shape)\n",
      "print(x_test.shape)\n",
      "=====\n",
      "plt.imshow(x_train[10,:,:]);\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib\n",
      "plt.show()\n",
      "matplotlib.style.use(jupyter_string)\n",
      "drivingLog = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string],header=None)\n",
      "\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "--------------------\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "=====\n",
      "fig, axes = plt.subplots(nrows=4, ncols=1,figsize=(20,10))\n",
      "drivingLog[jupyter_string].plot(ax=axes[0],color=jupyter_string); axes[0].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[1],color=jupyter_string); axes[1].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[2],color=jupyter_string); axes[2].set_title(jupyter_string);\n",
      "drivingLog[jupyter_string].plot(ax=axes[3],color=jupyter_string); axes[3].set_title(jupyter_string);\n",
      "--------------------\n",
      "drivingLog = pd.read_csv(jupyter_string,names=[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string],header=None)\n",
      "\n",
      "plt.figure(figsize=(20,10));\n",
      "drivingLog.plot();\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "import csv\n",
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "\n",
      "\n",
      "class AngleNum(dict):\n",
      "\tdef __missing__(self, key):\n",
      "\t\treturn 0\n",
      "\n",
      "angle_num = AngleNum()\n",
      "\n",
      "\n",
      "with open(jupyter_string) as csvfile:\n",
      "    spamreader = csv.reader(csvfile)\n",
      "    for row in spamreader:\n",
      "    \tangle = round(float(row[3]),3)*100\n",
      "    \tangle_num[angle] = angle_num[angle]+1\n",
      "\n",
      "        \n",
      "labels=[]\n",
      "label_num=[]\n",
      "\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,1000,100)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "--------------------\n",
      "df=df[df['angle' <unk>]>=30]\n",
      "df=df[df['angle' <unk>]<=100]\n",
      "df=df[df['angle' <unk>]>=-100]\n",
      "=====\n",
      "labels=[]\n",
      "label_num=[]\n",
      "print(angle_num[0],angle_num[100],angle_num[-100])\n",
      "angle_num[0]=20\n",
      "angle_num[100]=angle_num[-100]=0\n",
      "print(angle_num[100],angle_num[-100])\n",
      "sum=0\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "    if label!=0:\n",
      "        sum+=num\n",
      "    \n",
      "print(sum)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,100,10)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "--------------------\n",
      "labels=[]\n",
      "label_num=[]\n",
      "for (label, num) in angle_num.items():\n",
      "    labels.append(label)\n",
      "    label_num.append(num)\n",
      "    if label!=0:\n",
      "        sum+=num\n",
      "    \n",
      "print(sum)\n",
      "\n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,100,10)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels,label_num)\n",
      "plt.show()\n",
      "=====\n",
      "class AngleNum1(dict):\n",
      "\tdef __missing__(self, key):\n",
      "\t\treturn 0\n",
      "\n",
      "angle_num_m = AngleNum1()\n",
      "\n",
      "with open(jupyter_string) as csvfile:\n",
      "    spamreader = csv.reader(csvfile)\n",
      "    for row in spamreader:\n",
      "    \tangle_m = round(float(row[3]),3)*100\n",
      "    \tangle_num_m[angle_m] = angle_num_m[angle_m]+1\n",
      "\n",
      "labels_m=[]\n",
      "labels_num_m=[]\n",
      "print(jupyter_string,angle_num_m[0])\n",
      "angle_num_m[0]=250\n",
      "for (label, num) in angle_num_m.items():\n",
      "    labels_m.append(label)\n",
      "    labels_num_m.append(num)\n",
      "\n",
      "\n",
      "    \n",
      "x_label = np.arange(-150,150,5)\n",
      "y_label = np.arange(0,300,10)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels_m,labels_num_m)\n",
      "plt.show()\n",
      "--------------------\n",
      "x_train_br = np.array(x_train_br)\n",
      "y_train = np.array(y_train)\n",
      "=====\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "factors = [10,2]\n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)  \n",
      "    val = np.array(x_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    img.set_title(y_train[i])    \n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "    \n",
      "--------------------\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)  \n",
      "    val = np.array(x_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    img.set_title(y_train[i])    \n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "    \n",
      "=====\n",
      "def add_random_shadow(image):\n",
      "    top_y = 320*np.random.uniform()\n",
      "    top_x = 0\n",
      "    bot_x = 160\n",
      "    bot_y = 320*np.random.uniform()\n",
      "    image_hls = cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
      "    shadow_mask = 0*image_hls[:,:,1]\n",
      "    X_m = np.mgrid[0:image.shape[0],0:image.shape[1]][0]\n",
      "    Y_m = np.mgrid[0:image.shape[0],0:image.shape[1]][1]\n",
      "    shadow_mask[((X_m-top_x)*(bot_y-top_y) -(bot_x - top_x)*(Y_m-top_y) >=0)]=1\n",
      "    \n",
      "    if np.random.randint(2)==1:\n",
      "        random_bright = .5\n",
      "        cond1 = shadow_mask==1\n",
      "        cond0 = shadow_mask==0\n",
      "        if np.random.randint(2)==1:\n",
      "            image_hls[:,:,1][cond1] = image_hls[:,:,1][cond1]*random_bright\n",
      "        else:\n",
      "            image_hls[:,:,1][cond0] = image_hls[:,:,1][cond0]*random_bright    \n",
      "    image = cv2.cvtColor(image_hls,cv2.COLOR_HLS2RGB)\n",
      "    return image\n",
      "\n",
      "for i in range(len(x_train)):\n",
      "    x_train[i] = add_random_shadow(np.array(x_train[i]))\n",
      "\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,30))\n",
      "factors = [10,2]\n",
      "    \n",
      "for ind in range(20):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)\n",
      "    val = np.array(x_train[i])\n",
      "    img.set_title(y_train[i])\n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "--------------------\n",
      "from sklearn.utils import shuffle\n",
      "x_train,y_train = shuffle(x_train,y_train)\n",
      "x_train = x_train.reshape(x_train.shape[0],64,64,1)\n",
      "x_test = x_test.reshape(x_test.shape[0],64,64,1)\n",
      "x_valid = x_valid.reshape(x_valid.shape[0],64,64,1)\n",
      "=====\n",
      "for i in range(len(x_train)):\n",
      "    x_train[i] = cv2.resize(np.array(x_train[i]),(200,66),interpolation=cv2.INTER_AREA) \n",
      "\n",
      "x_len = len(x_train)\n",
      "figure = plt.figure(figsize=(10,10))\n",
      "factors = [4,4]\n",
      "    \n",
      "for ind in range(16):\n",
      "    img = figure.add_subplot(factors[0],factors[1],ind + 1)\n",
      "    i = random.randint(0,x_len)\n",
      "    val = np.array(x_train[i])\n",
      "    img.set_title(y_train[i])\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(val, cmap=jupyter_string)\n",
      "--------------------\n",
      "plt.plot(women_degrees['Year' <unk>], women_degrees['Biology' <unk>], c=jupyter_string, label=jupyter_string)\n",
      "plt.plot(women_degrees['Year' <unk>], 100-women_degrees['Biology' <unk>], c=jupyter_string, label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.show()\n",
      "\n",
      "leitor[jupyter_string] = 100 - leitor[\"Biology\"]\n",
      "leitor.plot(kind = jupyter_string, x = [\"Year\"], y = [\"Biology\", jupyter_string], \n",
      "            color = [jupyter_string, jupyter_string], title = jupyter_string).legend(loc = jupyter_string, labels = [jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "data.drop('ID' <unk>, axis=1, inplace=True)\n",
      "=====\n",
      "data.var3.replace(-999999, 2)\n",
      "data.drop([\"ID\"], axis =1)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
      "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=42)\n",
      "=====\n",
      "from sklearn.utils import shuffle\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "x_train, y_train = shuffle(x_train, y_train)\n",
      "x_train = np.array(x_train)\n",
      "y_train = np.array(y_train)\n",
      "\n",
      "print(x_train.shape)\n",
      "\n",
      "\n",
      "\n",
      "train_features, test_features, train_labels, test_labels = train_test_split(\n",
      "    x_train,\n",
      "    y_train,\n",
      "    test_size=0.1,\n",
      "    random_state=40)\n",
      "\n",
      "\n",
      "\n",
      "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
      "    train_features,\n",
      "    train_labels,\n",
      "    test_size=0.2,\n",
      "    random_state=11)\n",
      "--------------------\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Activation, Dropout, Flatten\n",
      "from keras.layers import Convolution2D, MaxPooling2D\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Convolution2D(32, 3, 3, border_mode=jupyter_string, input_shape=(32, 32, 3)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Convolution2D(64, 3, 3, border_mode=jupyter_string))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten())\n",
      "=====\n",
      "from keras.models import Sequential\n",
      "from keras.layers.core import Dense, Activation, Flatten,Dropout,Lambda\n",
      "from keras.layers.convolutional import Convolution2D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Lambda(lambda x: x/255.-0.5, input_shape=(33,100,3),))\n",
      "model.add(Convolution2D(24, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(36, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(48, 5, 5, border_mode=jupyter_string, subsample=(2,2)))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(64, 3, 3))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Convolution2D(64, 3, 3))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(1164))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "\n",
      "model.add(Dense(100))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(50))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(10))\n",
      "model.add(Activation(jupyter_string))\n",
      "model.add(Dropout(0.5))\n",
      "\n",
      "model.add(Dense(1,name=jupyter_string))\n",
      "model.summary()\n",
      "\n",
      "--------------------\n",
      "x_train = np.array(x_train)\n",
      "y_train = np.array(y_train)\n",
      "print(x_train.shape)\n",
      "print(y_train.shape)\n",
      "=====\n",
      "y_num = AngleNum()\n",
      "\n",
      "\n",
      "for angle in y_train:\n",
      "    y_num[angle*100] = y_num[angle*100]+1\n",
      "    \n",
      "        \n",
      "labels_m=[]\n",
      "labels_num_m=[]\n",
      "for (label, num) in y_num.items():\n",
      "    labels_m.append(label)\n",
      "    labels_num_m.append(num)\n",
      "\n",
      "\n",
      "x_label = np.arange(-100,100,10)\n",
      "y_label = np.arange(0,20,2)\n",
      "plt.xticks(x_label,x_label,ha=jupyter_string,rotation=45)\n",
      "plt.yticks(y_label,y_label)\n",
      "plt.bar(labels_m,labels_num_m)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df=pd.read_csv(jupyter_string)\n",
      "df.describe()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(df['ENTRIESn_hourly' <unk>])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(df['ENTRIESn_hourly' <unk>])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "df.plot(x=['day' <<unk>>],y=['a' madeupword0002],kind=jupyter_string)\n",
      "df.plot(x='day' <<unk>>,y='b' <<unk>>,kind=jupyter_string)\n",
      "df.plot(x='day' <<unk>>,y='c' <<unk>>,kind=jupyter_string)\n",
      "df.plot(x='day' <<unk>>,y='d' <<unk>>,kind=jupyter_string)\n",
      "df.plot(x='day' <<unk>>,y='e' <<unk>>,kind=jupyter_string)\n",
      "df.plot(x='day' <<unk>>,y='f' <<unk>>,kind=jupyter_string)\n",
      "--------------------\n",
      "df['a' <unk>].fillna(df['a' <unk>].mean(),inplace=True)\n",
      "df['d' <unk>].fillna(df['d' <unk>].mean(),inplace=True)\n",
      "=====\n",
      "df_clean=df.copy() \n",
      "\n",
      "df_clean['a' madeupword0002]=df['a' madeupword0002].replace(0,np.nan);\n",
      "df_clean['d' <<unk>>]=df['d' <<unk>>].replace(1.0,np.nan);\n",
      "--------------------\n",
      "df_clean[jupyter_string]=np.log(df_clean[jupyter_string])\n",
      "df_clean[jupyter_string]=np.log(df_clean[jupyter_string])\n",
      "=====\n",
      "plist=list(jupyter_string)\n",
      "logdf=np.log(df_clean[plist])\n",
      "logdf['day' <<unk>>]=df_clean['day' <<unk>>].copy()\n",
      "logdf['timestr' <<unk>>]=df_clean['timestr' <<unk>>].copy()\n",
      "logdf=logdf[df_clean.columns.tolist()]\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(logdf[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.plot(logdf[jupyter_string])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "logdf.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "--------------------\n",
      "logdf.loc[:,'a' <unk>:'f' <unk>].mean().plot()\n",
      "=====\n",
      "dayseries=df_clean['day' <<unk>>].unique();\n",
      "logdf_day=pd.DataFrame({'day' <<unk>>: dayseries,'a' madeupword0002: dayseries,'b' <<unk>>: dayseries,'c' <<unk>>: dayseries,'d' <<unk>>: dayseries,'e' <<unk>>: dayseries,'f' <<unk>>: dayseries})\n",
      "logdf_day=logdf_day[['day' <<unk>>,'a' madeupword0002,'b' <<unk>>,'c' <<unk>>,'d' <<unk>>,'e' <<unk>>,'f' <<unk>>]]\n",
      "\n",
      "for idx,ii in enumerate(logdf_day['day' <<unk>>]):\n",
      "    logdf_day.iloc[idx,1:]=np.log(df_clean[df_clean['day' <<unk>>]==ii].mean(axis=0))\n",
      "--------------------\n",
      "logdf_day.head()\n",
      "=====\n",
      "logdf_day.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "=====\n",
      "train_data = data.as_matrix()\n",
      "Targets = train_data[:,-1]\n",
      "Targets = Targets.reshape((-1,1))\n",
      "train_data = np.delete(train_data, -1, 1)\n",
      "print (train_data.shape)\n",
      "\n",
      "--------------------\n",
      "logdf_day.loc[:,'a' <unk>].plot()\n",
      "logdf_day.loc[:,'b' <unk>].plot()\n",
      "logdf_day.loc[:,'c' <unk>].plot()\n",
      "logdf_day.loc[:,'d' <unk>].plot()\n",
      "=====\n",
      "df_diff=logdf[['day' <<unk>>]+plist].diff(1)\n",
      "normdiff=df_diff[df_diff['day' <<unk>>]==0][plist]\n",
      "daydiff=df_diff[df_diff['day' <<unk>>]==1][plist]\n",
      "enddiff=df_diff[df_diff['day' <<unk>>]>1][plist]\n",
      "--------------------\n",
      "normdiff.plot()\n",
      "daydiff.plot()\n",
      "enddiff.plot()\n",
      "=====\n",
      "normdiff[normdiff['c' <<unk>>]>-0.6].hist(bins=40);plt.show() \n",
      "daydiff.hist(bins=40);plt.show()\n",
      "enddiff.hist(bins=30);plt.show()\n",
      "--------------------\n",
      "from scipy import stats\n",
      "stats.ks_2samp(daydiff['c' <unk>],enddiff['c' <unk>])\n",
      "=====\n",
      "ks_normday=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "ks_normend=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "ks_dayend=pd.DataFrame(np.zeros([2,len(plist)]),columns=plist)\n",
      "for ii in plist:\n",
      "    ks_normday[ii]=stats.ks_2samp(normdiff[ii],daydiff[ii])\n",
      "    ks_normend[ii]=stats.ks_2samp(normdiff[ii],enddiff[ii])\n",
      "    ks_dayend[ii]=stats.ks_2samp(daydiff[ii],enddiff[ii])\n",
      "print(jupyter_string)\n",
      "print(ks_normday.iloc[1,:])\n",
      "print(jupyter_string)\n",
      "print(ks_normend.iloc[1,:])\n",
      "print(jupyter_string)\n",
      "print(ks_dayend.iloc[1,:])\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "=====\n",
      "logdflt327=logdf[logdf['day' <<unk>>]<327]\n",
      "logdfgt327=logdf[logdf['day' <<unk>>]>327]\n",
      "logdf327temp=logdf[logdf['day' <<unk>>]==327].copy() \n",
      "new_index=pd.Index(logdf[logdf['day' <<unk>>]==330]['timestr' <<unk>>]) \n",
      "logdf327=logdf327temp.set_index('timestr' <<unk>>).reindex(new_index).reset_index() \n",
      "\n",
      "logdf327['day' <<unk>>]=327\n",
      "\n",
      "logdf_clean=pd.concat([logdflt327,logdf327,logdfgt327],ignore_index=True) \n",
      "logdf_clean=logdf_clean[['day' <<unk>>,'timestr' <<unk>>]+plist] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "logdf_clean.head()\n",
      "=====\n",
      "t_d = 391 \n",
      "t_m = t_d*21 \n",
      "R_m = logdf_clean[plist].shift(-t_m) - logdf_clean[plist] \n",
      "\n",
      "R_m[['day' <<unk>>,'timestr' <<unk>>]]=logdf_clean[['day' <<unk>>,'timestr' <<unk>>]].copy();R_m=R_m[['day' <<unk>>,'timestr' <<unk>>]+plist]\n",
      "\n",
      "sigma_m = logdf_clean.rolling(t_d,min_periods=t_d//2).std(ddof=1)\n",
      "sigma_m[['day' <<unk>>,'timestr' <<unk>>]]=R_m[['day' <<unk>>,'timestr' <<unk>>]].copy()\n",
      "sigma_m=sigma_m[['day' <<unk>>,'timestr' <<unk>>]+plist]\n",
      "sigma_m.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "plt.ylim([0,0.1])\n",
      "--------------------\n",
      "sigma_m.loc[:,'a' <unk>].plot()\n",
      "plt.ylim([0,0.1])\n",
      "=====\n",
      "sigma_m_coarse=pd.DataFrame(np.random.randn(len(logdf_clean['day' <<unk>>].unique()),7),columns=[['day' <<unk>>]+plist])\n",
      "sigma_m_coarse['day' <<unk>>]=logdf_clean['day' <<unk>>].unique().copy()\n",
      "\n",
      "\n",
      "for ii in logdf_clean['day' <<unk>>].unique():\n",
      "    \n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'a' madeupword0002]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['a' madeupword0002].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'b' <<unk>>]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['b' <<unk>>].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'c' <<unk>>]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['c' <<unk>>].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'d' <<unk>>]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['d' <<unk>>].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'e' <<unk>>]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['e' <<unk>>].std(ddof=1)\n",
      "    sigma_m_coarse.loc[sigma_m_coarse['day' <<unk>>]==ii,'f' <<unk>>]=logdf_clean[logdf_clean['day' <<unk>>]==ii]['f' <<unk>>].std(ddof=1)\n",
      "--------------------\n",
      "sigma_m_coarse.loc[:,'a' <unk>].plot()\n",
      "plt.ylim([0,0.1])\n",
      "=====\n",
      "sigma_m_coarse.loc[:,'a' madeupword0002:'f' <<unk>>].plot()\n",
      "plt.ylim([0,0.1])\n",
      "--------------------\n",
      "fig=plt.figure(figsize=(12,8))\n",
      "plt.title(jupyter_string)\n",
      "ax1=fig.add_subplot(121)\n",
      "fig = sm.graphics.tsa.plot_acf(sigma_m_coarse[jupyter_string],lags=30,ax=ax1)\n",
      "ax2=fig.add_subplot(122)\n",
      "fig = sm.graphics.tsa.plot_pacf(sigma_m_coarse[jupyter_string],lags=30,ax=ax2)\n",
      "=====\n",
      "pq = pd.DataFrame(index=[jupyter_string,jupyter_string],columns=plist)\n",
      "pq.set_value(jupyter_string,'a' madeupword0002,1);pq.set_value(jupyter_string,'a' madeupword0002,0)\n",
      "pq.set_value(jupyter_string,'b' <<unk>>,3);pq.set_value(jupyter_string,'b' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'c' <<unk>>,0);pq.set_value(jupyter_string,'c' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'d' <<unk>>,0);pq.set_value(jupyter_string,'d' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'e' <<unk>>,0);pq.set_value(jupyter_string,'e' <<unk>>,0)\n",
      "pq.set_value(jupyter_string,'f' <<unk>>,1);pq.set_value(jupyter_string,'f' <<unk>>,1)\n",
      "--------------------\n",
      "from statsmodels.tsa.arima_model import ARMA\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "=====\n",
      "a_model = sm.tsa.ARMA(sigma_m_coarse['a' madeupword0002].values,(1,0)).fit(disp=False)\n",
      "b_model = sm.tsa.ARMA(sigma_m_coarse['b' <<unk>>].values,(3,0)).fit(disp=False)\n",
      "f_model = sm.tsa.ARMA(sigma_m_coarse['f' <<unk>>].values,(1,1)).fit(disp=False)\n",
      "\n",
      "--------------------\n",
      "a_pred = a_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "b_pred = b_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "f_pred = f_model.predict(start=jupyter_string,end=jupyter_string,dynamic=False)\n",
      "=====\n",
      "beg=len(sigma_m_coarse['a' madeupword0002].values)\n",
      "predict_a = a_model.predict(start=beg,end=beg+21)\n",
      "predict_b = b_model.predict(start=beg,end=beg+21)\n",
      "predict_f = f_model.predict(start=beg,end=beg+21)\n",
      "print(predict_a)\n",
      "print(predict_b)\n",
      "print(predict_f)\n",
      "--------------------\n",
      "test_data = data.as_matrix()\n",
      "Targets = test_data[:,-1]\n",
      "Targets = Targets.reshape((-1,1))\n",
      "test_data = np.delete(test_data, -1, 1)\n",
      "print (test_data.shape)\n",
      "=====\n",
      "lda = ql.LDA()\n",
      "lda.train(train_data, Targets)\n",
      "pclass, probabilities, discriminants = lda.use(train_data)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "--------------------\n",
      "plt.plot(sigma_m_coarse['a' <unk>],label=jupyter_string)\n",
      "plt.plot(sigma_m_coarse['b' <unk>],label=jupyter_string)\n",
      "plt.plot(sigma_m_coarse['f' <unk>],label=jupyter_string)\n",
      "plt.plot(predict_a,label=jupyter_string)\n",
      "plt.plot(predict_b,label=jupyter_string)\n",
      "plt.plot(predict_f,label=jupyter_string)\n",
      "plt.legend()\n",
      "=====\n",
      "a_model.summary()\n",
      "--------------------\n",
      "b_model.summary()\n",
      "=====\n",
      "b_model.summary()\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "sns.boxplot(x=jupyter_string, y=jupyter_string, data=df)\n",
      "plt.show()\n",
      "=====\n",
      "plt.figure()\n",
      "sns.boxplot(data=sigma_m_coarse[['b' <<unk>>,'c' <<unk>>,'d' <<unk>>]])\n",
      "plt.ylim([0,0.05])\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "plt.show()\n",
      "\n",
      "bls_all = pd.read_csv(jupyter_string)\n",
      "bls_all.head()\n",
      "--------------------\n",
      "bls_2008_01 = bls_all[bls_all['year' <unk>] == 2008]\n",
      "bls_2008_01.head()\n",
      "=====\n",
      "bls_all[jupyter_string] = bls_all['2018-04' madeupword0100] >= bls_all['2008-01' madeupword0271]\n",
      "bls_recovered = bls_all[bls_all[jupyter_string] == True]\n",
      "print(jupyter_string + str(len(bls_recovered)))\n",
      "\n",
      "bls_not_recovered = bls_all[bls_all[jupyter_string] == False]\n",
      "bls_not_recovered['state' madeupword0192]\n",
      "--------------------\n",
      "bls_recovered['state' <unk>].unique()\n",
      "=====\n",
      "ct = bls_all[bls_all['state' madeupword0192] == jupyter_string]\n",
      "wv = bls_all[bls_all['state' madeupword0192] == jupyter_string]\n",
      "nm = bls_all[bls_all['state' madeupword0192] == jupyter_string]\n",
      "\n",
      "print(ct[['state' madeupword0192, '2008-01' madeupword0271]])\n",
      "print(ct[['state' madeupword0192, '2018-04' madeupword0100]])\n",
      "\n",
      "print(wv[['state' madeupword0192, '2008-01' madeupword0271]])\n",
      "print(wv[['state' madeupword0192, '2018-04' madeupword0100]])\n",
      "\n",
      "print(nm[['state' madeupword0192, '2008-01' madeupword0271]])\n",
      "print(nm[['state' madeupword0192, '2018-04' madeupword0100]])\n",
      "\n",
      "\n",
      "id = bls_all[bls_all['state' madeupword0192] == jupyter_string]\n",
      "print(id[['state' madeupword0192, '2008-01' madeupword0271]])\n",
      "print(id[['state' madeupword0192, '2018-04' madeupword0100]])\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "plt.rcParams[jupyter_string] = (10, 6)\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "plt.rcParams[jupyter_string] = jupyter_string\n",
      "=====\n",
      "bls_n = pd.read_excel(jupyter_string)\n",
      "\n",
      "bls_n.head(15)\n",
      "--------------------\n",
      "bls_n.info()\n",
      "=====\n",
      "bls_n = bls_n[12:]\n",
      "bls_n.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "                jupyter_string, jupyter_string]\n",
      "\n",
      "\n",
      "bls_n[jupyter_string] = (bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string]\\\n",
      "                                    + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string]\\\n",
      "                                    + bls_n[jupyter_string] + bls_n[jupyter_string] + bls_n[jupyter_string])\\\n",
      "                                    / 12.0\n",
      "\n",
      "bls_n.head()\n",
      "--------------------\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "bls_n = bls_n[bls_n[jupyter_string] != jupyter_string]\n",
      "=====\n",
      "bls_n.Year_Average.plot()\n",
      "--------------------\n",
      "CT_n.Year_Average.plot()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "ct_by_mo = ct.transpose()\n",
      "ct_by_mo\n",
      "--------------------\n",
      "qda = ql.QDA()\n",
      "qda.train(train_data, Targets)\n",
      "pclass, probabilities, discriminants = qda.use(train_data)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "=====\n",
      "qda = ql.QDA()\n",
      "qda.train(train_data, Targets)\n",
      "pc, prob, d = qda.use(train_data)\n",
      "showResults(pc, Targets, jupyter_string)\n",
      "--------------------\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "plt.plot(ct_by_mo.index, ct_by_mo[jupyter_string])\n",
      "=====\n",
      "ct_by_mo = ct_by_mo[1:]\n",
      "ct_by_mo = ct_by_mo[0:-5]\n",
      "\n",
      "\n",
      "\n",
      "ct_by_mo = ct_by_mo[191:]\n",
      "\n",
      "ct_by_mo.plot()\n",
      "--------------------\n",
      "ct_by_mo = ct_by_mo[1:]\n",
      "ct_by_mo = ct_by_mo[0:-5]\n",
      "\n",
      "\n",
      "\n",
      "ct_by_mo = ct_by_mo[191:]\n",
      "\n",
      "ct_by_mo.plot()\n",
      "=====\n",
      "ct_by_mo.tail(6)\n",
      "--------------------\n",
      "ct_by_mo.count()\n",
      "=====\n",
      "ct_3_mo = ct_by_mo.tail(3)\n",
      "feb_to_apr_loss = (ct_3_mo.iloc[0] - ct_3_mo.iloc[2]) * 1000\n",
      "print(jupyter_string + str(feb_to_apr_loss))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "plt.show()\n",
      "df=pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "def divide_int_float_strings(df):\n",
      "    list_remove=[]\n",
      "    new_list=[]\n",
      "    for items in df:\n",
      "        if isinstance(df[items][0],str):\n",
      "            list_remove.append(items)\n",
      "        else:\n",
      "            new_list.append(items)\n",
      "    return new_list,list_remove\n",
      "new_list=divide_int_float_strings(df)[0]\n",
      "--------------------\n",
      "df=df[new_list]\n",
      "df.head()\n",
      "=====\n",
      "df[new_list].corr()\n",
      "--------------------\n",
      "df[jupyter_string]=np.log(df[jupyter_string])\n",
      "df[jupyter_string]=np.sqrt(df[jupyter_string])\n",
      "df.head()\n",
      "=====\n",
      "def better_relations(df,new_list):\n",
      "    better_relation_dict={}\n",
      "    better_relation_name_d={}\n",
      "    for items in new_list:\n",
      "        log=df[jupyter_string].corr(np.log10(df[items]))\n",
      "        square=df[jupyter_string].corr(np.square(df[items]))\n",
      "        sqrt=df[jupyter_string].corr(np.sqrt(df[items]))\n",
      "        normal=df[jupyter_string].corr(df[items])\n",
      "        method=[log,square,sqrt,normal]\n",
      "        max1=np.nanmax(method)\n",
      "        better_relation_dict[items]=max1\n",
      "        if log==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if square==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if sqrt==max1:\n",
      "            better_relation_name_d[items]=jupyter_string\n",
      "        if normal==max1:\n",
      "            better_relation_name_d[items]=jupyter_string       \n",
      "    return better_relation_dict,better_relation_name_d\n",
      "transformer=better_relations(df)[1]\n",
      "--------------------\n",
      "df[jupyter_string]=np.log10(df[jupyter_string])\n",
      "=====\n",
      "df_copy=df.copy()\n",
      "df_copy[jupyter_string]=np.log10(df_copy[jupyter_string])\n",
      "improve_score(better_relations(df_copy)[0],better_relations(df)[0])\n",
      "--------------------\n",
      "transformed_data=transform_data(df,better_relations(df)[1])\n",
      "transformed_data[new_list].corr()\n",
      "=====\n",
      "df_corr=pd.DataFrame(better_relations(df)[0])\n",
      "df_corr=df_corr.sort_values(jupyter_string,ascending=False)\n",
      "df_corr=df_corr.drop(df_corr.index[0])\n",
      "df_corr\n",
      "--------------------\n",
      "model = ExtraTreesClassifier(random_state=361)\n",
      "grid_obj = GridSearchCV(model, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_obj = grid_obj.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "trainNN, evaluateNN = ml.trainValidateTestKFoldsClassification(trainNN, evaluateNN, train_data, Targets, [[[5], 10]], nFolds=10, shuffle=True, verbose=False)\n",
      "printResults(jupyter_string, resultsNN)\n",
      "=====\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(random_state=1729)\n",
      "data = data.drop([\"TARGET\"], axis =1)\n",
      "selector = clf.fit(data, Targets)\n",
      "\n",
      "\n",
      "feat_imp = pandas.Series(clf.feature_importances_, index = data.columns.values).sort_values(ascending=False)\n",
      "feat_imp[:40].plot(kind=jupyter_string, title=jupyter_string, figsize=(12, 8))\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "fs = SelectFromModel(selector, prefit=True)\n",
      "\n",
      "newTrain = fs.transform(data)\n",
      "--------------------\n",
      "X_train_dict = OrderedDict([(jupyter_string, pcaed_X_train),\n",
      "                            (jupyter_string, subspaced_X_train),\n",
      "                            (jupyter_string, fs_and_pca_X_train),\n",
      "                            (jupyter_string, X_train)])\n",
      "X_test_dict = OrderedDict([(jupyter_string, pcaed_X_test),\n",
      "                           (jupyter_string, subspaced_X_test),\n",
      "                           (jupyter_string, fs_and_pca_X_test),\n",
      "                           (jupyter_string, X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "=====\n",
      "y_train_et = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_et = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_test_rf)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.plot([0, 1], [0, 1], jupyter_string)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "_ = joblib.dump(clf, jupyter_string)\n",
      "--------------------\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import PandasTools\n",
      "from rdkit.Chem import Pandas\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "clf = GradientBoostingClassifier(random_state=42)\n",
      "clf.fit(X_train, y_train)\n",
      "=====\n",
      "X_total = sample[:, :-1]\n",
      "X_total.shape, y.shape\n",
      "--------------------\n",
      "clf.fit(X_total, y)\n",
      "=====\n",
      "clf.fit(X_total, y)\n",
      "y_hat = clf.predict_proba(X_total)[:, 1]\n",
      "roc_auc_score(y, y_hat)\n",
      "--------------------\n",
      "np.sort(y_test_rf)[-int(np.sum(y_test)):][::-1]\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist(y_hat, 100)\n",
      "--------------------\n",
      "df[pd.isnull(df[jupyter_string])]\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "unlabeled_indices = np.where(y_hat == 1)[0]\n",
      "unlabeled_indices_test = np.where(y_test == 1)[0]\n",
      "=====\n",
      "n_examples = 3\n",
      "indices = y_hat[border:].argsort()[-n_examples:][::-1].tolist()\n",
      "indices = [x + border for x in indices]\n",
      "indices\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist(y_hat[indices], 100)\n",
      "=====\n",
      "found_candidates = random_smiles.iloc[indices, 0].values.tolist()\n",
      "found_candidates\n",
      "--------------------\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(random_state=1729)\n",
      "data = data.drop([\"TARGET\"], axis =1)\n",
      "selector = clf.fit(data, Targets)\n",
      "\n",
      "\n",
      "feat_imp = pandas.Series(clf.feature_importances_, index = data.columns.values).sort_values(ascending=False)\n",
      "feat_imp[:40].plot(kind=jupyter_string, title=jupyter_string, figsize=(12, 8))\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.subplots_adjust(bottom=0.3)\n",
      "plt.savefig(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "fs = SelectFromModel(selector, prefit=True)\n",
      "\n",
      "newTest = fs.transform(test)\n",
      "print (newTest.shape)\n",
      "=====\n",
      "lda = ql.LDA()\n",
      "lda.train(newTrain, Targets)\n",
      "pclass, probabilities, discriminants = lda.use(newTrain)\n",
      "showResults(pclass,Targets, jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string] = df[jupyter_string].apply(smiles_to_numpy)\n",
      "df.head()\n",
      "=====\n",
      "path_to_zinc = jupyter_string\n",
      "if os.path.isfile(path_to_zinc):\n",
      "    zinc_smiles = pd.read_csv(path_to_zinc)\n",
      "else:\n",
      "    zinc_smiles = pd.read_csv(jupyter_string +\n",
      "                              jupyter_string +\n",
      "                              jupyter_string)\n",
      "zinc_smiles.head()\n",
      "--------------------\n",
      "zinc_smiles.drop(zinc_smiles.index[random_indices], inplace=True)\n",
      "=====\n",
      "random_smiles = zinc_smiles.iloc[random_indices, :]\n",
      "random_smiles.head()\n",
      "--------------------\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem\n",
      "=====\n",
      "positives = np.hstack((geroprotectors, np.ones((geroprotectors.shape[0], 1))))\n",
      "negatives = np.hstack((random_compounds, np.zeros((random_compounds.shape[0], 1))))\n",
      "sample = np.vstack((positives, negatives))\n",
      "sample.shape\n",
      "--------------------\n",
      "df = pd.DataFrame(sample, columns=[jupyter_string, jupyter_string])\n",
      "df.head()\n",
      "=====\n",
      "border = positives.shape[0]\n",
      "--------------------\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
      "plt.plot(fpr, tpr)\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "np.mean(positives[:, 1152])\n",
      "--------------------\n",
      "np.mean(negatives[:, 1152])\n",
      "=====\n",
      "np.mean(negatives[:, 1152])\n",
      "--------------------\n",
      "plt.scatter(positives[:, 1152], negatives[:, 1152])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.hist([scores[key] for key in scores.keys()], 50)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "=====\n",
      "weights = np.array([abs(scores[key] - 0.5) for key in scores.keys()])\n",
      "weighted_positives = positives[:, :-1] * weights\n",
      "weighted_negatives = negatives[:, :-1] * weights\n",
      "weighted_sample = sample[:, :-1] * weights\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "ax_one.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 1], c=jupyter_string, s=50)\n",
      "_ = ax_one.scatter(pcaed_positives[:, 0], pcaed_positives[:, 1], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 0], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 1], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 1], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "sns.kdeplot(pcaed_positives[:, 0], pcaed_positives[:, 2],\n",
      "            cmap=jupyter_string, shade=True, shade_lowest=False, ax=ax_one)\n",
      "ax_two = fig.add_subplot(122, sharex=ax_one, sharey=ax_one)\n",
      "ax_two.set_title(jupyter_string)\n",
      "ax_two.set_xlabel(jupyter_string)\n",
      "ax_two.set_ylabel(jupyter_string)\n",
      "_ = sns.kdeplot(pcaed_negatives[:, 0], pcaed_negatives[:, 2],\n",
      "            cmap=jupyter_string, shade=True, shade_lowest=False, ax=ax_two)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import math\n",
      "import numpy as np\n",
      "from pylab import *\n",
      "import seaborn as sns\n",
      "import scipy.stats\n",
      "import random\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(15, 7.5))\n",
      "ax_one = fig.add_subplot(121)\n",
      "ax_one.set_title(jupyter_string)\n",
      "ax_one.set_xlabel(jupyter_string)\n",
      "ax_one.set_ylabel(jupyter_string)\n",
      "_ = ax_one.scatter(pcaed_negatives[:, 0], pcaed_negatives[:, 1], c=jupyter_string, s=50)\n",
      "_ = ax_one.scatter(pcaed_positives[:, 0], pcaed_positives[:, 1], c=jupyter_string, s=50)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15, 15))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.scatter(pcaed_negatives[:, 1], pcaed_negatives[:, 2])\n",
      "_ = ax.scatter(pcaed_positives[:, 1], pcaed_positives[:, 2], c=jupyter_string, s=50)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "=====\n",
      "X = sample[:, :-1]\n",
      "y = sample[:, -1]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=361)\n",
      "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "X_train = pca.fit_transform(X_train)\n",
      "X_test = pca.transform(X_test)\n",
      "X_train.shape, X_test.shape\n",
      "=====\n",
      "scores = {}\n",
      "for i in range(X_train.shape[1]):\n",
      "    scores[i] = roc_auc_score(y_train, X_train[:, i])\n",
      "weights = np.array([abs(scores[key] - 0.5) for key in scores.keys()])\n",
      "weighted_X_train = X_train * weights\n",
      "weighted_X_test = X_test * weights\n",
      "--------------------\n",
      "plt.plot(evr)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.plot(evr.cumsum())\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(10, 7.5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "_ = ax.plot(evr.cumsum())\n",
      "=====\n",
      "pca = PCA(n_components=20, random_state=361)\n",
      "pcaed_X_train = pca.fit_transform(weighted_X_train)\n",
      "pcaed_X_test = pca.transform(weighted_X_test)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import f_classif\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "selector = SelectKBest(f_classif, k=20)\n",
      "selector.fit(X_train, y_train)\n",
      "\n",
      "X_train_selected = selector.transform(X_train)\n",
      "X_test_selected = selector.transform(X_test)\n",
      "=====\n",
      "feature_selector = SelectKBest(chi2, k=80)\n",
      "subspaced_X_train = feature_selector.fit_transform(X_train, y_train)\n",
      "subspaced_X_test = feature_selector.transform(X_test)\n",
      "--------------------\n",
      "subspaced_X_train = pd.DataFrame(subspaced_X_train)\n",
      "subspaced_X_test = pd.DataFrame(subspaced_X_test)\n",
      "=====\n",
      "fs_and_pca_X_train = np.hstack((subspaced_X_train, pcaed_X_train))\n",
      "fs_and_pca_X_test = np.hstack((subspaced_X_test, pcaed_X_test))\n",
      "fs_and_pca_X_train.shape, fs_and_pca_X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "X_train = fs_and_pca_X_train[:, fs_and_pca_X_train.shape[1] - 1]\n",
      "X_test = fs_and_pca_X_test[:, fs_and_pca_X_test.shape[1] - 1]\n",
      "=====\n",
      "all_and_pca_X_train = np.hstack((X_train, pcaed_X_train))\n",
      "all_and_pca_X_test = np.hstack((X_test, pcaed_X_test))\n",
      "all_and_pca_X_train.shape, all_and_pca_X_test.shape, y_train.shape, y_test.shape\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "X_train = pd.DataFrame(X_train_dict)\n",
      "X_test = pd.DataFrame(X_test_dict)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "temp = df['temperature' <<unk>>]\n",
      "print(np.mean(temp))\n",
      "print(np.std(temp))\n",
      "sns.set()\n",
      "sns.distplot(temp)\n",
      "sns.plt.show()\n",
      "--------------------\n",
      "gs = GridSearchCV(estimator= X_train_dict[jupyter_string],\n",
      "                  param_grid= grid_params,\n",
      "                  scoring= jupyter_string,\n",
      "                  cv= kf,\n",
      "                  n_jobs= -1)\n",
      "gs = gs.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "grid_search = GridSearchCV(clf, grid_params, scoring=jupyter_string, cv=kf)\n",
      "grid_search.fit(X_train_dict[jupyter_string], y_train)\n",
      "=====\n",
      "tuner = ModelsTuner(X_train_dict, X_test_dict, y_train, y_test)\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "--------------------\n",
      "X_train_dict = OrderedDict([(jupyter_string, pcaed_X_train),\n",
      "                            (jupyter_string, subspaced_X_train),\n",
      "                            (jupyter_string, fs_and_pca_X_train),\n",
      "                            (jupyter_string, X_train)])\n",
      "X_test_dict = OrderedDict([(jupyter_string, pcaed_X_test),\n",
      "                           (jupyter_string, subspaced_X_test),\n",
      "                           (jupyter_string, fs_and_pca_X_test),\n",
      "                           (jupyter_string, X_test)])\n",
      "clf = tuner.tune(clf, grid_params, kf, jupyter_string)\n",
      "=====\n",
      "y_train_gb = clf.predict_proba(X_train_dict[tuner.best_subspace_key_])[:, 1]\n",
      "y_test_gb = clf.predict_proba(X_test_dict[tuner.best_subspace_key_])[:, 1]\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "classes = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "classes = classes.values[:, 1]\n",
      "print(classes)\n",
      "print(classes.shape)\n",
      "--------------------\n",
      "predicates = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "predicates = predicates.values[:, 1]\n",
      "print(predicates)\n",
      "print(predicates.shape)\n",
      "=====\n",
      "predicates = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "predicates = predicates.values[:,1]\n",
      "print(predicates.shape)\n",
      "print(predicates)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "classes = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "classes = classes.values[:, 1]\n",
      "print(classes)\n",
      "print(classes.shape)\n",
      "=====\n",
      "dataset = pd.read_csv(jupyter_string, header=None, delimiter=rjupyter_string)\n",
      "dataset = dataset.values\n",
      "print(dataset.shape)\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(dataset)\n",
      "X = pca.transform(dataset)\n",
      "print(X.shape)\n",
      "=====\n",
      "projected = np.matmul(direction, dataset.T)\n",
      "projected.shape\n",
      "--------------------\n",
      "plt.scatter(projected[:,0], projected[:,1])\n",
      "plt.show()\n",
      "=====\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "from pylab import rcParams\n",
      "from IPython.display import set_matplotlib_formats\n",
      "set_matplotlib_formats(jupyter_string)\n",
      "rcParams[jupyter_string] = 13,13\n",
      "\n",
      "plt.scatter(projected[0,:], projected[1, :])\n",
      "\n",
      "for i in range(classes.shape[0]):\n",
      "    x = projected[0, i]\n",
      "    y = projected[1, i]\n",
      "    plt.annotate(classes[i], xy=(x,y), xytext=(x+1,y+1))\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "plt.hist(x, bins=20, alpha=0.75)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "l = list(df[jupyter_string])\n",
      "n, bins, patches = plt.hist(l, 20, normed=1, facecolor=jupyter_string, alpha=0.75)\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "l = list(df[jupyter_string])\n",
      "n, bins, patches = plt.hist(l, 20, normed=1, facecolor=jupyter_string, alpha=0.75)\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "=====\n",
      "param = norm.fit(l)\n",
      "x = linspace(0,1,100) \n",
      "pdf_fitted = norm.pdf(x, loc=param[0], scale=param[1])\n",
      "\n",
      "title(jupyter_string)\n",
      "text(0.1,1.7, rjupyter_string)\n",
      "grid()\n",
      "xlabel(jupyter_string)\n",
      "ylabel(jupyter_string)\n",
      "plot(x, pdf_fitted, jupyter_string)\n",
      "hist(l, normed=1,alpha=0.5, bins=20)\n",
      "savefig(jupyter_string, bbox_inches=jupyter_string)\n",
      "show()\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "df.loc[df.gender == jupyter_string, 'gender' madeupword0002] = 0\n",
      "df.loc[df.gender == jupyter_string, 'gender' madeupword0002] = 1\n",
      "\n",
      "chi2, p, dof, ex = scipy.stats.chi2_contingency(df.head())\n",
      "print(jupyter_string, chi2)\n",
      "print(jupyter_string, p)\n",
      "print(jupyter_string,dof)\n",
      "\n",
      "--------------------\n",
      "m.components_[0].__str__().split(jupyter_string)[1].split(jupyter_string)[0]\n",
      "=====\n",
      "x = linspace(0,1,100) \n",
      "\n",
      "\n",
      "f1 = norm.pdf(x, 0.769444751466, 0.1)\n",
      "f2 = norm.pdf(x, 0.564284885974, 0.1)\n",
      "f3 = norm.pdf(x, 0.483220658218, 0.146212605803)\n",
      "f4 = norm.pdf(x, 0.769446855025, 0.1)\n",
      "f5 = norm.pdf(x, 0.397788035948, 0.136193314069)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig, ax1 = plt.subplots()\n",
      "title(jupyter_string)\n",
      "grid()\n",
      "xlabel(jupyter_string)\n",
      "ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "n, bins, patches = ax1.hist(l, normed=False,alpha=0.5, bins=20)\n",
      "\n",
      "ax2 = ax1.twinx()\n",
      "ax2.plot(x, f1, jupyter_string,\n",
      "     x, f2, jupyter_string,\n",
      "     x, f3, jupyter_string,\n",
      "     x, f4, jupyter_string,\n",
      "     x, f5, jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "ax2.get_yaxis().set_ticks([])\n",
      "savefig(jupyter_string, bbox_inches=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.style.use(jupyter_string)\n",
      "matplotlib.rcParams[jupyter_string] = [16.0, 10.0]\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data.boxplot(['radius_mean' <<unk>>, 'texture_mean' <<unk>>, 'perimeter_mean' <<unk>>, 'texture_worst' <<unk>>, 'perimeter_worst' <<unk>>, 'area_se' <<unk>>, 'radius_worst' <<unk>>, 'texture_worst' <<unk>>])\n",
      "plt.show()\n",
      "--------------------\n",
      "X = data.drop('diagnosis' <unk>, axis=1)\n",
      "y = data['diagnosis' <unk>]\n",
      "=====\n",
      "data.boxplot(['area_mean' <<unk>>, 'area_worst' <<unk>>])\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data.hist()\n",
      "plt.show()\n",
      "--------------------\n",
      "data = data.drop(['Unnamed: 32' <unk>, 'id' <unk>, 'diagnosis' <unk>], axis=1)\n",
      "=====\n",
      "data.corr(method=jupyter_string, min_periods=1)\n",
      "--------------------\n",
      "data.corr(method=jupyter_string, min_periods=1)\n",
      "=====\n",
      "import seaborn as sns \n",
      "corr = data.corr()\n",
      "plt.subplots(figsize=(16,16))\n",
      "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, \n",
      "            cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)\n",
      "plt.show()\n",
      "--------------------\n",
      "from matplotlib.colors import ListedColormap\n",
      "X_set, Y_set = X_std, Y\n",
      "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
      "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
      "plt.contourf(X1, X2, clf.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
      "             alpha = 0.75, cmap = ListedColormap([jupyter_string, jupyter_string]))\n",
      "plt.xlim(X1.min(),\n",
      "=====\n",
      "no_effect_features = ['smoothness_mean' <<unk>>, 'symmetry_mean' <<unk>>, 'fractal_dimension_mean' <<unk>>, 'texture_se' <<unk>>, 'smoothness_se' <<unk>>, 'compactness_se' <<unk>>, 'concavity_se' <<unk>>, 'symmetry_se' <<unk>>, 'fractal_dimension_se' <<unk>>, 'smoothness_worst' <<unk>>, 'symmetry_worst' <<unk>>, 'fractal_dimension_worst' <<unk>>]\n",
      "\n",
      "\n",
      "data_reduced = data.drop(no_effect_features, axis=1)\n",
      "--------------------\n",
      "df.groupby('gender' <unk>).describe()\n",
      "=====\n",
      "scipy.stats.ttest_1samp(df.temperature, 98.6)\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.hist(data_reduced['diagnosis' <unk>])\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "data_reduced.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df08 = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "df12 = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "df08 = df08[df08[jupyter_string] != 1]\n",
      "=====\n",
      "filtered_cols = [jupyter_string, jupyter_string,\n",
      "                 jupyter_string, jupyter_string,\n",
      "                 jupyter_string, jupyter_string]\n",
      "is_invalid_filter = (df08[filtered_cols] > 1).any(axis=1)\n",
      "n_rows_to_drop = is_invalid_filter[is_invalid_filter].shape[0]\n",
      "print(jupyter_string.format(n_rows_to_drop))\n",
      "df08 = df08[~is_invalid_filter]\n",
      "df08.shape[0]\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08[jupyter_string], ax=ax[0])\n",
      "sns.distplot(df08[jupyter_string], ax=ax[1])\n",
      "--------------------\n",
      "fig, ax = plt.subplots(ncols=2)\n",
      "sns.distplot(df08[jupyter_string], ax=ax[0])\n",
      "sns.distplot(df08[jupyter_string], ax=ax[1])\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2016' <<unk>>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "--------------------\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2016' <unk>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2012' <<unk>>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "--------------------\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2013' <unk>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "=====\n",
      "alt.Chart(df08).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X('total_2008' <<unk>>,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string\n",
      ")\n",
      "--------------------\n",
      "year_melt = pd.melt(frame=df08,\n",
      "                    id_vars=['year' <unk>],\n",
      "                    value_vars=['total_2012' <unk>, 'total_2008' <unk>])\n",
      "year_melt.head()\n",
      "=====\n",
      "melted = df08.melt(id_vars=['fips_code' <<unk>>, 'county' <<unk>>])\n",
      "\n",
      "new_cols = pd.DataFrame(melted[jupyter_string].str.rsplit(jupyter_string, 1).tolist(), columns=[jupyter_string, jupyter_string])\n",
      "\n",
      "reshaped = pd.concat([melted, new_cols], axis=1).drop(jupyter_string, axis=1)\n",
      "reshaped.head()\n",
      "--------------------\n",
      "reshaped = reshaped.rename(columns={jupyter_string: jupyter_string, jupyter_string: jupyter_string, jupyter_string: jupyter_string})\n",
      "reshaped.head()\n",
      "=====\n",
      "data = reshaped.pivot_table(index=['fips_code' <<unk>>, 'county' <<unk>>, jupyter_string], columns=[jupyter_string])\n",
      "data.head()\n",
      "--------------------\n",
      "data = data.reset_index()\n",
      "data.head()\n",
      "=====\n",
      "flattened = data.copy()\n",
      "flattened.columns = flattened.columns.get_level_values(1)\n",
      "flattened = pd.DataFrame(flattened.to_records())\n",
      "--------------------\n",
      "temp_mean = np.mean(df.temperature)\n",
      "temp_mean\n",
      "=====\n",
      "m = df.loc[df.gender == 1, 'temperature' <<unk>>]\n",
      "f = df.loc[df.gender == 0, 'temperature' <<unk>>]\n",
      "--------------------\n",
      "flattened.head()\n",
      "=====\n",
      "alt.data_transformers.enable(jupyter_string, max_rows=1000000)\n",
      "alt.Chart(flattened).mark_point(strokeOpacity=.3).encode(\n",
      "    x=alt.X(jupyter_string,\n",
      "            scale=alt.Scale(type=jupyter_string)),\n",
      "    y=jupyter_string,\n",
      "    color=jupyter_string)\n",
      "--------------------\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "flattened[jupyter_string] = flattened[jupyter_string] + flattened[jupyter_string]\n",
      "=====\n",
      "def choose_winner(row):\n",
      "    ''jupyter_string''\n",
      "    if row[jupyter_string] > row[jupyter_string] and row[jupyter_string] > row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    elif row[jupyter_string] >= row[jupyter_string] and row[jupyter_string] >= row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    elif row[jupyter_string] >= row[jupyter_string] and row[jupyter_string] >= row[jupyter_string]:\n",
      "        return jupyter_string\n",
      "    else:\n",
      "        return jupyter_string\n",
      "flattened[jupyter_string] = flattened.apply(choose_winner, axis=1)\n",
      "flattened[jupyter_string] = flattened[jupyter_string] - flattened[jupyter_string]\n",
      "flattened.head()\n",
      "--------------------\n",
      "flattened.to_csv(jupyter_string)\n",
      "=====\n",
      "flattened.to_csv(jupyter_string, index=False)\n",
      "--------------------\n",
      "flattened.head()\n",
      "=====\n",
      "pd.read_csv(jupyter_string).head()\n",
      "--------------------\n",
      "plot_decision_regions(X, y, classifier=ppn)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plot_decision_regions(X, y, classifier=ppn)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.scatter(df[0],df[1])\n",
      "plt.show()\n",
      "=====\n",
      "y = df.iloc[0:100, 4].values\n",
      "y = np.where(y == jupyter_string, -1, 1)\n",
      "X = df.iloc[0:100, [0, 2]].values\n",
      "\n",
      "plt.scatter(X[:50, 0], X[:50, 1], color=jupyter_string, marker=jupyter_string, label=jupyter_string)\n",
      "plt.scatter(X[50:100, 0], X[50:100, 1], color=jupyter_string, marker=jupyter_string, label=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "iris = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "\n",
      "df.head(df.size)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "train = pd.read_csv(jupyter_string)\n",
      "test = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "\n",
      "np.random.seed(42)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams[jupyter_string] = 14\n",
      "plt.rcParams[jupyter_string] = 12\n",
      "plt.rcParams[jupyter_string] = 12\n",
      "\n",
      "train_data=pd.read_csv(jupyter_string)\n",
      "test_data=pd.read_csv(jupyter_string)\n",
      "train_data.head()\n",
      "--------------------\n",
      "test_data.head()\n",
      "=====\n",
      "train_data.info()\n",
      "--------------------\n",
      "temp_m = np.mean(m)\n",
      "temp_f = np.mean(f)\n",
      "temp_m, temp_f\n",
      "=====\n",
      "diff_mean = m.mean() - f.mean()\n",
      "m_var = np.var(m)\n",
      "f_var = np.var(f)\n",
      "var = math.sqrt((m_var/len(m)) + (f_var/len(f)))\n",
      "\n",
      "lim = 1.96 * var\n",
      "up = diff_mean + lim\n",
      "low = diff_mean - lim\n",
      "\n",
      "test = 1.65 * var\n",
      "print(jupyter_string,abs(diff_mean))\n",
      "print(jupyter_string,test)\n",
      "print(jupyter_string, up,jupyter_string,low,jupyter_string)\n",
      "--------------------\n",
      "train_data.describe()\n",
      "=====\n",
      "test_data.info()\n",
      "--------------------\n",
      "test_data.describe()\n",
      "=====\n",
      "train_data.describe()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_dev, y_train, y_dev = train_test_split(train_data.drop('Survived' <unk>, axis=1), train_data['Survived' <unk>], test_size=0.33, random_state=42)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_set, dev_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
      "dev_set.head()\n",
      "--------------------\n",
      "strat_test_set[\"Pclass\"].value_counts()\n",
      "=====\n",
      "def Pclass_proportions(data):\n",
      "    return data[\"Pclass\"].value_counts() / len(data)\n",
      "\n",
      "train_set, dev_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
      "\n",
      "compare_props = pd.DataFrame({\n",
      "    jupyter_string: Pclass_proportions(train_data),\n",
      "    jupyter_string: Pclass_proportions(strat_test_set),\n",
      "    jupyter_string: Pclass_proportions(dev_set),\n",
      "}).sort_index()\n",
      "compare_props[jupyter_string] = 100 * compare_props[jupyter_string] / compare_props[jupyter_string] - 100\n",
      "compare_props[jupyter_string] = 100 * compare_props[jupyter_string] / compare_props[jupyter_string] - 100\n",
      "--------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "train_set=strat_train_set.copy()\n",
      "--------------------\n",
      "train_set.drop('name' <unk>,axis=1,inplace=True)\n",
      "=====\n",
      "train_set.info()\n",
      "--------------------\n",
      "corr_matrix = train_set.corr()\n",
      "corr_matrix\n",
      "=====\n",
      "corr_mat=train_set.corr()\n",
      "--------------------\n",
      "corr_mat['Survived' <unk>].sort_values(ascending=False)\n",
      "=====\n",
      "train_data[jupyter_string] = pd.cut(train_data['Age' <<unk>>], 5)\n",
      "train_data[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean().sort_values(by=jupyter_string, ascending=True)\n",
      "--------------------\n",
      "train_data[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean().sort_values(by=jupyter_string, ascending=True)\n",
      "=====\n",
      "train_data[jupyter_string] = train_data['SibSp' <<unk>>] + train_data['Parch' <<unk>>] + 1\n",
      "\n",
      "train_data[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean().sort_values(by='Survived' <<unk>>, ascending=False)\n",
      "--------------------\n",
      "train_data[jupyter_string] = train_data['SibSp' <unk>] + train_data['Parch' <unk>] + 1\n",
      "\n",
      "train_data[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean().sort_values(by='Survived' <unk>)\n",
      "=====\n",
      "train_data[jupyter_string]=0\n",
      "train_data.loc[train_data[jupyter_string]==1,jupyter_string]=1\n",
      "\n",
      "train_data[[jupyter_string, 'Survived' <<unk>>]].groupby([jupyter_string], as_index=False).mean().sort_values(by='Survived' <<unk>>, ascending=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import urllib\n",
      "urllib.urlretrieve(jupyter_string, jupyter_string)\n",
      "--------------------\n",
      "train_data[jupyter_string]=0\n",
      "train_data.loc[train_data[jupyter_string]==1,jupyter_string]=1\n",
      "\n",
      "train_data[[jupyter_string, 'Survived' <unk>]].groupby([jupyter_string], as_index=False).mean().sort_values(by='Survived' <unk>, ascending=False)\n",
      "=====\n",
      "train_data[jupyter_string] = pd.qcut(train_data['Fare' <<unk>>], 4)\n",
      "train_data[[jupyter_string,'Survived' <<unk>>]].groupby([jupyter_string],as_index=False).mean().sort_values(by='Survived' <<unk>>, ascending=False)\n",
      "--------------------\n",
      "train_data.head()\n",
      "=====\n",
      "train_data=train_data.drop({jupyter_string,jupyter_string,jupyter_string,jupyter_string},axis=1)\n",
      "train_data\n",
      "--------------------\n",
      "train_data[[jupyter_string,jupyter_string]].groupby([jupyter_string],as_index=False).mean().sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "g = sns.FacetGrid(train_set, col='Sex' <<unk>>,row='Survived' <<unk>>)\n",
      "g.map(plt.hist, 'Age' <<unk>>, bins=20)\n",
      "g.add_legend\n",
      "--------------------\n",
      "g = sns.FacetGrid(train_set, col='Pclass' <unk>,row='Survived' <unk>)\n",
      "g.map(plt.hist, 'Age' <unk>, bins=20)\n",
      "g.add_legend()\n",
      "=====\n",
      "g = sns.FacetGrid(train_set, col='Embarked' <<unk>>,row='Survived' <<unk>>)\n",
      "g.map(plt.hist, 'Age' <<unk>>, bins=20)\n",
      "g.add_legend\n",
      "--------------------\n",
      "g = sns.FacetGrid(train_set, col='Embarked' <unk>,row='Pclass' <unk>)\n",
      "g.map(plt.hist, 'Age' <unk>, bins=20)\n",
      "g.add_legend()\n",
      "=====\n",
      "g = sns.FacetGrid(train_set,row='Embarked' <<unk>>, size=2.2, aspect=1.6) \n",
      "g.map(sns.pointplot,'Pclass' <<unk>>,'Survived' <<unk>>, 'Sex' <<unk>>,palette=jupyter_string) \n",
      "g.add_legend()\n",
      "--------------------\n",
      "titanic_df = titanic_df.drop(['PassengerId' <unk>,'Cabin' <unk>,'Ticket' <unk>],axis=1)\n",
      "titanic_df.head()\n",
      "=====\n",
      "train_set=strat_train_set.copy()\n",
      "train_set_labels=train_set[\"Survived\"].copy()\n",
      "train_set=train_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "train_set\n",
      "--------------------\n",
      "test_set=strat_test_set.copy()\n",
      "test_set_labels=test_set[\"Survived\"].copy()\n",
      "test_set=test_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "test_set\n",
      "=====\n",
      "dev_set=strat_test_set.copy()\n",
      "dev_set.info()\n",
      "--------------------\n",
      "test_set=strat_test_set.copy()\n",
      "test_set.info()\n",
      "=====\n",
      "dev_set_labels=dev_set[\"Survived\"].copy()\n",
      "dev_set=dev_set.drop([\"Survived\",\"PassengerId\"],axis=1)\n",
      "test_set=test_data.copy()\n",
      "test_set=test_set.drop([\"PassengerId\"],axis=1)\n",
      "--------------------\n",
      "dev_set.info()\n",
      "=====\n",
      "train_set_cat=train_set[[\"Name\",\"Embarked\",\"Fare\"]].copy()\n",
      "train_set_num=train_set.drop([\"Name\",\"Embarked\",\"Fare\"],axis=1)\n",
      "cat_attribs=list(train_set_cat)\n",
      "num_attribs=list(train_set_num)\n",
      "num_attribs\n",
      "--------------------\n",
      "def get_title(name):\n",
      "    title_search = re.search(jupyter_string, name)\n",
      "    if title_search:\n",
      "        return title_search.group(1)\n",
      "    return jupyter_string\n",
      "\n",
      "train_set[jupyter_string]=train_set[\"Name\"].apply(get_title)\n",
      "test_set[jupyter_string]=test_set[\"Name\"].apply(get_title)\n",
      "=====\n",
      "train_set_cat[jupyter_string]=pd.Series(train_set_cat['Name' <<unk>>]).str.extract(jupyter_string, expand=False)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "train_set_cat.head()\n",
      "=====\n",
      "pd.crosstab(train_set_cat[jupyter_string], train_set_num['Sex' <<unk>>])\n",
      "--------------------\n",
      "pd.crosstab(train_set_cat[jupyter_string], train_set_num['Pclass' <unk>])\n",
      "=====\n",
      "train_set_cat[jupyter_string] = train_set_cat[jupyter_string].replace([jupyter_string, jupyter_string,jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string], jupyter_string)\n",
      "train_set_cat[jupyter_string] = train_set_cat[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "train_set_cat[jupyter_string] = train_set_cat[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "train_set_cat[jupyter_string] = train_set_cat[jupyter_string].replace(jupyter_string, jupyter_string)\n",
      "train_set_cat\n",
      "--------------------\n",
      "train_set_cat = train_set_cat.drop(['Name' <unk>], axis=1)\n",
      "=====\n",
      "train_set_cat=train_set_cat.drop('Name' <<unk>>,axis=1)\n",
      "--------------------\n",
      "train_set_num=train_set_num.drop('Embarked' <unk>,axis=1)\n",
      "=====\n",
      "freq=pd.Series([train_set_cat[c].value_counts().index[0] for c in train_set_cat], index=train_set_cat.columns)\n",
      "\n",
      "freq\n",
      "--------------------\n",
      "train_set_cat['Embarked' <unk>]=train_set_cat['Embarked' <unk>].fillna(jupyter_string)\n",
      "=====\n",
      "train_set_cat['Embarked' <<unk>>].fillna(freq)\n",
      "train_set_cat['Embarked' <<unk>>].value_counts()\n",
      "--------------------\n",
      "cat_pipeline = Pipeline([\n",
      "        (jupyter_string, FeatureUnion([\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "            (jupyter_string, num_pipeline),\n",
      "        ])),\n",
      "        (jupyter_string, CategoricalEncoder(encoding=jupyter_string\n",
      "=====\n",
      "from sklearn.pipeline import Pipeline\n",
      "cat_pipeline=Pipeline([(jupyter_string,DataFrameSelector(cat_attribs)),\n",
      "                       (jupyter_string,CreateFeatures()),\n",
      "                       (jupyter_string,MostFrequentImputer()),\n",
      "                        (jupyter_string, CategoricalEncoder(encoding=jupyter_string))]) \n",
      "cat_pipeline.fit_transform(train_set)\n",
      "--------------------\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "full_pipeline=FeatureUnion([(jupyter_string,num_pipeline),(jupyter_string,cat_pipeline)])\n",
      "full_pipeline.fit_transform(train_set)\n",
      "=====\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "full_pipeline=FeatureUnion(transformer_list=[(jupyter_string,num_pipeline),(jupyter_string,cat_pipeline)])\n",
      "\n",
      "X_train=full_pipeline.fit_transform(train_set)\n",
      "X_train.shape\n",
      "--------------------\n",
      "forest_clf.fit(X_train,y_train)\n",
      "=====\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = {\n",
      "    jupyter_string: range(100, 400, 50),\n",
      "    jupyter_string: range(2, 7, 1),\n",
      "    jupyter_string:[jupyter_string,jupyter_string]\n",
      "}\n",
      "cross_validation = StratifiedKFold(n_splits=5)\n",
      "grid_clf = GridSearchCV(estimator = forest_clf, param_grid = param_grid, scoring=jupyter_string, cv=cross_validation)\n",
      "grid_clf.fit(X_train, y_train)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "--------------------\n",
      "forest_clf=RandomForestClassifier(max_features=jupyter_string)\n",
      "forest_clf.fit(X_train, y_train)\n",
      "forest_clf.score(X_dev, y_dev)\n",
      "=====\n",
      "X_dev=full_pipeline.transform(dev_set)\n",
      "y_dev=dev_set_labels\n",
      "--------------------\n",
      "forest_clf.fit(X_train, y_train)\n",
      "forest_clf.score(X_dev, y_dev)\n",
      "=====\n",
      "y_pred=grid_clf.best_estimator_.predict(X_dev)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = df.fillna(0)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "train_data=train_data.drop(['PassengerId' madeupword0002],axis=1)\n",
      "--------------------\n",
      "X=train_data.drop(['Survived' <unk>],axis=1)\n",
      "y=train_data['Survived' <unk>]\n",
      "=====\n",
      "y_full=train_data[\"Survived\"]\n",
      "X_full=train_data.drop(\"Survived\",axis=1)\n",
      "X_full=full_pipeline.fit_transform(X_full)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.33, random_state=42)\n",
      "=====\n",
      "grid_clf.fit(X_full, y_full)\n",
      "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n",
      "--------------------\n",
      "df_climate = pd.read_csv(jupyter_string)\n",
      "df_climate.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df_climate = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "columns = s.split(jupyter_string)\n",
      "columns\n",
      "=====\n",
      "column_labels_list = s.split(jupyter_string)\n",
      "column_labels_list\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.drop(columns_to_drop, axis=1, inplace=True)\n",
      "=====\n",
      "new_df = df.drop(columns_to_drop, axis=jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "new_df.head()\n",
      "=====\n",
      "new_df.head()\n",
      "--------------------\n",
      "new_df[jupyter_string] = pd.to_datetime(new_df[jupyter_string])\n",
      "new_df[jupyter_string] = pd.to_datetime(new_df[jupyter_string])\n",
      "new_df.head()\n",
      "=====\n",
      "new_df[jupyter_string] = new_df[jupyter_string].astype(str)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = new_df[jupyter_string].apply(lambda x:jupyter_string.format(x))\n",
      "\n",
      "\n",
      "date_string = new_df[jupyter_string] + new_df[jupyter_string]\n",
      "\n",
      "\n",
      "date_times = pd.to_datetime(date_string, format=jupyter_string)\n",
      "\n",
      "\n",
      "new_df = new_df.set_index(date_times)\n",
      "\n",
      "\n",
      "print(new_df.head())\n",
      "--------------------\n",
      "new_df[jupyter_string] = new_df[jupyter_string].astype(str)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = new_df[jupyter_string].apply(lambda x:jupyter_string.format(x))\n",
      "\n",
      "\n",
      "date_string = new_df[jupyter_string] + new_df[jupyter_string]\n",
      "\n",
      "\n",
      "date_times = pd.to_datetime(date_string, format=jupyter_string)\n",
      "\n",
      "\n",
      "new_df = new_df.set_index(date_times)\n",
      "\n",
      "\n",
      "print(new_df.head())\n",
      "=====\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "new_df[jupyter_string] = pd.to_numeric(new_df[jupyter_string], errors=jupyter_string)\n",
      "\n",
      "\n",
      "print(new_df.loc[jupyter_string:jupyter_string, [jupyter_string, jupyter_string, jupyter_string]])\n",
      "--------------------\n",
      "new_df = new_df[new_df[jupyter_string] >= 1984]\n",
      "new_df = new_df[new_df[jupyter_string] <= 2010]\n",
      "new_df = new_df.reset_index(drop=True)\n",
      "=====\n",
      "df_climate.head()\n",
      "--------------------\n",
      "df_climate['date' <unk>] = pd.to_datetime(df_climate['date' <unk>])\n",
      "df_climate = df_climate.set_index('date' <unk>)\n",
      "df_climate.head()\n",
      "=====\n",
      "df_climate['Date' <<unk>>] = df_climate['Date' <<unk>>].astype(str)\n",
      "\n",
      "\n",
      "date_times = pd.to_datetime(df_climate['Date' <<unk>>], format=jupyter_string)\n",
      "\n",
      "\n",
      "df_climate = df_climate.set_index(date_times)\n",
      "\n",
      "\n",
      "df_climate = df_climate.drop(['Date' <<unk>>], axis=jupyter_string)\n",
      "--------------------\n",
      "df_climate.head()\n",
      "=====\n",
      "df_climate.head()\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "new_df.describe()\n",
      "\n",
      "--------------------\n",
      "new_df.info()\n",
      "=====\n",
      "df_climate.describe()\n",
      "--------------------\n",
      "df_2011 = df_df[df_df['year' <unk>] == 2011]\n",
      "df_2011.head()\n",
      "=====\n",
      "daily_mean_2011 = new_df.resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "daily_mean_2011.head()\n",
      "--------------------\n",
      "daily_temp_2010 = daily_mean_2010[jupyter_string].values\n",
      "daily_temp_2010[:10]\n",
      "=====\n",
      "daily_climate = df_climate.resample(jupyter_string).mean()\n",
      "daily_climate.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.corr()\n",
      "--------------------\n",
      "daily_temp_2011 = daily_climate[jupyter_string].values\n",
      "daily_temp_2011[:10]\n",
      "=====\n",
      "daily_temp_climate = daily_climate.reset_index()['Temperature' <<unk>>]\n",
      "daily_temp_climate[:10]\n",
      "--------------------\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011[jupyter_string] = daily_temp_2011[jupyter_string] - daily_temp_2011[jupyter_string]\n",
      "daily_temp_2011.head()\n",
      "=====\n",
      "sunny = new_df.loc[new_df[jupyter_string] == jupyter_string]\n",
      "\n",
      "\n",
      "overcast = new_df.loc[new_df[jupyter_string].str.contains(jupyter_string)]\n",
      "--------------------\n",
      "temp_sunny = sunny.groupby(jupyter_string)[jupyter_string].mean()\n",
      "temp_overcast = overcast.groupby(jupyter_string)[jupyter_string].mean()\n",
      "=====\n",
      "sunny_daily_max = sunny.resample(jupyter_string).max()\n",
      "overcast_daily_max = overcast.resample(jupyter_string).max()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "weekly_mean = new_df[[jupyter_string, jupyter_string]].resample(jupyter_string).mean()\n",
      "\n",
      "\n",
      "print(weekly_mean.corr())\n",
      "\n",
      "\n",
      "weekly_mean.plot(subplots=True)\n",
      "plt.show()\n",
      "--------------------\n",
      "weekly_mean.plot(kind=jupyter_string, subplots=True)\n",
      "plt.show()\n",
      "=====\n",
      "sunny = new_df[jupyter_string] == jupyter_string\n",
      "\n",
      "\n",
      "sunny_hours = sunny.resample(jupyter_string).sum()\n",
      "\n",
      "\n",
      "total_hours = sunny.resample(jupyter_string).count()\n",
      "\n",
      "\n",
      "sunny_fraction = sunny_hours / total_hours\n",
      "\n",
      "\n",
      "sunny_fraction.plot(kind=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "months = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "=====\n",
      "monthly_max = new_df[[jupyter_string, jupyter_string]].resample(jupyter_string).max()\n",
      "\n",
      "\n",
      "monthly_max.plot(kind=jupyter_string, bins=8, alpha=0.5, subplots=True)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "august_2011 = monthly_max[monthly_max.index.month == 8]\n",
      "august_2010 = monthly_max[monthly_max.index.month == 9]\n",
      "august_2011 = august_2011.resample(jupyter_string).max()\n",
      "august_2010 = august_2010.resample(jupyter_string).max()\n",
      "=====\n",
      "august_max = df_climate.loc[jupyter_string,'Temperature' <<unk>>].max()\n",
      "print(august_max)\n",
      "--------------------\n",
      "august_min = df_climate.loc[jupyter_string,'Temperature' <unk>].min()\n",
      "print(august_min)\n",
      "=====\n",
      "august_2011 = new_df.loc[jupyter_string,jupyter_string].resample(jupyter_string).max()\n",
      "print(august_2011[:10])\n",
      "--------------------\n",
      "august_2011.plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "august_2011_high = august_2011.loc[august_2011 > august_max]\n",
      "\n",
      "\n",
      "august_2011_high.plot(kind=jupyter_string, normed=True, cumulative=True, bins=25, legend=jupyter_string, grid=True)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "dt = DecisionTreeClassifier()\n",
      "dt.fit(X_train, y_train)\n",
      "dt.score(X_test, y_test)\n",
      "=====\n",
      "ens = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_split=30)\n",
      "ens.fit(X_train,y_train)\n",
      "sk_rf_pred = ens.predict(X_test)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "model = smf.ols(formula=formula, data=df)\n",
      "res=model.fit()\n",
      "\n",
      "--------------------\n",
      "iris = load_iris()\n",
      "X = iris.data[:, :2]\n",
      "y = iris.target\n",
      "=====\n",
      "Xy = pd.read_csv(jupyter_string,header=None)\n",
      "Xy[60] = Xy[60].map({'R' <<unk>>:0,jupyter_string:1})\n",
      "X = np.array(Xy.iloc[:,:-1])\n",
      "y = np.array(Xy.iloc[:,-1])\n",
      "Xy = np.array(Xy)\n",
      "--------------------\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "forest = random_forest(num_trees=10)\n",
      "forest.fit(X_train, y_train)\n",
      "forest.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "Xy = pd.read_csv(jupyter_string,header=None)\n",
      "Xy[60] = Xy[60].map({'R' <<unk>>:0,jupyter_string:1})\n",
      "X = np.array(Xy.iloc[:,:-1])\n",
      "y = np.array(Xy.iloc[:,-1])\n",
      "Xy = np.array(Xy)\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=35)\n",
      "--------------------\n",
      "df = pandas.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dataframe = pandas.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "dataframe\n",
      "--------------------\n",
      "dataframe = dataframe.rename(columns={'name' <unk>: jupyter_string, 'score' <unk>: jupyter_string})\n",
      "\n",
      "\n",
      "dataframe\n",
      "=====\n",
      "dataframe = dataframe.set_index(\"name\")\n",
      "\n",
      "\n",
      "dataframe\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "dataframe.plot()\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "=====\n",
      "dataframe.plot(subplots=True)\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "=====\n",
      "dataframe.plot(kind=jupyter_string)\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string, stacked=True)\n",
      "=====\n",
      "dataframe.plot(kind=jupyter_string, stacked=True)\n",
      "--------------------\n",
      "dataframe.plot(kind=jupyter_string, x='score_1' <unk>, y='score_2' <unk>)\n",
      "=====\n",
      "graph = dataframe[['score_1' madeupword0002,'score_2' <<unk>>]].plot.scatter(x='score_1' madeupword0002,y='score_2' <<unk>>,c=jupyter_string)\n",
      "\n",
      "\n",
      "for score1, score2 in dataframe[['score_1' madeupword0002,'score_2' <<unk>>]].iterrows():\n",
      "    graph.annotate(score1, score2)\n",
      "--------------------\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "from scipy.stats import uniform as sp_uniform\n",
      "=====\n",
      "from scipy.stats import randint\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "param_dist = {jupyter_string: randint(1, 9),\n",
      "              jupyter_string: randint(1, 9)}\n",
      "n_iter_search = 20\n",
      "random_search = RandomizedSearchCV(rfc, param_distributions=param_dist,\n",
      "                                   n_iter=n_iter_search)\n",
      "random_search.fit(X_train_std, Y_train)\n",
      "random_search.best_score_\n",
      "random_search.best_params_\n",
      "--------------------\n",
      "res.summary()\n",
      "=====\n",
      "res.summary()\n",
      "--------------------\n",
      "rfc = RandomForestClassifier(n_estimators=100, max_features=4)\n",
      "rfc.fit(X_train_std, Y_train)\n",
      "rfc.score(X_train_std, Y_train)\n",
      "=====\n",
      "rfc_opt = RandomForestClassifier(random_state=3, max_features=4)\n",
      "rfc_opt.fit(X_train_std, Y_train)\n",
      "Y_pred = rfc_opt.predict(X_test_std)\n",
      "rfc_opt_score_train = rfc_opt.score(X_train_std, Y_train)\n",
      "print(jupyter_string,rfc_opt_score_train)\n",
      "rfc_opt_score_test = rfc_opt.score(X_test_std, Y_test)\n",
      "print(jupyter_string,rfc_opt_score_test)\n",
      "print(jupyter_string % metrics.f1_score(Y_test, Y_pred, average=jupyter_string))\n",
      "--------------------\n",
      "df.describe()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "df.salary.value_counts().plot(kind=jupyter_string)\n",
      "=====\n",
      "df.hist(column='average_montly_hours' <<unk>>)\n",
      "--------------------\n",
      "df.hist(column='last_evaluation' <unk>)\n",
      "=====\n",
      "df.hist(column='last_evaluation' <<unk>>)\n",
      "--------------------\n",
      "df.hist(column='last_evaluation' <unk>)\n",
      "=====\n",
      "df.hist(column='satisfaction_level' <<unk>>)\n",
      "--------------------\n",
      "df_left = df[df.left == 1]\n",
      "df_no_left = df[df.left == 0]\n",
      "=====\n",
      "dfleft, dfstay = [x for _, x in df.groupby(df[\"left\"]==0)]\n",
      "dfleft.describe()\n",
      "--------------------\n",
      "dfstay.describe()\n",
      "=====\n",
      "dfstay.describe()\n",
      "--------------------\n",
      "sns.distplot(dfleft['last_evaluation' <unk>])\n",
      "sns.distplot(dfstay['last_evaluation' <unk>])\n",
      "=====\n",
      "dfleft.hist(column='satisfaction_level' <<unk>>)\n",
      "dfstay.hist(column='satisfaction_level' <<unk>>)\n",
      "dfleft.hist(column='number_project' <<unk>>)\n",
      "dfstay.hist(column='number_project' <<unk>>)\n",
      "--------------------\n",
      "dfleft = pd.get_dummies(dfleft, columns=['sales' <unk>])\n",
      "dfstay = pd.get_dummies(dfstay, columns=['sales' <unk>])\n",
      "=====\n",
      "cols_to_transform = ['salary' <<unk>>, 'sales' <<unk>>]\n",
      "df = pd.get_dummies(df, columns = cols_to_transform )\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X = df.drop('left' <unk>, axis = 1)\n",
      "y = df['left' <unk>]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
      "=====\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X = df.drop('left' <<unk>>, axis=1)\n",
      "Y = df['left' <<unk>>]\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=5, stratify=Y)\n",
      "--------------------\n",
      "sns.regplot(x=res.fittedvalues, y=res.resid)\n",
      "=====\n",
      "plt.scatter(df['medv' <<unk>>], res.fittedvalues)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_test = scaler.transform(X_test)\n",
      "=====\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "stdsc = StandardScaler()\n",
      "X_train_std = stdsc.fit_transform(X_train)\n",
      "X_test_std = stdsc.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_std, y_train)\n",
      "print(jupyter_string.format(lr.score(X_train_std, y_train)))\n",
      "print(jupyter_string.format(lr.score(X_test_std, y_test)))\n",
      "=====\n",
      "from sklearn import linear_model, metrics, linear_model\n",
      "logit = linear_model.LogisticRegression()\n",
      "logit.fit(X_train_std, Y_train)\n",
      "Y_pred = logit.predict(X_test_std)\n",
      "logit_score_train = logit.score(X_train_std, Y_train)\n",
      "print(jupyter_string,logit_score_train)\n",
      "logit_score_test = logit.score(X_test_std, Y_test)\n",
      "print(jupyter_string,logit_score_test)\n",
      "print(jupyter_string % metrics.f1_score(Y_test, Y_pred, average=jupyter_string))\n",
      "--------------------\n",
      "all_user_predicted_ratings = np.dot(np.dot(u, sigma), vt)\n",
      "all_user_predicted_ratings.shape\n",
      "=====\n",
      "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n",
      "all_user_predicted_ratings\n",
      "--------------------\n",
      "predictions = all_user_predicted_ratings[all_user_predicted_ratings.nonzero()]\n",
      "=====\n",
      "cf_preds_df = pd.DataFrame(all_user_predicted_ratings, columns = users_items_pivot_matrix_df.columns, index=users_ids).transpose()\n",
      "cf_preds_df.head(10)\n",
      "--------------------\n",
      "articles = pd.read_csv(jupyter_string)\n",
      "articles.head()\n",
      "=====\n",
      "articles_df = pd.read_csv(jupyter_string)\n",
      "articles_df = articles_df[articles_df['eventType' <<unk>>] == jupyter_string]\n",
      "articles_df.head(5)\n",
      "--------------------\n",
      "users_interactions_df = pd.read_csv(jupyter_string)\n",
      "users_interactions_df.head(5)\n",
      "=====\n",
      "interactions_df = pd.read_csv(jupyter_string)\n",
      "interactions_df.head(10)\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "=====\n",
      "print(jupyter_string)\n",
      "cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\n",
      "print(jupyter_string % cf_global_metrics)\n",
      "cf_detailed_results_df.head(10)\n",
      "--------------------\n",
      "weights_df = pd.read_csv(jupyter_string)\n",
      "weights_df.head(10)\n",
      "=====\n",
      "event_type_strength = {\n",
      "   jupyter_string: 1.0,\n",
      "   jupyter_string: 2.0, \n",
      "   jupyter_string: 2.5, \n",
      "   jupyter_string: 3.0,\n",
      "   jupyter_string: 4.0,  \n",
      "}\n",
      "\n",
      "interactions_df[jupyter_string] = interactions_df['eventType' <<unk>>].apply(lambda x: event_type_strength[x])\n",
      "--------------------\n",
      "interactions_df = interactions_df[interactions_df[jupyter_string] > 5]\n",
      "interactions_df.head()\n",
      "=====\n",
      "users_interactions_count_df = interactions_df.groupby(['personId' <<unk>>, 'contentId' <<unk>>]).size().groupby('personId' <<unk>>).size()\n",
      "print(jupyter_string % len(users_interactions_count_df))\n",
      "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId' <<unk>>]]\n",
      "print(jupyter_string % len(users_with_enough_interactions_df))\n",
      "--------------------\n",
      "users_with_enough_interactions_df.head()\n",
      "=====\n",
      "print(jupyter_string % len(interactions_df))\n",
      "interactions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n",
      "               how = jupyter_string,\n",
      "               left_on = 'personId' <<unk>>,\n",
      "               right_on = 'personId' <<unk>>)\n",
      "print(jupyter_string % len(interactions_from_selected_users_df))\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "=====\n",
      "Number_variables=range(len(data_train.columns[:-1]))\n",
      "OLS_R_2_OS_F=[]\n",
      "OLS_R_2_IS_F=[]\n",
      "\n",
      "R2_IS_Lasso=[]\n",
      "R2_OS_Lasso=[]\n",
      "\n",
      "R2_IS_Ridge=[]\n",
      "R2_OS_Ridge=[]\n",
      "\n",
      "for i in Number_variables:\n",
      "    \n",
      "    lm = smf.ols(formula = jupyter_string+ jupyter_string.join(data_train.columns[:i+1]), data = data_train).fit()\n",
      "    R2 = modelEval(lm)\n",
      "    OLS_R_2_IS_F.append(lm.rsquared)\n",
      "    OLS_R_2_OS_F.append(R2 if R2 > 0 else 0)\n",
      "    \n",
      "for i in Number_variables:\n",
      "    \n",
      "    Lasso=linear_model.Lasso(fit_intercept=True,alpha=30, )\n",
      "    Lasso.fit(X_train,y_train)\n",
      "    \n",
      "    p_IS=Lasso.predict(X_train)\n",
      "    err_IS=p_IS-y_train.T\n",
      "    R2_IS_Lasso.append(1-np.var(err_IS.T)/np.var(y_train))\n",
      "    \n",
      "    p_OS=Lasso.predict(X_test)\n",
      "    err_OS=p_OS-y_test.T\n",
      "    R2_OS_Lasso.append(1-np.var(err_OS.T)/np.var(y_test))\n",
      "    \n",
      "    \n",
      "for i in Number_variables:\n",
      "    \n",
      "    Ridge=linear_model.Ridge(fit_intercept=True,alpha=3000)\n",
      "    Ridge.fit(X_train,y_train)\n",
      "    \n",
      "    p_OS=Ridge.predict(X_test)\n",
      "    err_OS=p_OS-y_test\n",
      "    R2_OS_Ridge.append(1-np.var(err_OS)/np.var(y_test))\n",
      "    \n",
      "    p_IS=Ridge.predict(X_train)\n",
      "    err_IS=p_IS-y_train\n",
      "    R2_IS_Ridge.append(1-np.var(err_IS)/np.var(y_train))\n",
      "    \n",
      "figure(figsize=(10,10))\n",
      "plt.title(jupyter_string)\n",
      "plt.plot(Number_variables,OLS_R_2_OS_F,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,OLS_R_2_IS_F,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_OS_Lasso,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_IS_Lasso,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_OS_Ridge,jupyter_string,label=jupyter_string)\n",
      "plt.plot(Number_variables,R2_IS_Ridge,jupyter_string,label=jupyter_string)\n",
      "plt.legend(loc=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "interactions_from_selected_users_df[jupyter_string] = np.log(interactions_from_selected_users_df[jupyter_string] + 1)\n",
      "interactions_from_selected_users_df.head()\n",
      "=====\n",
      "def smooth_user_preference(x):\n",
      "    return math.log(1+x, 2)\n",
      "    \n",
      "interactions_full_df = interactions_from_selected_users_df \\\n",
      "                    .groupby(['personId' <<unk>>, 'contentId' <<unk>>])[jupyter_string].sum() \\\n",
      "                    .apply(smooth_user_preference).reset_index()\n",
      "print(jupyter_string % len(interactions_full_df))\n",
      "interactions_full_df.head(10)\n",
      "--------------------\n",
      "user_item_matrix_train_df = interactions_train_df.pivot_table(index='personId' <unk>, columns='contentId' <unk>, values=jupyter_string)\n",
      "user_item_matrix_train_df.fillna(0, inplace=True)\n",
      "user_item_matrix_train_df.head()\n",
      "=====\n",
      "interactions_full_indexed_df = interactions_full_df.set_index('personId' <<unk>>)\n",
      "interactions_train_indexed_df = interactions_train_df.set_index('personId' <<unk>>)\n",
      "interactions_test_indexed_df = interactions_test_df.set_index('personId' <<unk>>)\n",
      "--------------------\n",
      "popularity_model = gl.popularity_recommender.create(interactions_full_indexed_df, user_id=jupyter_string, item_id=jupyter_string, target=jupyter_string)\n",
      "=====\n",
      "item_popularity_df = interactions_full_df.groupby('contentId' <<unk>>)[jupyter_string].sum().sort_values(ascending=False).reset_index()\n",
      "item_popularity_df.head(10)\n",
      "--------------------\n",
      "popularity_recommender = PopularityRecommender(item_popularity_df, items_df=interactions_full_df)\n",
      "popularity_recommender.get_model_name()\n",
      "=====\n",
      "print(jupyter_string)\n",
      "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\n",
      "print(jupyter_string % pop_global_metrics)\n",
      "pop_detailed_results_df.head(10)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "users_items_pivot_matrix_df = interactions_train_df.pivot(index='personId' <<unk>>, \n",
      "                                                          columns='contentId' <<unk>>, \n",
      "                                                          values=jupyter_string).fillna(0)\n",
      "\n",
      "users_items_pivot_matrix_df.head(10)\n",
      "--------------------\n",
      "users_items_pivot_matrix = users_items_pivot_matrix_df.as_matrix()\n",
      "=====\n",
      "users_items_pivot_matrix = users_items_pivot_matrix_df.as_matrix()\n",
      "users_items_pivot_matrix[:10]\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pickle\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import gaussian_kde\n",
      "from scipy.spatial import distance\n",
      "from scipy.cluster import hierarchy\n",
      "\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas.tools.plotting as pdplt\n",
      "import seaborn as sns\n",
      "\n",
      "plt.show()\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "clr_plt = sns.color_palette()\n",
      "pd.options.display.float_format = jupyter_string.format\n",
      "--------------------\n",
      "ensemble = pd.read_csv(jupyter_string)\n",
      "ensemble.head()\n",
      "=====\n",
      "na_ensemble = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_ensemble.head()\n",
      "--------------------\n",
      "na_ensemble[jupyter_string] = (na_ensemble[jupyter_string] - na_ensemble[jupyter_string].mean()) / na_ensemble[jupyter_string].std()\n",
      "na_ensemble.head()\n",
      "=====\n",
      "p_ranges = pd.read_csv(jupyter_string, index_col='name' <<unk>>)\n",
      "\n",
      "p_ranges\n",
      "--------------------\n",
      "na_cluster = na_params[jupyter_string]\n",
      "na_cluster\n",
      "=====\n",
      "na_clusters = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_clusters.head()\n",
      "--------------------\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.\n",
      "=====\n",
      "lambdas = np.exp(np.linspace(0,13,200))\n",
      "lambda_r_optimal=Regularization_fit_lambda(1,X_train,y_train,lambdas,p=0.3,Graph=True)\n",
      "print(jupyter_string.format(lambda_r_optimal))\n",
      "--------------------\n",
      "na_cluster_centers = na_clusters.mean(axis=1)\n",
      "na_cluster_centers.head()\n",
      "=====\n",
      "na_centroids = pd.read_csv(jupyter_string, index_col=0)\n",
      "\n",
      "na_centroids.head()\n",
      "--------------------\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "na_labels = {\n",
      "    'P_0' <<unk>>: jupyter_string,\n",
      "    'K_r12' <<unk>>: jupyter_string,\n",
      "    'K_d' <<unk>>: jupyter_string,\n",
      "    'K_dd' <<unk>>: jupyter_string,\n",
      "    'K_g' <<unk>>: jupyter_string,\n",
      "    'h_init' madeupword0002: jupyter_string,\n",
      "    'misfit' <<unk>>: jupyter_string,\n",
      "    'D_mean' <<unk>>: jupyter_string,\n",
      "    'RMSD' <<unk>>: 'RMSD' <<unk>>,\n",
      "    'cluster' <<unk>>: 'cluster' <<unk>>\n",
      "}\n",
      "\n",
      "\n",
      "pd.options.display.float_format = jupyter_string.format\n",
      "\n",
      "\n",
      "pd.options.mode.chained_assignment = None\n",
      "\n",
      "\n",
      "cat_clr = [mpl.colors.rgb2hex(rgb)\n",
      "           for rgb in sns.color_palette(n_colors=20)]\n",
      "quant_cmap = sns.cubehelix_palette(start=.5, rot=-.75,\n",
      "                                   as_cmap=True, reverse=True)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(quant_cmap)\n",
      "=====\n",
      "def scatter_misfit_stats(dataset):\n",
      "    \"\"jupyter_string\"\"\n",
      "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "    col_pairs = (('RMSD' <<unk>>, 'D_mean' <<unk>>),\n",
      "                 ('RMSD' <<unk>>, 'misfit' <<unk>>),\n",
      "                 ('D_mean' <<unk>>, 'misfit' <<unk>>))\n",
      "\n",
      "    for (x, y), ax in zip(col_pairs, axes):\n",
      "        dataset.plot(kind=jupyter_string, x=x, y=y,\n",
      "                     ax=ax, alpha=0.2, c=jupyter_string)\n",
      "        plt.setp(ax, xlabel=na_labels[x],\n",
      "                 ylabel=na_labels[y])\n",
      "\n",
      "    fig.tight_layout()\n",
      "    return fig, axes\n",
      "\n",
      "fig, axes = scatter_misfit_stats(na_ensemble)\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "\n",
      "for (x, y), ax in zip(col_pairs, axes):\n",
      "    dataset.plot(kind=jupyter_string, x=x, y=y,\n",
      "                 ax=ax, alpha=0.2, c=jupyter_string)\n",
      "    plt.setp(ax, xlabel=na_labels[x],\n",
      "             ylabel=na_labels[y])\n",
      "\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "\n",
      "for col, ax in zip(['D_mean' <<unk>>, 'RMSD' <<unk>>, 'misfit' <<unk>>], axes.flatten()):\n",
      "    trace = na_ensemble.get(col)\n",
      "    na_ensemble.plot(kind=jupyter_string, x='NA_iter' <<unk>>, y=col, ax=ax,\n",
      "                     s=50, alpha=0.15, color=jupyter_string)\n",
      "    plt.setp(ax, xlim=[-1, na_params[jupyter_string]])\n",
      "    plt.setp(ax, ylabel=na_labels[col], xlabel=jupyter_string)\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "from sklearn.metrics import adjusted_rand_score\n",
      "from sklearn.metrics import adjusted_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "from sklearn.metrics import normalized_mutual_\n",
      "=====\n",
      "misfit_max = -5.4\n",
      "\n",
      "drop_cols = ['misfit' <<unk>>, 'D_mean' <<unk>>, 'RMSD' <<unk>>]\n",
      "\n",
      "\n",
      "\n",
      "na_misfit_sel = na_ensemble.misfit < misfit_max\n",
      "na_subset = na_ensemble[na_misfit_sel]\n",
      "na_subset.drop('NA_iter' <<unk>>, axis=1, inplace=True)\n",
      "na_subset_log = log_df(na_subset)\n",
      "na_subset_norm = norm_df(na_subset_log)\n",
      "na_subset_norm_p = na_subset_norm.drop(drop_cols, axis=1)\n",
      "\n",
      "\n",
      "dist_matrix = distance.cdist(na_subset_norm_p, na_subset_norm_p)\n",
      "\n",
      "\n",
      "link_matrix = hierarchy.linkage(distance.squareform(dist_matrix),\n",
      "                                method=jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(10, 11))\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "dendrogram(\n",
      "    link_matrix,\n",
      "    truncate_mode=jupyter_string, \n",
      ")\n",
      "plt.show()\n",
      "=====\n",
      "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
      "\n",
      "\n",
      "dist_tresh = 0.6  \n",
      "\n",
      "cat_clr = [jupyter_string] * 5\n",
      "hierarchy.set_link_color_palette(cat_clr)\n",
      "\n",
      "dendrogram = hierarchy.dendrogram(link_matrix,\n",
      "                                  color_threshold=dist_tresh,\n",
      "                                  labels=na_subset_norm.index,\n",
      "                                  orientation=jupyter_string, ax=axes[1])\n",
      "\n",
      "plt.setp(axes[1], xlabel=jupyter_string,\n",
      "         ylabel=jupyter_string, xticklabels=[])\n",
      "\n",
      "\n",
      "na_clusters_log = log_df(na_clusters)\n",
      "na_clusters_lognorm = norm_df(na_clusters_log)\n",
      "na_centroids_lognorm = na_clusters_lognorm.groupby('cluster' <<unk>>).mean()\n",
      "\n",
      "cluster_markers = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "cat_clr = [jupyter_string] * 5\n",
      " \n",
      "pdplt.parallel_coordinates(\n",
      "    na_clusters_lognorm.sort('cluster' <<unk>>), 'cluster' <<unk>>,\n",
      "    alpha=0.4, color=cat_clr, ax=axes[0],\n",
      "    axvlines=False, xticks=range(9)\n",
      ")\n",
      "axes[0].legend_.remove()\n",
      "axes[0].grid(False)\n",
      "\n",
      "lines = []\n",
      "for c, ctr in enumerate(na_centroids_lognorm.values):\n",
      "    l, = axes[0].plot(ctr, linewidth=1.5,\n",
      "                 label=jupyter_string.format(c + 1),\n",
      "                 marker=cluster_markers[c], markersize=13,\n",
      "                 markerfacecolor=jupyter_string, color=jupyter_string)\n",
      "    lines.append(l)\n",
      "\n",
      "leg = axes[0].legend(handles=lines, loc=jupyter_string, ncol=3,\n",
      "                     frameon=True, labelspacing=1.3)\n",
      "leg.get_frame().set_color(jupyter_string)\n",
      "\n",
      "for x in range(9):\n",
      "    axes[0].axvline(x=x, color=jupyter_string, linestyle=jupyter_string)\n",
      "\n",
      "xlbls = [jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "         jupyter_string, jupyter_string, jupyter_string, jupyter_string, 'RMSD' <<unk>>,]\n",
      "plt.setp(axes[0], ylabel=jupyter_string,\n",
      "         xticklabels=xlbls, ylim=[-1, 1])\n",
      "\n",
      "fig.tight_layout()\n",
      "\n",
      "fig.savefig(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_city_data = pd.read_csv(jupyter_string)\n",
      "df_global_data = pd.read_csv(jupyter_string)\n",
      "df_city_list = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "df.corr()\n",
      "=====\n",
      "c_pune.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.ticker as ticker\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "\n",
      "plt.show()  \n",
      "--------------------\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib as mpl\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option(jupyter_string, 500)\n",
      "pd.set_option(jupyter_string, 100)\n",
      "pd.set_option(jupyter_string, True)\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "mu = 3.35294117647\n",
      "variance = 0.485071250073\n",
      "sigma = math.sqrt(variance)\n",
      "x = np.linspace(mu - 5*sigma, mu + 5*sigma, 100)\n",
      "plt.plot(x,mlab.normpdf(x, mu, sigma))\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_sgd = pd.read_csv(jupyter_string, header=0, index_col='DATE' <<unk>>)\n",
      "df_jpy = pd.read_csv(jupyter_string, header=0, index_col='DATE' <<unk>>)\n",
      "--------------------\n",
      "df_sgd = df_sgd.fillna(method=jupyter_string)\n",
      "df_jpy = df_jpy.fillna(method=jupyter_string)\n",
      "=====\n",
      "df_sgd = df_sgd[jupyter_string:]\n",
      "df_sgd.DEXSIUS = pd.to_numeric(df_sgd.DEXSIUS, errors=jupyter_string)\n",
      "df_sgd = df_sgd.fillna(method=jupyter_string)\n",
      "--------------------\n",
      "df_sgd.info()\n",
      "=====\n",
      "df_jpy = df_jpy[jupyter_string:]\n",
      "df_jpy.DEXJPUS = pd.to_numeric(df_jpy.DEXJPUS, errors=jupyter_string)\n",
      "df_jpy = df_jpy.fillna(method=jupyter_string)\n",
      "--------------------\n",
      "df_sgd = df_sgd.dropna()\n",
      "df_jpy = df_jpy.dropna()\n",
      "=====\n",
      "df = df_sgd.join(df_jpy, how=jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.plot(df_pc.index, df_pc['SGD' <unk>], label=jupyter_string)\n",
      "plt.plot(df_pc.index, df_pc['JPY' <unk>], label=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "=====\n",
      "df['DEXSIUS' <<unk>>].plot(title=jupyter_string)\n",
      "--------------------\n",
      "df_pc['DEXSIUS' <unk>].plot(title=jupyter_string)\n",
      "=====\n",
      "df['DEXJPUS' madeupword0002].plot(title=jupyter_string)\n",
      "--------------------\n",
      "df['DEXJPUS' <unk>].plot(title=jupyter_string)\n",
      "=====\n",
      "df[jupyter_string].plot(title=jupyter_string)\n",
      "--------------------\n",
      "df[jupyter_string].plot(title=jupyter_string)\n",
      "=====\n",
      "df_pc.plot()\n",
      "--------------------\n",
      "df_sgd.plot()\n",
      "=====\n",
      "df_pc.describe()\n",
      "--------------------\n",
      "from scipy import stats\n",
      "stats.probplot(df_pc, dist=jupyter_string, plot=pylab)\n",
      "pylab.show()\n",
      "=====\n",
      "fig=plt.figure(figsize=(19, 8), dpi= 80, facecolor=jupyter_string, edgecolor=jupyter_string)\n",
      "plt.subplot(1, 3, 1)\n",
      "plt.plot(df_pc['DEXJPUS' madeupword0002])\n",
      "plt.subplot(1, 3, 2)\n",
      "plt.plot(df_pc['DEXSIUS' <<unk>>])\n",
      "plt.subplot(1, 3, 3)\n",
      "plt.plot(df_pc[jupyter_string])\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "regressor = LinearRegression()\n",
      "regressor.fit(X_train, y_train)\n",
      "y_pred = regressor.predict(X_test)\n",
      "\n",
      "print(jupyter_string, regressor.score(X_test, y_test))\n",
      "print(jupyter_string, r2_score(y_test, y_pred))\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "data_train=pd.read_csv(jupyter_string)\n",
      "data_test=pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "data_train.drop('Unnamed: 0' <<unk>>,axis=1, inplace=True)\n",
      "data_test.drop('Unnamed: 0' <<unk>>,axis=1, inplace=True)\n",
      "--------------------\n",
      "df_pc.corr()\n",
      "=====\n",
      "print(jupyter_string.format(df_pc['DEXJPUS' madeupword0002].corr(df_pc['DEXSIUS' <<unk>>])))\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.scatter(x=df_pc['DEXJPUS' madeupword0002], y=df_pc['DEXSIUS' <<unk>>])\n",
      "--------------------\n",
      "print(jupyter_string.format(df_pc['JPUS' <unk>].corr(df_pc['SIUS' <unk>])))\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "_ = plt.scatter(x=df_pc['JPUS' <unk>], y=df_pc['SIUS' <unk>])\n",
      "=====\n",
      "print(jupyter_string.format(df_pc['DEXJPUS' madeupword0002].corr(df_pc[jupyter_string])))\n",
      "_ = plt.scatter(x=df_pc['DEXJPUS' madeupword0002], y=df_pc[jupyter_string])\n",
      "--------------------\n",
      "print(jupyter_string.format(df_pc['DEXJPUS' <unk>].corr(df_pc[jupyter_string])))\n",
      "_ = plt.scatter(x=df_pc['DEXJPUS' <unk>], y=df_pc[jupyter_string])\n",
      "=====\n",
      "print(jupyter_string.format(df_pc['DEXSIUS' <<unk>>].corr(df_pc[jupyter_string])))\n",
      "_ = plt.scatter(x=df_pc['DEXSIUS' <<unk>>], y=df_pc[jupyter_string])\n",
      "--------------------\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "from statsmodels.tsa.arima_model import ARMA\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima_model import ARIMAResults\n",
      "from statsmodels.tsa.arima\n",
      "=====\n",
      "df_pc['DEXJPUS' madeupword0002].autocorr()\n",
      "--------------------\n",
      "df_pc['DEXJPUS' <unk>].autocorr(lag=1)\n",
      "=====\n",
      "df_pc['DEXSIUS' <<unk>>].autocorr()\n",
      "--------------------\n",
      "df_pc['DEXJPUS' <unk>].autocorr()\n",
      "=====\n",
      "df_pc[jupyter_string].autocorr()\n",
      "--------------------\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "=====\n",
      "df_pc.index = pd.to_datetime(df_pc.index)\n",
      "df_pc_weekly = df_pc.dropna().resample(rule=jupyter_string).last()\n",
      "df_pc_weekly.head()\n",
      "--------------------\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot_acf\n",
      "from statsmodels.graphics.tsaplots import plot_pacf\n",
      "from statsmodels.graphics.tsaplots import plot\n",
      "=====\n",
      "df_pc_weekly['DEXSIUS' <<unk>>].autocorr()\n",
      "--------------------\n",
      "df_pc_weekly['DEXSIUS' <unk>].plot()\n",
      "=====\n",
      "df_pc_weekly['DEXJPUS' madeupword0002].autocorr()\n",
      "--------------------\n",
      "df_pc_monthly['DEXJPUS' <unk>].autocorr()\n",
      "=====\n",
      "df_pc_monthly = df_pc.resample(jupyter_string).last()\n",
      "--------------------\n",
      "X_train=data_train.drop('y' <unk>,axis=1)\n",
      "y_train=data_train['y' <unk>]\n",
      "X_test=data_test.drop('y' <unk>,axis=1)\n",
      "y_test=data_test['y' <unk>]\n",
      "=====\n",
      "data_train.head()\n",
      "--------------------\n",
      "df_pc_monthly[jupyter_string].autocorr()\n",
      "=====\n",
      "df_pc_monthly['DEXSIUS' <<unk>>].autocorr()\n",
      "--------------------\n",
      "df_pc_monthly['DEXSIUS' <unk>].autocorr(lag=1)\n",
      "=====\n",
      "df_pc_monthly['DEXJPUS' madeupword0002].autocorr()\n",
      "--------------------\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.plot(index_months, df_pc[jupyter_string], label=jupyter_string)\n",
      "plt.legend()\n",
      "\n",
      "=====\n",
      "df_std_by_months = df_pc.dropna().groupby(index_months).std()\n",
      "df_std_by_months\n",
      "--------------------\n",
      "df_std_by_months.plot()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "--------------------\n",
      "df_std_by_months = df_pc.dropna().groupby(index_months).std()\n",
      "_ = df_std_by_months.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "=====\n",
      "df_std_by_months_years = df_pc.dropna().groupby([df_pc.dropna().index.year, df_pc.dropna().index.month]).std()\n",
      "\n",
      "_ = df_std_by_months_years.loc[2000].plot(title=jupyter_string)\n",
      "--------------------\n",
      "_ = df_std_by_months_years.plot()\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "_ = plt.title(jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months_years.loc[2001].plot(title=jupyter_string)\n",
      "--------------------\n",
      "_ = df_std_by_months_years.loc[2000].plot(title=jupyter_string)\n",
      "_ = df_std_by_months_years.loc[2001].plot(title=jupyter_string)\n",
      "=====\n",
      "_ = df_std_by_months_years.loc[2002].plot(title=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "sample_mean = np.mean(df.temperature)\n",
      "sample_std = np.std(df.temperature)\n",
      "sample_size = len(df.temperature)\n",
      "sample_mean, sample_std, sample_size\n",
      "=====\n",
      "conf_int = np.percentile(df.temperature,[2.5,97.5])\n",
      "conf_int\n",
      "--------------------\n",
      "data_test.head()\n",
      "=====\n",
      "data_test.head()\n",
      "--------------------\n",
      "temp_mean = np.mean(df.temperature)\n",
      "temp_std = np.std(df.temperature)\n",
      "temp_std\n",
      "=====\n",
      "mean = df['temperature' <<unk>>].mean()\n",
      "std = df['temperature' <<unk>>].std()\n",
      "--------------------\n",
      "male_temp = df[df.gender == jupyter_string]['temperature' <unk>]\n",
      "female_temp = df[df.gender == jupyter_string]['temperature' <unk>]\n",
      "\n",
      "\n",
      "male_temp = np.array(male_temp)\n",
      "female_temp = np.array(female_temp)\n",
      "=====\n",
      "_ = sns.swarmplot(x='gender' madeupword0002, y='temperature' <<unk>>, data=df)\n",
      "\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "_ = sns.swarmplot(x='gender' <unk>, y='temperature' <unk>, hue='gender' <unk>, data=df)\n",
      "\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "=====\n",
      "_ = sns.boxplot(x='gender' madeupword0002, y='temperature' <<unk>>, data=df)\n",
      "\n",
      "_ = plt.xlabel(jupyter_string)\n",
      "_ = plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "training_data = pd.read_csv(jupyter_string)\n",
      "training_data['Well Name' <<unk>>] = training_data['Well Name' <<unk>>].astype(jupyter_string)\n",
      "training_data['Formation' <<unk>>] = training_data['Formation' <<unk>>].astype(jupyter_string)\n",
      "\n",
      "blind = pd.read_csv(jupyter_string)\n",
      "blind['Well Name' <<unk>>] = blind['Well Name' <<unk>>].astype(jupyter_string)\n",
      "blind['Formation' <<unk>>] = blind['Formation' <<unk>>].astype(jupyter_string)\n",
      "--------------------\n",
      "y_pred = V_classifier.fit(X_train, y_train).predict(X_test)\n",
      "=====\n",
      "y_blind = V_classifier.fit(X, y).predict(X_blind) \n",
      "blind['Facies' <<unk>>] = y_blind\n",
      "blind.to_csv(jupyter_string)\n",
      "--------------------\n",
      "training_data['Well Name' <unk>] = training_data['Well Name' <unk>].astype(jupyter_string)\n",
      "blind['Well Name' <unk>] = blind['Well Name' <unk>].astype(jupyter_string)\n",
      "=====\n",
      "above = []\n",
      "below = []\n",
      "\n",
      "cols = ['GR' <<unk>>, 'ILD_log10' <<unk>>, 'DeltaPHI' <<unk>>, 'PHIND' <<unk>>, 'PE' madeupword0002, 'NM_M' <<unk>>, 'RELPOS' <<unk>>]\n",
      "\n",
      "for i, group in training_data.groupby('Well Name' <<unk>>):\n",
      "    df = group.sort_values('Depth' <<unk>>)\n",
      "    dfa = df.shift(-1).fillna(method=jupyter_string)\n",
      "    dfb = df.shift(1).fillna(method=jupyter_string)\n",
      "        \n",
      "    above.append(dfa[cols])\n",
      "    below.append(dfb[cols])\n",
      "\n",
      "above_df = pd.concat(above)\n",
      "below_df = pd.concat(below)\n",
      "\n",
      "above_df.columns = [jupyter_string+ column for column in above_df.columns]\n",
      "below_df.columns = [jupyter_string+ column for column in below_df.columns]\n",
      "\n",
      "training_data = pd.concat((training_data, above_df, below_df), axis=1)\n",
      "    \n",
      "y = training_data['Facies' <<unk>>].values\n",
      "X = training_data.drop(['Formation' <<unk>>, 'Well Name' <<unk>>,'Facies' <<unk>>], axis=1)\n",
      "scaler = preprocessing.StandardScaler().fit(X)\n",
      "X = scaler.transform(X)\n",
      "\n",
      "--------------------\n",
      "X_blind = blind.drop(['Well Name' <unk>,'Facies' <unk>], axis=1)\n",
      "scaler = preprocessing.StandardScaler().fit(X_blind)\n",
      "X_blind = scaler.transform(X_blind)\n",
      "=====\n",
      "above = []\n",
      "below = []\n",
      "\n",
      "cols = ['GR' <<unk>>, 'ILD_log10' <<unk>>, 'DeltaPHI' <<unk>>, 'PHIND' <<unk>>, 'PE' madeupword0002, 'NM_M' <<unk>>, 'RELPOS' <<unk>>]\n",
      "\n",
      "for i, group in blind.groupby('Well Name' <<unk>>):\n",
      "    df = group.sort_values('Depth' <<unk>>)\n",
      "    dfa = df.shift(-1).fillna(method=jupyter_string)\n",
      "    dfb = df.shift(1).fillna(method=jupyter_string)\n",
      "        \n",
      "    above.append(dfa[cols])\n",
      "    below.append(dfb[cols])\n",
      "\n",
      "above_df = pd.concat(above)\n",
      "below_df = pd.concat(below)\n",
      "\n",
      "above_df.columns = [jupyter_string+ column for column in above_df.columns]\n",
      "below_df.columns = [jupyter_string+ column for column in below_df.columns]\n",
      "\n",
      "blind = pd.concat((blind, above_df, below_df), axis=1)\n",
      "\n",
      "X_blind = np.array(blind.drop(['Formation' <<unk>>, 'Well Name' <<unk>>], axis=1)) \n",
      "X_blind = scaler.transform(X_blind) \n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "world_firearms = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "world_firearms.head()\n",
      "=====\n",
      "world_firearms.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "=====\n",
      "X_train = data_train.iloc[:,:40]\n",
      "\n",
      "y_train = data_train.iloc[:,40:]\n",
      "\n",
      "X_test = data_train.iloc[:,:40]\n",
      "\n",
      "y_test = data_train.iloc[:,40:]\n",
      "--------------------\n",
      "world_Firearms = world_Firearms.dropna()\n",
      "world_Firearms.head()\n",
      "=====\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "world_firearms.drop(jupyter_string, axis=1, inplace=True)\n",
      "\n",
      "\n",
      "world_firearms = world_firearms.dropna(axis=0, how=jupyter_string)\n",
      "--------------------\n",
      "world_firearms.head()\n",
      "=====\n",
      "world_firearms.to_csv(jupyter_string)\n",
      "--------------------\n",
      "deaths = pd.read_csv(jupyter_string)\n",
      "deaths.head()\n",
      "=====\n",
      "law_score = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "law_score.head()\n",
      "=====\n",
      "law_score.head()\n",
      "--------------------\n",
      "law_score = law_score.set_index('State' <unk>)\n",
      "law_score.head()\n",
      "=====\n",
      "law_score['TOTAL STATE POINTS' <<unk>>]=(law_score['TOTAL STATE POINTS' <<unk>>] - law_score['TOTAL STATE POINTS' <<unk>>].min()\n",
      "                              ) / (law_score['TOTAL STATE POINTS' <<unk>>].max() - law_score['TOTAL STATE POINTS' <<unk>>].min())\n",
      "\n",
      "\n",
      "law_score[jupyter_string]=pd.cut(law_score['TOTAL STATE POINTS' <<unk>>], 5, labels=[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "\n",
      "law_score[jupyter_string]=pd.qcut(law_score['TOTAL STATE POINTS' <<unk>>], 5, labels=[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "=====\n",
      "population = pd.read_csv(jupyter_string)\n",
      "gun_ownership = pd.read_csv(jupyter_string)\n",
      "\n",
      "gun_ownership = population.merge(gun_ownership, left_on=jupyter_string, right_on='Location' madeupword0002, how=jupyter_string)\n",
      "--------------------\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string,jupyter_string)\n",
      "=====\n",
      "gun_ownership.head()\n",
      "--------------------\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "gun_ownership[jupyter_string] = gun_ownership[jupyter_string].str.replace(jupyter_string, jupyter_string)\n",
      "=====\n",
      "gun_ownership.drop('Location' madeupword0002, axis=1, inplace=True)\n",
      "\n",
      "gun_ownership['Gun Ownership by April 2017' <<unk>>] = gun_ownership['Gun Ownership by April 2017' <<unk>>].astype(int)\n",
      "\n",
      "gun_ownership[jupyter_string] = gun_ownership['Gun Ownership by April 2017' <<unk>>] / gun_ownership[\n",
      "    jupyter_string] * 1000\n",
      "\n",
      "gun_ownership[jupyter_string]=pd.qcut(gun_ownership[jupyter_string], 3, labels=[jupyter_string, jupyter_string, jupyter_string])\n",
      "\n",
      "gun_ownership.to_csv(jupyter_string)\n",
      "--------------------\n",
      "data[jupyter_string] = data['Adj Close' <unk>].rolling(window=9).mean()\n",
      "plot_data(data, symbol, dates)\n",
      "=====\n",
      "data[jupyter_string] = exponential_moving_avg(data['Adj Close' <<unk>>], 12)\n",
      "data[jupyter_string] = exponential_moving_avg(data['Adj Close' <<unk>>], 26)\n",
      "data[jupyter_string] = (data[jupyter_string] - data[jupyter_string])\n",
      "data[jupyter_string] = rolling_mean(data[jupyter_string], 7)\n",
      "data.tail()\n",
      "\n",
      "--------------------\n",
      "data[jupyter_string] = data[jupyter_string].shift(-1)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-2)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-3)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-4)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-5)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-6)\n",
      "data[jupyter_string] = data[jupyter_string].shift(-7)\n",
      "=====\n",
      "pd.options.mode.chained_assignment = None\n",
      "\n",
      "for i in range(1, len(data)):\n",
      "    if data[jupyter_string].iloc[i] == 1:\n",
      "        data[jupyter_string].iloc[i] = data['Adj Close' <<unk>>].iloc[i] - data['Adj Close' <<unk>>].iloc[i-1]\n",
      "    elif data[jupyter_string].iloc[i] == -1:\n",
      "        data[jupyter_string].iloc[i] = - (data['Adj Close' <<unk>>].iloc[i] - data['Adj Close' <<unk>>].iloc[i-1])\n",
      "\n",
      "total_gains = data[jupyter_string].sum()\n",
      "print(total_gains)\n",
      "        \n",
      "--------------------\n",
      "Lasso=linear_model.Lasso(fit_intercept=False, alpha=30) \n",
      "\n",
      "\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train\n",
      "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "\n",
      "Lasso_coef=Lasso.coef_\n",
      "\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test\n",
      "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "=====\n",
      "Lasso=linear_model.Lasso(fit_intercept=False,alpha=30)\n",
      "\n",
      "\n",
      "Lasso.fit(X_train,y_train)\n",
      "\n",
      "p_IS=Lasso.predict(X_train)\n",
      "err_IS=p_IS-y_train.T\n",
      "\n",
      "R_2_IS_Lasso=1-np.var(err_IS.T)/np.var(y_train)\n",
      "print(jupyter_string.format(R_2_IS_Lasso))\n",
      "\n",
      "\n",
      "p_OS=Lasso.predict(X_test)\n",
      "err_OS=p_OS-y_test.T\n",
      "R_2_OS_Lasso=1-np.var(err_OS.T)/np.var(y_test)\n",
      "print(jupyter_string.format(R_2_OS_Lasso))\n",
      "\n",
      "--------------------\n",
      "dates = pd.date_range(start=jupyter_string, end=jupyter_string, freq=jupyter_string)\n",
      "df = get_data(symbol, dates)\n",
      "df.head()\n",
      "=====\n",
      "dates = pd.date_range(jupyter_string, jupyter_string)\n",
      "symbol = jupyter_string\n",
      "data = get_data(symbol, dates)\n",
      "data.head()\n",
      "    \n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "df_feature = pd.read_csv(jupyter_string, index_col = 0)\n",
      "df_genre = pd.read_csv(jupyter_string, index_col = 0)\n",
      "--------------------\n",
      "x_train = x_train.drop(['music' <unk>, 'musical' <unk>], axis = 1)\n",
      "x_test = x_test.drop(['music' <unk>, 'musical' <unk>], axis = 1)\n",
      "=====\n",
      "x_train.to_csv(jupyter_string)\n",
      "x_test.to_csv(jupyter_string)\n",
      "y_train.to_csv(jupyter_string)\n",
      "y_test.to_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_train = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "df_train.head()\n",
      "=====\n",
      "df_train.head()\n",
      "--------------------\n",
      "sns.distplot(train['SalePrice' <unk>])\n",
      "=====\n",
      "sns.distplot(df_train[jupyter_string]);\n",
      "--------------------\n",
      "sns.distplot(df_train[jupyter_string]);\n",
      "=====\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "df_train.plot.scatter(x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "df_train.plot.box()\n",
      "=====\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(8, 6))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "--------------------\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(8, 6))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "=====\n",
      "var = jupyter_string\n",
      "data = pd.concat([df_train[jupyter_string], df_train[var]], axis=1)\n",
      "f, ax = plt.subplots(figsize=(16, 8))\n",
      "fig = sns.boxplot(x=var, y=jupyter_string, data=data)\n",
      "fig.axis(ymin=0, ymax=800000);\n",
      "plt.xticks(rotation=90);\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import utils\n",
      "\n",
      "df = utils.read_csv(jupyter_string)\n",
      "df.head()\n",
      "--------------------\n",
      "corrmat = df_train.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat, vmax=.8, square=True);\n",
      "=====\n",
      "corrmat = df_train.corr()\n",
      "f, ax = plt.subplots(figsize=(12, 9))\n",
      "sns.heatmap(corrmat, vmax=.8, square=True);\n",
      "--------------------\n",
      "sns.set()\n",
      "cols = ['SalePrice' <unk>, 'OverallQual' <unk>, 'GrLivArea' <unk>, 'GarageCars' <unk>, 'TotalBsmtSF' <unk>, '1stFlrSF' <unk>, 'FullBath' <unk>, 'YearBuilt' <unk>]\n",
      "sns.pairplot(df_train[cols], size = 2.5)\n",
      "plt.show();\n",
      "=====\n",
      "sns.set()\n",
      "cols = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "sns.pairplot(df_train[cols], height = 2.5);\n",
      "plt.show();\n",
      "--------------------\n",
      "df_train.isnull().sum()\n",
      "=====\n",
      "missing_features = df_train.isnull().sum()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "names_allbp =[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "data_allbp = pd.read_csv(jupyter_string, names=names_allbp, index_col=False, na_values=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "--------------------\n",
      "data_allbp.head()\n",
      "=====\n",
      "data_allbp.drop(jupyter_string, axis = 1, inplace = True)\n",
      "data_allbp.head()\n",
      "\n",
      "--------------------\n",
      "data_allbp.info()\n",
      "=====\n",
      "data_allbp[jupyter_string] = data_allbp[jupyter_string].apply( lambda x: x.split(jupyter_string)[0])\n",
      "data_allbp[jupyter_string].value_counts()\n",
      "\n",
      "\n",
      "--------------------\n",
      "data_allhyper = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "names_allhyper =[jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string,jupyter_string, jupyter_string, jupyter_string]\n",
      "data_allhyper = pd.read_csv(jupyter_string, names=names_allhyper, index_col=False, na_values=jupyter_string)\n",
      "data_allhyper.head()\n",
      "--------------------\n",
      "data_allhyper = pd.read_csv(jupyter_string, index_col=False, na_values=jupyter_string)\n",
      "data_allhyper.head()\n",
      "=====\n",
      "names_new_thyroid = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "data_new_thyroid = pd.read_csv(jupyter_string, names = names_new_thyroid, index_col = False)\n",
      "\n",
      "data_new_thyroid.head()\n",
      "--------------------\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X = df[[\"flarea\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "fig.tight_layout()\n",
      "\n",
      "\n",
      "def h(x, beta1):\n",
      "    return x * beta1\n",
      "\n",
      "\n",
      "\n",
      "beta1 = 8\n",
      "\n",
      "\n",
      "ax0.scatter(X, y, color = jupyter_string)\n",
      "\n",
      "\n",
      "xvals = np.linspace(-500, 500, 3)\n",
      "ax0.plot(xvals, h(xvals, beta1), color = jupyter_string)\n",
      "\n",
      "\n",
      "ax1.scatter(beta1, J(X, y, beta1), color = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X = df[[\"flarea\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "\n",
      "=====\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "fig.tight_layout()\n",
      "\n",
      "\n",
      "beta1s = np.linspace(-10, 10, 21)\n",
      "\n",
      "\n",
      "ax0.scatter(X, y, color = jupyter_string)\n",
      "\n",
      "\n",
      "xvals = np.linspace(-500, 500, 3)\n",
      "for beta1 in beta1s:\n",
      "    ax0.plot(xvals, h(xvals, beta1), color = jupyter_string)\n",
      "\n",
      "\n",
      "ax1.scatter(beta1s, [J(X, y, beta1) for beta1 in beta1s], color = jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "df[jupyter_string].mean()\n",
      "=====\n",
      "stacked = df.stack().reset_index()\n",
      "stacked.columns = [jupyter_string, jupyter_string, jupyter_string]\n",
      "stacked.head()\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(8, 4)) \n",
      "gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1]) \n",
      "\n",
      "ax0 = plt.subplot(gs[0])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-500, 500)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1000)\n",
      "\n",
      "ax1 = plt.subplot(gs[1])\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.xlim(-10, 10)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.ylim(0, 1500000)\n",
      "fig.tight_layout()\n",
      "\n",
      "\n",
      "beta1s = np.linspace(-10, 10, 21)\n",
      "\n",
      "\n",
      "ax0.scatter(X, y,\n",
      "=====\n",
      "X = df[[\"bdrms\", \"bthrms\"]].values\n",
      "y = df[\"price\"].values \n",
      "\n",
      "fig = plt.figure() \n",
      "ax = Axes3D(fig)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_zlabel(jupyter_string)\n",
      "xvals = np.linspace(-100, 200, 301)\n",
      "yvals = np.linspace(-100, 200, 301)\n",
      "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
      "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
      "zvals = zs.reshape(xxvals.shape)\n",
      "ax.plot_surface(xxvals, yyvals, zvals)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig = plt.figure() \n",
      "ax = Axes3D(fig)\n",
      "ax.set_title(jupyter_string)\n",
      "ax.set_xlabel(jupyter_string)\n",
      "ax.set_ylabel(jupyter_string)\n",
      "ax.set_zlabel(jupyter_string)\n",
      "xvals = np.linspace(-100, 200, 301)\n",
      "yvals = np.linspace(-100, 200, 301)\n",
      "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
      "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
      "zvals = zs.reshape(xxvals.shape)\n",
      "ax.plot_surface(xxvals, yyvals, zvals)\n",
      "plt.show()\n",
      "=====\n",
      "fig = plt.figure(figsize = (6, 6)) \n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "xvals = np.linspace(-100, 200, 301)\n",
      "yvals = np.linspace(-100, 200, 301)\n",
      "xxvals, yyvals = np.meshgrid(xvals, yvals)\n",
      "zs = np.array([J(X, y, [beta2, beta3]) for beta2, beta3 in zip(xxvals.flatten(), yyvals.flatten())])\n",
      "zvals = zs.reshape(xxvals.shape)\n",
      "C = plt.contour(xxvals, yyvals, zvals, 15, colors = jupyter_string, linewidth = 0.5)\n",
      "plt.clabel(C, inline=1, fontsize=10)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X_without_dummy = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "X = add_dummy_feature(X_without_dummy)\n",
      "\n",
      "\n",
      "beta = npla.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "\n",
      "\n",
      "X_without_dummy = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
      "y = df[\"price\"].values\n",
      "\n",
      "\n",
      "X = add_dummy_feature(X_without_dummy)\n",
      "\n",
      "\n",
      "beta = nplt.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "=====\n",
      "beta = npla.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "\n",
      "\n",
      "beta\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, index_col='id' <<unk>>)\n",
      "df.head()\n",
      "--------------------\n",
      "df.info()\n",
      "=====\n",
      "df.describe()\n",
      "--------------------\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = res\n",
      "importances_bal = imps\n",
      "=====\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "df['trust' <unk>] = np.where(df['trust' <unk>] > 0.75, 1, 0)\n",
      "df.head()\n",
      "=====\n",
      "trust_threshold = 0.75\n",
      "df[jupyter_string] = df.apply(lambda row: jupyter_string if row.trust_value >= trust_threshold else jupyter_string, axis=1)\n",
      "df.head()  \n",
      "--------------------\n",
      "res, imps = test_classification(df_routeset)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "=====\n",
      "res, imps = test_classification(df_routesets)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_bal = results_bal.append(res, ignore_index=True)\n",
      "importances_bal = importances_bal.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "df = df.drop(jupyter_string, axis=1)\n",
      "df.head()\n",
      "=====\n",
      "df.drop('trust_value' <<unk>>, axis=1, inplace=True)\n",
      "df.shape  \n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "REsample1 = pd.read_csv(jupyter_string)\n",
      "REsample1.head()\n",
      "--------------------\n",
      "base = pd.pivot_table(stacked, index=jupyter_string, columns=jupyter_string, values=jupyter_string, aggfunc=jupyter_string)\n",
      "base.head()\n",
      "=====\n",
      "mean_power = df.mean()\n",
      "mean_power.head()\n",
      "--------------------\n",
      "results_bal.to_csv(jupyter_string)\n",
      "importances_bal.to_csv(jupyter_string)\n",
      "=====\n",
      "results_unb[jupyter_string] = False\n",
      "results_bal[jupyter_string] = True\n",
      "results = results_unb.append(results_bal, ignore_index=True)\n",
      "--------------------\n",
      "buildings = df[df['buildings' <unk>] > 0]\n",
      "routes = df[df['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>] > 0]\n",
      "routes = routes[routes['routes' <unk>]\n",
      "=====\n",
      "df_buildings = df.filter(like=jupyter_string, axis=0)\n",
      "df_routes = df.filter(regex=jupyter_string, axis=0)\n",
      "df_routesets = df.filter(like=jupyter_string, axis=0)\n",
      "df_buildings.shape, df_routes.shape, df_routesets.shape  \n",
      "--------------------\n",
      "plt.plot(results[jupyter_string], results[jupyter_string], jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "plt.show()\n",
      "import seaborn as sns\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "results.Accuracy.plot(kind=jupyter_string)\n",
      "=====\n",
      "pal = sns.light_palette(jupyter_string, n_colors=3, reverse=True)\n",
      "g = sns.factorplot(data=results, x=jupyter_string, y=jupyter_string, hue=jupyter_string, col=jupyter_string,\n",
      "                   kind=jupyter_string, palette=pal, aspect=1.2, errwidth=1, capsize=0.04)\n",
      "g.set(ylim=(88, 98))\n",
      "--------------------\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_routes = res\n",
      "importances_routes = imps\n",
      "=====\n",
      "res, imps = test_classification(df_routes)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "res, imps = test_classification(df_routeset)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "=====\n",
      "res, imps = test_classification(df_routesets)\n",
      "\n",
      "\n",
      "res[jupyter_string] = jupyter_string\n",
      "imps[jupyter_string] = jupyter_string\n",
      "\n",
      "\n",
      "results_unb = results_unb.append(res, ignore_index=True)\n",
      "importances_unb = importances_unb.append(imps, ignore_index=True)\n",
      "--------------------\n",
      "buildings = pd.read_csv(jupyter_string)\n",
      "buildings.head()\n",
      "=====\n",
      "df_buildings.label.value_counts()\n",
      "--------------------\n",
      "df_routes = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df_routes.label.value_counts()\n",
      "--------------------\n",
      "df_routes.head()\n",
      "=====\n",
      "df_routesets.label.value_counts()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
      "\n",
      "reg = LinearRegression()\n",
      "reg.fit(X_train, Y_train)\n",
      "Y_pred = reg.predict(X_test)\n",
      "\n",
      "print(jupyter_string, reg.score(X_test, Y_test))\n",
      "print(jupyter_string, mean_squared_error(Y_test, Y_pred))\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Ypred = X.dot(coefs)\n",
      "    erro = Y - Ypred\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "mean_power.describe()\n",
      "=====\n",
      "mean_power = mean_power.reset_index()\n",
      "mean_power\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string, header=None, delim_whitespace=True)\n",
      "\n",
      "dfTrain = df[ :30]\n",
      "dfTest = df[30 : ]\n",
      "X = dfTrain.values[ : , : -1]\n",
      "n = np.ones(len(X))\n",
      "X = np.c_[n, X]\n",
      "Y = dfTrain.values[ : , -1]\n",
      "\n",
      "XTest = dfTest.values[ : , : -1]\n",
      "n = np.ones(len(XTest))\n",
      "XTest = np.c_[n, XTest]\n",
      "YTest = dfTest.values[ : , -1]\n",
      "\n",
      "--------------------\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Ypred = X.dot(coefs)\n",
      "    erro = Y - Ypred\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
      "list_lambd = []\n",
      "list_eqm = []\n",
      "for lambd in lambdas:\n",
      "    coefs = coefs_OLS(X,Y, lambd)\n",
      "    Yhat = XTest.dot(coefs)\n",
      "    erro = YTest - Yhat\n",
      "    erroQuad = erro**2\n",
      "    eqm = np.mean(erroQuad)\n",
      "    list_lambd.append(lambd)\n",
      "    list_eqm.append(eqm)\n",
      "    \n",
      "plt.plot(list_lambd, list_eqm)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "coefs_OLS = coefs_OLS(X_train, Y_train, 1)\n",
      "print(jupyter_string, coefs_OLS)\n",
      "=====\n",
      "lambdas = np.array([0,1,2,3,4,5])\n",
      "coef = coefs_OLS(X, Y , lambdas)\n",
      "print(coef)\n",
      "--------------------\n",
      "goldenpercent[jupyter_string]=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocio == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "=====\n",
      "goldenpercent1 = goldenpercent[['Tweet' <<unk>>,'Felicidad' <<unk>>,'Tristeza' <<unk>>,jupyter_string,'Asco' <<unk>>,'Miedo' <<unk>>,'Sorpresa' <<unk>>,'NoEmocion' <<unk>>,jupyter_string,jupyter_string]]\n",
      "goldenpercent1[:5]\n",
      "--------------------\n",
      "goldenpercent2 = goldenpercent1.groupby(['Tristeza' <unk>,'Asco' <unk>,'Miedo' <unk>,'Sorpresa' <unk>])['Tweet' <unk>].count()\n",
      "goldenpercent2[:5]\n",
      "=====\n",
      "goldenpercent.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "goldenpercent.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "--------------------\n",
      "df = pd.read_csv(goldencsvpath)\n",
      "df.head()\n",
      "=====\n",
      "usertweetinfo = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "usertweetinfo2 = usertweetinfo[['User' <<unk>>,'Golden trust' <<unk>>,'Majority trust' <<unk>>]]\n",
      "usertweetinfo2[17:19]\n",
      "--------------------\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "=====\n",
      "goldenpercent1.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "majority_report = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "majority_report.head()\n",
      "=====\n",
      "majoritypercent = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "\n",
      "majoritypercent[:5]\n",
      "--------------------\n",
      "goldenpercent1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=goldenpercent1)\n",
      "--------------------\n",
      "mean_power = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "housedf = pd.read_csv(jupyter_string)\n",
      "housedf.head()\n",
      "--------------------\n",
      "majoritypercent[jupyter_string] = 0\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 1, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 3, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 5, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 7, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 9, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 10, jupyter_string] = jupyter_string\n",
      "majoritypercent.loc[majoritypercent[jupyter_string] == 11, jupyter_string] = jupyter_string\n",
      "majoritypercent[:10]\n",
      "=====\n",
      "majoritypercent[jupyter_string]=0\n",
      "majoritypercent.FullAgreement = ((majoritypercent.Tristeza == 1) | (majoritypercent.CÃ³lera == 1) | (majoritypercent.NoEmocion == 1) | (majoritypercent.Asco == 1) | (majoritypercent.Miedo == 1) | (majoritypercent.Sorpresa == 1) | (majoritypercent.Felicidad == 1)).astype(int)\n",
      "majoritypercent[:5]\n",
      "--------------------\n",
      "majoritypercent = majoritypercent.drop(['Tristeza' <unk>,'CÃ³lera' <unk>,'NoEmocio' <unk>,'Asco' <unk>,'Miedo' <unk>,'Sorpresa' <unk>,'Felicidad' <unk>],axis=1)\n",
      "majoritypercent[:5]\n",
      "=====\n",
      "majority1 = majoritypercent[['Tweet' <<unk>>,'Felicidad' <<unk>>,'Tristeza' <<unk>>,jupyter_string,'Asco' <<unk>>,'Miedo' <<unk>>,'Sorpresa' <<unk>>,'NoEmocion' <<unk>>,jupyter_string,jupyter_string]]\n",
      "majority1[:5]\n",
      "--------------------\n",
      "majority1.groupby([jupyter_string])['Tweet' <unk>].count()\n",
      "=====\n",
      "import seaborn as sns\n",
      "majority1.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "majority1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=majority1)\n",
      "--------------------\n",
      "majority1[jupyter_string].value_counts()\n",
      "=====\n",
      "majority1.groupby(jupyter_string).size()\n",
      "--------------------\n",
      "majority1.groupby(jupyter_string).size().plot(kind=jupyter_string)\n",
      "=====\n",
      "sns.countplot(jupyter_string, data=majority1)\n",
      "--------------------\n",
      "golden = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "goldenpercent = pd.read_csv(jupyter_string, encoding=jupyter_string)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "goldenpercent[jupyter_string] = 0\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 1, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 3, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 5, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 7, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 9, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 10, jupyter_string] = jupyter_string\n",
      "goldenpercent.loc[goldenpercent[jupyter_string] == 11, jupyter_string] = jupyter_string\n",
      "goldenpercent[:10]\n",
      "=====\n",
      "goldenpercent[jupyter_string]=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "goldenpercent = goldenpercent[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "goldenpercent[:5]\n",
      "=====\n",
      "goldenpercent[jupyter_string]=0\n",
      "goldenpercent.FullAgreement = ((goldenpercent.Tristeza == 1) | (goldenpercent.CÃ³lera == 1) | (goldenpercent.NoEmocion == 1) | (goldenpercent.Asco == 1) | (goldenpercent.Miedo == 1) | (goldenpercent.Sorpresa == 1) | (goldenpercent.Felicidad == 1)).astype(int)\n",
      "goldenpercent[:5]\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string)\n",
      "data.head()\n",
      "--------------------\n",
      "df1 = pd.DataFrame({\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string],\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "})\n",
      "df1\n",
      "=====\n",
      "pd.merge(mean_power, housedf).head()\n",
      "--------------------\n",
      "X = data['v1' <unk>]\n",
      "y = data['v2' <unk>]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "=====\n",
      "X_data = data.drop('1' <<unk>>,axis=1)\n",
      "y_data = data['1' <<unk>>]\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "rad_recording01162018 = pd.read_csv(jupyter_string, header=None )\n",
      "rad_recording01162018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,rad_recording01162018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "rad_recording01162018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "rad_recording01162018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,rad_recording01162018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Baseline02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "Baseline02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Baseline02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Baseline02042018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "Baseline02042018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Baseline02042018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StreamingLTE02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "StreamingLTE02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,StreamingLTE02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02042018 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02042018.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02042018.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02062018v2 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02062018v2.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02062018v2.shape, jupyter_string)\n",
      "\n",
      "\n",
      "Detector02062018v3 = pd.read_csv(jupyter_string, header=None )\n",
      "Detector02062018v3.columns = [jupyter_string, jupyter_string]\n",
      "print(jupyter_string,Detector02062018v3.shape, jupyter_string)\n",
      "\n",
      "--------------------\n",
      "baseline02042018 = Baseline02042018.head(5)\n",
      "baseline02042018.head()\n",
      "=====\n",
      "Baseline02042018v2.head()\n",
      "--------------------\n",
      "baseline02042018v2.info()\n",
      "=====\n",
      "from matplotlib import pyplot\n",
      "import seaborn as sns; sns.set(color_codes=True)\n",
      "--------------------\n",
      "pyplot.plot(Baseline02042018v2[jupyter_string])\n",
      "pyplot.show()\n",
      "=====\n",
      "plt.show()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.tsplot(ax=ax, data=Baseline02042018v2[jupyter_string], ci=[68, 95], color=jupyter_string)\n",
      "--------------------\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], color=jupyter_string)\n",
      "=====\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "--------------------\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "myplot = sns.distplot(Baseline02042018v2[jupyter_string], ax=ax, hist=True, rug=True)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Baseline02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "b3_sample = rad_recording01162018v2[jupyter_string]\n",
      "print(stats.describe(b3_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "print(stats.ttest_ind(b1_sample, b3_sample, equal_var = False) )\n",
      "print(stats.ttest_ind(b2_sample, b3_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b3_sample, ax=ax, hist=False, rug=True)\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "b3_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b3_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "print(stats.ttest_ind(b1_sample, b3_sample, equal_var = False) )\n",
      "print(stats.ttest_ind(b2_sample, b3_sample, equal_var = False) )\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = StreamingLTE02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "--------------------\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = StreamingLTE02042018[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) )\n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "=====\n",
      "print(jupyter_string)\n",
      "print()\n",
      "b1_sample = Baseline02042018v2[jupyter_string]\n",
      "print(stats.describe(b1_sample))\n",
      "b2_sample = Detector02062018v2[jupyter_string]\n",
      "print(stats.describe(b2_sample))\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "print(stats.ttest_ind(b1_sample, b2_sample, equal_var = False) ) \n",
      "\n",
      "print()\n",
      "print(jupyter_string)\n",
      "print()\n",
      "fig, ax = pyplot.subplots(figsize=(18, 12))\n",
      "sns.distplot(b1_sample, ax=ax, hist=False, rug=True)\n",
      "myplot = sns.distplot(b2_sample, ax=ax, hist=False, rug=True)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string, header=None)\n",
      "df.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "=====\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\n",
      "    jupyter_string, header=None,\n",
      "    names=[\n",
      "        jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string,\n",
      "        jupyter_string, jupyter_string, jupyter_string\n",
      "    ])\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "pd.merge(mean_power, housedf, left_on=jupyter_string, right_on=jupyter_string)\n",
      "=====\n",
      "pd.merge(mean_power, housedf, left_on=jupyter_string, right_on='triplex_meter' madeupword0002).head()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "=====\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train_std = sc.fit_transform(X_train)\n",
      "X_test_std = sc.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "\n",
      "pca = PCA(0.95)\n",
      "X_train_pca = pca.fit_transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train_pca, y_train)\n",
      "print(jupyter_string, lr.score(X_train_pca, y_train))\n",
      "print(jupyter_string, lr.score(X_test_pca, y_test))\n",
      "=====\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "pca = PCA(n_components=num_components)\n",
      "\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "\n",
      "lr_pca = LogisticRegression().fit(X_train_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_pca.predict(X_train_pca)),\n",
      "    accuracy_score(y_test, lr_pca.predict(X_test_pca))))    \n",
      "\n",
      "\n",
      "lr = LogisticRegression().fit(X_train_std, y_train)\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr.predict(X_train_std)),\n",
      "    accuracy_score(y_test, lr.predict(X_test_std))))    \n",
      "\n",
      "--------------------\n",
      "X_train_std = np.copy(X_train)\n",
      "X_train_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
      "X_train_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
      "=====\n",
      "my_pca = MyPCA(n_components=num_components)\n",
      "\n",
      "X_train_my_pca = my_pca.fit_transform(X_train_std)\n",
      "X_test_my_pca = my_pca.transform(X_test_std)\n",
      "\n",
      "lr_my_pca = LogisticRegression().fit(X_train_my_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_my_pca.predict(X_train_my_pca)),\n",
      "    accuracy_score(y_test, lr_my_pca.predict(X_test_my_pca))))    \n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X_train)\n",
      "X_train_pca = pca.transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "\n",
      "print(jupyter_string, X_train_pca.shape)\n",
      "print(jupyter_string, X_test_pca.shape)\n",
      "=====\n",
      "my_svd_pca = MySVDPCA(n_components=num_components)\n",
      "\n",
      "X_train_my_svd_pca = my_pca.fit_transform(X_train_std)\n",
      "X_test_my_svd_pca = my_pca.transform(X_test_std)\n",
      "\n",
      "lr_my_svd_pca = LogisticRegression().fit(X_train_my_svd_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(\n",
      "    accuracy_score(y_train, lr_my_svd_pca.predict(X_train_my_svd_pca)),\n",
      "    accuracy_score(y_test, lr_my_svd_pca.predict(X_test_my_svd_pca))))    \n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_train_pca = pca.fit_transform(X_train_std)\n",
      "X_test_pca = pca.transform(X_test_std)\n",
      "\n",
      "lr_pca = LogisticRegression().fit(X_train_pca, y_train)\n",
      "\n",
      "print(jupyter_string.format(accuracy_score(y_train, lr_pca.predict(X_train_pca)), accuracy_score(y_test, lr_pca.predict(X_test_pca))))\n",
      "=====\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "faces_dataset = fetch_olivetti_faces(shuffle=True, random_state=0)\n",
      "faces = faces_dataset.images.reshape(len(faces_dataset.images), -1)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
      "init_notebook_mode(connected=True)\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "plt.show()\n",
      "\n",
      "co2_ice = pd.read_csv(jupyter_string)\n",
      "\n",
      "s = co2_ice.plot(x='ice Age' <<unk>>, y='CO2 ppmv' <<unk>>)\n",
      "--------------------\n",
      "import pandas as pd\n",
      "plt.show()\n",
      "\n",
      "co2_mauna = pd.read_csv(jupyter_string)\n",
      "\n",
      "s = co2_mauna.plot(x='Year' <unk>, y='CO2 ppmv' <unk>)\n",
      "=====\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "\n",
      "co2_mlo = pd.read_csv(jupyter_string)\n",
      "co2_mlo = co2_mlo.replace(-999.99, np.nan)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(co2_mlo['decimal' <<unk>>], co2_mlo['ppm' <<unk>>])\n",
      "None\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(co2_mlo['decimal' <unk>], co2_mlo['ppm' <unk>])\n",
      "None\n",
      "=====\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.plot(co2_mlo['decimal' <<unk>>], co2_mlo['ppm' <<unk>>],jupyter_string,color=jupyter_string, linewidth=2)\n",
      "ax.set_title(jupyter_string,loc=jupyter_string,fontsize=20)\n",
      "ax.set_xlabel(jupyter_string,fontsize=16)\n",
      "ax.set_ylabel(jupyter_string,fontsize=16)\n",
      "ax.set_title(jupyter_string,loc=jupyter_string,fontsize=20)\n",
      "None\n",
      "--------------------\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(co2_mlo['decimal' <unk>], co2_mlo['ppm' <unk>],jupyter_string,color=jupyter_string, linewidth=2)\n",
      "ax.set_title(jupyter_string,loc=jupyter_string,fontsize=20)\n",
      "ax.set_xlabel(jupyter_string,fontsize=16)\n",
      "ax.set_ylabel(jupyter_string,fontsize=16)\n",
      "ax.set_title(jupyter_string,loc=jupyter_string,fontsize=20)\n",
      "=====\n",
      "import seaborn as sns\n",
      "\n",
      "sns_plot = sns.distplot(co2_mlo['ppm' <<unk>>].dropna())\n",
      "sns_plot = sns.distplot(co2_ice['CO2 ppmv' <<unk>>].dropna())\n",
      "fig = sns_plot.get_figure()\n",
      "\n",
      "--------------------\n",
      "fig = sns_plot.get_figure()\n",
      "fig.savefig(jupyter_string)\n",
      "=====\n",
      "import pdvega\n",
      "\n",
      "co2_mlo.vgplot.line(x='decimal' <<unk>>, y='ppm' <<unk>>, alpha=0.5)\n",
      "--------------------\n",
      "pd.merge(mean_power, housedf, left_on=jupyter_string, right_on='triplex_meter' <unk>).head()\n",
      "=====\n",
      "housedf.columns = [jupyter_string, 'type' <<unk>>]\n",
      "pd.merge(mean_power, housedf).head()\n",
      "--------------------\n",
      "co2_ice.vgplot.line(x='decimal' <unk>, y='CO2 ppmv' <unk>, alpha=0.5)\n",
      "=====\n",
      "co2_mlo.vgplot.heatmap(x=jupyter_string, y='mon' <<unk>>, C='ppm' <<unk>>, gridsize=20)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "df_test = pd.read_csv(jupyter_string)\n",
      "df_train_val = pd.read_csv(jupyter_string)\n",
      "n,m = df_train_val.shape\n",
      "n_val = int(n*0.8)\n",
      "df_val = df_train_val.iloc[n_val:,:]\n",
      "df_train = df_train_val.iloc[:n_val-1:]\n",
      "print(df_train.shape,df_val.shape,df_test.shape)\n",
      "FOG_weight = (len(df_train)/np.sum(df_train.iloc[:,0]))\n",
      "--------------------\n",
      "sample_weight = df_train.iloc[:,0] * FOG_weight +1\n",
      "\n",
      "svm = SVC(C=1, kernel=jupyter_string)\n",
      "svm.fit(df_train.iloc[:,1:77],df_train.iloc[:,0])\n",
      "y_val_pred_svm = svm.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_svm, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_svm)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_svm = svm.predict(df_train_val.iloc[:,1:77])\n",
      "=====\n",
      "from sklearn import svm\n",
      "\n",
      "svml = svm.LinearSVC(class_weight=jupyter_string)\n",
      "svml.fit(df_train.iloc[:,1:77], df_train.iloc[:,0])\n",
      "\n",
      "y_val_pred_svm = svml.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_svm, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_svm)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_svm = svml.predict(df_train_val.iloc[:,1:77])\n",
      "--------------------\n",
      "from sklearn import tree\n",
      "\n",
      "dt = tree.DecisionTreeClassifier(class_weight=jupyter_string)\n",
      "dt.fit(df_train.iloc[:,1:77], df_train.iloc[:,0])\n",
      "\n",
      "y_val_pred_dt = dt.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dt, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dt)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_dt = dt.predict(df_train_val.iloc[:,1:77])\n",
      "=====\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "min_s_l_options = np.arange(0.1,0.5,0.025)\n",
      "max_depth_options = np.arange(1,11)\n",
      "max_features_option = np.arange(10,76,6)\n",
      "parameters = {jupyter_string: min_s_l_options, jupyter_string:max_depth_options} \n",
      "dtc = DecisionTreeClassifier(criterion = jupyter_string, class_weight = jupyter_string)\n",
      "dtc_cv = GridSearchCV(dtc, parameters,scoring=geo_cost)\n",
      "dtc_cv.fit(df_train.iloc[:,1:77],df_train.iloc[:,0])\n",
      "\n",
      "--------------------\n",
      "y_val_pred_dtc = dtc_cv.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dtc, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dtc)\n",
      "print(jupyter_string.format(cost))\n",
      "=====\n",
      "scores = dtc_cv.cv_results_[jupyter_string]\n",
      "scores = scores.reshape(len(min_s_l_options), len(max_depth_options))\n",
      "plt.imshow(scores)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "\n",
      "y_val_pred_dtc = dtc_cv.predict(df_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_val.iloc[:,0], y_val_pred_dtc, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_val.iloc[:,0], y_val_pred_dtc)\n",
      "print(jupyter_string.format(cost))\n",
      "y_tv_pred_dtc_cv = dtc_cv.predict(df_train_val.iloc[:,1:77])\n",
      "--------------------\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble\n",
      "=====\n",
      "log_en = LogisticRegression(solver=jupyter_string, max_iter= 200)\n",
      "X_train = np.hstack((y_tv_pred_log[:,np.newaxis],y_tv_pred_svm[:,np.newaxis],y_tv_pred_dtc_cv[:,np.newaxis]))\n",
      "\n",
      "sample_weight = df_train_val.iloc[:,0] * FOG_weight +1\n",
      "log_en.fit(X_train, df_train_val.iloc[:,0], sample_weight)\n",
      "y_tv_log_en_train = log_en.predict(X_train)\n",
      "\n",
      "\n",
      "print(classification_report(df_train_val.iloc[:,0], y_tv_log_en_train, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_train_val.iloc[:,0], y_tv_log_en_train)\n",
      "print(jupyter_string.format(cost))\n",
      "--------------------\n",
      "y_tv_log_en_cv = log_en.predict(df_train_val.iloc[:,1:77])\n",
      "\n",
      "print(classification_report(df_train_val.iloc[:,0], y_tv_log_en_cv, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_train_val.iloc[:,0], y_tv_log_en_cv)\n",
      "print(jupyter_string.format(cost))\n",
      "=====\n",
      "y_pred_log = log.predict(df_test.iloc[:,1:77])\n",
      "y_pred_svm = svml.predict(df_test.iloc[:,1:77])\n",
      "y_pred_dtc = dtc_cv.predict(df_test.iloc[:,1:77])\n",
      "\n",
      "X_test = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis]))\n",
      "y_tv_log_en = log_en.predict(X_test)\n",
      "\n",
      "\n",
      "print(classification_report(df_test.iloc[:,0], y_tv_log_en, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_test.iloc[:,0], y_tv_log_en)\n",
      "print(jupyter_string.format(cost))\n",
      "--------------------\n",
      "y_pred_log = log.predict(df_train.iloc[:,1:77])\n",
      "y_pred_svm = svml.predict(df_train.iloc[:,1:77])\n",
      "y_pred_dtc = dtc_cv.predict(df_train.iloc[:,1:77])\n",
      "\n",
      "X_train = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis]))\n",
      "=====\n",
      "list_of_tv_pred_train = (y_tv_pred_log[:,np.newaxis],y_tv_pred_svm[:,np.newaxis],y_tv_pred_dtc_cv[:,np.newaxis],y_tv_log_en_train[:,np.newaxis])\n",
      "list_of_tv_pred_test =(y_pred_log[:,np.newaxis] , y_pred_svm[:,np.newaxis], y_pred_dtc[:,np.newaxis], y_tv_log_en[:,np.newaxis] )\n",
      "\n",
      "cost_list_tv = list()\n",
      "cost_list_test = list()\n",
      "for i, tv_pred in enumerate(list_of_tv_pred_train):\n",
      "    cost_list_tv.append( geomean_cost(df_train_val.iloc[:,0], tv_pred))\n",
      "    cost_list_test.append( geomean_cost(df_test.iloc[:,0], list_of_tv_pred_test[i]))\n",
      "    \n",
      "    \n",
      "width = 0.8       \n",
      "\n",
      "ind_train = np.array([1,4,7,10])    \n",
      "ind_test = np.array([2,5,8,11])    \n",
      "\n",
      "p1 = plt.bar(ind_train,np.array(cost_list_tv), width, color=jupyter_string)\n",
      "p2 = plt.bar(ind_test,np.array(cost_list_test), width, color=jupyter_string)\n",
      "\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xticks(ind_train+0.5, (jupyter_string, jupyter_string, jupyter_string, jupyter_string))\n",
      "plt.yticks(np.arange(.65, .85, 0.05))\n",
      "plt.ylim(0.65,.85)\n",
      "plt.legend((p1[0], p2[0]), (jupyter_string, jupyter_string), loc = jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClass\n",
      "=====\n",
      "plt.scatter(np.arange(svml.coef_.shape[1]),np.abs(svml.coef_[0,:]))\n",
      "plt.plot(np.array([0,80]),np.array([1,1]))\n",
      "plt.show()\n",
      "\n",
      "extra_features = np.abs(svml.coef_) > 1\n",
      "print(df_train_val.iloc[:,1:77].columns.values[extra_features[0]])\n",
      "\n",
      "\n",
      "log_enp = LogisticRegression(solver=jupyter_string, max_iter= 200)\n",
      "X_train_plus = np.hstack((\n",
      "        np.hstack((y_tv_pred_log[:,np.newaxis],y_tv_pred_svm[:,np.newaxis],y_tv_pred_dtc_cv[:,np.newaxis])),\n",
      "        df_train_val.iloc[:,extra_features[0]]))\n",
      "\n",
      "sample_weight = df_train_val.iloc[:,0] * FOG_weight +1\n",
      "log_enp.fit(X_train_plus, df_train_val.iloc[:,0], sample_weight)\n",
      "y_tv_log_enp_train = log_enp.predict(X_train_plus)\n",
      "\n",
      "\n",
      "print(classification_report(df_train_val.iloc[:,0], y_tv_log_enp_train, target_names=[jupyter_string,jupyter_string]))\n",
      "cost = geomean_cost(df_train_val.iloc[:,0], y_tv_log_enp_train)\n",
      "print(jupyter_string.format(cost))\n",
      "cost_list_tv.append(cost)\n",
      "\n",
      "X_test = np.hstack((y_pred_log[:,np.newaxis],y_pred_svm[:,np.newaxis],y_pred_dtc[:,np.newaxis],df_test.iloc[:,extra_features[0]]))\n",
      "y_tv_log_enp = log_enp.predict(X_test)\n",
      "cost_list_test.append( geomean_cost(df_test.iloc[:,0], y_tv_log_enp[:,np.newaxis]))\n",
      "--------------------\n",
      "plt.scatter(np.arange(log_enp.coef_.shape[1]),np.abs(log_enp.coef_[0,:]))\n",
      "plt.plot(np.array([0,80]),np.array([1,1]))\n",
      "plt.show()\n",
      "=====\n",
      "ind_train = np.array([1,4,7,10,13])    \n",
      "ind_test = np.array([2,5,8,11,14])    \n",
      "\n",
      "print((cost_list_tv,cost_list_test))\n",
      "p1 = plt.bar(ind_train,np.array(cost_list_tv), width, color=jupyter_string)\n",
      "p2 = plt.bar(ind_test,np.array(cost_list_test), width, color=jupyter_string)\n",
      "\n",
      "\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xticks(ind_train+0.5, (jupyter_string, jupyter_string, jupyter_string, jupyter_string,jupyter_string))\n",
      "plt.yticks(np.arange(.65, .85, 0.05))\n",
      "plt.ylim(0.65,.85)\n",
      "plt.legend((p1[0], p2[0]), (jupyter_string, jupyter_string), loc = jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "pd.merge(mean_power, housedf, on=jupyter_string, how=jupyter_string).head()\n",
      "=====\n",
      "merged = pd.merge(mean_power, housedf, on=jupyter_string)\n",
      "merged.head()\n",
      "--------------------\n",
      "X_test_counts = vect.transform(X_test)\n",
      "X_test_tfidf = tfidf.transform(X_test_counts)\n",
      "y_pred = clf.predict(X_test_tfidf)\n",
      "=====\n",
      "X_test_counts = vect.transform(X_test)\n",
      "X_test_tfidf = tfidf.transform(X_test_counts)\n",
      "\n",
      "\n",
      "y_pred = clf.predict(X_test_tfidf)\n",
      "--------------------\n",
      "pd.crosstab(y_test, y_pred, rownames=[jupyter_string], colnames=[jupyter_string])\n",
      "=====\n",
      "labels = np.unique(y_pred)\n",
      "confusion_mat = confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
      "accuracy = accuracy = (y_pred == y_test).mean()\n",
      "\n",
      "print(jupyter_string, labels)\n",
      "print(jupyter_string, confusion_mat)\n",
      "print(jupyter_string, accuracy)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "plt.show()\n",
      "import os\n",
      "import sys\n",
      "import numpy \n",
      "import pandas \n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import LogNorm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.formula.api import ols\n",
      "    \n",
      "\n",
      "pandas.set_option(jupyter_string, True)\n",
      "pandas.set_option(jupyter_string, 20)\n",
      "pandas.set_option(jupyter_string, 50)\n",
      "\n",
      "from decimal import getcontext, Decimal\n",
      "\n",
      "getcontext().prec = 2\n",
      "--------------------\n",
      "df = pandas.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "dm = pandas.read_csv(jupyter_string)\n",
      "dm = dm[(dm['advantagetypeshooter' <<unk>>]==jupyter_string)] \n",
      "dm = dm[dm['zone' <<unk>>]==jupyter_string]\n",
      "--------------------\n",
      "dm.head()\n",
      "=====\n",
      "dm.head(2)\n",
      "--------------------\n",
      "dm.tail(2)\n",
      "=====\n",
      "dm.describe()\n",
      "--------------------\n",
      "dm.groupby('shot_made_flag' <unk>).shot_length.describe()\n",
      "=====\n",
      "dm.groupby(['eventtype' <<unk>>])[['XNorm' <<unk>>, 'YNorm' <<unk>>]].describe()\n",
      "--------------------\n",
      "dm.groupby(['shot_type' <unk>])[['XNorm' <unk>, 'YNorm' <unk>]].describe()\n",
      "=====\n",
      "_t = dm.groupby(['eventtype' <<unk>>])[['XNorm' <<unk>>, 'YNorm' <<unk>>]].describe()\n",
      "_t.swapaxes(0,1)\n",
      "--------------------\n",
      "dm.groupby(['eventtype' <unk>, 'shot_type' <unk>])[['XNorm' <unk>, 'YNorm' <unk>]].describe()\n",
      "=====\n",
      "pandas.crosstab(dm['shotType' <<unk>>], dm['eventtype' <<unk>>], margins=True)\n",
      "--------------------\n",
      "dm.groupby(['shotType' <unk>, 'eventtype' <unk>])[['XNorm' <unk>, 'YNorm' <unk>]].describe()\n",
      "=====\n",
      "pandas.crosstab(dm['shotType' <<unk>>], dm['eventtype' <<unk>>], margins=True, normalize=True)\n",
      "--------------------\n",
      "merged.groupby('type' <unk>)['power' <unk>].mean()\n",
      "=====\n",
      "grp = merged.groupby('type' <<unk>>)\n",
      "type(grp)\n",
      "--------------------\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "dm[jupyter_string] = jupyter_string\n",
      "=====\n",
      "dm[jupyter_string] = dm['eventtype' <<unk>>]==jupyter_string\n",
      "dm[jupyter_string] = 99 - dm['XNorm' <<unk>>]\n",
      "dm.groupby(['eventtype' <<unk>>])[[jupyter_string, 'YNorm' <<unk>>]].describe()\n",
      "--------------------\n",
      "dm.groupby(['shotType' <unk>])[[jupyter_string, 'YNorm' <unk>]].describe()\n",
      "=====\n",
      "dm[jupyter_string] = dm[jupyter_string] + 1\n",
      "dm[jupyter_string] = numpy.absolute(dm['YNorm' <<unk>>])\n",
      "dm[jupyter_string] = dm[jupyter_string] + 1\n",
      "\n",
      "dm[jupyter_string] = numpy.log(dm[jupyter_string])\n",
      "dm[jupyter_string] = numpy.log(dm[jupyter_string])\n",
      "\n",
      "dm[jupyter_string] = dm['tgoals' <<unk>>] - dm['ogoals' <<unk>>]\n",
      "dm[jupyter_string] = dm.apply(lambda x: 1 if (x['teamcode' <<unk>>] == x['hteamcode' <<unk>>]) else x['vteamcode' <<unk>>], axis=1)\n",
      "--------------------\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "dm = dm[dm[jupyter_string] == jupyter_string]\n",
      "=====\n",
      "dw = dm[dm['shotType' <<unk>>]==jupyter_string]\n",
      "--------------------\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.heatmap(dm.corr(), annot=True)\n",
      "=====\n",
      "plt.hist2d(dw[jupyter_string], dw['YNorm' <<unk>>],bins=30)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.hist2d(dw[jupyter_string], dw['YNorm' <unk>],bins=30)\n",
      "plt.show()\n",
      "=====\n",
      "dg = dw[dw['eventtype' <<unk>>]==jupyter_string]\n",
      "plt.hist2d(dg[jupyter_string], dg['YNorm' <<unk>>],bins=30)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.formula.api as smf\n",
      "import statsmodels.api as sm\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "plt.style.use(jupyter_string)\n",
      "=====\n",
      "m1 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "\n",
      "\n",
      "m1.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "\n",
      "--------------------\n",
      "m2 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string]).fit())\n",
      "\n",
      "\n",
      "m2.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "=====\n",
      "dw[jupyter_string] = m1.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string,x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "dw = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "dw.summary()\n",
      "=====\n",
      "m2 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[jupyter_string])).fit()\n",
      "m2.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "\n",
      "\n",
      "--------------------\n",
      "dw[jupyter_string] = m2.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string,x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "dw[jupyter_string] = m1.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "--------------------\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "m3 = sm.Logit(dw[jupyter_string], sm.add_constant(dw[[jupyter_string, jupyter_string]])).fit()\n",
      "m3.get_margeff(method=jupyter_string, at=jupyter_string).summary()\n",
      "--------------------\n",
      "grouped = grp.mean()\n",
      "type(grouped)\n",
      "=====\n",
      "grp.agg([jupyter_string, jupyter_string])\n",
      "--------------------\n",
      "dw[jupyter_string] = m3.predict()\n",
      "\n",
      "\n",
      "dw.plot(kind=jupyter_string, x=jupyter_string, y=jupyter_string)\n",
      "=====\n",
      "dw[jupyter_string] = m3.predict()\n",
      "--------------------\n",
      "dw[jupyter_string] = dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string] + dw[jupyter_string]\n",
      "=====\n",
      "dw.describe()\n",
      "--------------------\n",
      "dw.head()\n",
      "=====\n",
      "dw.groupby(['season' <<unk>>])[[jupyter_string, jupyter_string]].sum()\n",
      "--------------------\n",
      "dw.groupby(['season' <unk>])[[jupyter_string, jupyter_string]].sum()\n",
      "=====\n",
      "dw.groupby(['period' <<unk>>])[[jupyter_string, jupyter_string]].sum()\n",
      "\n",
      "--------------------\n",
      "dw.groupby([jupyter_string, jupyter_string])[[jupyter_string, jupyter_string]].sum()\n",
      "=====\n",
      "dw.groupby(['teamcode' <<unk>>])[[jupyter_string, jupyter_string]].sum()\n",
      "--------------------\n",
      "dw.groupby(['teamcode' <unk>, jupyter_string])[[jupyter_string, jupyter_string]].sum()\n",
      "=====\n",
      "dw.groupby([jupyter_string])[[jupyter_string, jupyter_string]].sum()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data_uncleaned=pd.read_csv(jupyter_string)\n",
      "\n",
      "data=pd.read_csv(rjupyter_string, encoding=jupyter_string)\n",
      "--------------------\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "=====\n",
      "Intro_Course_Required = []\n",
      "for row in data.values[:,9]:\n",
      "    Intro_Course_Required.append(row)\n",
      "Intro_Course_Required_Y = Intro_Course_Required.count(jupyter_string)\n",
      "Intro_Course_Required_N = Intro_Course_Required.count(jupyter_string)\n",
      "Prop_Intro_Course_Required_Y = Intro_Course_Required_Y/(Intro_Course_Required_Y+Intro_Course_Required_N)\n",
      "Prop_Intro_Course_Required_N = Intro_Course_Required_N/(Intro_Course_Required_Y+Intro_Course_Required_N)\n",
      "\n",
      "Intro_Course_Elective = []\n",
      "for row in data.values[:,10]:\n",
      "    Intro_Course_Elective.append(row)\n",
      "Intro_Course_Elective_Y = Intro_Course_Elective.count(jupyter_string)\n",
      "Intro_Course_Elective_N = Intro_Course_Elective.count(jupyter_string)\n",
      "Prop_Intro_Course_Elective_Y = Intro_Course_Elective_Y/(Intro_Course_Elective_Y+Intro_Course_Elective_N)\n",
      "Prop_Intro_Course_Elective_N = Intro_Course_Elective_N/(Intro_Course_Elective_Y+Intro_Course_Elective_N)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.pie([Intro_Course_Required_Y,Intro_Course_Required_N],labels=[jupyter_string,jupyter_string],explode=(0,0.1),autopct=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.subplot(1,2,2)\n",
      "plt.pie([Intro_Course_Elective_Y,Intro_Course_Elective_N],labels=[jupyter_string,jupyter_string],explode=(0,0.1),autopct=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Com_Dir_dist_ax1 = [Prop_Intro_Course_Required_Y, Prop_Intro_Course_Required_N]\n",
      "Com_Dir_dist_ax2 = [Prop_Intro_Course_Elective_Y, Prop_Intro_Course_Elective_N]\n",
      "ax2.bar(ticks2,Com_Dir_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Com_Dir_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Com_Dir_dist_ax1 = [Prop_Deg_Comp_Y_and_Taught_Intro_Y, Prop_Deg_Comp_Y_and_Taught_Intro_N]\n",
      "Com_Dir_dist_ax2 = [Prop_Deg_Comp_Y_and_Taught_Intro_Y, Prop_Deg_Comp_Y_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Com_Dir_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Com_Dir_dist_ax1,width\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2a_det_3a_dist_ax1 = [Prop_Deg_Comp_Y_and_Taught_Intro_Y, Prop_Deg_Comp_Y_and_Taught_Intro_N]\n",
      "Q2a_det_3a_dist_ax2 = [Prop_Deg_Comp_N_and_Taught_Intro_Y, Prop_Deg_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "grp.mean()\n",
      "=====\n",
      "merged.pivot(index=jupyter_string, columns='type' <<unk>>, values=jupyter_string).head()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2a_det_3a_dist_ax1 = [Prop_Min_Comp_Y_and_Taught_Intro_Y, Prop_Min_Comp_Y_and_Taught_Intro_N]\n",
      "Q2a_det_3a_dist_ax2 = [Prop_Min_Comp_N_and_Taught_Intro_Y, Prop_Min_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "Q2b_det_3a_dist_ax1 = [Prop_Min_Comp_Y_and_Taught_Intro_Y, Prop_Min_Comp_Y_and_Taught_Intro_N]\n",
      "Q2b_det_3a_dist_ax2 = [Prop_Min_Comp_N_and_Taught_Intro_Y, Prop_Min_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks2 = [1,2]\n",
      "Q2c_det_3a_dist_ax1 = [Prop_Cer_Comp_Y_and_Taught_Intro_Y, Prop_Cer_Comp_Y_and_Taught_Intro_N]\n",
      "Q2c_det_3a_dist_ax2 = [Prop_Cer_Comp_N_and_Taught_Intro_Y, Prop_Cer_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2c_det_\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2c_det_3a_dist_ax1 = [Prop_Cer_Comp_Y_and_Taught_Intro_Y, Prop_Cer_Comp_Y_and_Taught_Intro_N]\n",
      "Q2c_det_3a_dist_ax2 = [Prop_Cer_Comp_N_and_Taught_Intro_Y, Prop_Cer_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2c_det_3a_dist_ax1 = [Prop_Oth_Comp_Y_and_Taught_Intro_Y, Prop_Oth_Comp_Y_and_Taught_Intro_N]\n",
      "Q2c_det_3a_dist_ax2 = [Prop_Oth_Comp_N_and_Taught_Intro_Y, Prop_Oth_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2d_det_3a_dist_ax1 = [Prop_Oth_Comp_Y_and_Taught_Intro_Y, Prop_Oth_Comp_Y_and_Taught_Intro_N]\n",
      "Q2d_det_3a_dist_ax2 = [Prop_Oth_Comp_N_and_Taught_Intro_Y, Prop_Oth_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2d_det_3a_dist_ax1 = [Prop_Non_Comp_Y_and_Taught_Intro_Y, Prop_Non_Comp_Y_and_Taught_Intro_N]\n",
      "Q2d_det_3a_dist_ax2 = [Prop_Non_Comp_N_and_Taught_Intro_Y, Prop_Non_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2d_det_\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q2e_det_3a_dist_ax1 = [Prop_Non_Comp_Y_and_Taught_Intro_Y, Prop_Non_Comp_Y_and_Taught_Intro_N]\n",
      "Q2e_det_3a_dist_ax2 = [Prop_Non_Comp_N_and_Taught_Intro_Y, Prop_Non_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "Q2e_det_3a_dist_ax1 = [Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_N]\n",
      "Q2e_det_3a_dist_ax2 = [Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q2e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q2e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6a_det_3a_dist_ax1 = [Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_Y_and_Taught_Intro_N]\n",
      "Q6a_det_3a_dist_ax2 = [Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_Y, Prop_Comp_Ans_Hmwk_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6a_det_3a_dist_ax1 = [Prop_Comp_Intr_Act_Y_and_Taught_Intro_Y, Prop_Comp_Intr_Act_Y_and_Taught_Intro_N]\n",
      "Q6a_det_3a_dist_ax2 = [Prop_Comp_Intr_Act_Y_and_Taught_Intro_Y, Prop_Comp_Intr_Act_Y_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6b_det_3a_dist_ax1 = [Prop_Comp_Intr_Act_Y_and_Taught_Intro_Y, Prop_Comp_Intr_Act_Y_and_Taught_Intro_N]\n",
      "Q6b_det_3a_dist_ax2 = [Prop_Comp_Intr_Act_N_and_Taught_Intro_Y, Prop_Comp_Intr_Act_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6c_det_3a_dist_ax1 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "Q6c_det_3a_dist_ax2 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6c_det_3a_dist_ax1 = [Prop_Comp_Exm_Ques_Y_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_Y_and_Taught_Intro_N]\n",
      "Q6c_det_3a_dist_ax2 = [Prop_Comp_Exm_Ques_N_and_Taught_Intro_Y, Prop_Comp_Exm_Ques_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6c_det_3a_dist_ax1 = [Prop_Comp_Pre_Con_Y_and_Taught_Intro_Y, Prop_Comp_Pre_Con_Y_and_Taught_Intro_N]\n",
      "Q6c_det_3a_dist_ax2 = [Prop_Comp_Pre_Con_N_and_Taught_Intro_Y, Prop_Comp_Pre_Con_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6d_det_3a_dist_ax1 = [Prop_Comp_Pre_Con_Y_and_Taught_Intro_Y, Prop_Comp_Pre_Con_Y_and_Taught_Intro_N]\n",
      "Q6d_det_3a_dist_ax2 = [Prop_Comp_Pre_Con_N_and_Taught_Intro_Y, Prop_Comp_Pre_Con_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6e_det_3a_dist_ax1 = [Prop_Comp_Pre_Con_Y_and_Taught_Intro_Y, Prop_Comp_Pre_Con_Y_and_Taught_Intro_N]\n",
      "Q6e_det_3a_dist_ax2 = [Prop_Comp_Pre_Con_N_and_Taught_Intro_Y, Prop_Comp_Pre_Con_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6e_det_3a_dist_ax1 = [Prop_Comp_Sim_Cla_Y_and_Taught_Intro_Y, Prop_Comp_Sim_Cla_Y_and_Taught_Intro_N]\n",
      "Q6e_det_3a_dist_ax2 = [Prop_Comp_Sim_Cla_N_and_Taught_Intro_Y, Prop_Comp_Sim_Cla_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6e_det_3a_dist_ax1 = [Prop_Comp_Sim_Out_Y_and_Taught_Intro_Y, Prop_Comp_Sim_Out_Y_and_Taught_Intro_N]\n",
      "Q6e_det_3a_dist_ax2 = [Prop_Comp_Sim_Out_N_and_Taught_Intro_Y, Prop_Comp_Sim_Out_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6f_det_3a_dist_ax1 = [Prop_Comp_Sim_Out_Y_and_Taught_Intro_Y, Prop_Comp_Sim_Out_Y_and_Taught_Intro_N]\n",
      "Q6f_det_3a_dist_ax2 = [Prop_Comp_Sim_Out_N_and_Taught_Intro_Y, Prop_Comp_Sim_Out_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6f_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "REsample12=REsample1.loc[100:]\n",
      "\n",
      "RE12w0=sum(REsample12.sale_price*REsample12.gross_sq_feet)/sum(REsample12.gross_sq_feet**2)\n",
      "print(jupyter_string.format(RE12w0))\n",
      "--------------------\n",
      "merged.pivot(index=jupyter_string, columns='type' <unk>, values=jupyter_string).mean()\n",
      "=====\n",
      "merged.pivot(index=jupyter_string, columns='type' <<unk>>, values=jupyter_string).boxplot(return_type=jupyter_string)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6f_det_3a_dist_ax1 = [Comp_Crs_Pro_Y_and_Taught_Intro_Y, Comp_Crs_Pro_Y_and_Taught_Intro_N]\n",
      "Q6f_det_3a_dist_ax2 = [Comp_Crs_Pro_Y_and_Taught_Intro_Y, Comp_Crs_Pro_Y_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6g_det_3a_dist_ax1 = [Prop_Comp_Crs_Pro_Y_and_Taught_Intro_Y, Prop_Comp_Crs_Pro_Y_and_Taught_Intro_N]\n",
      "Q6g_det_3a_dist_ax2 = [Prop_Comp_Crs_Pro_N_and_Taught_Intro_Y, Prop_Comp_Crs_Pro_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6g_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6g_det_3a_dist_ax1 = [Prop_Comp_Oth_Y_and_Taught_Intro_Y, Prop_Comp_Oth_Y_and_Taught_Intro_N]\n",
      "Q6g_det_3a_dist_ax2 = [Prop_Comp_Oth_N_and_Taught_Intro_Y, Prop_Comp_Oth_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6h_det_3a_dist_ax1 = [Prop_Comp_Oth_Y_and_Taught_Intro_Y, Prop_Comp_Oth_Y_and_Taught_Intro_N]\n",
      "Q6h_det_3a_dist_ax2 = [Prop_Comp_Oth_N_and_Taught_Intro_Y, Prop_Comp_Oth_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6h_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6h_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q6h_det_3a_dist_ax1 = [Prop_Teach_Doc_Y_and_Taught_Intro_Y, Prop_Teach_Doc_Y_and_Taught_Intro_N]\n",
      "Q6h_det_3a_dist_ax2 = [Prop_Teach_Doc_N_and_Taught_Intro_Y, Prop_Teach_Doc_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q6h_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q6\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q7a_det_3a_dist_ax1 = [Prop_Teach_Doc_Y_and_Taught_Intro_Y, Prop_Teach_Doc_Y_and_Taught_Intro_N]\n",
      "Q7a_det_3a_dist_ax2 = [Prop_Teach_Doc_N_and_Taught_Intro_Y, Prop_Teach_Doc_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q7a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q7a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q7a_det_3a_dist_ax1 = [Prop_Teach_Debug_Y_and_Taught_Intro_Y, Prop_Teach_Debug_Y_and_Taught_Intro_N]\n",
      "Q7a_det_3a_dist_ax2 = [Prop_Teach_Debug_N_and_Taught_Intro_Y, Prop_Teach_Debug_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q7a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q7\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q7b_det_3a_dist_ax1 = [Prop_Teach_Debug_Y_and_Taught_Intro_Y, Prop_Teach_Debug_Y_and_Taught_Intro_N]\n",
      "Q7b_det_3a_dist_ax2 = [Prop_Teach_Debug_N_and_Taught_Intro_Y, Prop_Teach_Debug_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q7b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q7b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q7b_det_3a_dist_ax1 = [Prop_Teach_Ver_Y_and_Taught_Intro_Y, Prop_Teach_Ver_Y_and_Taught_Intro_N]\n",
      "Q7b_det_3a_dist_ax2 = [Prop_Teach_Ver_N_and_Taught_Intro_Y, Prop_Teach_Ver_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q7b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q7\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q7c_det_3a_dist_ax1 = [Prop_Teach_Ver_Y_and_Taught_Intro_Y, Prop_Teach_Ver_Y_and_Taught_Intro_N]\n",
      "Q7c_det_3a_dist_ax2 = [Prop_Teach_Ver_N_and_Taught_Intro_Y, Prop_Teach_Ver_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q7c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q7c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q9_det_3a_dist_ax1 = [Prop_Matlab_Lcns_Y_and_Taught_Intro_Y, Prop_Matlab_Lcns_Y_and_Taught_Intro_N]\n",
      "Q9_det_3a_dist_ax2 = [Prop_Matlab_Lcns_N_and_Taught_Intro_Y, Prop_Matlab_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q9_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(t\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q11a_det_3a_dist_ax1 = [Prop_Matlab_Lcns_Y_and_Taught_Intro_Y, Prop_Matlab_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11a_det_3a_dist_ax2 = [Prop_Matlab_Lcns_N_and_Taught_Intro_Y, Prop_Matlab_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "Q11a_det_3a_dist_ax1 = [Prop_Mtmtca_Lcns_Y_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11a_det_3a_dist_ax2 = [Prop_Mtmtca_Lcns_N_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q11b_det_3a_dist_ax1 = [Prop_Mtmtca_Lcns_Y_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11b_det_3a_dist_ax2 = [Prop_Mtmtca_Lcns_N_and_Taught_Intro_Y, Prop_Mtmtca_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "Q11b_det_3a_dist_ax1 = [Prop_Maple_Lcns_Y_and_Taught_Intro_Y, Prop_Maple_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11b_det_3a_dist_ax2 = [Prop_Maple_Lcns_N_and_Taught_Intro_Y, Prop_Maple_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q11c_det_3a_dist_ax1 = [Prop_Maple_Lcns_Y_and_Taught_Intro_Y, Prop_Maple_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11c_det_3a_dist_ax2 = [Prop_Maple_Lcns_N_and_Taught_Intro_Y, Prop_Maple_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "plt.savefig(jupyter_string)\n",
      "=====\n",
      "houses = merged.pivot(index=jupyter_string, columns='type' <<unk>>, values=jupyter_string)\n",
      "houses.to_csv(jupyter_string)\n",
      "--------------------\n",
      "Q11c_det_3a_dist_ax1 = [Prop_Oth_Lcns_Y_and_Taught_Intro_Y, Prop_Oth_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11c_det_3a_dist_ax2 = [Prop_Oth_Lcns_N_and_Taught_Intro_Y, Prop_Oth_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q11d_det_3a_dist_ax1 = [Prop_Oth_Lcns_Y_and_Taught_Intro_Y, Prop_Oth_Lcns_Y_and_Taught_Intro_N]\n",
      "Q11d_det_3a_dist_ax2 = [Prop_Oth_Lcns_N_and_Taught_Intro_Y, Prop_Oth_Lcns_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q11d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q11d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20,10))\n",
      "ticks4 = [1,2,3,4]\n",
      "xticklabelsagreestr=[jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "ax1.bar(ticks4,Fact_Rel_Time_Y_and_Taught_Intro_Y,width=.25, align=jupyter_string)\n",
      "ax2.bar(ticks4,Fact_Rel_Time_Y_and_Taught_Intro_N,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks4)\n",
      "ax1.set_xticklabels(xticklabelsagreestr, rotation=90)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14a_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Y_and_Taught_Intro_N]\n",
      "Q14a_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14a_det_3a_dist_ax1 = [Prop_Fact_Stip_Y_and_Taught_Intro_Y, Prop_Fact_Stip_Y_and_Taught_Intro_N]\n",
      "Q14a_det_3a_dist_ax2 = [Prop_Fact_Stip_N_and_Taught_Intro_Y, Prop_Fact_Stip_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14b_det_3a_dist_ax1 = [Prop_Fact_Stip_Y_and_Taught_Intro_Y, Prop_Fact_Stip_Y_and_Taught_Intro_N]\n",
      "Q14b_det_3a_dist_ax2 = [Prop_Fact_Stip_N_and_Taught_Intro_Y, Prop_Fact_Stip_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14b_det_3a_dist_ax1 = [Prop_TA_Teach_Y_and_Taught_Intro_Y, Prop_TA_Teach_Y_and_Taught_Intro_N]\n",
      "Q14b_det_3a_dist_ax2 = [Prop_TA_Teach_N_and_Taught_Intro_Y, Prop_TA_Teach_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14c_det_3a_dist_ax1 = [Prop_TA_Teach_Y_and_Taught_Intro_Y, Prop_TA_Teach_Y_and_Taught_Intro_N]\n",
      "Q14c_det_3a_dist_ax2 = [Prop_TA_Teach_N_and_Taught_Intro_Y, Prop_TA_Teach_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14c_det_3a_dist_ax1 = [Prop_Fact_Disc_Y_and_Taught_Intro_Y, Prop_Fact_Disc_Y_and_Taught_Intro_N]\n",
      "Q14c_det_3a_dist_ax2 = [Prop_Fact_Disc_N_and_Taught_Intro_Y, Prop_Fact_Disc_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14c_det_\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14d_det_3a_dist_ax1 = [Prop_Fact_Disc_Y_and_Taught_Intro_Y, Prop_Fact_Disc_Y_and_Taught_Intro_N]\n",
      "Q14d_det_3a_dist_ax2 = [Prop_Fact_Disc_N_and_Taught_Intro_Y, Prop_Fact_Disc_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q14d_det_3a_dist_ax1 = [Prop_Dept_Apathetic_Y_and_Taught_Intro_Y, Prop_Dept_Apathetic_Y_and_Taught_Intro_N]\n",
      "Q14d_det_3a_dist_ax2 = [Prop_Dept_Apathetic_N_and_Taught_Intro_Y, Prop_Dept_Apathetic_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14e_det_3a_dist_ax1 = [Prop_Dept_Apathetic_Y_and_Taught_Intro_Y, Prop_Dept_Apathetic_Y_and_Taught_Intro_N]\n",
      "Q14e_det_3a_dist_ax2 = [Prop_Dept_Apathetic_N_and_Taught_Intro_Y, Prop_Dept_Apathetic_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q14e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q14e_det_3a_dist_ax1 = [Prop_Fact_Access_Lab_Y_and_Taught_Intro_Y, Prop_Fact_Access_Lab_Y_and_Taught_Intro_N]\n",
      "Q14e_det_3a_dist_ax2 = [Prop_Fact_Access_Lab_N_and_Taught_Intro_Y, Prop_Fact_Access_Lab_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q14e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15a_det_3a_dist_ax1 = [Prop_Fact_Access_Lab_Y_and_Taught_Intro_Y, Prop_Fact_Access_Lab_Y_and_Taught_Intro_N]\n",
      "Q15a_det_3a_dist_ax2 = [Prop_Fact_Access_Lab_N_and_Taught_Intro_Y, Prop_Fact_Access_Lab_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "orange_probability = data[data['orange' <unk>] == 1].shape[0] / data.shape[0]\n",
      "stripes_probability = data[data['stripes' <unk>] > 1].shape[0] / data.shape[0]\n",
      "=====\n",
      "total_countries = flags.shape[0]\n",
      "orange_probability = float(flags[flags[jupyter_string] == 1].shape[0]) / total_countries\n",
      "print(jupyter_string, orange_probability)\n",
      "\n",
      "stripe_probability = float(flags[flags[jupyter_string] > 1].shape[0]) / total_countries\n",
      "print(jupyter_string, stripe_probability)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15a_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_N]\n",
      "Q15a_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15b_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_Y_and_Taught_Intro_N]\n",
      "Q15b_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Soft_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_Y, Prop_Fact_Access_Spec_Hard_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q15c_det_3a_dist_ax1 = [Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_N]\n",
      "Q15c_det_3a_dist_ax2 = [Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15d_det_3a_dist_ax1 = [Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_Y_and_Taught_Intro_N]\n",
      "Q15d_det_3a_dist_ax2 = [Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_Y, Prop_Fact_Adtnl_Funds_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15e_det_3a_dist_ax1 = [Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_Y_and_Taught_Intro_N]\n",
      "Q15e_det_3a_dist_ax2 = [Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_Y, Prop_Fact_Rel_Time_Learn_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15f_det_3a_dist_ax1 = [Prop_Fact_Train_Intg_Comp_Y_and_Taught_Intro_Y, Prop_Fact_Train_Intg_Comp_Y_and_Taught_Intro_N]\n",
      "Q15f_det_3a_dist_ax2 = [Prop_Fact_Train_Intg_Comp_N_and_Taught_Intro_Y, Prop_Fact_Train_Intg_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15f_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q15f_det_3a_dist_ax1 = [Prop_Fact_Credit_Promo_Y_and_Taught_Intro_Y, Prop_Fact_Credit_Promo_Y_and_Taught_Intro_N]\n",
      "Q15f_det_3a_dist_ax2 = [Prop_Fact_Credit_Promo_N_and_Taught_Intro_Y, Prop_Fact_Credit_Promo_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15f_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15g_det_3a_dist_ax1 = [Prop_Fact_Credit_Promo_Y_and_Taught_Intro_Y, Prop_Fact_Credit_Promo_Y_and_Taught_Intro_N]\n",
      "Q15g_det_3a_dist_ax2 = [Prop_Fact_Credit_Promo_N_and_Taught_Intro_Y, Prop_Fact_Credit_Promo_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15g_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15g_det_3a_dist_ax1 = [Prop_Fact_Stip_Y_and_Taught_Intro_Y, Prop_Fact_Stip_Y_and_Taught_Intro_N]\n",
      "Q15g_det_3a_dist_ax2 = [Prop_Fact_Stip_N_and_Taught_Intro_Y, Prop_Fact_Stip_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15h_det_3a_dist_ax1 = [Prop_Fact_Stip_Y_and_Taught_Intro_Y, Prop_Fact_Stip_Y_and_Taught_Intro_N]\n",
      "Q15h_det_3a_dist_ax2 = [Prop_Fact_Stip_N_and_Taught_Intro_Y, Prop_Fact_Stip_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15h_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15h_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q15h_det_3a_dist_ax1 = [Prop_Rel_Time_Dev_Y_and_Taught_Intro_Y, Prop_Rel_Time_Dev_Y_and_Taught_Intro_N]\n",
      "Q15h_det_3a_dist_ax2 = [Prop_Rel_Time_Dev_N_and_Taught_Intro_Y, Prop_Rel_Time_Dev_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15h_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15h_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax2\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15i_det_3a_dist_ax1 = [Prop_Rel_Time_Dev_Y_and_Taught_Intro_Y, Prop_Rel_Time_Dev_Y_and_Taught_Intro_N]\n",
      "Q15i_det_3a_dist_ax2 = [Prop_Rel_Time_Dev_N_and_Taught_Intro_Y, Prop_Rel_Time_Dev_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15i_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15i_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15i_det_3a_dist_ax1 = [Prop_Other_Y_and_Taught_Intro_Y, Prop_Other_Y_and_Taught_Intro_N]\n",
      "Q15i_det_3a_dist_ax2 = [Prop_Other_N_and_Taught_Intro_Y, Prop_Other_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15i_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15i_det_3a_dist_ax1,\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15j_det_3a_dist_ax1 = [Prop_Other_Y_and_Taught_Intro_Y, Prop_Other_Y_and_Taught_Intro_N]\n",
      "Q15j_det_3a_dist_ax2 = [Prop_Other_N_and_Taught_Intro_Y, Prop_Other_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15j_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15j_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15j_det_3a_dist_ax1 = [Prop_None_Above_Y_and_Taught_Intro_Y, Prop_None_Above_Y_and_Taught_Intro_N]\n",
      "Q15j_det_3a_dist_ax2 = [Prop_None_Above_N_and_Taught_Intro_Y, Prop_None_Above_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15j_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15j_det_\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15k_det_3a_dist_ax1 = [Prop_None_Above_Y_and_Taught_Intro_Y, Prop_None_Above_Y_and_Taught_Intro_N]\n",
      "Q15k_det_3a_dist_ax2 = [Prop_None_Above_N_and_Taught_Intro_Y, Prop_None_Above_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15k_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15k_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "total_count = flags.shape[0]\n",
      "red_count = flags[flags[jupyter_string] == 1].shape[0]\n",
      "\n",
      "one_red = (float(red_count) / total_count) \n",
      "print(jupyter_string, one_red)\n",
      "\n",
      "two_red = one_red * (float(red_count - 1) / (total_count - 1))\n",
      "print(jupyter_string, two_red)\n",
      "\n",
      "three_red = two_red * (float(red_count - 2) / (total_count - 2))\n",
      "print(jupyter_string, three_red)\n",
      "--------------------\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q15k_comp_lab_ax1 = [Prop_Stu_Comp_Lab_Y_and_Taught_Intro_Y, Prop_Stu_Comp_Lab_Y_and_Taught_Intro_N]\n",
      "Q15k_comp_lab_ax2 = [Prop_Stu_Comp_Lab_N_and_Taught_Intro_Y, Prop_Stu_Comp_Lab_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q15k_comp_lab_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q15k\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16a_det_3a_dist_ax1 = [Prop_Stu_Comp_Lab_Y_and_Taught_Intro_Y, Prop_Stu_Comp_Lab_Y_and_Taught_Intro_N]\n",
      "Q16a_det_3a_dist_ax2 = [Prop_Stu_Comp_Lab_N_and_Taught_Intro_Y, Prop_Stu_Comp_Lab_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Q16a_det_3a_dist_ax1 = [Prop_Stu_Prov_PC_Y_and_Taught_Intro_Y, Prop_Stu_Prov_PC_Y_and_Taught_Intro_N]\n",
      "Q16a_det_3a_dist_ax2 = [Prop_Stu_Prov_PC_N_and_Taught_Intro_Y, Prop_Stu_Prov_PC_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16b_det_3a_dist_ax1 = [Prop_Stu_Prov_PC_Y_and_Taught_Intro_Y, Prop_Stu_Prov_PC_Y_and_Taught_Intro_N]\n",
      "Q16b_det_3a_dist_ax2 = [Prop_Stu_Prov_PC_N_and_Taught_Intro_Y, Prop_Stu_Prov_PC_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16b_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16b_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Stu_Prov_PC_Y_and_Taught_Intro_Y=0\n",
      "Stu_Prov_PC_Y_and_Taught_Intro_N=0\n",
      "Stu_Prov_PC_Y_and_Taught_Intro_Y=0\n",
      "Stu_Prov_PC_N_and_Taught_Intro_Y=0\n",
      "Stu_Prov_PC_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Stu_Prov_PC_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Stu_Prov_PC_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Stu_Prov_PC_Y_and_Taught_Intro_N+=\n",
      "=====\n",
      "Adq_Comp_Pwr_YN = []\n",
      "\n",
      "for row in data.values[:,84]:\n",
      "    Adq_Comp_Pwr_YN.append(row)\n",
      "Adq_Comp_Pwr_Y_and_Taught_Intro_Y=0\n",
      "Adq_Comp_Pwr_Y_and_Taught_Intro_N=0\n",
      "Adq_Comp_Pwr_N_and_Taught_Intro_Y=0\n",
      "Adq_Comp_Pwr_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Adq_Comp_Pwr_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Adq_Comp_Pwr_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Adq_Comp_Pwr_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Adq_Comp_Pwr_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Adq_Comp_Pwr_N_and_Taught_Intro_N+=1\n",
      "Prop_Adq_Comp_Pwr_Y_and_Taught_Intro_Y = Adq_Comp_Pwr_Y_and_Taught_Intro_Y/(Adq_Comp_Pwr_Y_and_Taught_Intro_Y+Adq_Comp_Pwr_Y_and_Taught_Intro_N)  \n",
      "Prop_Adq_Comp_Pwr_Y_and_Taught_Intro_N = Adq_Comp_Pwr_Y_and_Taught_Intro_N/(Adq_Comp_Pwr_Y_and_Taught_Intro_Y+Adq_Comp_Pwr_Y_and_Taught_Intro_N)  \n",
      "Prop_Adq_Comp_Pwr_N_and_Taught_Intro_Y = Adq_Comp_Pwr_N_and_Taught_Intro_Y/(Adq_Comp_Pwr_N_and_Taught_Intro_Y+Adq_Comp_Pwr_N_and_Taught_Intro_N)  \n",
      "Prop_Adq_Comp_Pwr_N_and_Taught_Intro_N = Adq_Comp_Pwr_N_and_Taught_Intro_N/(Adq_Comp_Pwr_N_and_Taught_Intro_Y+Adq_Comp_Pwr_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16c_det_3a_dist_ax1 = [Prop_Adq_Comp_Pwr_Y_and_Taught_Intro_Y, Prop_Adq_Comp_Pwr_Y_and_Taught_Intro_N]\n",
      "Q16c_det_3a_dist_ax2 = [Prop_Adq_Comp_Pwr_N_and_Taught_Intro_Y, Prop_Adq_Comp_Pwr_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16c_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16c_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Adq_Comp_Pwr_YN = []\n",
      "\n",
      "for row in data.values[:,84]:\n",
      "    Adq_Comp_Pwr_YN.append(row)\n",
      "Adq_Comp_Pwr_Y_and_Taught_Intro_Y=0\n",
      "Adq_Comp_Pwr_Y_and_Taught_Intro_N=0\n",
      "Adq_Comp_Pwr_N_and_Taught_Intro_Y=0\n",
      "Adq_Comp_Pwr_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Adq_Comp_Pwr_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Adq_Comp_Pwr_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j\n",
      "=====\n",
      "Acc_Reas_Fund_YN = []\n",
      "\n",
      "for row in data.values[:,85]:\n",
      "    Acc_Reas_Fund_YN.append(row)\n",
      "Acc_Reas_Fund_Y_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Fund_Y_and_Taught_Intro_N=0\n",
      "Acc_Reas_Fund_N_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Fund_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Acc_Reas_Fund_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Fund_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Fund_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Fund_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Fund_N_and_Taught_Intro_N+=1\n",
      "Prop_Acc_Reas_Fund_Y_and_Taught_Intro_Y = Acc_Reas_Fund_Y_and_Taught_Intro_Y/(Acc_Reas_Fund_Y_and_Taught_Intro_Y+Acc_Reas_Fund_Y_and_Taught_Intro_N)  \n",
      "Prop_Acc_Reas_Fund_Y_and_Taught_Intro_N = Acc_Reas_Fund_Y_and_Taught_Intro_N/(Acc_Reas_Fund_Y_and_Taught_Intro_Y+Acc_Reas_Fund_Y_and_Taught_Intro_N)  \n",
      "Prop_Acc_Reas_Fund_N_and_Taught_Intro_Y = Acc_Reas_Fund_N_and_Taught_Intro_Y/(Acc_Reas_Fund_N_and_Taught_Intro_Y+Acc_Reas_Fund_N_and_Taught_Intro_N)  \n",
      "Prop_Acc_Reas_Fund_N_and_Taught_Intro_N = Acc_Reas_Fund_N_and_Taught_Intro_N/(Acc_Reas_Fund_N_and_Taught_Intro_Y+Acc_Reas_Fund_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16d_det_3a_dist_ax1 = [Prop_Acc_Reas_Fund_Y_and_Taught_Intro_Y, Prop_Acc_Reas_Fund_Y_and_Taught_Intro_N]\n",
      "Q16d_det_3a_dist_ax2 = [Prop_Acc_Reas_Fund_N_and_Taught_Intro_Y, Prop_Acc_Reas_Fund_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16d_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16d_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Acc_Reas_Experience_YN = []\n",
      "\n",
      "for row in data.values[:,86]:\n",
      "    Acc_Reas_Experience_YN.append(row)\n",
      "Acc_Reas_Experience_Y_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Experience_Y_and_Taught_Intro_N=0\n",
      "Acc_Reas_Experience_N_and_Taught_Intro_Y=0\n",
      "Acc_Reas_Experience_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Acc_Reas_Experience_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_Reas_Experience_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Acc_\n",
      "=====\n",
      "Reas_Exp_Opp_YN = []\n",
      "\n",
      "for row in data.values[:,86]:\n",
      "    Reas_Exp_Opp_YN.append(row)\n",
      "Reas_Exp_Opp_Y_and_Taught_Intro_Y=0\n",
      "Reas_Exp_Opp_Y_and_Taught_Intro_N=0\n",
      "Reas_Exp_Opp_N_and_Taught_Intro_Y=0\n",
      "Reas_Exp_Opp_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Reas_Exp_Opp_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Reas_Exp_Opp_N_and_Taught_Intro_N+=1\n",
      "Prop_Reas_Exp_Opp_Y_and_Taught_Intro_Y = Reas_Exp_Opp_Y_and_Taught_Intro_Y/(Reas_Exp_Opp_Y_and_Taught_Intro_Y+Reas_Exp_Opp_Y_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_Y_and_Taught_Intro_N = Reas_Exp_Opp_Y_and_Taught_Intro_N/(Reas_Exp_Opp_Y_and_Taught_Intro_Y+Reas_Exp_Opp_Y_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_N_and_Taught_Intro_Y = Reas_Exp_Opp_N_and_Taught_Intro_Y/(Reas_Exp_Opp_N_and_Taught_Intro_Y+Reas_Exp_Opp_N_and_Taught_Intro_N)  \n",
      "Prop_Reas_Exp_Opp_N_and_Taught_Intro_N = Reas_Exp_Opp_N_and_Taught_Intro_N/(Reas_Exp_Opp_N_and_Taught_Intro_Y+Reas_Exp_Opp_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16e_det_3a_dist_ax1 = [Prop_Reas_Exp_Opp_Y_and_Taught_Intro_Y, Prop_Reas_Exp_Opp_Y_and_Taught_Intro_N]\n",
      "Q16e_det_3a_dist_ax2 = [Prop_Reas_Exp_Opp_N_and_Taught_Intro_Y, Prop_Reas_Exp_Opp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16e_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16e_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Taught_in_Intro_YN = []\n",
      "\n",
      "for row in data.values[:,86]:\n",
      "    Taught_in_Intro_YN.append(row)\n",
      "Taught_in_Intro_Y_and_Taught_Intro_Y=0\n",
      "Taught_in_Intro_Y_and_Taught_Intro_N=0\n",
      "Taught_in_Intro_N_and_Taught_Intro_Y=0\n",
      "Taught_in_Intro_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Taught_in_Intro_YN, Taught_in_Intro_Y_and_Taught_Intro_Y):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Taught_in_Intro_Y_and_Taught_Intro_Y+=\n",
      "=====\n",
      "Ind_Stud_Comp_YN = []\n",
      "\n",
      "for row in data.values[:,87]:\n",
      "    Ind_Stud_Comp_YN.append(row)\n",
      "Ind_Stud_Comp_Y_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Comp_Y_and_Taught_Intro_N=0\n",
      "Ind_Stud_Comp_N_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Comp_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Ind_Stud_Comp_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Comp_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Comp_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Comp_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Comp_N_and_Taught_Intro_N+=1\n",
      "Prop_Ind_Stud_Comp_Y_and_Taught_Intro_Y = Ind_Stud_Comp_Y_and_Taught_Intro_Y/(Ind_Stud_Comp_Y_and_Taught_Intro_Y+Ind_Stud_Comp_Y_and_Taught_Intro_N)  \n",
      "Prop_Ind_Stud_Comp_Y_and_Taught_Intro_N = Ind_Stud_Comp_Y_and_Taught_Intro_N/(Ind_Stud_Comp_Y_and_Taught_Intro_Y+Ind_Stud_Comp_Y_and_Taught_Intro_N)  \n",
      "Prop_Ind_Stud_Comp_N_and_Taught_Intro_Y = Ind_Stud_Comp_N_and_Taught_Intro_Y/(Ind_Stud_Comp_N_and_Taught_Intro_Y+Ind_Stud_Comp_N_and_Taught_Intro_N)  \n",
      "Prop_Ind_Stud_Comp_N_and_Taught_Intro_N = Ind_Stud_Comp_N_and_Taught_Intro_N/(Ind_Stud_Comp_N_and_Taught_Intro_Y+Ind_Stud_Comp_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16f_det_3a_dist_ax1 = [Prop_Ind_Stud_Comp_Y_and_Taught_Intro_Y, Prop_Ind_Stud_Comp_Y_and_Taught_Intro_N]\n",
      "Q16f_det_3a_dist_ax2 = [Prop_Ind_Stud_Comp_N_and_Taught_Intro_Y, Prop_Ind_Stud_Comp_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16f_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16f_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Ind_Stud_Support_YN = []\n",
      "\n",
      "for row in data.values[:,88]:\n",
      "    Ind_Stud_Support_YN.append(row)\n",
      "Ind_Stud_Support_Y_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Support_Y_and_Taught_Intro_N=0\n",
      "Ind_Stud_Support_N_and_Taught_Intro_Y=0\n",
      "Ind_Stud_Support_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Ind_Stud_Support_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Support_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Ind_Stud_Support_Y_and_\n",
      "=====\n",
      "Other_YN = []\n",
      "\n",
      "for row in data.values[:,88]:\n",
      "    Other_YN.append(row)\n",
      "Other_Y_and_Taught_Intro_Y=0\n",
      "Other_Y_and_Taught_Intro_N=0\n",
      "Other_N_and_Taught_Intro_Y=0\n",
      "Other_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Other_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_N+=1\n",
      "Prop_Other_Y_and_Taught_Intro_Y = Other_Y_and_Taught_Intro_Y/(Other_Y_and_Taught_Intro_Y+Other_Y_and_Taught_Intro_N)  \n",
      "Prop_Other_Y_and_Taught_Intro_N = Other_Y_and_Taught_Intro_N/(Other_Y_and_Taught_Intro_Y+Other_Y_and_Taught_Intro_N)  \n",
      "Prop_Other_N_and_Taught_Intro_Y = Other_N_and_Taught_Intro_Y/(Other_N_and_Taught_Intro_Y+Other_N_and_Taught_Intro_N)  \n",
      "Prop_Other_N_and_Taught_Intro_N = Other_N_and_Taught_Intro_N/(Other_N_and_Taught_Intro_Y+Other_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16g_det_3a_dist_ax1 = [Prop_Other_Y_and_Taught_Intro_Y, Prop_Other_Y_and_Taught_Intro_N]\n",
      "Q16g_det_3a_dist_ax2 = [Prop_Other_N_and_Taught_Intro_Y, Prop_Other_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16g_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16g_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "Other_Y_and_Taught_Intro_Y=0\n",
      "Other_Y_and_Taught_Intro_N=0\n",
      "Other_N_and_Taught_Intro_Y=0\n",
      "Other_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Other_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Other_N_and_Taught_Int\n",
      "=====\n",
      "None_Above_YN = []\n",
      "\n",
      "for row in data.values[:,89]:\n",
      "    None_Above_YN.append(row)\n",
      "None_Above_Y_and_Taught_Intro_Y=0\n",
      "None_Above_Y_and_Taught_Intro_N=0\n",
      "None_Above_N_and_Taught_Intro_Y=0\n",
      "None_Above_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(None_Above_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        None_Above_N_and_Taught_Intro_N+=1\n",
      "Prop_None_Above_Y_and_Taught_Intro_Y = None_Above_Y_and_Taught_Intro_Y/(None_Above_Y_and_Taught_Intro_Y+None_Above_Y_and_Taught_Intro_N)  \n",
      "Prop_None_Above_Y_and_Taught_Intro_N = None_Above_Y_and_Taught_Intro_N/(None_Above_Y_and_Taught_Intro_Y+None_Above_Y_and_Taught_Intro_N)  \n",
      "Prop_None_Above_N_and_Taught_Intro_Y = None_Above_N_and_Taught_Intro_Y/(None_Above_N_and_Taught_Intro_Y+None_Above_N_and_Taught_Intro_N)  \n",
      "Prop_None_Above_N_and_Taught_Intro_N = None_Above_N_and_Taught_Intro_N/(None_Above_N_and_Taught_Intro_Y+None_Above_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q16j_det_3a_dist_ax1 = [Prop_None_Above_Y_and_Taught_Intro_Y, Prop_None_Above_Y_and_Taught_Intro_N]\n",
      "Q16j_det_3a_dist_ax2 = [Prop_None_Above_N_and_Taught_Intro_Y, Prop_None_Above_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q16j_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q16j_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(Taught_in_Intro_YN, bins=20)\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(None_Above_YN, bins=20)\n",
      "plt.show()\n",
      "=====\n",
      "Teach_Mot_Reas_Int_YN = []\n",
      "\n",
      "for row in data.values[:,91]:\n",
      "    Teach_Mot_Reas_Int_YN.append(row)\n",
      "Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y=0\n",
      "Teach_Mot_Reas_Int_Y_and_Taught_Intro_N=0\n",
      "Teach_Mot_Reas_Int_N_and_Taught_Intro_Y=0\n",
      "Teach_Mot_Reas_Int_N_and_Taught_Intro_N=0\n",
      "for i,j in zip(Teach_Mot_Reas_Int_YN, Taught_in_Intro_YN):\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_Y_and_Taught_Intro_N+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+=1\n",
      "    if i == jupyter_string and j == jupyter_string:\n",
      "        Teach_Mot_Reas_Int_N_and_Taught_Intro_N+=1\n",
      "Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y = Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y/(Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+Teach_Mot_Reas_Int_Y_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_N = Teach_Mot_Reas_Int_Y_and_Taught_Intro_N/(Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y+Teach_Mot_Reas_Int_Y_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_Y = Teach_Mot_Reas_Int_N_and_Taught_Intro_Y/(Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+Teach_Mot_Reas_Int_N_and_Taught_Intro_N)  \n",
      "Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_N = Teach_Mot_Reas_Int_N_and_Taught_Intro_N/(Teach_Mot_Reas_Int_N_and_Taught_Intro_Y+Teach_Mot_Reas_Int_N_and_Taught_Intro_N)  \n",
      "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "ticks2 = [1,2]\n",
      "xticklabels=[jupyter_string, jupyter_string]\n",
      "Q17a_det_3a_dist_ax1 = [Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_Y, Prop_Teach_Mot_Reas_Int_Y_and_Taught_Intro_N]\n",
      "Q17a_det_3a_dist_ax2 = [Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_Y, Prop_Teach_Mot_Reas_Int_N_and_Taught_Intro_N]\n",
      "ax2.bar(ticks2,Q17a_det_3a_dist_ax2,width=.25, align=jupyter_string)\n",
      "ax1.bar(ticks2,Q17a_det_3a_dist_ax1,width=.25, align=jupyter_string)\n",
      "ax1.set_title(jupyter_string)\n",
      "ax2.set_title(jupyter_string)\n",
      "ax1.set_xticks(ticks2)\n",
      "ax1.set_xticklabels(xticklabels, rotation=90)\n",
      "ax2.set_xticks(ticks2)\n",
      "ax2.set_xticklabels(xticklabels, rotation=90)\n",
      "plt.show()\n",
      "--------------------\n",
      "most_bars_country = flags[flags['bars' <unk>] == flags['bars' <unk>].max()]['name' <unk>]\n",
      "highest_population_country = flags[flags['population' <unk>] == flags['population' <unk>].max()]['name' <unk>]\n",
      "=====\n",
      "import pandas\n",
      "\n",
      "flags = pandas.read_csv(jupyter_string)\n",
      "flags.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "                 jupyter_string, jupyter_string, 'green' <<unk>>, jupyter_string, jupyter_string, jupyter_string, 'black' <<unk>>, jupyter_string, jupyter_string, jupyter_string, \n",
      "                 jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, \n",
      "                 jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "print(jupyter_string, flags[:5])\n",
      "\n",
      "bars_sorted = flags.sort(jupyter_string, ascending=[0])\n",
      "most_bars_country = bars_sorted[jupyter_string].iloc[0]\n",
      "\n",
      "population_sorted = flags.sort(jupyter_string, ascending=[0])\n",
      "highest_population_country = population_sorted[jupyter_string].iloc[0]\n",
      "print(jupyter_string, highest_population_country)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "animals = pd.read_csv(jupyter_string, index_col='AnimalID' <<unk>>)\n",
      "animals.head(3)\n",
      "--------------------\n",
      "X = animals[[jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]]\n",
      "y = animals[jupyter_string]\n",
      "=====\n",
      "columns = ['AnimalType' <<unk>>, 'Sex' <<unk>>, 'IsNamed' <<unk>>, 'IsMixed' <<unk>>, 'IsIntact' <<unk>>, 'AgeCategory' <<unk>>, 'ColorCategory' <<unk>>, 'OutcomeType' <<unk>>]\n",
      "animals = pd.DataFrame(animals, columns=columns)\n",
      "animals.head()\n",
      "--------------------\n",
      "dogs = dogs.sample(frac=1).reset_index(drop=True)\n",
      "dogs.head()\n",
      "=====\n",
      "animals = pd.get_dummies(animals, columns=['Sex' <<unk>>, 'AgeCategory' <<unk>>, 'ColorCategory' <<unk>>])\n",
      "animals.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "vote = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "vote.head(5)\n",
      "--------------------\n",
      "X = animals.drop('OutcomeType' <unk>, axis=1)\n",
      "y = animals['OutcomeType' <unk>]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
      "=====\n",
      "dogs = animals.loc[animals['AnimalType' <<unk>>] == jupyter_string].drop(['AnimalType' <<unk>>], axis=1)\n",
      "cats = animals.loc[animals['AnimalType' <<unk>>] == jupyter_string].drop(['AnimalType' <<unk>>], axis=1)\n",
      "--------------------\n",
      "dogs.head()\n",
      "=====\n",
      "X_dogs = dogs.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "=====\n",
      "clf_RF_dogs = RandomForestClassifier()\n",
      "clf_RF_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_RF_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_RF_dogs.score(X_test_dogs, y_test_dogs))\n",
      "--------------------\n",
      "clf_RF_dogs = RandomForestClassifier()\n",
      "clf_RF_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_RF_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_RF_dogs.score(X_test_dogs, y_test_dogs))\n",
      "=====\n",
      "clf_RF_cats = RandomForestClassifier()\n",
      "clf_RF_cats.fit(X_train_cats, y_train_cats)\n",
      "print(clf_RF_cats.score(X_train_cats, y_train_cats))\n",
      "print(clf_RF_cats.score(X_test_cats, y_test_cats))\n",
      "--------------------\n",
      "print(pd.Series(clf_RF_dogs.feature_importances_,\n",
      "                index=X_dogs.columns).sort_values(ascending=False))\n",
      "=====\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
      "pd.Series(clf_RF_dogs.feature_importances_,\n",
      "                index=X_dogs.columns).sort_values(ascending=False).plot(ax=ax1, title=jupyter_string)\n",
      "pd.Series(clf_RF_cats.feature_importances_,\n",
      "                index=X_cats.columns).sort_values(ascending=False).plot(ax=ax2, title=jupyter_string)\n",
      "--------------------\n",
      "X_dogs = X_dogs.drop(['IsIntact' <unk>, 'IsNamed' <unk>, 'AgeCategory_baby' <unk>], axis=1)\n",
      "X_dogs.head()\n",
      "=====\n",
      "dogs_fi = dogs.drop([jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string ], axis=1)\n",
      "cats_fi = cats.drop([jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string], axis=1)\n",
      "--------------------\n",
      "dogs_fi.head()\n",
      "=====\n",
      "X_dogs = dogs_fi.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs_fi.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import classification_report\n",
      "=====\n",
      "X = dogs_train.drop('OutcomeType' <<unk>>, axis=1)\n",
      "y = dogs_train['OutcomeType' <<unk>>]\n",
      "\n",
      "clf_LR_dogs = LogisticRegression().fit(X, y)\n",
      "--------------------\n",
      "y_pred_dogs = clf_LR_dogs.predict(dogs_test)\n",
      "=====\n",
      "dogs_train[jupyter_string] = clf_LR_dogs.predict(X)\n",
      "dogs_train.head()\n",
      "--------------------\n",
      "dogs_train[jupyter_string] = clf_LR_dogs.predict_proba(dogs_train[feature_cols])[:,1]\n",
      "dogs_train.head()\n",
      "=====\n",
      "dogs_train['OutcomeType' <<unk>>].unique()\n",
      "--------------------\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = vote.loc[vote.Party == jupyter_string, jupyter_string] - vote.loc[vote.Party == jupyter_string, jupyter_string]\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = vote.loc[vote.Party == jupyter_string, jupyter_string] - vote.loc[vote.Party == jupyter_string, jupyter_string]\n",
      "=====\n",
      "vote[jupyter_string] = vote.Gen1 / (vote.Gen1 + vote.Gen2)\n",
      "vote[jupyter_string] = vote.Pri1 / (vote.Pri1 + vote.Pri2)\n",
      "vote[jupyter_string] = vote.GenShare - vote.PriShare\n",
      "vote[jupyter_string] = vote.DIME1 - vote.DIME2\n",
      "vote[jupyter_string] = vote.PriOpp / vote.PriTotal * 100\n",
      "vote[jupyter_string] = vote.OppShare * vote.DIMEChange\n",
      "vote.head(5)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "knn.fit(dogs_train[dogs_train.columns[:-1]], dogs_train[dogs_train.columns[-1]])\n",
      "dogs_train[jupyter_string] = knn.predict(dogs_train[dogs_train.columns[:-1]])\n",
      "=====\n",
      "X_dogs = dogs.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_dogs = dogs.OutcomeType\n",
      "\n",
      "X_train_dogs, X_test_dogs, y_train_dogs, y_test_dogs = train_test_split(X_dogs, y_dogs, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "knn_dogs = KNeighborsClassifier(n_neighbors=5)\n",
      "knn_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "knn_dogs.score(X_test_dogs, y_test_dogs)\n",
      "=====\n",
      "X_cats = cats.drop(['OutcomeType' <<unk>>], axis=1)\n",
      "y_cats = cats.OutcomeType\n",
      "\n",
      "X_train_cats, X_test_cats, y_train_cats, y_test_cats = train_test_split(X_cats, y_cats, train_size=0.7, random_state=456784)\n",
      "--------------------\n",
      "y_dogs = dogs.OutcomeType\n",
      "X_dogs = dogs.drop(['OutcomeType' <unk>], axis=1)\n",
      "=====\n",
      "clf_KNN_dogs = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_KNN_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_KNN_dogs.score(X_test_dogs, y_test_dogs))\n",
      "--------------------\n",
      "clf_KNN_dogs = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_dogs.fit(X_train_dogs, y_train_dogs)\n",
      "print(clf_KNN_dogs.score(X_train_dogs, y_train_dogs))\n",
      "print(clf_KNN_dogs.score(X_test_dogs, y_test_dogs))\n",
      "=====\n",
      "clf_KNN_cats = neighbors.KNeighborsClassifier()\n",
      "\n",
      "clf_KNN_cats.fit(X_train_cats, y_train_cats)\n",
      "print(clf_KNN_cats.score(X_train_cats, y_train_cats))\n",
      "print(clf_KNN_cats.score(X_test_cats, y_test_cats))\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "np.mean([2,4,6])\n",
      "--------------------\n",
      "os.getcwd()\n",
      "=====\n",
      "import os\n",
      "\n",
      "\n",
      "os.getcwd()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "mousedata = pd.read_csv(jupyter_string)\n",
      "\n",
      "mousedata\n",
      "--------------------\n",
      "mousedata.head()\n",
      "=====\n",
      "mousedata.head()\n",
      "--------------------\n",
      "mousedata.describe()\n",
      "=====\n",
      "mousedata.describe()\n",
      "--------------------\n",
      "data_m.mean()\n",
      "=====\n",
      "np.mean(data_m)\n",
      "--------------------\n",
      "vote[jupyter_string] = -1*vote.DIMEChange\n",
      "vote.head(5)\n",
      "=====\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] *=  -1\n",
      "vote[[jupyter_string,jupyter_string,jupyter_string,jupyter_string]].head(5)\n",
      "--------------------\n",
      "df = pd.DataFrame({\n",
      "    jupyter_string: [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string\n",
      "=====\n",
      "group_mouse = mousedata.groupby(['Sex' madeupword0002])\n",
      "--------------------\n",
      "group_mouse.mean()\n",
      "=====\n",
      "group_mouse.get_group(jupyter_string)\n",
      "--------------------\n",
      "group_mouse.apply(np.mean)\n",
      "=====\n",
      "mousedata.groupby(['Sex' madeupword0002]).apply(np.mean)\n",
      "--------------------\n",
      "sns.distplot(df['Weight' <unk>])\n",
      "=====\n",
      "sns.distplot(mousedata.Weight)\n",
      "--------------------\n",
      "sns.boxplot(x='Sex' madeupword0002, y='Median' <unk>, hue='Pclass' <unk>, data=df)\n",
      "=====\n",
      "sns.boxplot(x = \"Sex\", y=\"Weight\", data=mousedata)\n",
      "\n",
      "\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "maze_files = os.listdir(jupyter_string)\n",
      "maze_files\n",
      "=====\n",
      "import os\n",
      "\n",
      "path = jupyter_string\n",
      "file_list = os.listdir(path)\n",
      "file_list\n",
      "\n",
      "--------------------\n",
      "mouse_list = pd.concat(mouse_list)\n",
      "\n",
      "mouse_list\n",
      "=====\n",
      "big_mouse_frame = pd.concat(mouse_list)\n",
      "\n",
      "big_mouse_frame\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "point_stats = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "point_stats.head()\n",
      "=====\n",
      "point_stats.head()\n",
      "--------------------\n",
      "point_stats.info()\n",
      "=====\n",
      "point_stats.tail()\n",
      "--------------------\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = jupyter_string\n",
      "vote.loc[vote.Party == jupyter_string, jupyter_string] = jupyter_string\n",
      "vote.head(5)\n",
      "=====\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "--------------------\n",
      "point_stats.Name.unique()\n",
      "=====\n",
      "point_stats.Name.unique()\n",
      "--------------------\n",
      "point_stats[point_stats[\"ID_Object\"]==0].head()\n",
      "=====\n",
      "point_stats[point_stats[\"Name\"]==jupyter_string].head(20)\n",
      "\n",
      "point_stats[point_stats[\"ID_StatisticsType\"]==237].head(20)\n",
      "--------------------\n",
      "point_stats_matrix = point_stats.pivot(index=\"Name\", columns=\"ID_StatisticsType\", values=\"Value\")\n",
      "point_stats_matrix.head()\n",
      "=====\n",
      "point_stats_matrix = point_stats.pivot(index='ID_Object' <<unk>>, columns='Name' <<unk>>, values='Value' <<unk>>)\n",
      "point_stats_matrix.head()\n",
      "--------------------\n",
      "point_stats_matrix = point_stats_matrix.drop(-1)\n",
      "point_stats_matrix.head()\n",
      "=====\n",
      "point_stats_matrix = point_stats_matrix.drop(-1)\n",
      "point_stats_matrix.head(20)\n",
      "--------------------\n",
      "colnames = [jupyter_string, jupyter_string, jupyter_string]\n",
      "point_stats.loc[:, colnames]\n",
      "=====\n",
      "colnames = [jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "diameter = point_stats_matrix.loc[:,colnames]\n",
      "\n",
      "diameter.head(10)\n",
      "--------------------\n",
      "point_stats_matrix.filter(regex=jupyter_string, axis=1)\n",
      "=====\n",
      "diameter=point_stats_matrix.filter(regex=jupyter_string, axis=1)\n",
      "diameter.head()\n",
      "--------------------\n",
      "sns.distplot(point_stats_matrix['Area' <unk>])\n",
      "=====\n",
      "sns.distplot(point_stats_matrix.Area)\n",
      "--------------------\n",
      "sns.boxplot(point_stats_matrix.Area)\n",
      "=====\n",
      "sns.boxplot(y=jupyter_string, data=point_stats_matrix)\n",
      "--------------------\n",
      "mask = (df['Volume' <unk>] < 0.8)\n",
      "filtered_points = df[mask]\n",
      "=====\n",
      "mask = point_stats_matrix.Volume <= 0.8\n",
      "filtered_points=point_stats_matrix[mask]\n",
      "filtered_points.head()\n",
      "--------------------\n",
      "plt.scatter(filtered_points.Longitude, filtered_points.Latitude)\n",
      "plt.show()\n",
      "=====\n",
      "sns.lmplot(x=jupyter_string, y=jupyter_string, fit_reg=False, data=filtered_points)\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "=====\n",
      "import statsmodels.api as sm\n",
      "model = sm.OLS(vote.loc[vote.Opp == jupyter_string].VoteChange, sm.add_constant(vote.loc[vote.Opp == jupyter_string].DIMEChange)).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "g.map(sns.distplot, 'Weight' <unk>)\n",
      "=====\n",
      "g = sns.FacetGrid(mousedata, col=\"Strain\", row=\"Sex\")\n",
      "\n",
      "g = g.map(sns.distplot, \"Weight\", bins = 30)\n",
      "--------------------\n",
      "point_stats = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "points_value = pd.read_table(jupyter_string, header=None)\n",
      "points_value.head()\n",
      "--------------------\n",
      "points_value.columns = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "points_value.head()\n",
      "=====\n",
      "new_col_names = [\"ID_Time\",\"ID_Object\",\"ID_StatisticsType\",\"Value\"]\n",
      "points_value.columns = new_col_names\n",
      "points_value.head()\n",
      "--------------------\n",
      "points_statistics_type = pd.read_csv(jupyter_string)\n",
      "points_statistics_type.head()\n",
      "=====\n",
      "points_type = pd.read_table(jupyter_string, header=None)\n",
      "\n",
      "new_col_names = [\"ID_StatisticsType\", \"ID_Category\", \"ID_FactorList\", \"Name\", \"Unit\"]\n",
      "points_type.columns = new_col_names\n",
      "\n",
      "points_type.head()\n",
      "--------------------\n",
      "pd.merge(left=points_value, right=points_type, left_on=\"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
      "=====\n",
      "merged_table = pd.merge(left=points_value, right=points_type, \n",
      "         left_on = \"ID_StatisticsType\", right_on=\"ID_StatisticsType\")\n",
      "\n",
      "merged_table.head()\n",
      "--------------------\n",
      "merged_table.to_csv(jupyter_string)\n",
      "=====\n",
      "point_stats_matrix.to_csv(jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "food_info = pd.read_csv(jupyter_string)\n",
      "print(type(food_info))\n",
      "--------------------\n",
      "food_info.head()\n",
      "=====\n",
      "food_info.head()\n",
      "--------------------\n",
      "food_info[[\"Energ_Kcal\", \"Protein\"]].head()\n",
      "=====\n",
      "col_names = food_info.columns.tolist()\n",
      "\n",
      "gram_columns = []\n",
      "for el in col_names:\n",
      "    if el.endswith(jupyter_string):\n",
      "        gram_columns.append(el)\n",
      "print(gram_columns,jupyter_string)\n",
      "\n",
      "gram_df = food_info[gram_columns]\n",
      "print(gram_df)\n",
      "--------------------\n",
      "food_info[jupyter_string] = food_info[\"Carbohydrt_(g)\"] + food_info[\"Water_(g)\"]\n",
      "food_info\n",
      "=====\n",
      "food_info[jupyter_string] = water_energy\n",
      "food_info.head()\n",
      "--------------------\n",
      "s22=sigma**-2; s12=RE11s**-2\n",
      "RE12w=(s22*sum(REsample1.sale_price*REsample1.gross_sq_feet)+s12*RE11w)/(s22*sum(REsample1.gross_sq_feet**2)+s12)\n",
      "RE12s=(s22*sum(REsample1.gross_sq_feet**2)+s12)**(-0.5)\n",
      "print(jupyter_string.format(RE12w,RE12s))\n",
      "=====\n",
      "lm = smf.ols(formula=jupyter_string, data = REsample1).fit()\n",
      "print(lm.summary())\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(111)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "vote.loc[vote.Opp == jupyter_string].plot.scatter(jupyter_string, jupyter_string, color=jupyter_string, ax=ax)\n",
      "model.summary()\n",
      "=====\n",
      "model = sm.OLS(vote.loc[vote.Opp == jupyter_string].VoteChange, sm.add_constant(vote.loc[vote.Opp == jupyter_string].DIMEChange)).fit()\n",
      "model.summary()\n",
      "--------------------\n",
      "food_info = food_info.drop(jupyter_string, axis=1)\n",
      "food_info.head()\n",
      "=====\n",
      "del food_info[jupyter_string]\n",
      "food_info.head()\n",
      "--------------------\n",
      "food_info.sort_values(jupyter_string, ascending=False).head()\n",
      "=====\n",
      "food_info.sort_values(\"Energ_Kcal\", inplace=True, ascending=False)\n",
      "food_info.head()\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "data_df = pd.read_csv(jupyter_string, sep=jupyter_string)\n",
      "--------------------\n",
      "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
      "=====\n",
      "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "=====\n",
      "count_vect = CountVectorizer()\n",
      "X_train_counts = count_vect.fit_transform(X_train)\n",
      "X_test_counts = count_vect.transform(X_test)\n",
      "--------------------\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "nb = MultinomialNB()\n",
      "nb.fit(X_train_counts, y_train)\n",
      "=====\n",
      "gnb = GaussianNB()\n",
      "y_pred = gnb.fit(X_train_counts.toarray(), y_train).predict(X_test_counts.toarray())\n",
      "--------------------\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(y_test, y_pred))\n",
      "=====\n",
      "rocscores = roc_curve(y_test, y_pred)\n",
      "plt.plot(rocscores[0], rocscores[1])\n",
      "plt.plot(rocscores[0],rocscores[0])\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "from collections import Counter\n",
      "\n",
      "\n",
      "pd.set_option(jupyter_string,10)\n",
      "pd.set_option(jupyter_string,20)\n",
      "--------------------\n",
      "rebates = pd.read_csv(jupyter_string)\n",
      "rebates.head()\n",
      "=====\n",
      "rebates = pd.read_csv(jupyter_string)\n",
      "rebates.columns = [c.replace(jupyter_string, jupyter_string) for c in rebates.columns] \n",
      "rebates\n",
      "--------------------\n",
      "rebates[\"County\"] = rebates[\"County\"].str.strip()\n",
      "=====\n",
      "rebates[\"County\"] = rebates[\"County\"].str.strip()\n",
      "Counter(rebates[\"County\"])\n",
      "--------------------\n",
      "vote.loc[(vote.VoteChange > .15) & (vote.DIMEChange < .5), 'Party' <unk>] = jupyter_string\n",
      "vote.loc[(vote.VoteChange < .15) & (vote.DIMEChange > .5), 'Party' <unk>] = jupyter_string\n",
      "vote.loc[(vote.VoteChange > .15) & (vote.DIMEChange < .5), 'Party' <unk>] = jupyter_string\n",
      "vote.loc[(vote.VoteChange < .15) & (vote.DIMEChange > .5), 'Party' <unk>] = jupyter_string\n",
      "vote.loc[(vote.VoteChange < .15) & (vote.DIMEChange < .5), 'Party' <unk>] = jupyter_string\n",
      "vote.loc[(vote.VoteChange < .15) & (vote.DIMEChange > .5), 'Party' <unk>]\n",
      "=====\n",
      "wooutlie = vote.loc[(vote.VoteChange < .15) | (vote.DIMEChange > -.5)]\n",
      "model_nooutlie = sm.OLS(wooutlie.VoteChange, sm.add_constant(wooutlie.DIMEChange)).fit()\n",
      "model_nooutlie.summary()\n",
      "--------------------\n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.strip()\n",
      "Counter(rebates[\"Make\"])\n",
      "=====\n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.title() \n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.strip() \n",
      "rebates[\"Make\"] = rebates[\"Make\"].str.replace(jupyter_string,jupyter_string) \n",
      "Counter(rebates[\"Make\"]).most_common()\n",
      "--------------------\n",
      "rebates[\"Model\"] =rebates[\"Model\"].str.strip()\n",
      "=====\n",
      "rebates[\"Model\"] = rebates[\"Model\"].str.strip() \n",
      "Counter(rebates[\"Model\"]).most_common()\n",
      "--------------------\n",
      "rebates[\"Vehicle Type\"].value_counts()\n",
      "=====\n",
      "Counter(rebates[jupyter_string]).most_common()\n",
      "--------------------\n",
      "rebates[\"City\"].value_counts()\n",
      "=====\n",
      "rebates[\"City\"] = rebates[\"City\"].str.strip() \n",
      "Counter(rebates[\"City\"]).most_common()\n",
      "--------------------\n",
      "rebates[jupyter_string] = pd.to_datetime(rebates[jupyter_string])\n",
      "=====\n",
      "rebates[jupyter_string] = pd.to_datetime(rebates[jupyter_string])\n",
      "rebates[jupyter_string] < jupyter_string\n",
      "--------------------\n",
      "rebates.to_csv(jupyter_string, index=False)\n",
      "=====\n",
      "rebates.to_csv(jupyter_string,index=False)\n",
      "--------------------\n",
      "plt.scatter(data.Survived,data.Fare)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel('Fare' <unk>)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "data.Fare[data.Survived==0].plot(kind=jupyter_string)\n",
      "data.Fare[data.Survived==1].plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel('Fare' <<unk>>)\n",
      "plt.legend((jupyter_string,'Survived' <<unk>>),loc=jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pandas import DataFrame,Series\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "MEDIUM_SIZE = 14\n",
      "BIGGER_SIZE = 16\n",
      "\n",
      "plt.rc(jupyter_string, size=MEDIUM_SIZE)          \n",
      "plt.rc(jupyter_string, titlesize=BIGGER_SIZE)     \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, labelsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, fontsize=MEDIUM_SIZE)    \n",
      "plt.rc(jupyter_string, titlesize=BIGGER_SIZE)  \n",
      "\n",
      "\n",
      "data = pd.read_csv(jupyter_string)\n",
      "print(jupyter_string,data.shape)\n",
      "--------------------\n",
      "data.head()\n",
      "=====\n",
      "data.head()\n",
      "--------------------\n",
      "sns.barplot(x='Sex' madeupword0002,y='Survived' <unk>,hue='Pclass' <unk>,data=titanic_df)\n",
      "=====\n",
      "facet = sns.FacetGrid(data,hue='Survived' <<unk>>,row='Sex' madeupword0002,aspect=4)\n",
      "facet.map(sns.kdeplot,'Age' <<unk>>, shade=True)\n",
      "facet.set(xlim=(0,data['Age' <<unk>>].max()))\n",
      "facet.add_legend()\n",
      "plt.show()\n",
      "--------------------\n",
      "vote = vote[vote.columns[10:]]\n",
      "=====\n",
      "vote2 = vote[vote.columns[10:]]\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.loc[vote2.VoteChange < 0, jupyter_string] *= -1\n",
      "vote2.head(5)\n",
      "--------------------\n",
      "data.info()\n",
      "=====\n",
      "data.info()\n",
      "--------------------\n",
      "data['Age' <unk>] = data['Age' <unk>].fillna(data['Age' <unk>].mean())\n",
      "=====\n",
      "data.loc[(data.Age.isnull()), 'Age' <<unk>>] = data.Age.mean()\n",
      "data['Age' <<unk>>].hasnans\n",
      "--------------------\n",
      "data.describe()\n",
      "=====\n",
      "data.describe()\n",
      "--------------------\n",
      "sns.heatmap(titanic_df.corr(), annot=True)\n",
      "=====\n",
      "f, ax =plt.subplots(figsize=(15,10))\n",
      "sns.heatmap(data.corr(),\n",
      "            annot = True,\n",
      "            ax = ax)\n",
      "sns.plt.show()\n",
      "--------------------\n",
      "f, ax =plt.subplots(figsize=(15,10))\n",
      "sns.countplot(x=jupyter_string, hue=jupyter_string, data=data)\n",
      "sns.plt.show()\n",
      "=====\n",
      "plt.figure(figsize=(16,3))\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "data.Sex.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "data.Pclass.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "           \n",
      "plt.subplot(1,3,3)\n",
      "data.Embarked.value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "--------------------\n",
      "plt.figure(figsize=(16,3))\n",
      "\n",
      "plt.subplot(1,3,1)\n",
      "data.Sex.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "\n",
      "plt.subplot(1,3,2)\n",
      "data.Pclass.value_counts().plot(kind =jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "           \n",
      "plt.subplot(1,3,3)\n",
      "data.Embarked.value_counts().plot(kind=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "plt.close()\n",
      "=====\n",
      "suvival_by_sex = data.groupby(['Sex' madeupword0002,'Survived' <<unk>>]).size().unstack()\n",
      "suvival_by_sex.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "\n",
      "survival_by_pclass = data.groupby(['Pclass' <<unk>>,'Survived' <<unk>>]).size().unstack()\n",
      "survival_by_pclass.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "\n",
      "survival_by_embarked = data.groupby(['Embarked' <<unk>>,'Survived' <<unk>>]).size().unstack()\n",
      "survival_by_embarked.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "survival_by_pclass = data.groupby(['Pclass' <unk>,'Survived' <unk>]).size().unstack()\n",
      "survival_by_pclass.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <unk>])\n",
      "\n",
      "\n",
      "survival_by_embarked = data.groupby(['Embarked' <unk>,'Survived' <unk>]).size().unstack()\n",
      "survival_by_embarked.T.plot(kind=jupyter_string,stacked=True)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <unk>])\n",
      "\n",
      "plt.show()\n",
      "=====\n",
      "fig=plt.figure(figsize=(25,20))\n",
      "fig.set(alpha=0.65)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "ax1 = fig.add_subplot(141)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass != 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax1.set_xticklabels(['Survived' <<unk>>, jupyter_string], rotation=0)\n",
      "ax1.legend([jupyter_string], loc=jupyter_string)\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(142, sharey=ax1)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass == 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax2.set_xticklabels(['Survived' <<unk>>, jupyter_string], rotation=0)\n",
      "ax2.legend([jupyter_string], loc=jupyter_string)\n",
      "\n",
      "ax3 = fig.add_subplot(143, sharey=ax1)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass != 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax3.set_xticklabels(['Survived' <<unk>>, jupyter_string], rotation=0)\n",
      "ax3.legend([jupyter_string], loc=jupyter_string)\n",
      "\n",
      "ax4 = fig.add_subplot(144, sharey=ax1)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass == 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax4.set_xticklabels(['Survived' <<unk>>, jupyter_string], rotation=0)\n",
      "ax4.legend([jupyter_string], loc=jupyter_string)\n",
      "\n",
      "plt.show()\n",
      "--------------------\n",
      "fig=plt.figure(figsize=(25,20))\n",
      "fig.set(alpha=0.65)\n",
      "plt.title(jupyter_string)\n",
      "\n",
      "ax1 = fig.add_subplot(141)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass != 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax1.set_xticklabels(['Survived' <unk>, jupyter_string], rotation=0)\n",
      "ax1.legend([jupyter_string], loc=jupyter_string)\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(142, sharey=ax1)\n",
      "data.Survived[data.Sex == jupyter_string][data.Pclass == 3].value_counts().plot(kind=jupyter_string, label=jupyter_string, color=jupyter_string)\n",
      "ax2.set_xticklabels(['Survived' <unk>\n",
      "=====\n",
      "survival_by_sibSp = data.groupby(['SibSp' <<unk>>,'Survived' <<unk>>]).sum()\n",
      "\n",
      "print(jupyter_string)\n",
      "survival_by_sibSp\n",
      "--------------------\n",
      "survival_by_parch = data.groupby(['Parch' <unk>,'Survived' <unk>]).sum()\n",
      "\n",
      "print(jupyter_string)\n",
      "survival_by_parch\n",
      "=====\n",
      "survival_by_Parch = data.groupby(['Parch' <<unk>>,'Survived' <<unk>>]).sum()\n",
      "print(jupyter_string)\n",
      "survival_by_Parch\n",
      "--------------------\n",
      "sns.distplot(data['Age' <unk>].dropna(),kde=False)\n",
      "=====\n",
      "plt.scatter(data.Survived,data.Age)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel('Age' <<unk>>)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <<unk>>])\n",
      "plt.show()\n",
      "--------------------\n",
      "data.loc[1980:1989][[jupyter_string, jupyter_string, jupyter_string]]\n",
      "=====\n",
      "results = data.loc[1980:1989][[jupyter_string, jupyter_string, jupyter_string]]\n",
      "results.to_csv(jupyter_string)\n",
      "--------------------\n",
      "plt.scatter(data.Survived,data.Pclass)\n",
      "plt.title(jupyter_string)\n",
      "plt.ylabel('Pclass' <unk>)\n",
      "plt.xticks([0,1],[jupyter_string,'Survived' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "data_fare = data['Fare' <<unk>>]\n",
      "((data_fare - data_fare.mean())/data_fare.std()).plot()\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "plt.imshow(X_train[0])\n",
      "plt.show()\n",
      "=====\n",
      "import pandas as pd\n",
      "import csv\n",
      "import random\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "traffic_db = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "traffic_db.head()\n",
      "=====\n",
      "fig = plt.figure(figsize=(50, 50))\n",
      "fig.suptitle(jupyter_string, fontsize=40)\n",
      "for id in traffic_db['ClassId' madeupword0002]:\n",
      "    fig.add_subplot(11, 4, id+1)\n",
      "    plt.title(str(id)+jupyter_string+traffic_db['SignName' <<unk>>][id], fontsize=30)\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(X_train_clean[random.choice(class_indices[id])])\n",
      "--------------------\n",
      "fig = plt.figure(figsize=(50, 50))\n",
      "fig.suptitle(jupyter_string, fontsize=40)\n",
      "for id in traffic_db['ClassId' <unk>]:\n",
      "    fig.add_subplot(11, 4, id+1)\n",
      "    plt.title(str(id)+jupyter_string+traffic_db['SignName' <unk>][id], fontsize=30)\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.hist(y_train_clean[y_train_clean['ClassId' <unk>] == id])\n",
      "=====\n",
      "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
      " \n",
      "count=[]\n",
      "for i in range(43):\n",
      "    count.append(len(class_indices[i]))\n",
      "plt.bar(range(43), count)\n",
      "plt.xticks(range(0,43,2), range(1,44,2))\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "--------------------\n",
      "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)\n",
      "X_valid = X_valid.reshape(X_valid.shape[0], 32, 32, 1)\n",
      "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)\n",
      "=====\n",
      "X_train, y_train = shuffle(X_train, y_train)\n",
      "\n",
      "class_indices_new = defaultdict(list)\n",
      "for c in range(n_classes):\n",
      "    for i in range(np.shape(X_train)[0]):\n",
      "        if y_train[i] == c:\n",
      "            class_indices_new[c].append(i)\n",
      "\n",
      "class_id = random.choice(range(43))\n",
      "\n",
      "images = [X_train[random.choice(class_indices_new[class_id])].squeeze()]\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "images.append(X_train[random.choice(class_indices_new[class_id])].squeeze())\n",
      "\n",
      "\n",
      "fig = plt.figure(figsize=(3, 3))\n",
      "fig.suptitle(jupyter_string, fontsize=10)\n",
      "image_index=0\n",
      "for image in images:\n",
      "    image_index+=1\n",
      "    fig.add_subplot(2, 2, image_index)\n",
      "    \n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(image, cmap=jupyter_string)\n",
      "--------------------\n",
      "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
      "y = tf.placeholder(tf.int32, (None))\n",
      "one_hot_y = tf.one_hot(y, n_classes)\n",
      "\n",
      "rate = 0.001\n",
      "\n",
      "logits = LeNet(x)\n",
      "\n",
      "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
      "\n",
      "loss_operation = tf.reduce_mean(cross_entropy)\n",
      "\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
      "\n",
      "training_operation = optimizer.minimize(loss_operation)\n",
      "=====\n",
      "x = tf.placeholder(tf.float32, (None, image_shape[0], image_shape[1], 1))\n",
      "y = tf.placeholder(tf.int32, (None))\n",
      "one_hot_y = tf.one_hot(y, n_classes)\n",
      "keep_prob = tf.placeholder(tf.float32)\n",
      "\n",
      "\n",
      "EPOCHS = 10\n",
      "BATCH_SIZE = 32\n",
      "\n",
      "global_step = tf.Variable(0, trainable=False)\n",
      "starter_learning_rate = 0.0005\n",
      "rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
      "\n",
      "logits = LeNet(x)\n",
      "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
      "loss_operation = tf.reduce_mean(cross_entropy)\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
      "training_operation = optimizer.minimize(loss_operation)\n",
      "\n",
      "\n",
      "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
      "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "def evaluate(X_data, y_data):\n",
      "    num_examples = len(X_data)\n",
      "    total_accuracy = 0\n",
      "    sess = tf.get_default_session()\n",
      "    for offset in range(0, num_examples, BATCH_SIZE):\n",
      "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
      "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
      "        total_accuracy += (accuracy * len(batch_x))\n",
      "    return total_accuracy / num_examples\n",
      "\n",
      "\n",
      "with tf.Session() as sess:\n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    num_examples = len(X_train)\n",
      "    \n",
      "    print(jupyter_string)\n",
      "    print()\n",
      "    for i in range(EPOCHS):\n",
      "        X_train, y_train = shuffle(X_train, y_train)\n",
      "        for offset in range(0, num_examples, BATCH_SIZE):\n",
      "            end = offset + BATCH_SIZE\n",
      "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
      "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.75})\n",
      "            \n",
      "        validation_accuracy = evaluate(X_valid, y_valid)\n",
      "        print(jupyter_string.format(i+1))\n",
      "        print(jupyter_string.format(validation_accuracy))\n",
      "        print()\n",
      "        \n",
      "    saver.save(sess, jupyter_string)\n",
      "    print(jupyter_string)\n",
      "--------------------\n",
      "import glob\n",
      "import matplotlib.image as mpimg\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "new_images = glob.glob(jupyter_string)\n",
      "new_images = np.array(new_images)\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "new_images = new_images.astype(jupyter_string)\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "new_images = np.reshape(new_images, (new_images.shape[0], 32, 32, 1))\n",
      "=====\n",
      "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
      "import numpy as np\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "images = plt.imread(jupyter_string)\n",
      "images = np.expand_dims(images, axis=0)\n",
      "images = np.insert(images, 1, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 2, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 3, plt.imread(jupyter_string), axis=0)\n",
      "images = np.insert(images, 4, plt.imread(jupyter_string), axis=0)\n",
      "\n",
      "labels = [2, 14, 11, 25, 14]\n",
      "\n",
      "images, labels = shuffle(images, labels)\n",
      "\n",
      "fig = plt.figure(figsize=(8, 2))\n",
      "fig.suptitle(jupyter_string, fontsize=10)\n",
      "image_index=0\n",
      "for image in images:\n",
      "    fig.add_subplot(1, 5, image_index+1)\n",
      "    plt.title(jupyter_string+str(labels[image_index])+jupyter_string+traffic_db['SignName' <<unk>>][labels[image_index]], fontsize=5)\n",
      "    plt.axis(jupyter_string)\n",
      "    plt.imshow(image.squeeze())\n",
      "    image_index+=1\n",
      "plt.show()\n",
      "--------------------\n",
      "plt.scatter(sales[jupyter_string], sales[jupyter_string])\n",
      "plt.plot(sales[jupyter_string], simple_model_predictions)\n",
      "plt.show()\n",
      "=====\n",
      "plt.scatter(sales.sqft_living, sales.price, color=jupyter_string, s = 2)\n",
      "plt.plot(sales.sqft_living, simple_model_predictions, color=jupyter_string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel('sqft_living' <<unk>>)\n",
      "plt.ylabel('price' <<unk>>)\n",
      "plt.ticklabel_format(style = jupyter_string)\n",
      "--------------------\n",
      "house = pd.read_csv(jupyter_string)\n",
      "house.head()\n",
      "=====\n",
      "sales = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "sales.head()\n",
      "=====\n",
      "sales.head()\n",
      "--------------------\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.show()\n",
      "\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "N = 100\n",
      "for _ in xrange(5):\n",
      "    s = np.zeros(N)\n",
      "    s[1:] = np.random.binomial(1, .5, size=(N-1,))*2-1\n",
      "    s = pd.Series(s)\n",
      "    s = s.cumsum()\n",
      "    plt.ylim([-50, 50])\n",
      "    s.plot()\n",
      "plt.show()\n",
      "--------------------\n",
      "predictions = simple_model.predict(test_data[simple_features])\n",
      "=====\n",
      "house1 = sales[sales.id== 5309101200]\n",
      "house2 = sales[sales.id == 1925069082]\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import median_absolute_error\n",
      "from sklearn.metrics import median_squared_error\n",
      "from sklearn.metrics import mean_squared_log_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "=====\n",
      "sales[simple_features+['price' <<unk>>]].head()\n",
      "--------------------\n",
      "house1_pred = regressor.predict(house1[feature_cols])\n",
      "house2_pred = regressor.predict(house2[feature_cols])\n",
      "=====\n",
      "house1.head()\n",
      "--------------------\n",
      "sns.regplot(x='sqft_living' <unk>,y='price' <unk>,data=sales)\n",
      "=====\n",
      "plt.scatter(x= sales.sqft_living, y = sales.price, color=jupyter_string, s = 3)\n",
      "plt.xlabel('sqft_living' <<unk>>)\n",
      "plt.ylabel('price' <<unk>>)\n",
      "plt.ticklabel_format(style = jupyter_string)\n",
      "\n",
      "--------------------\n",
      "house2.head()\n",
      "=====\n",
      "house2.head()\n",
      "--------------------\n",
      "house3 = pd.DataFrame(bill_gates)\n",
      "house3.head()\n",
      "=====\n",
      "house3 = pd.DataFrame(data=bill_gates)\n",
      "--------------------\n",
      "house3.head()\n",
      "=====\n",
      "house3.head()\n",
      "--------------------\n",
      "model = linear_model.LinearRegression()\n",
      "model.fit(train_data[[jupyter_string]], train_data[jupyter_string])\n",
      "=====\n",
      "simple_model = linear_model.LinearRegression()\n",
      "X = train_data[simple_features]\n",
      "y = train_data.price.to_frame()\n",
      "--------------------\n",
      "simple_model.fit(X,y)\n",
      "=====\n",
      "import warnings\n",
      "warnings.filterwarnings(action=jupyter_string, module=jupyter_string, message=jupyter_string)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "app_store = pd.read_csv(jupyter_string,delimiter=jupyter_string)\n",
      "\n",
      "number_of_apps = len(app_store.id)\n",
      "print(jupyter_string + str(number_of_apps))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "name = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "sex = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "age = [20, 21, 18, 22, 19, 20, 20, 19, 20]\n",
      "rank = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "ID = range(9)\n",
      "aid = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "GPA = [3.8, 3.5, 3.0, 3.9, 2.8, 2.9, 3.8, 3.4, 3.7]\n",
      "mathID = [0, 1, 5, 6, 3]\n",
      "mathGd = [4.0, 3.0, 3.5, 3.0, 4.0]\n",
      "major = [jupyter_string, jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "studentInfo = pd.DataFrame({jupyter_string: ID, jupyter_string: name, jupyter_string: sex, jupyter_string: age, jupyter_string: rank})\n",
      "otherInfo = pd.DataFrame({jupyter_string: ID, jupyter_string: GPA, jupyter_string: aid})\n",
      "mathInfo = pd.DataFrame({jupyter_string: mathID, jupyter_string: mathGd, jupyter_string: major})\n",
      "--------------------\n",
      "app_store.head()\n",
      "=====\n",
      "app_store.info()\n",
      "--------------------\n",
      "genres = list(app_store['Genres' <unk>].unique())\n",
      "genres.sort()\n",
      "print(genres)\n",
      "=====\n",
      "index, count = np.unique(app_store[\"prime_genre\"],return_counts=True)\n",
      "unique = app_store[\"prime_genre\"].nunique()\n",
      "print(jupyter_string.format(unique))\n",
      "count = count/count.sum()\n",
      "print(jupyter_string.format(count[7]))\n",
      "\n",
      "\n",
      "pie_chart(count, index, jupyter_string)\n",
      "\n",
      "--------------------\n",
      "plot_count('prime_genre' <unk>, jupyter_string, jupyter_string, \n",
      "           jupyter_string, hue='user_rating' <unk>, legend_title=jupyter_string, rotation=60)\n",
      "=====\n",
      "stats = app_store.groupby(\"prime_genre\")['user_rating' <<unk>>]\n",
      "genres, count = np.unique(app_store[\"prime_genre\"],return_counts=True)\n",
      "describe = stats.describe()\n",
      "describe[jupyter_string]=genres\n",
      "bar_chart(describe[jupyter_string],index,(20,8),jupyter_string, jupyter_string,jupyter_string ,60)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.title(jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.hist(df['Price' <unk>])\n",
      "plt.show()\n",
      "=====\n",
      "app_store_data = app_store.copy()\n",
      "\n",
      "plot = sns.regplot(x=app_store[jupyter_string], y=app_store['price' <<unk>>])\n",
      "plot.set(xlabel=jupyter_string, ylabel=jupyter_string, title=jupyter_string)\n",
      "--------------------\n",
      "plot = sns.distplot(app_store['price' <unk>])\n",
      "plot.set(xlabel=jupyter_string, ylabel=jupyter_string, title=jupyter_string)\n",
      "=====\n",
      "new_prices = []\n",
      "indices = []\n",
      "counted = 0\n",
      "summation = 0\n",
      "prices = app_store_data['price' <<unk>>].tolist()\n",
      "for price in prices:\n",
      "    if price > 0:\n",
      "        new_prices.append(1)\n",
      "        counted = counted + 1\n",
      "        summation = summation + price\n",
      "    else:\n",
      "        new_prices.append(0)\n",
      "        \n",
      "series = pd.Series(new_prices)\n",
      "app_store_data[jupyter_string] = series.values\n",
      "max = app_store_data[\"price\"].max()\n",
      "min = app_store_data[\"price\"].min()\n",
      "mean = app_store_data[\"price\"].mean()\n",
      "mean_of_paid = summation/counted\n",
      "print(mean_of_paid)\n",
      "print(mean)\n",
      "print(max)\n",
      "--------------------\n",
      "count = np.unique(app_store_data[jupyter_string],return_counts=True)\n",
      "unique = app_store_data[jupyter_string].nunique()\n",
      "print(count)\n",
      "count = count/count.sum()\n",
      "\n",
      "print(jupyter_string.format(count[0]))\n",
      "\n",
      "pie_chart(count, index, jupyter_string)\n",
      "=====\n",
      "revenue = []\n",
      "\n",
      "for index in range(7197):\n",
      "\n",
      "    revenue.append(app_store_data.loc[[index]]['price' <<unk>>][index]*app_store_data.loc[[index]]['rating_count_tot' <<unk>>][index])\n",
      "    \n",
      "revenue_series = pd.Series(revenue)\n",
      "total_revenue = revenue_series.sum()\n",
      "app_store_data[jupyter_string] = revenue_series.values\n",
      "\n",
      "print(total_revenue)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.bar(range(len(total_revenue)), total_revenue, align=jupyter_string)\n",
      "plt.xticks(range(len(total_revenue)), total_revenue.index, rotation=jupyter_string)\n",
      "plt.xlabel(jupyter_string)\n",
      "plt.ylabel(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "total_revenue = revenue_series.sum()\n",
      "print(total_revenue)\n",
      "--------------------\n",
      "plt.figure(figsize=(10,5))\n",
      "plt.bar(range(len(total_revenue)), total_revenue)\n",
      "plt.xticks(range(len(total_revenue)), total_revenue.index, rotation=jupyter_string)\n",
      "plt.title(jupyter_string)\n",
      "plt.show()\n",
      "=====\n",
      "index, count = np.unique(app_store_data[\"prime_genre\"],return_counts=True)\n",
      "stats = app_store_data.groupby(\"prime_genre\")[jupyter_string].sum()\n",
      "print(stats)\n",
      "type(stats)\n",
      "devices, count = np.unique(app_store_data[\"prime_genre\"],return_counts=True)\n",
      "describe = stats.describe()\n",
      "describe[jupyter_string]=devices\n",
      "\n",
      "\n",
      "bar_chart(stats,index,(20,8),jupyter_string, jupyter_string,jupyter_string ,60)\n",
      "\n",
      "--------------------\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.show()\n",
      "import warnings\n",
      "warnings.filterwarnings(jupyter_string)\n",
      "=====\n",
      "''jupyter_string''\n",
      "app_store_data['user_rating' <<unk>>] = app_store_data['user_rating' <<unk>>].replace([3.5, 3, 2.5, 2, 1.5, 1, 0.5], 0)\n",
      "app_store_data['user_rating' <<unk>>] = app_store_data['user_rating' <<unk>>].replace([4, 4.5, 5], 1)\n",
      "app_store_data.head()\n",
      "--------------------\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "\n",
      "C = np.logspace(0, 4, 20)\n",
      "gamma = [jupyter_string, jupyter_string]\n",
      "hyperparameters = dict(C=C, gamma=gamma)\n",
      "\n",
      "\n",
      "clf = GridSearchCV(svm_model, hyperparameters, cv=5, verbose=0)\n",
      "\n",
      "best_model = clf.fit(X, y)\n",
      "\n",
      "\n",
      "selected_C=best_model.best_estimator_.get_params()[jupyter_string]\n",
      "selected_gamma=best_model.best_estimator_.get_params()[jupyter_string]\n",
      "=====\n",
      "labels = new_df2['user_rating' <<unk>>]\n",
      "\n",
      "--------------------\n",
      "studentInfo.head()\n",
      "=====\n",
      "studentInfo.info()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
      "=====\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler().fit(new_df2.drop('user_rating' <<unk>>, axis=1))\n",
      "\n",
      "new_df = scaler.transform(new_df2.drop('user_rating' <<unk>>, axis=1))\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "df.head()\n",
      "=====\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(jupyter_string, index_col=0)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "pd.set_option(jupyter_string, 140)\n",
      "\n",
      "df.head()\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.\n",
      "=====\n",
      "df['tweet' <<unk>>].head(n=5).apply(lambda x: x.split())\n",
      "--------------------\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics\n",
      "=====\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "sklearn_default_preprocessor = CountVectorizer(strip_accents=jupyter_string, stop_words=jupyter_string).build_analyzer()\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(sklearn_default_preprocessor)\n",
      "--------------------\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import strip_accents\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "\n",
      "=====\n",
      "from gensim.parsing.preprocessing import preprocess_string\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(preprocess_string)\n",
      "--------------------\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "from gensim.parsing.preprocessing import remove_short_tokens\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import remove_punctuation\n",
      "\n",
      "=====\n",
      "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \\\n",
      "    remove_stopwords\n",
      "\n",
      "def drop_short(tweet):\n",
      "    \n",
      "    return jupyter_string.join(x for x in tweet.split() if len(x) >= 3)\n",
      "\n",
      "def to_lowercase(tweet):\n",
      "    return tweet  \n",
      "\n",
      "def drop_usernames(tweet):\n",
      "    return tweet  \n",
      "    \n",
      "my_filters = [ to_lowercase, drop_usernames, strip_multiple_whitespaces, strip_punctuation, strip_numeric,\n",
      "               remove_stopwords, drop_short ]\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(lambda x: preprocess_string(x, my_filters))\n",
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style(jupyter_string)\n",
      "sns.set_context(jupyter_string)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_context(jupyter_string, font_scale=1.5)\n",
      "sns.set_palette(jupyter_string)\n",
      "sns.set_context(jupyter_string, font_scale=1.5)\n",
      "sns.set_palette(jupyter_string)\n",
      "=====\n",
      "import seaborn as sns\n",
      "\n",
      "plt.show()\n",
      "\n",
      "sns.countplot(df['class' <<unk>>])\n",
      "--------------------\n",
      "from gensim.parsing.preprocessing import remove_stopwords\n",
      "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_punctuation, strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_numeric\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_punctuation\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing import strip_tags\n",
      "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
      "from gensim.parsing.preprocessing\n",
      "=====\n",
      "ngram_preprocessor = CountVectorizer(strip_accents=jupyter_string, analyzer=..., ngram_range=...).build_analyzer()\n",
      "\n",
      "df['tweet' <<unk>>].head(n=5).apply(...)\n",
      "--------------------\n",
      "df = pd.read_csv(jupyter_string)\n",
      "=====\n",
      "df = pd.read_csv(jupyter_string)\n",
      "--------------------\n",
      "otherInfo.info()\n",
      "=====\n",
      "otherInfo.info()\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.isnull().sum().sort_values(ascending=False)\n",
      "--------------------\n",
      "df.head()\n",
      "=====\n",
      "df.head()\n",
      "--------------------\n",
      "plt.figure(figsize=(15,5))\n",
      "plt.subplot(1,2,1)\n",
      "plt.title(jupyter_string)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.subplot(1,2,2)\n",
      "plt.title(jupyter_string)\n",
      "sns.distplot(df[jupyter_string])\n",
      "plt.show()\n",
      "=====\n",
      "sns.distplot(df['GDP_perCap' <<unk>>], bins=15)\n",
      "--------------------\n",
      "sns.distplot(df['GDP_total' <unk>], bins=15)\n",
      "=====\n",
      "sns.distplot(df['GDP_Total' <<unk>>], bins=15)\n",
      "--------------------\n",
      "df.groupby('IncomeGroup' <unk>)['GDP_perCap' <unk>].describe()\n",
      "=====\n",
      "order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "\n",
      "GDPbyIncome = df.groupby('Income_Group' <<unk>>)['GDP_Total' <<unk>>].mean().reset_index()\n",
      "\n",
      "\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "\n",
      "sns.barplot(data=GDPbyIncome, x='Income_Group' <<unk>>, y='GDP_Total' <<unk>>, order=order, palette=jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "\n",
      "sns.barplot(data=GDPbyIncome, x='Income_Group' <unk>, y='GDP_Total' <unk>, order=order, palette=jupyter_string)\n",
      "=====\n",
      "order = [jupyter_string, jupyter_string, jupyter_string, jupyter_string]\n",
      "\n",
      "\n",
      "GDPbyIncome = df.groupby('Income_Group' <<unk>>)['GDP_perCap' <<unk>>].mean().reset_index()\n",
      "\n",
      "\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "\n",
      "sns.barplot(data=GDPbyIncome, x='Income_Group' <<unk>>, y='GDP_perCap' <<unk>>, order=order, palette=jupyter_string)\n",
      "--------------------\n",
      "sns.heatmap(df.corr(), annot=True)\n",
      "=====\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "\n",
      "sns.heatmap(df.corr(), annot=True, cmap=jupyter_string)\n",
      "--------------------\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "\n",
      "sns.boxplot(x='Indicator Name' <unk>, y='Value' <unk>, data=df)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15,20))\n",
      "\n",
      "\n",
      "ax1 = fig.add_subplot(821)\n",
      "\n",
      "ax1.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Life_Expect' <<unk>>, ax=ax1, width=0.3)\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(822)\n",
      "\n",
      "ax2.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='School_Enrl' <<unk>>, ax=ax2, width=0.3)\n",
      "\n",
      "\n",
      "ax3 = fig.add_subplot(823)\n",
      "\n",
      "ax3.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Adult_Lit' <<unk>>, ax=ax3, width=0.3)\n",
      "\n",
      "\n",
      "ax4 = fig.add_subplot(824)\n",
      "\n",
      "ax4.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='BusReg' <<unk>>, ax=ax4, width=0.3)\n",
      "\n",
      "\n",
      "ax5 = fig.add_subplot(825)\n",
      "\n",
      "ax5.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Comm_Tax' <<unk>>, ax=ax5, width=0.3)\n",
      "\n",
      "\n",
      "ax6 = fig.add_subplot(826)\n",
      "\n",
      "ax6.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Elec_Access' <<unk>>, ax=ax6, width=0.3)\n",
      "\n",
      "\n",
      "ax7 = fig.add_subplot(827)\n",
      "\n",
      "ax7.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Elec_Consumption' <<unk>>, ax=ax7, width=0.3)\n",
      "\n",
      "\n",
      "ax8 = fig.add_subplot(828)\n",
      "\n",
      "ax8.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='ElecGen_OilGasCoal' <<unk>>, ax=ax8, width=0.3)\n",
      "\n",
      "\n",
      "ax9 = fig.add_subplot(829)\n",
      "\n",
      "ax9.set_title('Emissions' <<unk>>, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Emissions' <<unk>>, ax=ax9, width=0.3)\n",
      "\n",
      "\n",
      "ax10 = fig.add_subplot(8,2,10)\n",
      "\n",
      "ax10.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='FossFuel_Cons' madeupword0002, ax=ax10, width=0.3)\n",
      "\n",
      "\n",
      "ax11 = fig.add_subplot(8,2,11)\n",
      "\n",
      "ax11.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Inf_Mortality' <<unk>>, ax=ax11, width=0.3)\n",
      "\n",
      "\n",
      "ax12 = fig.add_subplot(8,2,12)\n",
      "\n",
      "ax12.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Inflation' <<unk>>, ax=ax12, width=0.3)\n",
      "\n",
      "\n",
      "ax13 = fig.add_subplot(8,2,13)\n",
      "\n",
      "ax13.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='OreMetal_exports' <<unk>>, ax=ax13, width=0.3)\n",
      "\n",
      "\n",
      "ax14 = fig.add_subplot(8,2,14)\n",
      "\n",
      "ax14.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Imp_San_Access' <<unk>>, ax=ax14, width=0.3)\n",
      "\n",
      "\n",
      "ax15 = fig.add_subplot(8,2,15)\n",
      "\n",
      "ax15.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "sns.boxplot(data=df, y='Tech_Exports' <<unk>>, ax=ax15, width=0.3)\n",
      "\n",
      "\n",
      "plt.tight_layout()\n",
      "--------------------\n",
      "df = df[df['Inflation' <unk>]<60]\n",
      "=====\n",
      "df = df.drop(labels=df[df['Inflation' <<unk>>]>2500].index, axis=0).reset_index(drop=True)\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoost\n",
      "=====\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "--------------------\n",
      "crime = pd.read_csv(jupyter_string)\n",
      "crime.head()\n",
      "=====\n",
      "data = pd.read_csv(jupyter_string, header=1, index_col=0)\n",
      "data.head()\n",
      "--------------------\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.neural_network\n",
      "=====\n",
      "rowsToUse = ['Corr_Perc_Index' <<unk>>, 'School_Enrl' <<unk>>, 'Adult_Lit' <<unk>>,'BusReg' <<unk>>, 'Comm_Tax' <<unk>>, 'Elec_Consumption' <<unk>>,\n",
      "             'Emissions' <<unk>>, 'FossFuel_Cons' madeupword0002, 'Inf_Mortality' <<unk>>, 'Inflation' <<unk>>, 'OreMetal_exports' <<unk>>, 'Imp_San_Access' <<unk>>,\n",
      "             'Tech_Exports' <<unk>>]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "\n",
      "\n",
      "X = df.loc[:,rowsToUse]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "--------------------\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "\n",
      "model = LinearRegression()\n",
      "\n",
      "\n",
      "model.fit(X_train, y_train)\n",
      "=====\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train, y_train)\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "importances = model1.feature_importances_\n",
      "std = np.std([tree.feature_importances_ for tree in model1.estimators_],\n",
      "             axis=0)\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "\n",
      "print(jupyter_string)\n",
      "\n",
      "for f in range(X_train.shape[1]):\n",
      "    print(jupyter_string % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "plt.title(jupyter_string)\n",
      "plt.bar(range(X_train.shape[1]), importances[indices],\n",
      "       color=jupyter_string, yerr=std[indices], align=jupyter_string)\n",
      "plt.xticks(range(X_train.shape[1]), indices)\n",
      "plt.xlim([-1, X_train\n",
      "=====\n",
      "weights = pd.DataFrame({jupyter_string: X.columns.values, jupyter_string: model5.feature_importances_, jupyter_string: model3.coef_})\n",
      "weights\n",
      "--------------------\n",
      "weights.sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "weights.set_index(keys=jupyter_string)\n",
      "--------------------\n",
      "weights.sort_values(by=jupyter_string, ascending=False)\n",
      "=====\n",
      "fig = plt.figure(figsize=(15,10))\n",
      "\n",
      "\n",
      "ax1 = fig.add_subplot(211)\n",
      "\n",
      "ax1.set_ylabel(jupyter_string)\n",
      "ax1.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "weights.set_index(keys=jupyter_string)[jupyter_string].plot(kind=jupyter_string, ax=ax1)\n",
      "\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(212)\n",
      "ax2.set_ylabel(jupyter_string)\n",
      "ax2.set_title(jupyter_string, fontweight=jupyter_string)\n",
      "\n",
      "weights.set_index(keys=jupyter_string)[jupyter_string].plot(kind=jupyter_string, ax=ax2)\n",
      "\n",
      "plt.tight_layout()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_train_pca = pca.fit_transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "explained_variance = pca.explained_variance_ratio_\n",
      "explained_variance\n",
      "=====\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "\n",
      "\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "X.head()\n",
      "--------------------\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "=====\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "pca.explained_variance_ratio_\n",
      "=====\n",
      "pcaDF = pd.DataFrame(data=pca.components_, index=X.columns)\n",
      "\n",
      "\n",
      "pcaDF.head()\n",
      "--------------------\n",
      "plt.plot(range(1, len(exp_var)+1), exp_var, lw=2)\n",
      "plt.scatter(range(1, len(exp_var)+1), exp_var, s=120)\n",
      "\n",
      "\n",
      "plt.xlabel(jupyter_string, fontsize=16)\n",
      "plt.ylabel(jupyter_string, fontsize=16)\n",
      "=====\n",
      "pcaDF[0].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "data['Crime_Rate' <unk>] = data['Crime_Rate' <unk>].astype(float)\n",
      "data.head()\n",
      "=====\n",
      "data[jupyter_string] = data[jupyter_string] / data[jupyter_string]\n",
      "data.tail()\n",
      "--------------------\n",
      "pcaDF[1].plot(kind=jupyter_string, figsize=(15,6))\n",
      "=====\n",
      "pcaDF[1].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "pcaDF[2].plot(kind=jupyter_string, figsize=(15,6))\n",
      "=====\n",
      "pcaDF[2].plot(kind=jupyter_string, figsize=(15,6))\n",
      "--------------------\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(pcaDF)\n",
      "pcaDF = pca.transform(pcaDF)\n",
      "=====\n",
      "pcaX_train = pca.transform(X_train)\n",
      "\n",
      "\n",
      "pcaX_test = pca.transform(X_test)\n",
      "\n",
      "\n",
      "pcaX_train = pd.DataFrame(pcaX_train)\n",
      "\n",
      "\n",
      "pcaX_test = pd.DataFrame(pcaX_test)\n",
      "--------------------\n",
      "sns.heatmap(pcaX_train.corr(), annot=True)\n",
      "plt.show()\n",
      "=====\n",
      "sns.heatmap(X_train.corr())\n",
      "--------------------\n",
      "sns.heatmap(X_test.corr())\n",
      "=====\n",
      "sns.heatmap(pcaX_train.corr())\n",
      "--------------------\n",
      "model1.fit(pcaX_train, y_train)\n",
      "model2.fit(pcaX_train, y_train)\n",
      "model3.fit(pcaX_train, y_train)\n",
      "model4.fit(pcaX_train, y_train)\n",
      "model5.fit(pcaX_train, y_train)\n",
      "model6.fit(pcaX_train, y_train)\n",
      "model7.fit(pcaX_train, y_train)\n",
      "model8.fit(pcaX_train, y_train)\n",
      "=====\n",
      "model5.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "model7.fit(pcaX_train.iloc[:,:10], y_train)\n",
      "\n",
      "scoreRF = model5.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseRF = np.sqrt(mean_squared_error(y_test, model5.predict(pcaX_test.iloc[:,:10])))\n",
      "\n",
      "scoreGB = model7.score(pcaX_test.iloc[:,:10], y_test)\n",
      "rmseGB = np.sqrt(mean_squared_error(y_test, model7.predict(pcaX_test.iloc[:,:10])))\n",
      "               \n",
      "print(jupyter_string.format(scoreRF,rmseRF))\n",
      "print(jupyter_string.format(scoreGB,rmseGB))\n",
      "--------------------\n",
      "from tpot import TPOTClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "=====\n",
      "X = df.iloc[:,6:]\n",
      "\n",
      "target = df['GDP_perCap' <<unk>>]\n",
      "--------------------\n",
      "from tpot import TPOTRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.33, random_state=42)\n",
      "\n",
      "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)\n",
      "tpot.fit(X_train, y_train)\n",
      "tpot.score(X_test, y_test)\n",
      "=====\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=46)\n",
      "\n",
      "\n",
      "X_train = X_train.apply(zscore)\n",
      "X_test = X_test.apply(zscore)\n",
      "--------------------\n",
      "from tpot import TPOTRegressor\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "=====\n",
      "X_train.head()\n",
      "--------------------\n",
      "from tpot import TPOTClassifier\n",
      "\n",
      "tpot = TPOTClassifier()\n",
      "tpot.fit(X_train, y_train)\n",
      "tpot.score(X_test, y_test)\n",
      "=====\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from sklearn.svm import LinearSVR\n",
      "from tpot.builtins import StackingEstimator\n",
      "\n",
      "exported_pipeline = make_pipeline(\n",
      "    VarianceThreshold(threshold=0.3),\n",
      "    StackingEstimator(estimator=RidgeCV()),\n",
      "    StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, max_features=0.75, min_samples_leaf=20, min_samples_split=5, n_estimators=100)),\n",
      "    StackingEstimator(estimator=LinearSVR(C=0.1, dual=True, epsilon=0.1, loss=jupyter_string, tol=0.001)),\n",
      "    StackingEstimator(estimator=KNeighborsRegressor(n_neighbors=22, p=1, weights=jupyter_string)),\n",
      "    StackingEstimator(estimator=KNeighborsRegressor(n_neighbors=4, p=1, weights=jupyter_string)),\n",
      "    SelectFromModel(estimator=ExtraTreesRegressor(max_features=1.0, n_estimators=100), threshold=0.2),\n",
      "    ExtraTreesRegressor(bootstrap=False, max_features=1.0, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n",
      ")\n",
      "\n",
      "\n",
      "exported_pipeline.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "y_pred = exported_pipeline.predict(X_test)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for h, t in zip(hypothesis,reference):\n",
    "    #if h==t:\n",
    "    print(h)\n",
    "    print('='*5)\n",
    "    print(t)\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074bd02-87f5-4ff7-bcdf-ce06eefa0aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
